<!DOCTYPE html>
<html>
<head>
<title>Paper collected by Wang</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<style type="text/css">


/* BODY
=============================================================================*/

body {
  font-family: Helvetica, arial, freesans, clean, sans-serif;
  font-size: 14px;
  line-height: 1.6;
  color: #333;
  background-color: #fff;
  padding: 20px;
  max-width: 960px;
  margin: 0 auto;
}

span#pid {
  color:red;
  
}
span#filename{
  font-style: oblique;
}

span#title{
  font-family: Times New Roman, freesans, clean, sans-serif;
  font-style: italic;
  font-size: 20px;
  border:1px solid #B50;
}
span#abs{
  font-family: Times New Roman, freesans, clean, sans-serif;
  font-style: oblique;
  font-size: 18px;
}
</style>
</head>
<body>

</p></br></br><div id='section'>Paperid: <span id='pid'>1, <a href='https://arxiv.org/pdf/2510.04508.pdf' target='_blank'>https://arxiv.org/pdf/2510.04508.pdf</a></span>   <span><a href='https://github.com/xiewilliams/MARCO' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Lili Xie, Yi Zhang, Ruihong Qiu, Jiajun Liu, Sen Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.04508">MARCO: A Cooperative Knowledge Transfer Framework for Personalized Cross-domain Recommendations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recommender systems frequently encounter data sparsity issues, particularly when addressing cold-start scenarios involving new users or items. Multi-source cross-domain recommendation (CDR) addresses these challenges by transferring valuable knowledge from multiple source domains to enhance recommendations in a target domain. However, existing reinforcement learning (RL)-based CDR methods typically rely on a single-agent framework, leading to negative transfer issues caused by inconsistent domain contributions and inherent distributional discrepancies among source domains. To overcome these limitations, MARCO, a Multi-Agent Reinforcement Learning-based Cross-Domain recommendation framework, is proposed. It leverages cooperative multi-agent reinforcement learning, where each agent is dedicated to estimating the contribution from an individual source domain, effectively managing credit assignment and mitigating negative transfer. In addition, an entropy-based action diversity penalty is introduced to enhance policy expressiveness and stabilize training by encouraging diverse agents' joint actions. Extensive experiments across four benchmark datasets demonstrate MARCO's superior performance over state-of-the-art methods, highlighting its robustness and strong generalization capabilities. The code is at https://github.com/xiewilliams/MARCO.
<div id='section'>Paperid: <span id='pid'>2, <a href='https://arxiv.org/pdf/2510.04508.pdf' target='_blank'>https://arxiv.org/pdf/2510.04508.pdf</a></span>   <span><a href='https://github.com/xiewilliams/MARCO' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Lili Xie, Yi Zhang, Ruihong Qiu, Jiajun Liu, Sen Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.04508">MARCO: A Cooperative Knowledge Transfer Framework for Personalized Cross-domain Recommendations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recommender systems frequently encounter data sparsity issues, particularly when addressing cold-start scenarios involving new users or items. Multi-source cross-domain recommendation (CDR) addresses these challenges by transferring valuable knowledge from multiple source domains to enhance recommendations in a target domain. However, existing reinforcement learning (RL)-based CDR methods typically rely on a single-agent framework, leading to negative transfer issues caused by inconsistent domain contributions and inherent distributional discrepancies among source domains. To overcome these limitations, MARCO, a Multi-Agent Reinforcement Learning-based Cross-Domain recommendation framework, is proposed. It leverages cooperative multi-agent reinforcement learning, where each agent is dedicated to estimating the contribution from an individual source domain, effectively managing credit assignment and mitigating negative transfer. In addition, an entropy-based action diversity penalty is introduced to enhance policy expressiveness and stabilize training by encouraging diverse agents' joint actions. Extensive experiments across four benchmark datasets demonstrate MARCO's superior performance over state-of-the-art methods, highlighting its robustness and strong generalization capabilities. The code is at https://github.com/xiewilliams/MARCO.
<div id='section'>Paperid: <span id='pid'>3, <a href='https://arxiv.org/pdf/2510.03257.pdf' target='_blank'>https://arxiv.org/pdf/2510.03257.pdf</a></span>   <span><a href='https://github.com/RS2002/Triple-BERT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zijian Zhao, Sen Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.03257">Triple-BERT: Do We Really Need MARL for Order Dispatch on Ride-Sharing Platforms?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>On-demand ride-sharing platforms, such as Uber and Lyft, face the intricate real-time challenge of bundling and matching passengers-each with distinct origins and destinations-to available vehicles, all while navigating significant system uncertainties. Due to the extensive observation space arising from the large number of drivers and orders, order dispatching, though fundamentally a centralized task, is often addressed using Multi-Agent Reinforcement Learning (MARL). However, independent MARL methods fail to capture global information and exhibit poor cooperation among workers, while Centralized Training Decentralized Execution (CTDE) MARL methods suffer from the curse of dimensionality. To overcome these challenges, we propose Triple-BERT, a centralized Single Agent Reinforcement Learning (MARL) method designed specifically for large-scale order dispatching on ride-sharing platforms. Built on a variant TD3, our approach addresses the vast action space through an action decomposition strategy that breaks down the joint action probability into individual driver action probabilities. To handle the extensive observation space, we introduce a novel BERT-based network, where parameter reuse mitigates parameter growth as the number of drivers and orders increases, and the attention mechanism effectively captures the complex relationships among the large pool of driver and orders. We validate our method using a real-world ride-hailing dataset from Manhattan. Triple-BERT achieves approximately an 11.95% improvement over current state-of-the-art methods, with a 4.26% increase in served orders and a 22.25% reduction in pickup times. Our code, trained model parameters, and processed data are publicly available at the repository https://github.com/RS2002/Triple-BERT .
<div id='section'>Paperid: <span id='pid'>4, <a href='https://arxiv.org/pdf/2510.03257.pdf' target='_blank'>https://arxiv.org/pdf/2510.03257.pdf</a></span>   <span><a href='https://github.com/RS2002/Triple-BERT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zijian Zhao, Sen Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.03257">Triple-BERT: Do We Really Need MARL for Order Dispatch on Ride-Sharing Platforms?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>On-demand ride-sharing platforms, such as Uber and Lyft, face the intricate real-time challenge of bundling and matching passengers-each with distinct origins and destinations-to available vehicles, all while navigating significant system uncertainties. Due to the extensive observation space arising from the large number of drivers and orders, order dispatching, though fundamentally a centralized task, is often addressed using Multi-Agent Reinforcement Learning (MARL). However, independent MARL methods fail to capture global information and exhibit poor cooperation among workers, while Centralized Training Decentralized Execution (CTDE) MARL methods suffer from the curse of dimensionality. To overcome these challenges, we propose Triple-BERT, a centralized Single Agent Reinforcement Learning (MARL) method designed specifically for large-scale order dispatching on ride-sharing platforms. Built on a variant TD3, our approach addresses the vast action space through an action decomposition strategy that breaks down the joint action probability into individual driver action probabilities. To handle the extensive observation space, we introduce a novel BERT-based network, where parameter reuse mitigates parameter growth as the number of drivers and orders increases, and the attention mechanism effectively captures the complex relationships among the large pool of driver and orders. We validate our method using a real-world ride-hailing dataset from Manhattan. Triple-BERT achieves approximately an 11.95% improvement over current state-of-the-art methods, with a 4.26% increase in served orders and a 22.25% reduction in pickup times. Our code, trained model parameters, and processed data are publicly available at the repository https://github.com/RS2002/Triple-BERT .
<div id='section'>Paperid: <span id='pid'>5, <a href='https://arxiv.org/pdf/2510.01264.pdf' target='_blank'>https://arxiv.org/pdf/2510.01264.pdf</a></span>   <span><a href='https://github.com/DIRECTLab/IsaacLab-HARL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Isaac Peterson, Christopher Allred, Jacob Morrey, Mario Harper
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.01264">A Framework for Scalable Heterogeneous Multi-Agent Adversarial Reinforcement Learning in IsaacLab</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-Agent Reinforcement Learning (MARL) is central to robotic systems cooperating in dynamic environments. While prior work has focused on these collaborative settings, adversarial interactions are equally critical for real-world applications such as pursuit-evasion, security, and competitive manipulation. In this work, we extend the IsaacLab framework to support scalable training of adversarial policies in high-fidelity physics simulations. We introduce a suite of adversarial MARL environments featuring heterogeneous agents with asymmetric goals and capabilities. Our platform integrates a competitive variant of Heterogeneous Agent Reinforcement Learning with Proximal Policy Optimization (HAPPO), enabling efficient training and evaluation under adversarial dynamics. Experiments across several benchmark scenarios demonstrate the framework's ability to model and train robust policies for morphologically diverse multi-agent competition while maintaining high throughput and simulation realism. Code and benchmarks are available at: https://github.com/DIRECTLab/IsaacLab-HARL .
<div id='section'>Paperid: <span id='pid'>6, <a href='https://arxiv.org/pdf/2510.01264.pdf' target='_blank'>https://arxiv.org/pdf/2510.01264.pdf</a></span>   <span><a href='https://github.com/DIRECTLab/IsaacLab-HARL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Isaac Peterson, Christopher Allred, Jacob Morrey, Mario Harper
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.01264">A Framework for Scalable Heterogeneous Multi-Agent Adversarial Reinforcement Learning in IsaacLab</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-Agent Reinforcement Learning (MARL) is central to robotic systems cooperating in dynamic environments. While prior work has focused on these collaborative settings, adversarial interactions are equally critical for real-world applications such as pursuit-evasion, security, and competitive manipulation. In this work, we extend the IsaacLab framework to support scalable training of adversarial policies in high-fidelity physics simulations. We introduce a suite of adversarial MARL environments featuring heterogeneous agents with asymmetric goals and capabilities. Our platform integrates a competitive variant of Heterogeneous Agent Reinforcement Learning with Proximal Policy Optimization (HAPPO), enabling efficient training and evaluation under adversarial dynamics. Experiments across several benchmark scenarios demonstrate the framework's ability to model and train robust policies for morphologically diverse multi-agent competition while maintaining high throughput and simulation realism. Code and benchmarks are available at: https://github.com/DIRECTLab/IsaacLab-HARL .
<div id='section'>Paperid: <span id='pid'>7, <a href='https://arxiv.org/pdf/2509.25550.pdf' target='_blank'>https://arxiv.org/pdf/2509.25550.pdf</a></span>   <span><a href='https://dongsuleetech.github.io/projects/IWoL/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Dongsu Lee, Daehee Lee, Yaru Niu, Honguk Woo, Amy Zhang, Ding Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.25550">Learning to Interact in World Latent for Team Coordination</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work presents a novel representation learning framework, interactive world latent (IWoL), to facilitate team coordination in multi-agent reinforcement learning (MARL). Building effective representation for team coordination is a challenging problem, due to the intricate dynamics emerging from multi-agent interaction and incomplete information induced by local observations. Our key insight is to construct a learnable representation space that jointly captures inter-agent relations and task-specific world information by directly modeling communication protocols. This representation, we maintain fully decentralized execution with implicit coordination, all while avoiding the inherent drawbacks of explicit message passing, e.g., slower decision-making, vulnerability to malicious attackers, and sensitivity to bandwidth constraints. In practice, our representation can be used not only as an implicit latent for each agent, but also as an explicit message for communication. Across four challenging MARL benchmarks, we evaluate both variants and show that IWoL provides a simple yet powerful key for team coordination. Moreover, we demonstrate that our representation can be combined with existing MARL algorithms to further enhance their performance.
<div id='section'>Paperid: <span id='pid'>8, <a href='https://arxiv.org/pdf/2509.25550.pdf' target='_blank'>https://arxiv.org/pdf/2509.25550.pdf</a></span>   <span><a href='https://dongsuleetech.github.io/projects/IWoL/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Dongsu Lee, Daehee Lee, Yaru Niu, Honguk Woo, Amy Zhang, Ding Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.25550">Learning to Interact in World Latent for Team Coordination</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work presents a novel representation learning framework, interactive world latent (IWoL), to facilitate team coordination in multi-agent reinforcement learning (MARL). Building effective representation for team coordination is a challenging problem, due to the intricate dynamics emerging from multi-agent interaction and incomplete information induced by local observations. Our key insight is to construct a learnable representation space that jointly captures inter-agent relations and task-specific world information by directly modeling communication protocols. This representation, we maintain fully decentralized execution with implicit coordination, all while avoiding the inherent drawbacks of explicit message passing, e.g., slower decision-making, vulnerability to malicious attackers, and sensitivity to bandwidth constraints. In practice, our representation can be used not only as an implicit latent for each agent, but also as an explicit message for communication. Across four challenging MARL benchmarks, we evaluate both variants and show that IWoL provides a simple yet powerful key for team coordination. Moreover, we demonstrate that our representation can be combined with existing MARL algorithms to further enhance their performance.
<div id='section'>Paperid: <span id='pid'>9, <a href='https://arxiv.org/pdf/2508.20818.pdf' target='_blank'>https://arxiv.org/pdf/2508.20818.pdf</a></span>   <span><a href='https://github.com/DaRL-LibSignal/cMALC-D' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Anirudh Satheesh, Keenan Powell, Hua Wei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.20818">cMALC-D: Contextual Multi-Agent LLM-Guided Curriculum Learning with Diversity-Based Context Blending</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Many multi-agent reinforcement learning (MARL) algorithms are trained in fixed simulation environments, making them brittle when deployed in real-world scenarios with more complex and uncertain conditions. Contextual MARL (cMARL) addresses this by parameterizing environments with context variables and training a context-agnostic policy that performs well across all environment configurations. Existing cMARL methods attempt to use curriculum learning to help train and evaluate context-agnostic policies, but they often rely on unreliable proxy signals, such as value estimates or generalized advantage estimates that are noisy and unstable in multi-agent settings due to inter-agent dynamics and partial observability. To address these issues, we propose Contextual Multi-Agent LLM-Guided Curriculum Learning with Diversity-Based Context Blending (cMALC-D), a framework that uses Large Language Models (LLMs) to generate semantically meaningful curricula and provide a more robust evaluation signal. To prevent mode collapse and encourage exploration, we introduce a novel diversity-based context blending mechanism that creates new training scenarios by combining features from prior contexts. Experiments in traffic signal control domains demonstrate that cMALC-D significantly improves both generalization and sample efficiency compared to existing curriculum learning baselines. We provide code at https://github.com/DaRL-LibSignal/cMALC-D.
<div id='section'>Paperid: <span id='pid'>10, <a href='https://arxiv.org/pdf/2508.04416.pdf' target='_blank'>https://arxiv.org/pdf/2508.04416.pdf</a></span>   <span><a href='https://zhang9302002.github.io/thinkingwithvideos-page/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoji Zhang, Xin Gu, Jiawen Li, Chixiang Ma, Sule Bai, Chubin Zhang, Bowen Zhang, Zhichao Zhou, Dongliang He, Yansong Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.04416">Thinking With Videos: Multimodal Tool-Augmented Reinforcement Learning for Long Video Reasoning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The video reasoning ability of multimodal large language models (MLLMs) is crucial for downstream tasks like video question answering and temporal grounding. While recent approaches have explored text-based chain-of-thought (CoT) reasoning for MLLMs, these methods often suffer from limited cross-modal interaction and increased hallucination, especially with longer videos or reasoning chains. To address these challenges, we propose Video Intelligence via Tool-Augmented Learning (VITAL), a novel end-to-end agentic video reasoning framework. With a visual toolbox, the model can densely sample new video frames on demand and generate multimodal CoT for precise long video reasoning. We observe that temporal grounding and question answering are mutually beneficial for video understanding tasks. Therefore, we construct two high-quality multi-task video reasoning datasets MTVR-CoT-72k for supervised fine-tuning and MTVR-RL-110k for reinforcement learning. Moreover, we propose a Difficulty-aware Group Relative Policy Optimization algorithm (DGRPO) to mitigate difficulty imbalance in multi-task reinforcement learning. Extensive experiments on 11 challenging video understanding benchmarks demonstrate the advanced reasoning ability of VITAL, outperforming existing methods in video question answering and temporal grounding tasks, especially in long video scenarios. Code is available at https://zhang9302002.github.io/thinkingwithvideos-page/.
<div id='section'>Paperid: <span id='pid'>11, <a href='https://arxiv.org/pdf/2508.04416.pdf' target='_blank'>https://arxiv.org/pdf/2508.04416.pdf</a></span>   <span><a href='https://zhang9302002.github.io/thinkingwithvideos-page/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoji Zhang, Xin Gu, Jiawen Li, Chixiang Ma, Sule Bai, Chubin Zhang, Bowen Zhang, Zhichao Zhou, Dongliang He, Yansong Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.04416">Thinking With Videos: Multimodal Tool-Augmented Reinforcement Learning for Long Video Reasoning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The video reasoning ability of multimodal large language models (MLLMs) is crucial for downstream tasks like video question answering and temporal grounding. While recent approaches have explored text-based chain-of-thought (CoT) reasoning for MLLMs, these methods often suffer from limited cross-modal interaction and increased hallucination, especially with longer videos or reasoning chains. To address these challenges, we propose Video Intelligence via Tool-Augmented Learning (VITAL), a novel end-to-end agentic video reasoning framework. With a visual toolbox, the model can densely sample new video frames on demand and generate multimodal CoT for precise long video reasoning. We observe that temporal grounding and question answering are mutually beneficial for video understanding tasks. Therefore, we construct two high-quality multi-task video reasoning datasets MTVR-CoT-72k for supervised fine-tuning and MTVR-RL-110k for reinforcement learning. Moreover, we propose a Difficulty-aware Group Relative Policy Optimization algorithm (DGRPO) to mitigate difficulty imbalance in multi-task reinforcement learning. Extensive experiments on 11 challenging video understanding benchmarks demonstrate the advanced reasoning ability of VITAL, outperforming existing methods in video question answering and temporal grounding tasks, especially in long video scenarios. Code is available at https://zhang9302002.github.io/thinkingwithvideos-page/.
<div id='section'>Paperid: <span id='pid'>12, <a href='https://arxiv.org/pdf/2507.23172.pdf' target='_blank'>https://arxiv.org/pdf/2507.23172.pdf</a></span>   <span><a href='https://github.com/Viraj-Joshi/MTBench' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Viraj Joshi, Zifan Xu, Bo Liu, Peter Stone, Amy Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.23172">Benchmarking Massively Parallelized Multi-Task Reinforcement Learning for Robotics Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-task Reinforcement Learning (MTRL) has emerged as a critical training paradigm for applying reinforcement learning (RL) to a set of complex real-world robotic tasks, which demands a generalizable and robust policy. At the same time, \emph{massively parallelized training} has gained popularity, not only for significantly accelerating data collection through GPU-accelerated simulation but also for enabling diverse data collection across multiple tasks by simulating heterogeneous scenes in parallel. However, existing MTRL research has largely been limited to off-policy methods like SAC in the low-parallelization regime. MTRL could capitalize on the higher asymptotic performance of on-policy algorithms, whose batches require data from the current policy, and as a result, take advantage of massive parallelization offered by GPU-accelerated simulation. To bridge this gap, we introduce a massively parallelized $\textbf{M}$ulti-$\textbf{T}$ask $\textbf{Bench}$mark for robotics (MTBench), an open-sourced benchmark featuring a broad distribution of 50 manipulation tasks and 20 locomotion tasks, implemented using the GPU-accelerated simulator IsaacGym. MTBench also includes four base RL algorithms combined with seven state-of-the-art MTRL algorithms and architectures, providing a unified framework for evaluating their performance. Our extensive experiments highlight the superior speed of evaluating MTRL approaches using MTBench, while also uncovering unique challenges that arise from combining massive parallelism with MTRL. Code is available at https://github.com/Viraj-Joshi/MTBench
<div id='section'>Paperid: <span id='pid'>13, <a href='https://arxiv.org/pdf/2507.18059.pdf' target='_blank'>https://arxiv.org/pdf/2507.18059.pdf</a></span>   <span><a href='https://github.com/liyheng/MAGPO' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yueheng Li, Guangming Xie, Zongqing Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.18059">Multi-Agent Guided Policy Optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Due to practical constraints such as partial observability and limited communication, Centralized Training with Decentralized Execution (CTDE) has become the dominant paradigm in cooperative Multi-Agent Reinforcement Learning (MARL). However, existing CTDE methods often underutilize centralized training or lack theoretical guarantees. We propose Multi-Agent Guided Policy Optimization (MAGPO), a novel framework that better leverages centralized training by integrating centralized guidance with decentralized execution. MAGPO uses an auto-regressive joint policy for scalable, coordinated exploration and explicitly aligns it with decentralized policies to ensure deployability under partial observability. We provide theoretical guarantees of monotonic policy improvement and empirically evaluate MAGPO on 43 tasks across 6 diverse environments. Results show that MAGPO consistently outperforms strong CTDE baselines and matches or surpasses fully centralized approaches, offering a principled and practical solution for decentralized multi-agent learning. Our code and experimental data can be found in https://github.com/liyheng/MAGPO.
<div id='section'>Paperid: <span id='pid'>14, <a href='https://arxiv.org/pdf/2507.12110.pdf' target='_blank'>https://arxiv.org/pdf/2507.12110.pdf</a></span>   <span><a href='https://github.com/leoPub/tpemarl' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ye Han, Lijun Zhang, Dejian Meng, Zhuang Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.12110">Topology Enhanced MARL for Multi-Vehicle Cooperative Decision-Making of CAVs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The exploration-exploitation trade-off constitutes one of the fundamental challenges in reinforcement learning (RL), which is exacerbated in multi-agent reinforcement learning (MARL) due to the exponential growth of joint state-action spaces. This paper proposes a topology-enhanced MARL (TPE-MARL) method for optimizing cooperative decision-making of connected and autonomous vehicles (CAVs) in mixed traffic. This work presents two primary contributions: First, we construct a game topology tensor for dynamic traffic flow, effectively compressing high-dimensional traffic state information and decrease the search space for MARL algorithms. Second, building upon the designed game topology tensor and using QMIX as the backbone RL algorithm, we establish a topology-enhanced MARL framework incorporating visit counts and agent mutual information. Extensive simulations across varying traffic densities and CAV penetration rates demonstrate the effectiveness of TPE-MARL. Evaluations encompassing training dynamics, exploration patterns, macroscopic traffic performance metrics, and microscopic vehicle behaviors reveal that TPE-MARL successfully balances exploration and exploitation. Consequently, it exhibits superior performance in terms of traffic efficiency, safety, decision smoothness, and task completion. Furthermore, the algorithm demonstrates decision-making rationality comparable to or exceeding that of human drivers in both mixed-autonomy and fully autonomous traffic scenarios. Code of our work is available at \href{https://github.com/leoPub/tpemarl}{https://github.com/leoPub/tpemarl}.
<div id='section'>Paperid: <span id='pid'>15, <a href='https://arxiv.org/pdf/2507.06825.pdf' target='_blank'>https://arxiv.org/pdf/2507.06825.pdf</a></span>   <span><a href='https://github.com/strakam/generals-bots' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Matej Straka, Martin Schmid
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.06825">Artificial Generals Intelligence: Mastering Generals.io with Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce a real-time strategy game environment based on Generals.io, a game with thousands of weekly active players. Our environment is fully compatible with Gymnasium and PettingZoo and is capable of running thousands of frames per second on commodity hardware. We also present a reference agent, trained with supervised pre-training and self-play, which reached the top 0.003% of the 1v1 human leaderboard after only 36 hours on a single H100 GPU. To accelerate learning, we incorporate potential-based reward shaping and memory features. Our contributions of a modular RTS benchmark and a competitive baseline agent provide an accessible yet challenging platform for advancing multi-agent reinforcement learning research. The documented code, together with examples and tutorials, is available at https://github.com/strakam/generals-bots.
<div id='section'>Paperid: <span id='pid'>16, <a href='https://arxiv.org/pdf/2507.06690.pdf' target='_blank'>https://arxiv.org/pdf/2507.06690.pdf</a></span>   <span><a href='https://github.com/WindyLab/MT-MARL-SG' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Guobin Zhu, Rui Zhou, Wenkang Ji, Hongyin Zhang, Donglin Wang, Shiyu Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.06690">Multi-Task Multi-Agent Reinforcement Learning via Skill Graphs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-task multi-agent reinforcement learning (MT-MARL) has recently gained attention for its potential to enhance MARL's adaptability across multiple tasks. However, it is challenging for existing multi-task learning methods to handle complex problems, as they are unable to handle unrelated tasks and possess limited knowledge transfer capabilities. In this paper, we propose a hierarchical approach that efficiently addresses these challenges. The high-level module utilizes a skill graph, while the low-level module employs a standard MARL algorithm. Our approach offers two contributions. First, we consider the MT-MARL problem in the context of unrelated tasks, expanding the scope of MTRL. Second, the skill graph is used as the upper layer of the standard hierarchical approach, with training independent of the lower layer, effectively handling unrelated tasks and enhancing knowledge transfer capabilities. Extensive experiments are conducted to validate these advantages and demonstrate that the proposed method outperforms the latest hierarchical MAPPO algorithms. Videos and code are available at https://github.com/WindyLab/MT-MARL-SG
<div id='section'>Paperid: <span id='pid'>17, <a href='https://arxiv.org/pdf/2507.01823.pdf' target='_blank'>https://arxiv.org/pdf/2507.01823.pdf</a></span>   <span><a href='https://github.com/dmytro-kuzmenko/td-mpc-opt' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Dmytro Kuzmenko, Nadiya Shvai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.01823">TD-MPC-Opt: Distilling Model-Based Multi-Task Reinforcement Learning Agents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a novel approach to knowledge transfer in model-based reinforcement learning, addressing the critical challenge of deploying large world models in resource-constrained environments. Our method efficiently distills a high-capacity multi-task agent (317M parameters) into a compact model (1M parameters) on the MT30 benchmark, significantly improving performance across diverse tasks. Our distilled model achieves a state-of-the-art normalized score of 28.45, surpassing the original 1M parameter model score of 18.93. This improvement demonstrates the ability of our distillation technique to capture and consolidate complex multi-task knowledge. We further optimize the distilled model through FP16 post-training quantization, reducing its size by $\sim$50\%. Our approach addresses practical deployment limitations and offers insights into knowledge representation in large world models, paving the way for more efficient and accessible multi-task reinforcement learning systems in robotics and other resource-constrained applications. Code available at https://github.com/dmytro-kuzmenko/td-mpc-opt.
<div id='section'>Paperid: <span id='pid'>18, <a href='https://arxiv.org/pdf/2506.16718.pdf' target='_blank'>https://arxiv.org/pdf/2506.16718.pdf</a></span>   <span><a href='https://github.com/vcis-wangchenxu/MRDG.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chenxu Wang, Yonggang Jin, Cheng Hu, Youpeng Zhao, Zipeng Dai, Jian Zhao, Shiyu Huang, Liuyu Xiang, Junge Zhang, Zhaofeng He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.16718">Generalizable Agent Modeling for Agent Collaboration-Competition Adaptation with Multi-Retrieval and Dynamic Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Adapting a single agent to a new multi-agent system brings challenges, necessitating adjustments across various tasks, environments, and interactions with unknown teammates and opponents. Addressing this challenge is highly complex, and researchers have proposed two simplified scenarios, Multi-agent reinforcement learning for zero-shot learning and Ad-Hoc Teamwork. Building on these foundations, we propose a more comprehensive setting, Agent Collaborative-Competitive Adaptation (ACCA), which evaluates an agent to generalize across diverse scenarios, tasks, and interactions with both unfamiliar opponents and teammates. In ACCA, agents adjust to task and environmental changes, collaborate with unseen teammates, and compete against unknown opponents. We introduce a new modeling approach, Multi-Retrieval and Dynamic Generation (MRDG), that effectively models both teammates and opponents using their behavioral trajectories. This method incorporates a positional encoder for varying team sizes and a hypernetwork module to boost agents' learning and adaptive capabilities. Additionally, a viewpoint alignment module harmonizes the observational perspectives of retrieved teammates and opponents with the learning agent. Extensive tests in benchmark scenarios like SMAC, Overcooked-AI, and Melting Pot show that MRDG significantly improves robust collaboration and competition with unseen teammates and opponents, surpassing established baselines. Our code is available at: https://github.com/vcis-wangchenxu/MRDG.git
<div id='section'>Paperid: <span id='pid'>19, <a href='https://arxiv.org/pdf/2506.07548.pdf' target='_blank'>https://arxiv.org/pdf/2506.07548.pdf</a></span>   <span><a href='https://github.com/NICE-HKU/CL2MARL-SMAC' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Weiqiang Jin, Hongyang Du, Guizhong Liu, Dong In Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.07548">Curriculum Learning With Counterfactual Group Relative Policy Advantage For Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent reinforcement learning (MARL) has achieved strong performance in cooperative adversarial tasks. However, most existing methods typically train agents against fixed opponent strategies and rely on such meta-static difficulty conditions, which limits their adaptability to changing environments and often leads to suboptimal policies. Inspired by the success of curriculum learning (CL) in supervised tasks, we propose a dynamic CL framework for MARL that employs an self-adaptive difficulty adjustment mechanism. This mechanism continuously modulates opponent strength based on real-time agent training performance, allowing agents to progressively learn from easier to more challenging scenarios. However, the dynamic nature of CL introduces instability due to nonstationary environments and sparse global rewards. To address this challenge, we develop a Counterfactual Group Relative Policy Advantage (CGRPA), which is tightly coupled with the curriculum by providing intrinsic credit signals that reflect each agent's impact under evolving task demands. CGRPA constructs a counterfactual advantage function that isolates individual contributions within group behavior, facilitating more reliable policy updates throughout the curriculum. CGRPA evaluates each agent's contribution through constructing counterfactual action advantage function, providing intrinsic rewards that enhance credit assignment and stabilize learning under non-stationary conditions. Extensive experiments demonstrate that our method improves both training stability and final performance, achieving competitive results against state-of-the-art methods. The code is available at https://github.com/NICE-HKU/CL2MARL-SMAC.
<div id='section'>Paperid: <span id='pid'>20, <a href='https://arxiv.org/pdf/2506.01538.pdf' target='_blank'>https://arxiv.org/pdf/2506.01538.pdf</a></span>   <span><a href='https://windylab.github.io/LAMARL/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Guobin Zhu, Rui Zhou, Wenkang Ji, Shiyu Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.01538">LAMARL: LLM-Aided Multi-Agent Reinforcement Learning for Cooperative Policy Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Although Multi-Agent Reinforcement Learning (MARL) is effective for complex multi-robot tasks, it suffers from low sample efficiency and requires iterative manual reward tuning. Large Language Models (LLMs) have shown promise in single-robot settings, but their application in multi-robot systems remains largely unexplored. This paper introduces a novel LLM-Aided MARL (LAMARL) approach, which integrates MARL with LLMs, significantly enhancing sample efficiency without requiring manual design. LAMARL consists of two modules: the first module leverages LLMs to fully automate the generation of prior policy and reward functions. The second module is MARL, which uses the generated functions to guide robot policy training effectively. On a shape assembly benchmark, both simulation and real-world experiments demonstrate the unique advantages of LAMARL. Ablation studies show that the prior policy improves sample efficiency by an average of 185.9% and enhances task completion, while structured prompts based on Chain-of-Thought (CoT) and basic APIs improve LLM output success rates by 28.5%-67.5%. Videos and code are available at https://windylab.github.io/LAMARL/
<div id='section'>Paperid: <span id='pid'>21, <a href='https://arxiv.org/pdf/2506.00228.pdf' target='_blank'>https://arxiv.org/pdf/2506.00228.pdf</a></span>   <span><a href='https://github.com/social-ai-uoft/sorrel' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Rebekah A. GelpÃ­, Yibing Ju, Ethan C. Jackson, Yikai Tang, Shon Verch, Claas Voelcker, William A. Cunningham
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.00228">Sorrel: A simple and flexible framework for multi-agent reinforcement learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce Sorrel (https://github.com/social-ai-uoft/sorrel), a simple Python interface for generating and testing new multi-agent reinforcement learning environments. This interface places a high degree of emphasis on simplicity and accessibility, and uses a more psychologically intuitive structure for the basic agent-environment loop, making it a useful tool for social scientists to investigate how learning and social interaction leads to the development and change of group dynamics. In this short paper, we outline the basic design philosophy and features of Sorrel.
<div id='section'>Paperid: <span id='pid'>22, <a href='https://arxiv.org/pdf/2505.08459.pdf' target='_blank'>https://arxiv.org/pdf/2505.08459.pdf</a></span>   <span><a href='https://github.com/hsushuai/SAP' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuai Xu, Sijia Cui, Yanna Wang, Bo Xu, Qi Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.08459">Strategy-Augmented Planning for Large Language Models via Opponent Exploitation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Efficiently modeling and exploiting opponents is a long-standing challenge in adversarial domains. Large Language Models (LLMs) trained on extensive textual data have recently demonstrated outstanding performance in general tasks, introducing new research directions for opponent modeling. Some studies primarily focus on directly using LLMs to generate decisions based on the elaborate prompt context that incorporates opponent descriptions, while these approaches are limited to scenarios where LLMs possess adequate domain expertise. To address that, we introduce a two-stage Strategy-Augmented Planning (SAP) framework that significantly enhances the opponent exploitation capabilities of LLM-based agents by utilizing a critical component, the Strategy Evaluation Network (SEN). Specifically, in the offline stage, we construct an explicit strategy space and subsequently collect strategy-outcome pair data for training the SEN network. During the online phase, SAP dynamically recognizes the opponent's strategies and greedily exploits them by searching best response strategy on the well-trained SEN, finally translating strategy to a course of actions by carefully designed prompts. Experimental results show that SAP exhibits robust generalization capabilities, allowing it to perform effectively not only against previously encountered opponent strategies but also against novel, unseen strategies. In the MicroRTS environment, SAP achieves a $85.35\%$ performance improvement over baseline methods and matches the competitiveness of reinforcement learning approaches against state-of-the-art (SOTA) rule-based AI. Our code is available at https://github.com/hsushuai/SAP.
<div id='section'>Paperid: <span id='pid'>23, <a href='https://arxiv.org/pdf/2505.06771.pdf' target='_blank'>https://arxiv.org/pdf/2505.06771.pdf</a></span>   <span><a href='https://github.com/GT-STAR-Lab/JaxRobotarium' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/GT-STAR-Lab/JaxRobotarium' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shalin Anand Jain, Jiazhen Liu, Siva Kailas, Harish Ravichandar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.06771">JaxRobotarium: Training and Deploying Multi-Robot Policies in 10 Minutes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent reinforcement learning (MARL) has emerged as a promising solution for learning complex and scalable coordination behaviors in multi-robot systems. However, established MARL platforms (e.g., SMAC and MPE) lack robotics relevance and hardware deployment, leaving multi-robot learning researchers to develop bespoke environments and hardware testbeds dedicated to the development and evaluation of their individual contributions. The Multi-Agent RL Benchmark and Learning Environment for the Robotarium (MARBLER) is an exciting recent step in providing a standardized robotics-relevant platform for MARL, by bridging the Robotarium testbed with existing MARL software infrastructure. However, MARBLER lacks support for parallelization and GPU/TPU execution, making the platform prohibitively slow compared to modern MARL environments and hindering adoption. We contribute JaxRobotarium, a Jax-powered end-to-end simulation, learning, deployment, and benchmarking platform for the Robotarium. JaxRobotarium enables rapid training and deployment of multi-robot RL (MRRL) policies with realistic robot dynamics and safety constraints, supporting parallelization and hardware acceleration. Our generalizable learning interface integrates easily with SOTA MARL libraries (e.g., JaxMARL). In addition, JaxRobotarium includes eight standardized coordination scenarios, including four novel scenarios that bring established MARL benchmark tasks (e.g., RWARE and Level-Based Foraging) to a robotics setting. We demonstrate that JaxRobotarium retains high simulation fidelity while achieving dramatic speedups over baseline (20x in training and 150x in simulation), and provides an open-access sim-to-real evaluation pipeline through the Robotarium testbed, accelerating and democratizing access to multi-robot learning research and evaluation. Our code is available at https://github.com/GT-STAR-Lab/JaxRobotarium.
<div id='section'>Paperid: <span id='pid'>24, <a href='https://arxiv.org/pdf/2505.03586.pdf' target='_blank'>https://arxiv.org/pdf/2505.03586.pdf</a></span>   <span><a href='https://github.com/linkjoker1006' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Songchen Fu, Siang Chen, Shaojing Zhao, Letian Bai, Ta Li, Yonghong Yan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.03586">Rainbow Delay Compensation: A Multi-Agent Reinforcement Learning Framework for Mitigating Delayed Observation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In real-world multi-agent systems (MASs), observation delays are ubiquitous, preventing agents from making decisions based on the environment's true state. An individual agent's local observation often consists of multiple components from other agents or dynamic entities in the environment. These discrete observation components with varying delay characteristics pose significant challenges for multi-agent reinforcement learning (MARL). In this paper, we first formulate the decentralized stochastic individual delay partially observable Markov decision process (DSID-POMDP) by extending the standard Dec-POMDP. We then propose the Rainbow Delay Compensation (RDC), a MARL training framework for addressing stochastic individual delays, along with recommended implementations for its constituent modules. We implement the DSID-POMDP's observation generation pattern using standard MARL benchmarks, including MPE and SMAC. Experiments demonstrate that baseline MARL methods suffer severe performance degradation under fixed and unfixed delays. The RDC-enhanced approach mitigates this issue, remarkably achieving ideal delay-free performance in certain delay scenarios while maintaining generalizability. Our work provides a novel perspective on multi-agent delayed observation problems and offers an effective solution framework. The source code is available at https://anonymous.4open.science/r/RDC-pymarl-4512/.
<div id='section'>Paperid: <span id='pid'>25, <a href='https://arxiv.org/pdf/2505.02293.pdf' target='_blank'>https://arxiv.org/pdf/2505.02293.pdf</a></span>   <span><a href='https://dinamo-mit.github.io/Layered-Safe-MARL/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jason J. Choi, Jasmine Jerry Aloor, Jingqi Li, Maria G. Mendoza, Hamsa Balakrishnan, Claire J. Tomlin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.02293">Resolving Conflicting Constraints in Multi-Agent Reinforcement Learning with Layered Safety</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Preventing collisions in multi-robot navigation is crucial for deployment. This requirement hinders the use of learning-based approaches, such as multi-agent reinforcement learning (MARL), on their own due to their lack of safety guarantees. Traditional control methods, such as reachability and control barrier functions, can provide rigorous safety guarantees when interactions are limited only to a small number of robots. However, conflicts between the constraints faced by different agents pose a challenge to safe multi-agent coordination.
  To overcome this challenge, we propose a method that integrates multiple layers of safety by combining MARL with safety filters. First, MARL is used to learn strategies that minimize multiple agent interactions, where multiple indicates more than two. Particularly, we focus on interactions likely to result in conflicting constraints within the engagement distance. Next, for agents that enter the engagement distance, we prioritize pairs requiring the most urgent corrective actions. Finally, a dedicated safety filter provides tactical corrective actions to resolve these conflicts. Crucially, the design decisions for all layers of this framework are grounded in reachability analysis and a control barrier-value function-based filtering mechanism.
  We validate our Layered Safe MARL framework in 1) hardware experiments using Crazyflie drones and 2) high-density advanced aerial mobility (AAM) operation scenarios, where agents navigate to designated waypoints while avoiding collisions. The results show that our method significantly reduces conflict while maintaining safety without sacrificing much efficiency (i.e., shorter travel time and distance) compared to baselines that do not incorporate layered safety. The project website is available at https://dinamo-mit.github.io/Layered-Safe-MARL/
<div id='section'>Paperid: <span id='pid'>26, <a href='https://arxiv.org/pdf/2505.02293.pdf' target='_blank'>https://arxiv.org/pdf/2505.02293.pdf</a></span>   <span><a href='https://dinamo-mit.github.io/Layered-Safe-MARL/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jason J. Choi, Jasmine Jerry Aloor, Jingqi Li, Maria G. Mendoza, Hamsa Balakrishnan, Claire J. Tomlin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.02293">Resolving Conflicting Constraints in Multi-Agent Reinforcement Learning with Layered Safety</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Preventing collisions in multi-robot navigation is crucial for deployment. This requirement hinders the use of learning-based approaches, such as multi-agent reinforcement learning (MARL), on their own due to their lack of safety guarantees. Traditional control methods, such as reachability and control barrier functions, can provide rigorous safety guarantees when interactions are limited only to a small number of robots. However, conflicts between the constraints faced by different agents pose a challenge to safe multi-agent coordination. To overcome this challenge, we propose a method that integrates multiple layers of safety by combining MARL with safety filters. First, MARL is used to learn strategies that minimize multiple agent interactions, where multiple indicates more than two. Particularly, we focus on interactions likely to result in conflicting constraints within the engagement distance. Next, for agents that enter the engagement distance, we prioritize pairs requiring the most urgent corrective actions. Finally, a dedicated safety filter provides tactical corrective actions to resolve these conflicts. Crucially, the design decisions for all layers of this framework are grounded in reachability analysis and a control barrier-value function-based filtering mechanism. We validate our Layered Safe MARL framework in 1) hardware experiments using Crazyflie drones and 2) high-density advanced aerial mobility (AAM) operation scenarios, where agents navigate to designated waypoints while avoiding collisions. The results show that our method significantly reduces conflict while maintaining safety without sacrificing much efficiency (i.e., shorter travel time and distance) compared to baselines that do not incorporate layered safety. The project website is available at https://dinamo-mit.github.io/Layered-Safe-MARL/
<div id='section'>Paperid: <span id='pid'>27, <a href='https://arxiv.org/pdf/2505.02293.pdf' target='_blank'>https://arxiv.org/pdf/2505.02293.pdf</a></span>   <span><a href='https://dinamo-mit.github.io/Layered-Safe-MARL/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jason J. Choi, Jasmine Jerry Aloor, Jingqi Li, Maria G. Mendoza, Hamsa Balakrishnan, Claire J. Tomlin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.02293">Resolving Conflicting Constraints in Multi-Agent Reinforcement Learning with Layered Safety</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Preventing collisions in multi-robot navigation is crucial for deployment. This requirement hinders the use of learning-based approaches, such as multi-agent reinforcement learning (MARL), on their own due to their lack of safety guarantees. Traditional control methods, such as reachability and control barrier functions, can provide rigorous safety guarantees when interactions are limited only to a small number of robots. However, conflicts between the constraints faced by different agents pose a challenge to safe multi-agent coordination. To overcome this challenge, we propose a method that integrates multiple layers of safety by combining MARL with safety filters. First, MARL is used to learn strategies that minimize multiple agent interactions, where multiple indicates more than two. Particularly, we focus on interactions likely to result in conflicting constraints within the engagement distance. Next, for agents that enter the engagement distance, we prioritize pairs requiring the most urgent corrective actions. Finally, a dedicated safety filter provides tactical corrective actions to resolve these conflicts. Crucially, the design decisions for all layers of this framework are grounded in reachability analysis and a control barrier-value function-based filtering mechanism. We validate our Layered Safe MARL framework in 1) hardware experiments using Crazyflie drones and 2) high-density advanced aerial mobility (AAM) operation scenarios, where agents navigate to designated waypoints while avoiding collisions. The results show that our method significantly reduces conflict while maintaining safety without sacrificing much efficiency (i.e., shorter travel time and distance) compared to baselines that do not incorporate layered safety. The project website is available at https://dinamo-mit.github.io/Layered-Safe-MARL/
<div id='section'>Paperid: <span id='pid'>28, <a href='https://arxiv.org/pdf/2504.16129.pdf' target='_blank'>https://arxiv.org/pdf/2504.16129.pdf</a></span>   <span><a href='https://github.com/jwliao-ai/MARFT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Junwei Liao, Muning Wen, Jun Wang, Weinan Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.16129">MARFT: Multi-Agent Reinforcement Fine-Tuning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>LLM-based Multi-Agent Systems have demonstrated remarkable capabilities in addressing complex, agentic tasks, from generating high-quality presentation slides to even conducting sophisticated scientific research. Meanwhile, RL has been widely recognized for its effectiveness in enhancing agent intelligence, but limited research has investigated the fine-tuning of LaMAS using foundational RL techniques. Moreover, the direct application of MARL methods to LaMAS introduces significant challenges, stemming from the unique characteristics and mechanisms inherent to LaMAS. To address these challenges, this article presents a comprehensive study of LLM-based MARL and proposes a novel paradigm termed Multi-Agent Reinforcement Fine-Tuning (MARFT). We introduce a brand-new POMDP called Flex-POMDP, which aligns with the LaMAS optimization in real-world applications and a universal algorithmic framework tailored specifically for LaMAS, outlining the conceptual foundations, key distinctions, and practical implementation strategies. We review the evolution from RL to RFT, setting the stage for a parallel analysis in the multi-agent domain. In the context of LaMAS, we elucidate critical differences between MARL and MARFT. These differences motivate a transition toward a LaMAS-oriented formulation of RFT. Central to this work is a robust and scalable MARFT framework. We detail the core algorithm and provide a complete, open-source implementation to facilitate adoption and further research. The latter sections of the paper explore real-world application perspectives and opening challenges in MARFT. By bridging theoretical underpinnings with practical methodologies, this work serves as a roadmap for researchers seeking to advance MARFT toward resilient and adaptive solutions in agentic systems. Our implementation of the proposed framework is publicly available at: https://github.com/jwliao-ai/MARFT.
<div id='section'>Paperid: <span id='pid'>29, <a href='https://arxiv.org/pdf/2503.23626.pdf' target='_blank'>https://arxiv.org/pdf/2503.23626.pdf</a></span>   <span><a href='https://github.com/Asatheesh6561/MAPPO-LCE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Anirudh Satheesh, Keenan Powell
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.23626">A Constrained Multi-Agent Reinforcement Learning Approach to Autonomous Traffic Signal Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Traffic congestion in modern cities is exacerbated by the limitations of traditional fixed-time traffic signal systems, which fail to adapt to dynamic traffic patterns. Adaptive Traffic Signal Control (ATSC) algorithms have emerged as a solution by dynamically adjusting signal timing based on real-time traffic conditions. However, the main limitation of such methods is that they are not transferable to environments under real-world constraints, such as balancing efficiency, minimizing collisions, and ensuring fairness across intersections. In this paper, we view the ATSC problem as a constrained multi-agent reinforcement learning (MARL) problem and propose a novel algorithm named Multi-Agent Proximal Policy Optimization with Lagrange Cost Estimator (MAPPO-LCE) to produce effective traffic signal control policies. Our approach integrates the Lagrange multipliers method to balance rewards and constraints, with a cost estimator for stable adjustment. We also introduce three constraints on the traffic network: GreenTime, GreenSkip, and PhaseSkip, which penalize traffic policies that do not conform to real-world scenarios. Our experimental results on three real-world datasets demonstrate that MAPPO-LCE outperforms three baseline MARL algorithms by across all environments and traffic constraints (improving on MAPPO by 12.60%, IPPO by 10.29%, and QTRAN by 13.10%). Our results show that constrained MARL is a valuable tool for traffic planners to deploy scalable and efficient ATSC methods in real-world traffic networks. We provide code at https://github.com/Asatheesh6561/MAPPO-LCE.
<div id='section'>Paperid: <span id='pid'>30, <a href='https://arxiv.org/pdf/2503.15049.pdf' target='_blank'>https://arxiv.org/pdf/2503.15049.pdf</a></span>   <span><a href='https://github.com/RoboSafe-Lab/Sim4AD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Cheng Wang, Lingxin Kong, Massimiliano Tamborski, Stefano V. Albrecht
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.15049">HAD-Gen: Human-like and Diverse Driving Behavior Modeling for Controllable Scenario Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Simulation-based testing has emerged as an essential tool for verifying and validating autonomous vehicles (AVs). However, contemporary methodologies, such as deterministic and imitation learning-based driver models, struggle to capture the variability of human-like driving behavior. Given these challenges, we propose HAD-Gen, a general framework for realistic traffic scenario generation that simulates diverse human-like driving behaviors. The framework first clusters the vehicle trajectory data into different driving styles according to safety features. It then employs maximum entropy inverse reinforcement learning on each of the clusters to learn the reward function corresponding to each driving style. Using these reward functions, the method integrates offline reinforcement learning pre-training and multi-agent reinforcement learning algorithms to obtain general and robust driving policies. Multi-perspective simulation results show that our proposed scenario generation framework can simulate diverse, human-like driving behaviors with strong generalization capability. The proposed framework achieves a 90.96% goal-reaching rate, an off-road rate of 2.08%, and a collision rate of 6.91% in the generalization test, outperforming prior approaches by over 20% in goal-reaching performance. The source code is released at https://github.com/RoboSafe-Lab/Sim4AD.
<div id='section'>Paperid: <span id='pid'>31, <a href='https://arxiv.org/pdf/2503.14555.pdf' target='_blank'>https://arxiv.org/pdf/2503.14555.pdf</a></span>   <span><a href='https://github.com/chandar-lab/R3D2-A-Generalist-Hanabi-Agent' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Arjun V Sudhakar, Hadi Nekoei, Mathieu Reymond, Miao Liu, Janarthanan Rajendran, Sarath Chandar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.14555">A Generalist Hanabi Agent</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Traditional multi-agent reinforcement learning (MARL) systems can develop cooperative strategies through repeated interactions. However, these systems are unable to perform well on any other setting than the one they have been trained on, and struggle to successfully cooperate with unfamiliar collaborators. This is particularly visible in the Hanabi benchmark, a popular 2-to-5 player cooperative card-game which requires complex reasoning and precise assistance to other agents. Current MARL agents for Hanabi can only learn one specific game-setting (e.g., 2-player games), and play with the same algorithmic agents. This is in stark contrast to humans, who can quickly adjust their strategies to work with unfamiliar partners or situations. In this paper, we introduce Recurrent Replay Relevance Distributed DQN (R3D2), a generalist agent for Hanabi, designed to overcome these limitations. We reformulate the task using text, as language has been shown to improve transfer. We then propose a distributed MARL algorithm that copes with the resulting dynamic observation- and action-space. In doing so, our agent is the first that can play all game settings concurrently, and extend strategies learned from one setting to other ones. As a consequence, our agent also demonstrates the ability to collaborate with different algorithmic agents -- agents that are themselves unable to do so. The implementation code is available at: $\href{https://github.com/chandar-lab/R3D2-A-Generalist-Hanabi-Agent}{R3D2-A-Generalist-Hanabi-Agent}$
<div id='section'>Paperid: <span id='pid'>32, <a href='https://arxiv.org/pdf/2503.12122.pdf' target='_blank'>https://arxiv.org/pdf/2503.12122.pdf</a></span>   <span><a href='https://yanoyoshiki.github.io/ICCO/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yoshiki Yano, Kazuki Shibata, Maarten Kokshoorn, Takamitsu Matsubara
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.12122">ICCO: Learning an Instruction-conditioned Coordinator for Language-guided Task-aligned Multi-robot Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in Large Language Models (LLMs) have permitted the development of language-guided multi-robot systems, which allow robots to execute tasks based on natural language instructions. However, achieving effective coordination in distributed multi-agent environments remains challenging due to (1) misalignment between instructions and task requirements and (2) inconsistency in robot behaviors when they independently interpret ambiguous instructions. To address these challenges, we propose Instruction-Conditioned Coordinator (ICCO), a Multi-Agent Reinforcement Learning (MARL) framework designed to enhance coordination in language-guided multi-robot systems. ICCO consists of a Coordinator agent and multiple Local Agents, where the Coordinator generates Task-Aligned and Consistent Instructions (TACI) by integrating language instructions with environmental states, ensuring task alignment and behavioral consistency. The Coordinator and Local Agents are jointly trained to optimize a reward function that balances task efficiency and instruction following. A Consistency Enhancement Term is added to the learning objective to maximize mutual information between instructions and robot behaviors, further improving coordination. Simulation and real-world experiments validate the effectiveness of ICCO in achieving language-guided task-aligned multi-robot control. The demonstration can be found at https://yanoyoshiki.github.io/ICCO/.
<div id='section'>Paperid: <span id='pid'>33, <a href='https://arxiv.org/pdf/2503.11726.pdf' target='_blank'>https://arxiv.org/pdf/2503.11726.pdf</a></span>   <span><a href='https://github.com/funny-rl/SPECTra' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hyunwoo Park, Baekryun Seong, Sang-Ki Ko
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.11726">SPECTra: Scalable Multi-Agent Reinforcement Learning with Permutation-Free Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In cooperative multi-agent reinforcement learning (MARL), the permutation problem where the state space grows exponentially with the number of agents reduces sample efficiency. Additionally, many existing architectures struggle with scalability, relying on a fixed structure tied to a specific number of agents, limiting their applicability to environments with a variable number of entities. While approaches such as graph neural networks (GNNs) and self-attention mechanisms have progressed in addressing these challenges, they have significant limitations as dense GNNs and self-attention mechanisms incur high computational costs. To overcome these limitations, we propose a novel agent network and a non-linear mixing network that ensure permutation-equivariance and scalability, allowing them to generalize to environments with various numbers of agents. Our agent network significantly reduces computational complexity, and our scalable hypernetwork enables efficient weight generation for non-linear mixing. Additionally, we introduce curriculum learning to improve training efficiency. Experiments on SMACv2 and Google Research Football (GRF) demonstrate that our approach achieves superior learning performance compared to existing methods. By addressing both permutation-invariance and scalability in MARL, our work provides a more efficient and adaptable framework for cooperative MARL. Our code is available at https://github.com/funny-rl/SPECTra.
<div id='section'>Paperid: <span id='pid'>34, <a href='https://arxiv.org/pdf/2503.09501.pdf' target='_blank'>https://arxiv.org/pdf/2503.09501.pdf</a></span>   <span><a href='https://github.com/ziyuwan/ReMA-public' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziyu Wan, Yunxiang Li, Xiaoyu Wen, Yan Song, Hanjing Wang, Linyi Yang, Mark Schmidt, Jun Wang, Weinan Zhang, Shuyue Hu, Ying Wen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.09501">ReMA: Learning to Meta-think for LLMs with Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent research on Reasoning of Large Language Models (LLMs) has sought to further enhance their performance by integrating meta-thinking -- enabling models to monitor, evaluate, and control their reasoning processes for more adaptive and effective problem-solving. However, current single-agent work lacks a specialized design for acquiring meta-thinking, resulting in low efficacy. To address this challenge, we introduce Reinforced Meta-thinking Agents (ReMA), a novel framework that leverages Multi-Agent Reinforcement Learning (MARL) to elicit meta-thinking behaviors, encouraging LLMs to think about thinking. ReMA decouples the reasoning process into two hierarchical agents: a high-level meta-thinking agent responsible for generating strategic oversight and plans, and a low-level reasoning agent for detailed executions. Through iterative reinforcement learning with aligned objectives, these agents explore and learn collaboration, leading to improved generalization and robustness. Empirical results from single-turn experiments demonstrate that ReMA outperforms single-agent RL baselines on complex reasoning tasks, including competitive-level mathematical benchmarks and LLM-as-a-Judge benchmarks. Additionally, we further extend ReMA to multi-turn interaction settings, leveraging turn-level ratio and parameter sharing to improve efficiency. Comprehensive ablation studies further illustrate the evolving dynamics of each distinct agent, providing valuable insights into how the meta-thinking reasoning process enhances the reasoning capabilities of LLMs. Our code can be found in https://github.com/ziyuwan/ReMA-public
<div id='section'>Paperid: <span id='pid'>35, <a href='https://arxiv.org/pdf/2503.01017.pdf' target='_blank'>https://arxiv.org/pdf/2503.01017.pdf</a></span>   <span><a href='https://github.com/Lab-Work/marl-vsl-controller' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuhang Zhang, Zhiyao Zhang, Junyi Ji, Marcos QuiÃ±ones-Grueiro, William Barbour, Derek Gloudemans, Gergely ZachÃ¡r, Clay Weston, Gautam Biswas, Daniel B. Work
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.01017">Real-World Deployment and Assessment of a Multi-Agent Reinforcement Learning-Based Variable Speed Limit Control System</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This article presents the first field deployment of a multi-agent reinforcement learning (MARL) based variable speed limit (VSL) control system on Interstate 24 (I-24) near Nashville, Tennessee. We design and demonstrate a full pipeline from training MARL agents in a traffic simulator to a field deployment on a 17-mile segment of I-24 encompassing 67 VSL controllers. The system was launched on March 8th, 2024, and has made approximately 35 million decisions on 28 million trips in six months of operation. We apply an invalid action masking mechanism and several safety guards to ensure real-world constraints. The MARL-based implementation operates up to 98% of the time, with the safety guards overriding the MARL decisions for the remaining time. We evaluate the performance of the MARL-based algorithm in comparison to a previously deployed non-RL VSL benchmark algorithm on I-24. Results show that the MARL-based VSL control system achieves a superior performance. The accuracy of correctly warning drivers about slowing traffic ahead is improved by 14% and the response delay to non-recurrent congestion is reduced by 75%. The preliminary data shows that the VSL control system has reduced the crash rate by 26% and the secondary crash rate by 50%. We open-sourced the deployed MARL-based VSL algorithm at https://github.com/Lab-Work/marl-vsl-controller.
<div id='section'>Paperid: <span id='pid'>36, <a href='https://arxiv.org/pdf/2502.19717.pdf' target='_blank'>https://arxiv.org/pdf/2502.19717.pdf</a></span>   <span><a href='https://github.com/LXXXXR/ExpoComm' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinran Li, Xiaolu Wang, Chenjia Bai, Jun Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.19717">Exponential Topology-enabled Scalable Communication in Multi-agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In cooperative multi-agent reinforcement learning (MARL), well-designed communication protocols can effectively facilitate consensus among agents, thereby enhancing task performance. Moreover, in large-scale multi-agent systems commonly found in real-world applications, effective communication plays an even more critical role due to the escalated challenge of partial observability compared to smaller-scale setups. In this work, we endeavor to develop a scalable communication protocol for MARL. Unlike previous methods that focus on selecting optimal pairwise communication links-a task that becomes increasingly complex as the number of agents grows-we adopt a global perspective on communication topology design. Specifically, we propose utilizing the exponential topology to enable rapid information dissemination among agents by leveraging its small-diameter and small-size properties. This approach leads to a scalable communication protocol, named ExpoComm. To fully unlock the potential of exponential graphs as communication topologies, we employ memory-based message processors and auxiliary tasks to ground messages, ensuring that they reflect global information and benefit decision-making. Extensive experiments on large-scale cooperative benchmarks, including MAgent and Infrastructure Management Planning, demonstrate the superior performance and robust zero-shot transferability of ExpoComm compared to existing communication strategies. The code is publicly available at https://github.com/LXXXXR/ExpoComm.
<div id='section'>Paperid: <span id='pid'>37, <a href='https://arxiv.org/pdf/2502.10233.pdf' target='_blank'>https://arxiv.org/pdf/2502.10233.pdf</a></span>   <span><a href='http://github.com/LTluttmann/marl4msprp' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Laurin Luttmann, Lin Xie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.10233">Learning to Solve the Min-Max Mixed-Shelves Picker-Routing Problem via Hierarchical and Parallel Decoding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The Mixed-Shelves Picker Routing Problem (MSPRP) is a fundamental challenge in warehouse logistics, where pickers must navigate a mixed-shelves environment to retrieve SKUs efficiently. Traditional heuristics and optimization-based approaches struggle with scalability, while recent machine learning methods often rely on sequential decision-making, leading to high solution latency and suboptimal agent coordination. In this work, we propose a novel hierarchical and parallel decoding approach for solving the min-max variant of the MSPRP via multi-agent reinforcement learning. While our approach generates a joint distribution over agent actions, allowing for fast decoding and effective picker coordination, our method introduces a sequential action selection to avoid conflicts in the multi-dimensional action space. Experiments show state-of-the-art performance in both solution quality and inference speed, particularly for large-scale and out-of-distribution instances. Our code is publicly available at http://github.com/LTluttmann/marl4msprp.
<div id='section'>Paperid: <span id='pid'>38, <a href='https://arxiv.org/pdf/2502.05453.pdf' target='_blank'>https://arxiv.org/pdf/2502.05453.pdf</a></span>   <span><a href='https://happyeureka.github.io/damcs' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hanqing Yang, Jingdi Chen, Marie Siew, Tania Lorido-Botran, Carlee Joe-Wong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.05453">LLM-Powered Decentralized Generative Agents with Adaptive Hierarchical Knowledge Graph for Cooperative Planning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Developing intelligent agents for long-term cooperation in dynamic open-world scenarios is a major challenge in multi-agent systems. Traditional Multi-agent Reinforcement Learning (MARL) frameworks like centralized training decentralized execution (CTDE) struggle with scalability and flexibility. They require centralized long-term planning, which is difficult without custom reward functions, and face challenges in processing multi-modal data. CTDE approaches also assume fixed cooperation strategies, making them impractical in dynamic environments where agents need to adapt and plan independently. To address decentralized multi-agent cooperation, we propose Decentralized Adaptive Knowledge Graph Memory and Structured Communication System (DAMCS) in a novel Multi-agent Crafter environment. Our generative agents, powered by Large Language Models (LLMs), are more scalable than traditional MARL agents by leveraging external knowledge and language for long-term planning and reasoning. Instead of fully sharing information from all past experiences, DAMCS introduces a multi-modal memory system organized as a hierarchical knowledge graph and a structured communication protocol to optimize agent cooperation. This allows agents to reason from past interactions and share relevant information efficiently. Experiments on novel multi-agent open-world tasks show that DAMCS outperforms both MARL and LLM baselines in task efficiency and collaboration. Compared to single-agent scenarios, the two-agent scenario achieves the same goal with 63% fewer steps, and the six-agent scenario with 74% fewer steps, highlighting the importance of adaptive memory and structured communication in achieving long-term goals. We publicly release our project at: https://happyeureka.github.io/damcs.
<div id='section'>Paperid: <span id='pid'>39, <a href='https://arxiv.org/pdf/2502.04492.pdf' target='_blank'>https://arxiv.org/pdf/2502.04492.pdf</a></span>   <span><a href='https://github.com/sftekin/rl-focal' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Selim Furkan Tekin, Fatih Ilhan, Gaowen Liu, Ramana Rao Kompella, Ling Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.04492">Dynamic Optimizations of LLM Ensembles with Two-Stage Reinforcement Learning Agents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The advancement of LLMs and their accessibility have triggered renewed interest in multi-agent reinforcement learning as robust and adaptive frameworks for dynamically changing environments. This paper introduces RL-Focal, a two-stage RL agent framework that routes and ensembles LLMs. First, we develop the Decider RL-agent, which learns to dynamically select an ensemble of small size ($m_i$) among $N$ LLMs ($m_i \ll N$) for incoming queries from a user-defined downstream task $i$, by maximizing both error-diversity and reasoning-performance of the selected ensemble through iterative updates of task-adaptive rewards and policy. Second, to enable effective fusion of dynamically selected LLMs, we develop the stage-2 Fusion RL-agent, which learns to resolve reasoning conflicts from different LLMs and dynamically adapts to different ensemble teams composed by the Decider Agent for different downstream tasks. Third, we introduce the focal diversity metric to better model the error correlations among multiple LLMs, further improving the generalization performance of the Decider Agent, which actively prunes the ensemble combinations. By focal diversity, we enhance performance across tasks by effectively promoting reward-aware and policy-adaptive ensemble selection and inference fusion. Extensive evaluations on five benchmarks show that RL-Focal achieves the performance improvement of 8.48\% with an ensemble of small size compared to the best individual LLM in a pool and offers stronger robustness. Code is available at https://github.com/sftekin/rl-focal
<div id='section'>Paperid: <span id='pid'>40, <a href='https://arxiv.org/pdf/2502.04492.pdf' target='_blank'>https://arxiv.org/pdf/2502.04492.pdf</a></span>   <span><a href='https://github.com/sftekin/rl-focal' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Selim Furkan Tekin, Fatih Ilhan, Tiansheng Huang, Sihao Hu, Zachary Yahn, Ling Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.04492">Multi-Agent Reinforcement Learning with Focal Diversity Optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The advancement of Large Language Models (LLMs) and their finetuning strategies has triggered the renewed interests in multi-agent reinforcement learning. In this paper, we introduce a focal diversity-optimized multi-agent reinforcement learning approach, coined as MARL-Focal, with three unique characteristics. First, we develop an agent-fusion framework for encouraging multiple LLM based agents to collaborate in producing the final inference output for each LLM query. Second, we develop a focal-diversity optimized agent selection algorithm that can choose a small subset of the available agents based on how well they can complement one another to generate the query output. Finally, we design a conflict-resolution method to detect output inconsistency among multiple agents and produce our MARL-Focal output through reward-aware and policy-adaptive inference fusion. Extensive evaluations on five benchmarks show that MARL-Focal is cost-efficient and adversarial-robust. Our multi-agent fusion model achieves performance improvement of 5.51\% compared to the best individual LLM-agent and offers stronger robustness over the TruthfulQA benchmark. Code is available at https://github.com/sftekin/rl-focal
<div id='section'>Paperid: <span id='pid'>41, <a href='https://arxiv.org/pdf/2502.04028.pdf' target='_blank'>https://arxiv.org/pdf/2502.04028.pdf</a></span>   <span><a href='https://github.com/Nikunj-Gupta/dmcg-marl' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Nikunj Gupta, James Zachary Hare, Rajgopal Kannan, Viktor Prasanna
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.04028">Deep Meta Coordination Graphs for Multi-agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents deep meta coordination graphs (DMCG) for learning cooperative policies in multi-agent reinforcement learning (MARL). Coordination graph formulations encode local interactions and accordingly factorize the joint value function of all agents to improve efficiency in MARL. However, existing approaches rely solely on pairwise relations between agents, which potentially oversimplifies complex multi-agent interactions. DMCG goes beyond these simple direct interactions by also capturing useful higher-order and indirect relationships among agents. It generates novel graph structures accommodating multiple types of interactions and arbitrary lengths of multi-hop connections in coordination graphs to model such interactions. It then employs a graph convolutional network module to learn powerful representations in an end-to-end manner. We demonstrate its effectiveness in multiple coordination problems in MARL where other state-of-the-art methods can suffer from sample inefficiency or fail entirely. All codes can be found here: https://github.com/Nikunj-Gupta/dmcg-marl.
<div id='section'>Paperid: <span id='pid'>42, <a href='https://arxiv.org/pdf/2502.02844.pdf' target='_blank'>https://arxiv.org/pdf/2502.02844.pdf</a></span>   <span><a href='https://github.com/sunwoolee0504/WALL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sunwoo Lee, Jaebak Hwang, Yonghyeon Jo, Seungyul Han
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.02844">Wolfpack Adversarial Attack for Robust Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Traditional robust methods in multi-agent reinforcement learning (MARL) often struggle against coordinated adversarial attacks in cooperative scenarios. To address this limitation, we propose the Wolfpack Adversarial Attack framework, inspired by wolf hunting strategies, which targets an initial agent and its assisting agents to disrupt cooperation. Additionally, we introduce the Wolfpack-Adversarial Learning for MARL (WALL) framework, which trains robust MARL policies to defend against the proposed Wolfpack attack by fostering systemwide collaboration. Experimental results underscore the devastating impact of the Wolfpack attack and the significant robustness improvements achieved by WALL. Our code is available at https://github.com/sunwoolee0504/WALL.
<div id='section'>Paperid: <span id='pid'>43, <a href='https://arxiv.org/pdf/2502.00345.pdf' target='_blank'>https://arxiv.org/pdf/2502.00345.pdf</a></span>   <span><a href='https://github.com/Yurui-Li/CTC' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yurui Li, Yuxuan Chen, Li Zhang, Shijian Li, Gang Pan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.00345">The Composite Task Challenge for Cooperative Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The significant role of division of labor (DOL) in promoting cooperation is widely recognized in real-world applications.Many cooperative multi-agent reinforcement learning (MARL) methods have incorporated the concept of DOL to improve cooperation among agents.However, the tasks used in existing testbeds typically correspond to tasks where DOL is often not a necessary feature for achieving optimal policies.Additionally, the full utilize of DOL concept in MARL methods remains unrealized due to the absence of appropriate tasks.To enhance the generality and applicability of MARL methods in real-world scenarios, there is a necessary to develop tasks that demand multi-agent DOL and cooperation.In this paper, we propose a series of tasks designed to meet these requirements, drawing on real-world rules as the guidance for their design.We guarantee that DOL and cooperation are necessary condition for completing tasks and introduce three factors to expand the diversity of proposed tasks to cover more realistic situations.We evaluate 10 cooperative MARL methods on the proposed tasks.The results indicate that all baselines perform poorly on these tasks.To further validate the solvability of these tasks, we also propose simplified variants of proposed tasks.Experimental results show that baselines are able to handle these simplified variants, providing evidence of the solvability of the proposed tasks.The source files is available at https://github.com/Yurui-Li/CTC.
<div id='section'>Paperid: <span id='pid'>44, <a href='https://arxiv.org/pdf/2501.15228.pdf' target='_blank'>https://arxiv.org/pdf/2501.15228.pdf</a></span>   <span><a href='https://github.com/chenyiqun/MMOA-RAG' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiqun Chen, Lingyong Yan, Weiwei Sun, Xinyu Ma, Yi Zhang, Shuaiqiang Wang, Dawei Yin, Yiming Yang, Jiaxin Mao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.15228">Improving Retrieval-Augmented Generation through Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Retrieval-augmented generation (RAG) is extensively utilized to incorporate external, current knowledge into large language models, thereby minimizing hallucinations. A standard RAG pipeline may comprise several components, such as query rewriting, document retrieval, document filtering, and answer generation. However, these components are typically optimized separately through supervised fine-tuning, which can lead to misalignments between the objectives of individual modules and the overarching aim of generating accurate answers in question-answering (QA) tasks. Although recent efforts have explored reinforcement learning (RL) to optimize specific RAG components, these approaches often focus on overly simplistic pipelines with only two components or do not adequately address the complex interdependencies and collaborative interactions among the modules. To overcome these challenges, we propose treating the RAG pipeline as a multi-agent cooperative task, with each component regarded as an RL agent. Specifically, we present MMOA-RAG, a Multi-Module joint Optimization Algorithm for RAG, which employs multi-agent reinforcement learning to harmonize all agents' goals towards a unified reward, such as the F1 score of the final answer. Experiments conducted on various QA datasets demonstrate that MMOA-RAG improves the overall pipeline performance and outperforms existing baselines. Furthermore, comprehensive ablation studies validate the contributions of individual components and the adaptability of MMOA-RAG across different RAG components and datasets. The code of MMOA-RAG is on https://github.com/chenyiqun/MMOA-RAG.
<div id='section'>Paperid: <span id='pid'>45, <a href='https://arxiv.org/pdf/2501.13200.pdf' target='_blank'>https://arxiv.org/pdf/2501.13200.pdf</a></span>   <span><a href='https://github.com/Aloriosa/srmt' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Alsu Sagirova, Yuri Kuratov, Mikhail Burtsev
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.13200">SRMT: Shared Memory for Multi-agent Lifelong Pathfinding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent reinforcement learning (MARL) demonstrates significant progress in solving cooperative and competitive multi-agent problems in various environments. One of the principal challenges in MARL is the need for explicit prediction of the agents' behavior to achieve cooperation. To resolve this issue, we propose the Shared Recurrent Memory Transformer (SRMT) which extends memory transformers to multi-agent settings by pooling and globally broadcasting individual working memories, enabling agents to exchange information implicitly and coordinate their actions. We evaluate SRMT on the Partially Observable Multi-Agent Pathfinding problem in a toy Bottleneck navigation task that requires agents to pass through a narrow corridor and on a POGEMA benchmark set of tasks. In the Bottleneck task, SRMT consistently outperforms a variety of reinforcement learning baselines, especially under sparse rewards, and generalizes effectively to longer corridors than those seen during training. On POGEMA maps, including Mazes, Random, and MovingAI, SRMT is competitive with recent MARL, hybrid, and planning-based algorithms. These results suggest that incorporating shared recurrent memory into the transformer-based architectures can enhance coordination in decentralized multi-agent systems. The source code for training and evaluation is available on GitHub: https://github.com/Aloriosa/srmt.
<div id='section'>Paperid: <span id='pid'>46, <a href='https://arxiv.org/pdf/2501.10593.pdf' target='_blank'>https://arxiv.org/pdf/2501.10593.pdf</a></span>   <span><a href='https://github.com/andreyrisukhin/ColorGrid' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Andrey Risukhin, Kavel Rao, Ben Caffee, Alan Fan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.10593">ColorGrid: A Multi-Agent Non-Stationary Environment for Goal Inference and Assistance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autonomous agents' interactions with humans are increasingly focused on adapting to their changing preferences in order to improve assistance in real-world tasks. Effective agents must learn to accurately infer human goals, which are often hidden, to collaborate well. However, existing Multi-Agent Reinforcement Learning (MARL) environments lack the necessary attributes required to rigorously evaluate these agents' learning capabilities. To this end, we introduce ColorGrid, a novel MARL environment with customizable non-stationarity, asymmetry, and reward structure. We investigate the performance of Independent Proximal Policy Optimization (IPPO), a state-of-the-art (SOTA) MARL algorithm, in ColorGrid and find through extensive ablations that, particularly with simultaneous non-stationary and asymmetric goals between a ``leader'' agent representing a human and a ``follower'' assistant agent, ColorGrid is unsolved by IPPO. To support benchmarking future MARL algorithms, we release our environment code, model checkpoints, and trajectory visualizations at https://github.com/andreyrisukhin/ColorGrid.
<div id='section'>Paperid: <span id='pid'>47, <a href='https://arxiv.org/pdf/2501.02977.pdf' target='_blank'>https://arxiv.org/pdf/2501.02977.pdf</a></span>   <span><a href='https://github.com/ai4co/camp' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chuanbo Hua, Federico Berto, Jiwoo Son, Seunghyun Kang, Changhyun Kwon, Jinkyoo Park
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.02977">CAMP: Collaborative Attention Model with Profiles for Vehicle Routing Problems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The profiled vehicle routing problem (PVRP) is a generalization of the heterogeneous capacitated vehicle routing problem (HCVRP) in which the objective is to optimize the routes of vehicles to serve client demands subject to different vehicle profiles, with each having a preference or constraint on a per-client basis. While existing learning methods have shown promise for solving the HCVRP in real-time, no learning method exists to solve the more practical and challenging PVRP. In this paper, we propose a Collaborative Attention Model with Profiles (CAMP), a novel approach that learns efficient solvers for PVRP using multi-agent reinforcement learning. CAMP employs a specialized attention-based encoder architecture to embed profiled client embeddings in parallel for each vehicle profile. We design a communication layer between agents for collaborative decision-making across profiled embeddings at each decoding step and a batched pointer mechanism to attend to the profiled embeddings to evaluate the likelihood of the next actions. We evaluate CAMP on two variants of PVRPs: PVRP with preferences, which explicitly influence the reward function, and PVRP with zone constraints with different numbers of agents and clients, demonstrating that our learned solvers achieve competitive results compared to both classical state-of-the-art neural multi-agent models in terms of solution quality and computational efficiency. We make our code openly available at https://github.com/ai4co/camp.
<div id='section'>Paperid: <span id='pid'>48, <a href='https://arxiv.org/pdf/2412.17707.pdf' target='_blank'>https://arxiv.org/pdf/2412.17707.pdf</a></span>   <span><a href='https://github.com/devindeng94/smac-hard' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yue Deng, Yan Yu, Weiyu Ma, Zirui Wang, Wenhui Zhu, Jian Zhao, Yin Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.17707">SMAC-Hard: Enabling Mixed Opponent Strategy Script and Self-play on SMAC</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The availability of challenging simulation environments is pivotal for advancing the field of Multi-Agent Reinforcement Learning (MARL). In cooperative MARL settings, the StarCraft Multi-Agent Challenge (SMAC) has gained prominence as a benchmark for algorithms following centralized training with decentralized execution paradigm. However, with continual advancements in SMAC, many algorithms now exhibit near-optimal performance, complicating the evaluation of their true effectiveness. To alleviate this problem, in this work, we highlight a critical issue: the default opponent policy in these environments lacks sufficient diversity, leading MARL algorithms to overfit and exploit unintended vulnerabilities rather than learning robust strategies. To overcome these limitations, we propose SMAC-HARD, a novel benchmark designed to enhance training robustness and evaluation comprehensiveness. SMAC-HARD supports customizable opponent strategies, randomization of adversarial policies, and interfaces for MARL self-play, enabling agents to generalize to varying opponent behaviors and improve model stability. Furthermore, we introduce a black-box testing framework wherein agents are trained without exposure to the edited opponent scripts but are tested against these scripts to evaluate the policy coverage and adaptability of MARL algorithms. We conduct extensive evaluations of widely used and state-of-the-art algorithms on SMAC-HARD, revealing the substantial challenges posed by edited and mixed strategy opponents. Additionally, the black-box strategy tests illustrate the difficulty of transferring learned policies to unseen adversaries. We envision SMAC-HARD as a critical step toward benchmarking the next generation of MARL algorithms, fostering progress in self-play methods for multi-agent systems. Our code is available at https://github.com/devindeng94/smac-hard.
<div id='section'>Paperid: <span id='pid'>49, <a href='https://arxiv.org/pdf/2412.04233.pdf' target='_blank'>https://arxiv.org/pdf/2412.04233.pdf</a></span>   <span><a href='https://github.com/KaleabTessera/HyperMARL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kale-ab Abebe Tessera, Arrasy Rahman, Amos Storkey, Stefano V. Albrecht
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.04233">HyperMARL: Adaptive Hypernetworks for Multi-Agent RL</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Adaptability to specialised or homogeneous behaviours is critical in cooperative multi-agent reinforcement learning (MARL). Parameter sharing (PS) techniques, common for efficient adaptation, often limit behavioural diversity due to cross-agent gradient interference, which we show can be exacerbated by the coupling of observations and agent IDs. Current remedies typically add complexity through altered objectives, manual preset diversity levels, or sequential updates. We ask: can shared policies adapt without these complexities? We propose HyperMARL, a PS approach using hypernetworks for dynamic agent-specific parameters, without altering the RL objective or requiring preset diversity levels. HyperMARL's explicit decoupling of observation- and agent-conditioned gradients empirically reduces policy gradient variance, facilitates shared-policy adaptation (including specialisation), and helps mitigate cross-agent interference. Across diverse MARL benchmarks (up to 20 agents), requiring homogeneous, heterogeneous, or mixed behaviours, HyperMARL achieves competitive performance against key baselines -- fully shared, non-parameter sharing, and three diversity-promoting methods -- while preserving behavioural diversity comparable to non-parameter sharing. These findings establish HyperMARL as a versatile approach for adaptive MARL. The code is publicly available at https://github.com/KaleabTessera/HyperMARL.
<div id='section'>Paperid: <span id='pid'>50, <a href='https://arxiv.org/pdf/2411.04672.pdf' target='_blank'>https://arxiv.org/pdf/2411.04672.pdf</a></span>   <span><a href='https://github.com/qiongwu86/Semantic-Aware-Resource-Management-for-C-V2X-Platooning-via-Multi-Agent-Reinforcement-Learning' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenjun Zhang, Qiong Wu, Pingyi Fan, Kezhi Wang, Nan Cheng, Wen Chen, Khaled B. Letaief
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.04672">Semantic-Aware Resource Management for C-V2X Platooning via Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Semantic communication transmits the extracted features of information rather than raw data, significantly reducing redundancy, which is crucial for addressing spectrum and energy challenges in 6G networks. In this paper, we introduce semantic communication into a cellular vehicle-to-everything (C-V2X)- based autonomous vehicle platoon system for the first time, aiming to achieve efficient management of communication resources in a dynamic environment. Firstly, we construct a mathematical model for semantic communication in platoon systems, in which the DeepSC model and MU-DeepSC model are used to semantically encode and decode unimodal and multi-modal data, respectively. Then, we propose the quality of experience (QoE) metric based on semantic similarity and semantic rate. Meanwhile, we consider the success rate of semantic information transmission (SRS) metric to ensure the fairness of channel resource allocation. Next, the optimization problem is posed with the aim of maximizing the QoE in vehicle-to-vehicle (V2V) links while improving SRS. To solve this mixed integer nonlinear programming problem (MINLP) and adapt to time-varying channel conditions, the paper proposes a distributed semantic-aware multi-modal resource allocation (SAMRA) algorithm based on multi-agent reinforcement learning (MARL), referred to as SAMRAMARL. The algorithm can dynamically allocate channels and power and determine semantic symbol length based on the contextual importance of the transmitted information, ensuring efficient resource utilization. Finally, extensive simulations have demonstrated that SAMRAMARL outperforms existing methods, achieving significant gains in QoE, SRS, and communication delay in C-V2X platooning scenarios.
<div id='section'>Paperid: <span id='pid'>51, <a href='https://arxiv.org/pdf/2410.19372.pdf' target='_blank'>https://arxiv.org/pdf/2410.19372.pdf</a></span>   <span><a href='https://github.com/giangbang/Strong-Pareto-MARL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Bang Giang Le, Viet Cuong Ta
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.19372">Toward Finding Strong Pareto Optimal Policies in Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this work, we study the problem of finding Pareto optimal policies in multi-agent reinforcement learning problems with cooperative reward structures. We show that any algorithm where each agent only optimizes their reward is subject to suboptimal convergence. Therefore, to achieve Pareto optimality, agents have to act altruistically by considering the rewards of others. This observation bridges the multi-objective optimization framework and multi-agent reinforcement learning together. We first propose a framework for applying the Multiple Gradient Descent algorithm (MGDA) for learning in multi-agent settings. We further show that standard MGDA is subjected to weak Pareto convergence, a problem that is often overlooked in other learning settings but is prevalent in multi-agent reinforcement learning. To mitigate this issue, we propose MGDA++, an improvement of the existing algorithm to handle the weakly optimal convergence of MGDA properly. Theoretically, we prove that MGDA++ converges to strong Pareto optimal solutions in convex, smooth bi-objective problems. We further demonstrate the superiority of our MGDA++ in cooperative settings in the Gridworld benchmark. The results highlight that our proposed method can converge efficiently and outperform the other methods in terms of the optimality of the convergent policies. The source code is available at \url{https://github.com/giangbang/Strong-Pareto-MARL}.
<div id='section'>Paperid: <span id='pid'>52, <a href='https://arxiv.org/pdf/2410.08540.pdf' target='_blank'>https://arxiv.org/pdf/2410.08540.pdf</a></span>   <span><a href='https://github.com/LXXXXR/Kaleidoscope' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinran Li, Ling Pan, Jun Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.08540">Kaleidoscope: Learnable Masks for Heterogeneous Multi-agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In multi-agent reinforcement learning (MARL), parameter sharing is commonly employed to enhance sample efficiency. However, the popular approach of full parameter sharing often leads to homogeneous policies among agents, potentially limiting the performance benefits that could be derived from policy diversity. To address this critical limitation, we introduce \emph{Kaleidoscope}, a novel adaptive partial parameter sharing scheme that fosters policy heterogeneity while still maintaining high sample efficiency. Specifically, Kaleidoscope maintains one set of common parameters alongside multiple sets of distinct, learnable masks for different agents, dictating the sharing of parameters. It promotes diversity among policy networks by encouraging discrepancy among these masks, without sacrificing the efficiencies of parameter sharing. This design allows Kaleidoscope to dynamically balance high sample efficiency with a broad policy representational capacity, effectively bridging the gap between full parameter sharing and non-parameter sharing across various environments. We further extend Kaleidoscope to critic ensembles in the context of actor-critic algorithms, which could help improve value estimations.Our empirical evaluations across extensive environments, including multi-agent particle environment, multi-agent MuJoCo and StarCraft multi-agent challenge v2, demonstrate the superior performance of Kaleidoscope compared with existing parameter sharing approaches, showcasing its potential for performance enhancement in MARL. The code is publicly available at \url{https://github.com/LXXXXR/Kaleidoscope}.
<div id='section'>Paperid: <span id='pid'>53, <a href='https://arxiv.org/pdf/2410.08345.pdf' target='_blank'>https://arxiv.org/pdf/2410.08345.pdf</a></span>   <span><a href='https://github.com/hegasz/large-legislative-models' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Henry Gasztowtt, Benjamin Smith, Vincent Zhu, Qinxun Bai, Edwin Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.08345">Large Legislative Models: Towards Efficient AI Policymaking in Economic Simulations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The improvement of economic policymaking presents an opportunity for broad societal benefit, a notion that has inspired research towards AI-driven policymaking tools. AI policymaking holds the potential to surpass human performance through the ability to process data quickly at scale. However, existing RL-based methods exhibit sample inefficiency, and are further limited by an inability to flexibly incorporate nuanced information into their decision-making processes. Thus, we propose a novel method in which we instead utilize pre-trained Large Language Models (LLMs), as sample-efficient policymakers in socially complex multi-agent reinforcement learning (MARL) scenarios. We demonstrate significant efficiency gains, outperforming existing methods across three environments. Our code is available at https://github.com/hegasz/large-legislative-models.
<div id='section'>Paperid: <span id='pid'>54, <a href='https://arxiv.org/pdf/2410.06372.pdf' target='_blank'>https://arxiv.org/pdf/2410.06372.pdf</a></span>   <span><a href='https://github.com/mylad13/CATMiP' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Milad Farjadnasab, Shahin Sirouspour
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.06372">Cooperative and Asynchronous Transformer-based Mission Planning for Heterogeneous Teams of Mobile Robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cooperative mission planning for heterogeneous teams of mobile robots presents a unique set of challenges, particularly when operating under communication constraints and limited computational resources. To address these challenges, we propose the Cooperative and Asynchronous Transformer-based Mission Planning (CATMiP) framework, which leverages multi-agent reinforcement learning (MARL) to coordinate distributed decision making among agents with diverse sensing, motion, and actuation capabilities, operating under sporadic ad hoc communication. A Class-based Macro-Action Decentralized Partially Observable Markov Decision Process (CMacDec-POMDP) is also formulated to effectively model asynchronous decision-making for heterogeneous teams of agents. The framework utilizes an asynchronous centralized training and distributed execution scheme, enabled by the proposed Asynchronous Multi-Agent Transformer (AMAT) architecture. This design allows a single trained model to generalize to larger environments and accommodate varying team sizes and compositions. We evaluate CATMiP in a 2D grid-world simulation environment and compare its performance against planning-based exploration methods. Results demonstrate CATMiP's superior efficiency, scalability, and robustness to communication dropouts and input noise, highlighting its potential for real-world heterogeneous mobile robot systems. The code is available at https://github.com/mylad13/CATMiP
<div id='section'>Paperid: <span id='pid'>55, <a href='https://arxiv.org/pdf/2409.11741.pdf' target='_blank'>https://arxiv.org/pdf/2409.11741.pdf</a></span>   <span><a href='https://github.com/huawen-hu/HARP' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Huawen Hu, Enze Shi, Chenxi Yue, Shuocun Yang, Zihao Wu, Yiwei Li, Tianyang Zhong, Tuo Zhang, Tianming Liu, Shu Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.11741">HARP: Human-Assisted Regrouping with Permutation Invariant Critic for Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human-in-the-loop reinforcement learning integrates human expertise to accelerate agent learning and provide critical guidance and feedback in complex fields. However, many existing approaches focus on single-agent tasks and require continuous human involvement during the training process, significantly increasing the human workload and limiting scalability. In this paper, we propose HARP (Human-Assisted Regrouping with Permutation Invariant Critic), a multi-agent reinforcement learning framework designed for group-oriented tasks. HARP integrates automatic agent regrouping with strategic human assistance during deployment, enabling and allowing non-experts to offer effective guidance with minimal intervention. During training, agents dynamically adjust their groupings to optimize collaborative task completion. When deployed, they actively seek human assistance and utilize the Permutation Invariant Group Critic to evaluate and refine human-proposed groupings, allowing non-expert users to contribute valuable suggestions. In multiple collaboration scenarios, our approach is able to leverage limited guidance from non-experts and enhance performance. The project can be found at https://github.com/huawen-hu/HARP.
<div id='section'>Paperid: <span id='pid'>56, <a href='https://arxiv.org/pdf/2409.00985.pdf' target='_blank'>https://arxiv.org/pdf/2409.00985.pdf</a></span>   <span><a href='https://github.com/yuqian2003/Co_Learning' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiapeng Yu, Yuqian Wu, Yajing Zhan, Wenhao Guo, Zhou Xu, Raymond Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.00985">Co-Learning: Code Learning for Multi-Agent Reinforcement Collaborative Framework with Conversational Natural Language Interfaces</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Online question-and-answer (Q\&A) systems based on the Large Language Model (LLM) have progressively diverged from recreational to professional use. This paper proposed a Multi-Agent framework with environmentally reinforcement learning (E-RL) for code correction called Code Learning (Co-Learning) community, assisting beginners to correct code errors independently. It evaluates the performance of multiple LLMs from an original dataset with 702 error codes, uses it as a reward or punishment criterion for E-RL; Analyzes input error codes by the current agent; selects the appropriate LLM-based agent to achieve optimal error correction accuracy and reduce correction time. Experiment results showed that 3\% improvement in Precision score and 15\% improvement in time cost as compared with no E-RL method respectively. Our source code is available at: https://github.com/yuqian2003/Co_Learning
<div id='section'>Paperid: <span id='pid'>57, <a href='https://arxiv.org/pdf/2408.06656.pdf' target='_blank'>https://arxiv.org/pdf/2408.06656.pdf</a></span>   <span><a href='https://github.com/CCCC1dhcgd/A-MAPPO-PIS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yicheng Guo, Jiaqi Liu, Rongjie Yu, Peng Hang, Jian Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.06656">MAPPO-PIS: A Multi-Agent Proximal Policy Optimization Method with Prior Intent Sharing for CAVs' Cooperative Decision-Making</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vehicle-to-Vehicle (V2V) technologies have great potential for enhancing traffic flow efficiency and safety. However, cooperative decision-making in multi-agent systems, particularly in complex human-machine mixed merging areas, remains challenging for connected and autonomous vehicles (CAVs). Intent sharing, a key aspect of human coordination, may offer an effective solution to these decision-making problems, but its application in CAVs is under-explored. This paper presents an intent-sharing-based cooperative method, the Multi-Agent Proximal Policy Optimization with Prior Intent Sharing (MAPPO-PIS), which models the CAV cooperative decision-making problem as a Multi-Agent Reinforcement Learning (MARL) problem. It involves training and updating the agents' policies through the integration of two key modules: the Intention Generator Module (IGM) and the Safety Enhanced Module (SEM). The IGM is specifically crafted to generate and disseminate CAVs' intended trajectories spanning multiple future time-steps. On the other hand, the SEM serves a crucial role in assessing the safety of the decisions made and rectifying them if necessary. Merging area with human-machine mixed traffic flow is selected to validate our method. Results show that MAPPO-PIS significantly improves decision-making performance in multi-agent systems, surpassing state-of-the-art baselines in safety, efficiency, and overall traffic system performance. The code and video demo can be found at: \url{https://github.com/CCCC1dhcgd/A-MAPPO-PIS}.
<div id='section'>Paperid: <span id='pid'>58, <a href='https://arxiv.org/pdf/2407.11310.pdf' target='_blank'>https://arxiv.org/pdf/2407.11310.pdf</a></span>   <span><a href='https://github.com/qiongwu86/Digital-Twin-Vehicular-Edge-Computing-Network_Task-Offloading-and-Resource-Allocation' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yu Xie, Qiong Wu, Pingyi Fan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.11310">Digital Twin Vehicular Edge Computing Network: Task Offloading and Resource Allocation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the increasing demand for multiple applications on internet of vehicles. It requires vehicles to carry out multiple computing tasks in real time. However, due to the insufficient computing capability of vehicles themselves, offloading tasks to vehicular edge computing (VEC) servers and allocating computing resources to tasks becomes a challenge. In this paper, a multi task digital twin (DT) VEC network is established. By using DT to develop offloading strategies and resource allocation strategies for multiple tasks of each vehicle in a single slot, an optimization problem is constructed. To solve it, we propose a multi-agent reinforcement learning method on the task offloading and resource allocation. Numerous experiments demonstrate that our method is effective compared to other benchmark algorithms.
<div id='section'>Paperid: <span id='pid'>59, <a href='https://arxiv.org/pdf/2407.05800.pdf' target='_blank'>https://arxiv.org/pdf/2407.05800.pdf</a></span>   <span><a href='https://github.com/Pranabiitp/FedMRL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Pranab Sahoo, Ashutosh Tripathi, Sriparna Saha, Samrat Mondal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.05800">FedMRL: Data Heterogeneity Aware Federated Multi-agent Deep Reinforcement Learning for Medical Imaging</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite recent advancements in federated learning (FL) for medical image diagnosis, addressing data heterogeneity among clients remains a significant challenge for practical implementation. A primary hurdle in FL arises from the non-IID nature of data samples across clients, which typically results in a decline in the performance of the aggregated global model. In this study, we introduce FedMRL, a novel federated multi-agent deep reinforcement learning framework designed to address data heterogeneity. FedMRL incorporates a novel loss function to facilitate fairness among clients, preventing bias in the final global model. Additionally, it employs a multi-agent reinforcement learning (MARL) approach to calculate the proximal term $(Î¼)$ for the personalized local objective function, ensuring convergence to the global optimum. Furthermore, FedMRL integrates an adaptive weight adjustment method using a Self-organizing map (SOM) on the server side to counteract distribution shifts among clients' local data distributions. We assess our approach using two publicly available real-world medical datasets, and the results demonstrate that FedMRL significantly outperforms state-of-the-art techniques, showing its efficacy in addressing data heterogeneity in federated learning. The code can be found here~{\url{https://github.com/Pranabiitp/FedMRL}}.
<div id='section'>Paperid: <span id='pid'>60, <a href='https://arxiv.org/pdf/2407.02342.pdf' target='_blank'>https://arxiv.org/pdf/2407.02342.pdf</a></span>   <span><a href='https://github.com/qiongwu86/Optimizing-AoI-in-VEC-with-Federated-Graph-Neural-Network-Multi-Agent-Reinforcement-Learning' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenhua Wang, Qiong Wu, Pingyi Fan, Nan Cheng, Wen Chen, Jiangzhou Wang, Khaled B. Letaief
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.02342">Optimizing Age of Information in Vehicular Edge Computing with Federated Graph Neural Network Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the rapid development of intelligent vehicles and Intelligent Transport Systems (ITS), the sensors such as cameras and LiDAR installed on intelligent vehicles provides higher capacity of executing computation-intensive and delay-sensitive tasks, thereby raising deployment costs. To address this issue, Vehicular Edge Computing (VEC) has been proposed to process data through Road Side Units (RSUs) to support real-time applications. This paper focuses on the Age of Information (AoI) as a key metric for data freshness and explores task offloading issues for vehicles under RSU communication resource constraints. We adopt a Multi-agent Deep Reinforcement Learning (MADRL) approach, allowing vehicles to autonomously make optimal data offloading decisions. However, MADRL poses risks of vehicle information leakage during communication learning and centralized training. To mitigate this, we employ a Federated Learning (FL) framework that shares model parameters instead of raw data to protect the privacy of vehicle users. Building on this, we propose an innovative distributed federated learning framework combining Graph Neural Networks (GNN), named Federated Graph Neural Network Multi-Agent Reinforcement Learning (FGNN-MADRL), to optimize AoI across the system. For the first time, road scenarios are constructed as graph data structures, and a GNN-based federated learning framework is proposed, effectively combining distributed and centralized federated aggregation. Furthermore, we propose a new MADRL algorithm that simplifies decision making and enhances offloading efficiency, further reducing the decision complexity. Simulation results demonstrate the superiority of our proposed approach to other methods through simulations.
<div id='section'>Paperid: <span id='pid'>61, <a href='https://arxiv.org/pdf/2406.17818.pdf' target='_blank'>https://arxiv.org/pdf/2406.17818.pdf</a></span>   <span><a href='https://github.com/Canyizl/TPA-for-AVC' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Feiyang Xu, Shunyu Liu, Yunpeng Qing, Yihe Zhou, Yuwen Wang, Mingli Song
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.17818">Temporal Prototype-Aware Learning for Active Voltage Control on Power Distribution Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Active Voltage Control (AVC) on the Power Distribution Networks (PDNs) aims to stabilize the voltage levels to ensure efficient and reliable operation of power systems. With the increasing integration of distributed energy resources, recent efforts have explored employing multi-agent reinforcement learning (MARL) techniques to realize effective AVC. Existing methods mainly focus on the acquisition of short-term AVC strategies, i.e., only learning AVC within the short-term training trajectories of a singular diurnal cycle. However, due to the dynamic nature of load demands and renewable energy, the operation states of real-world PDNs may exhibit significant distribution shifts across varying timescales (e.g., daily and seasonal changes). This can render those short-term strategies suboptimal or even obsolete when performing continuous AVC over extended periods. In this paper, we propose a novel temporal prototype-aware learning method, abbreviated as TPA, to learn time-adaptive AVC under short-term training trajectories. At the heart of TPA are two complementary components, namely multi-scale dynamic encoder and temporal prototype-aware policy, that can be readily incorporated into various MARL methods. The former component integrates a stacked transformer network to learn underlying temporal dependencies at different timescales of the PDNs, while the latter implements a learnable prototype matching mechanism to construct a dedicated AVC policy that can dynamically adapt to the evolving operation states. Experimental results on the AVC benchmark with different PDN sizes demonstrate that the proposed TPA surpasses the state-of-the-art counterparts not only in terms of control performance but also by offering model transferability. Our code is available at https://github.com/Canyizl/TPA-for-AVC.
<div id='section'>Paperid: <span id='pid'>62, <a href='https://arxiv.org/pdf/2406.11318.pdf' target='_blank'>https://arxiv.org/pdf/2406.11318.pdf</a></span>   <span><a href='https://github.com/qiongwu86/RIS-VEC-MARL.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kangwei Qi, Qiong Wu, Pingyi Fan, Nan Cheng, Qiang Fan, Jiangzhou Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.11318">Reconfigurable Intelligent Surface Assisted VEC Based on Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vehicular edge computing (VEC) is an emerging technology that enables vehicles to perform high-intensity tasks by executing tasks locally or offloading them to nearby edge devices. However, obstacles such as buildings may degrade the communications and incur communication interruptions, and thus the vehicle may not meet the requirement for task offloading. Reconfigurable intelligent surfaces (RIS) is introduced to support vehicle communication and provide an alternative communication path. The system performance can be improved by flexibly adjusting the phase-shift of the RIS. For RIS-assisted VEC system where tasks arrive randomly, we design a control scheme that considers offloading power, local power allocation and phase-shift optimization. To solve this non-convex problem, we propose a new deep reinforcement learning (DRL) framework that employs modified multi-agent deep deterministic policy gradient (MADDPG) approach to optimize the power allocation for vehicle users (VUs) and block coordinate descent (BCD) algorithm to optimize the phase-shift of the RIS. Simulation results show that our proposed scheme outperforms the centralized deep deterministic policy gradient (DDPG) scheme and random scheme.
<div id='section'>Paperid: <span id='pid'>63, <a href='https://arxiv.org/pdf/2406.07875.pdf' target='_blank'>https://arxiv.org/pdf/2406.07875.pdf</a></span>   <span><a href='https://github.com/xwanghan/Carbon-Simulator' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Han Wang, Wenhao Li, Hongyuan Zha, Baoxiang Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.07875">Carbon Market Simulation with Adaptive Mechanism Design</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A carbon market is a market-based tool that incentivizes economic agents to align individual profits with the global utility, i.e., reducing carbon emissions to tackle climate change. Cap and trade stands as a critical principle based on allocating and trading carbon allowances (carbon emission credit), enabling economic agents to follow planned emissions and penalizing excess emissions. A central authority is responsible for introducing and allocating those allowances in cap and trade. However, the complexity of carbon market dynamics makes accurate simulation intractable, which in turn hinders the design of effective allocation strategies. To address this, we propose an adaptive mechanism design framework, simulating the market using hierarchical, model-free multi-agent reinforcement learning (MARL). Government agents allocate carbon credits, while enterprises engage in economic activities and carbon trading. This framework illustrates agents' behavior comprehensively. Numerical results show MARL enables government agents to balance productivity, equality, and carbon emissions. Our project is available at https://github.com/xwanghan/Carbon-Simulator.
<div id='section'>Paperid: <span id='pid'>64, <a href='https://arxiv.org/pdf/2406.03978.pdf' target='_blank'>https://arxiv.org/pdf/2406.03978.pdf</a></span>   <span><a href='https://github.com/tencent-ailab/mini-hok' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Lin Liu, Jian Zhao, Cheng Hu, Zhengtao Cao, Youpeng Zhao, Zhenbin Ye, Meng Meng, Wenjun Wang, Zhaofeng He, Houqiang Li, Xia Lin, Lanxiao Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.03978">Mini Honor of Kings: A Lightweight Environment for Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Games are widely used as research environments for multi-agent reinforcement learning (MARL), but they pose three significant challenges: limited customization, high computational demands, and oversimplification. To address these issues, we introduce the first publicly available map editor for the popular mobile game Honor of Kings and design a lightweight environment, Mini Honor of Kings (Mini HoK), for researchers to conduct experiments. Mini HoK is highly efficient, allowing experiments to be run on personal PCs or laptops while still presenting sufficient challenges for existing MARL algorithms. We have tested our environment on common MARL algorithms and demonstrated that these algorithms have yet to find optimal solutions within this environment. This facilitates the dissemination and advancement of MARL methods within the research community. Additionally, we hope that more researchers will leverage the Honor of Kings map editor to develop innovative and scientifically valuable new maps. Our code and user manual are available at: https://github.com/tencent-ailab/mini-hok.
<div id='section'>Paperid: <span id='pid'>65, <a href='https://arxiv.org/pdf/2405.18733.pdf' target='_blank'>https://arxiv.org/pdf/2405.18733.pdf</a></span>   <span><a href='https://github.com/noahadhikari/pettingzoo-chinese-checkers' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Noah Adhikari, Allen Gu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.18733">Efficient Learning in Chinese Checkers: Comparing Parameter Sharing in Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We show that multi-agent reinforcement learning (MARL) with full parameter sharing outperforms independent and partially shared architectures in the competitive perfect-information homogenous game of Chinese Checkers. To run our experiments, we develop a new MARL environment: variable-size, six-player Chinese Checkers. This custom environment was developed in PettingZoo and supports all traditional rules of the game including chaining jumps. This is, to the best of our knowledge, the first implementation of Chinese Checkers that remains faithful to the true game.
  Chinese Checkers is difficult to learn due to its large branching factor and potentially infinite horizons. We borrow the concept of branching actions (submoves) from complex action spaces in other RL domains, where a submove may not end a player's turn immediately. This drastically reduces the dimensionality of the action space. Our observation space is inspired by AlphaGo with many binary game boards stacked in a 3D array to encode information.
  The PettingZoo environment, training and evaluation logic, and analysis scripts can be found on \href{https://github.com/noahadhikari/pettingzoo-chinese-checkers}{Github}.
<div id='section'>Paperid: <span id='pid'>66, <a href='https://arxiv.org/pdf/2405.18209.pdf' target='_blank'>https://arxiv.org/pdf/2405.18209.pdf</a></span>   <span><a href='https://github.com/SafeRL-Lab/Safe-MARL-in-Autonomous-Driving.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhi Zheng, Shangding Gu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.18209">Safe Multi-Agent Reinforcement Learning with Bilevel Optimization in Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Ensuring safety in MARL, particularly when deploying it in real-world applications such as autonomous driving, emerges as a critical challenge. To address this challenge, traditional safe MARL methods extend MARL approaches to incorporate safety considerations, aiming to minimize safety risk values. However, these safe MARL algorithms often fail to model other agents and lack convergence guarantees, particularly in dynamically complex environments. In this study, we propose a safe MARL method grounded in a Stackelberg model with bi-level optimization, for which convergence analysis is provided. Derived from our theoretical analysis, we develop two practical algorithms, namely Constrained Stackelberg Q-learning (CSQ) and Constrained Stackelberg Multi-Agent Deep Deterministic Policy Gradient (CS-MADDPG), designed to facilitate MARL decision-making in autonomous driving applications. To evaluate the effectiveness of our algorithms, we developed a safe MARL autonomous driving benchmark and conducted experiments on challenging autonomous driving scenarios, such as merges, roundabouts, intersections, and racetracks. The experimental results indicate that our algorithms, CSQ and CS-MADDPG, outperform several strong MARL baselines, such as Bi-AC, MACPO, and MAPPO-L, regarding reward and safety performance. The demos and source code are available at {https://github.com/SafeRL-Lab/Safe-MARL-in-Autonomous-Driving.git}.
<div id='section'>Paperid: <span id='pid'>67, <a href='https://arxiv.org/pdf/2405.18110.pdf' target='_blank'>https://arxiv.org/pdf/2405.18110.pdf</a></span>   <span><a href='https://github.com/LXXXXR/ICES' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinran Li, Zifan Liu, Shibo Chen, Jun Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.18110">Individual Contributions as Intrinsic Exploration Scaffolds for Multi-agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In multi-agent reinforcement learning (MARL), effective exploration is critical, especially in sparse reward environments. Although introducing global intrinsic rewards can foster exploration in such settings, it often complicates credit assignment among agents. To address this difficulty, we propose Individual Contributions as intrinsic Exploration Scaffolds (ICES), a novel approach to motivate exploration by assessing each agent's contribution from a global view. In particular, ICES constructs exploration scaffolds with Bayesian surprise, leveraging global transition information during centralized training. These scaffolds, used only in training, help to guide individual agents towards actions that significantly impact the global latent state transitions. Additionally, ICES separates exploration policies from exploitation policies, enabling the former to utilize privileged global information during training. Extensive experiments on cooperative benchmark tasks with sparse rewards, including Google Research Football (GRF) and StarCraft Multi-agent Challenge (SMAC), demonstrate that ICES exhibits superior exploration capabilities compared with baselines. The code is publicly available at https://github.com/LXXXXR/ICES.
<div id='section'>Paperid: <span id='pid'>68, <a href='https://arxiv.org/pdf/2405.16854.pdf' target='_blank'>https://arxiv.org/pdf/2405.16854.pdf</a></span>   <span><a href='https://github.com/LiuZhihao2022/eSpark.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhihao Liu, Xianliang Yang, Zichuan Liu, Yifan Xia, Wei Jiang, Yuanyu Zhang, Lijuan Li, Guoliang Fan, Lei Song, Bian Jiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.16854">Knowing What Not to Do: Leverage Language Model Insights for Action Space Pruning in Multi-agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent reinforcement learning (MARL) is employed to develop autonomous agents that can learn to adopt cooperative or competitive strategies within complex environments. However, the linear increase in the number of agents leads to a combinatorial explosion of the action space, which may result in algorithmic instability, difficulty in convergence, or entrapment in local optima. While researchers have designed a variety of effective algorithms to compress the action space, these methods also introduce new challenges, such as the need for manually designed prior knowledge or reliance on the structure of the problem, which diminishes the applicability of these techniques. In this paper, we introduce Evolutionary action SPAce Reduction with Knowledge (eSpark), an exploration function generation framework driven by large language models (LLMs) to boost exploration and prune unnecessary actions in MARL. Using just a basic prompt that outlines the overall task and setting, eSpark is capable of generating exploration functions in a zero-shot manner, identifying and pruning redundant or irrelevant state-action pairs, and then achieving autonomous improvement from policy feedback. In reinforcement learning tasks involving inventory management and traffic light control encompassing a total of 15 scenarios, eSpark consistently outperforms the combined MARL algorithm in all scenarios, achieving an average performance gain of 34.4% and 9.9% in the two types of tasks respectively. Additionally, eSpark has proven to be capable of managing situations with a large number of agents, securing a 29.7% improvement in scalability challenges that featured over 500 agents. The code can be found in https://github.com/LiuZhihao2022/eSpark.git.
<div id='section'>Paperid: <span id='pid'>69, <a href='https://arxiv.org/pdf/2405.16386.pdf' target='_blank'>https://arxiv.org/pdf/2405.16386.pdf</a></span>   <span><a href='https://github.com/LucasCJYSDL/VOMASD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiayu Chen, Tian Lan, Vaneet Aggarwal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.16386">Variational Offline Multi-agent Skill Discovery</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Skills are effective temporal abstractions established for sequential decision making, which enable efficient hierarchical learning for long-horizon tasks and facilitate multi-task learning through their transferability. Despite extensive research, research gaps remain in multi-agent scenarios, particularly for automatically extracting subgroup coordination patterns in a multi-agent task. In this case, we propose two novel auto-encoder schemes: VO-MASD-3D and VO-MASD-Hier, to simultaneously capture subgroup- and temporal-level abstractions and form multi-agent skills, which firstly solves the aforementioned challenge. An essential algorithm component of these schemes is a dynamic grouping function that can automatically detect latent subgroups based on agent interactions in a task. Further, our method can be applied to offline multi-task data, and the discovered subgroup skills can be transferred across relevant tasks without retraining. Empirical evaluations on StarCraft tasks indicate that our approach significantly outperforms existing hierarchical multi-agent reinforcement learning (MARL) methods. Moreover, skills discovered using our method can effectively reduce the learning difficulty in MARL scenarios with delayed and sparse reward signals. The codebase is available at https://github.com/LucasCJYSDL/VOMASD.
<div id='section'>Paperid: <span id='pid'>70, <a href='https://arxiv.org/pdf/2405.11778.pdf' target='_blank'>https://arxiv.org/pdf/2405.11778.pdf</a></span>   <span><a href='https://github.com/liuqh16/MAZero' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Qihan Liu, Jianing Ye, Xiaoteng Ma, Jun Yang, Bin Liang, Chongjie Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.11778">Efficient Multi-agent Reinforcement Learning by Planning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent reinforcement learning (MARL) algorithms have accomplished remarkable breakthroughs in solving large-scale decision-making tasks. Nonetheless, most existing MARL algorithms are model-free, limiting sample efficiency and hindering their applicability in more challenging scenarios. In contrast, model-based reinforcement learning (MBRL), particularly algorithms integrating planning, such as MuZero, has demonstrated superhuman performance with limited data in many tasks. Hence, we aim to boost the sample efficiency of MARL by adopting model-based approaches. However, incorporating planning and search methods into multi-agent systems poses significant challenges. The expansive action space of multi-agent systems often necessitates leveraging the nearly-independent property of agents to accelerate learning. To tackle this issue, we propose the MAZero algorithm, which combines a centralized model with Monte Carlo Tree Search (MCTS) for policy search. We design a novel network structure to facilitate distributed execution and parameter sharing. To enhance search efficiency in deterministic environments with sizable action spaces, we introduce two novel techniques: Optimistic Search Lambda (OS($Î»$)) and Advantage-Weighted Policy Optimization (AWPO). Extensive experiments on the SMAC benchmark demonstrate that MAZero outperforms model-free approaches in terms of sample efficiency and provides comparable or better performance than existing model-based methods in terms of both sample and computational efficiency. Our code is available at https://github.com/liuqh16/MAZero.
<div id='section'>Paperid: <span id='pid'>71, <a href='https://arxiv.org/pdf/2405.05542.pdf' target='_blank'>https://arxiv.org/pdf/2405.05542.pdf</a></span>   <span><a href='https://github.com/SICC-Group/DDFG' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuchen Shi, Shihong Duan, Cheng Xu, Ran Wang, Fangwen Ye, Chau Yuen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.05542">Dynamic Deep Factor Graph for Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work introduces a novel value decomposition algorithm, termed \textit{Dynamic Deep Factor Graphs} (DDFG). Unlike traditional coordination graphs, DDFG leverages factor graphs to articulate the decomposition of value functions, offering enhanced flexibility and adaptability to complex value function structures. Central to DDFG is a graph structure generation policy that innovatively generates factor graph structures on-the-fly, effectively addressing the dynamic collaboration requirements among agents. DDFG strikes an optimal balance between the computational overhead associated with aggregating value functions and the performance degradation inherent in their complete decomposition. Through the application of the max-sum algorithm, DDFG efficiently identifies optimal policies. We empirically validate DDFG's efficacy in complex scenarios, including higher-order predator-prey tasks and the StarCraft II Multi-agent Challenge (SMAC), thus underscoring its capability to surmount the limitations faced by existing value decomposition algorithms. DDFG emerges as a robust solution for MARL challenges that demand nuanced understanding and facilitation of dynamic agent collaboration. The implementation of DDFG is made publicly accessible, with the source code available at \url{https://github.com/SICC-Group/DDFG}.
<div id='section'>Paperid: <span id='pid'>72, <a href='https://arxiv.org/pdf/2405.02198.pdf' target='_blank'>https://arxiv.org/pdf/2405.02198.pdf</a></span>   <span><a href='https://proroklab.github.io/cambridge-robomaster' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jan Blumenkamp, Ajay Shankar, Matteo Bettini, Joshua Bird, Amanda Prorok
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.02198">The Cambridge RoboMaster: An Agile Multi-Robot Research Platform</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Compact robotic platforms with powerful compute and actuation capabilities are key enablers for practical, real-world deployments of multi-agent research. This article introduces a tightly integrated hardware, control, and simulation software stack on a fleet of holonomic ground robot platforms designed with this motivation. Our robots, a fleet of customised DJI Robomaster S1 vehicles, offer a balance between small robots that do not possess sufficient compute or actuation capabilities and larger robots that are unsuitable for indoor multi-robot tests. They run a modular ROS2-based optimal estimation and control stack for full onboard autonomy, contain ad-hoc peer-to-peer communication infrastructure, and can zero-shot run multi-agent reinforcement learning (MARL) policies trained in our vectorized multi-agent simulation framework. We present an in-depth review of other platforms currently available, showcase new experimental validation of our system's capabilities, and introduce case studies that highlight the versatility and reliability of our system as a testbed for a wide range of research demonstrations. Our system as well as supplementary material is available online. https://proroklab.github.io/cambridge-robomaster
<div id='section'>Paperid: <span id='pid'>73, <a href='https://arxiv.org/pdf/2404.11014.pdf' target='_blank'>https://arxiv.org/pdf/2404.11014.pdf</a></span>   <span><a href='https://github.com/Edun-Eyes/TSC' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kang Wang, Zhishu Shen, Zhen Lei, Tiehua Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.11014">Towards Multi-agent Reinforcement Learning based Traffic Signal Control through Spatio-temporal Hypergraphs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Traffic signal control systems (TSCSs) are integral to intelligent traffic management, fostering efficient vehicle flow. Traditional approaches often simplify road networks into standard graphs, which results in a failure to consider the dynamic nature of traffic data at neighboring intersections, thereby neglecting higher-order interconnections necessary for real-time control. To address this, we propose a novel TSCS framework to realize intelligent traffic control. This framework collaborates with multiple neighboring edge computing servers to collect traffic information across the road network. To elevate the efficiency of traffic signal control, we have crafted a multi-agent soft actor-critic (MA-SAC) reinforcement learning algorithm. Within this algorithm, individual agents are deployed at each intersection with a mandate to optimize traffic flow across the road network collectively. Furthermore, we introduce hypergraph learning into the critic network of MA-SAC to enable the spatio-temporal interactions from multiple intersections in the road network. This method fuses hypergraph and spatio-temporal graph structures to encode traffic data and capture the complex spatio-temporal correlations between multiple intersections. Our empirical evaluation, tested on varied datasets, demonstrates the superiority of our framework in minimizing average vehicle travel times and sustaining high-throughput performance. This work facilitates the development of more intelligent urban traffic management solutions. We release the code to support the reproducibility of this work at https://github.com/Edun-Eyes/TSC
<div id='section'>Paperid: <span id='pid'>74, <a href='https://arxiv.org/pdf/2403.17421.pdf' target='_blank'>https://arxiv.org/pdf/2403.17421.pdf</a></span>   <span><a href='https://github.com/chenyiqun/MA4DIV' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiqun Chen, Jiaxin Mao, Yi Zhang, Dehong Ma, Long Xia, Jun Fan, Daiting Shi, Zhicong Cheng, Simiu Gu, Dawei Yin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.17421">MA4DIV: Multi-Agent Reinforcement Learning for Search Result Diversification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Search result diversification (SRD), which aims to ensure that documents in a ranking list cover a broad range of subtopics, is a significant and widely studied problem in Information Retrieval and Web Search. Existing methods primarily utilize a paradigm of "greedy selection", i.e., selecting one document with the highest diversity score at a time or optimize an approximation of the objective function. These approaches tend to be inefficient and are easily trapped in a suboptimal state. To address these challenges, we introduce Multi-Agent reinforcement learning (MARL) for search result DIVersity, which called MA4DIV. In this approach, each document is an agent and the search result diversification is modeled as a cooperative task among multiple agents. By modeling the SRD ranking problem as a cooperative MARL problem, this approach allows for directly optimizing the diversity metrics, such as $Î±$-NDCG, while achieving high training efficiency. We conducted experiments on public TREC datasets and a larger scale dataset in the industrial setting. The experiemnts show that MA4DIV achieves substantial improvements in both effectiveness and efficiency than existing baselines, especially on the industrial dataset. The code of MA4DIV can be seen on https://github.com/chenyiqun/MA4DIV.
<div id='section'>Paperid: <span id='pid'>75, <a href='https://arxiv.org/pdf/2403.16015.pdf' target='_blank'>https://arxiv.org/pdf/2403.16015.pdf</a></span>   <span><a href='https://github.com/ziyanx02/multiagent-quadruped-environment' target='_blank'>  GitHub</a></span> <span><a href='https://ziyanx02.github.io/multiagent-quadruped-environment/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziyan Xiong, Bo Chen, Shiyu Huang, Wei-Wei Tu, Zhaofeng He, Yang Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.16015">MQE: Unleashing the Power of Interaction with Multi-agent Quadruped Environment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The advent of deep reinforcement learning (DRL) has significantly advanced the field of robotics, particularly in the control and coordination of quadruped robots. However, the complexity of real-world tasks often necessitates the deployment of multi-robot systems capable of sophisticated interaction and collaboration. To address this need, we introduce the Multi-agent Quadruped Environment (MQE), a novel platform designed to facilitate the development and evaluation of multi-agent reinforcement learning (MARL) algorithms in realistic and dynamic scenarios. MQE emphasizes complex interactions between robots and objects, hierarchical policy structures, and challenging evaluation scenarios that reflect real-world applications. We present a series of collaborative and competitive tasks within MQE, ranging from simple coordination to complex adversarial interactions, and benchmark state-of-the-art MARL algorithms. Our findings indicate that hierarchical reinforcement learning can simplify task learning, but also highlight the need for advanced algorithms capable of handling the intricate dynamics of multi-agent interactions. MQE serves as a stepping stone towards bridging the gap between simulation and practical deployment, offering a rich environment for future research in multi-agent systems and robot learning. For open-sourced code and more details of MQE, please refer to https://ziyanx02.github.io/multiagent-quadruped-environment/ .
<div id='section'>Paperid: <span id='pid'>76, <a href='https://arxiv.org/pdf/2403.07559.pdf' target='_blank'>https://arxiv.org/pdf/2403.07559.pdf</a></span>   <span><a href='https://github.com/ai4co/eph-mapf' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Huijie Tang, Federico Berto, Jinkyoo Park
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.07559">Ensembling Prioritized Hybrid Policies for Multi-agent Pathfinding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-Agent Reinforcement Learning (MARL) based Multi-Agent Path Finding (MAPF) has recently gained attention due to its efficiency and scalability. Several MARL-MAPF methods choose to use communication to enrich the information one agent can perceive. However, existing works still struggle in structured environments with high obstacle density and a high number of agents. To further improve the performance of the communication-based MARL-MAPF solvers, we propose a new method, Ensembling Prioritized Hybrid Policies (EPH). We first propose a selective communication block to gather richer information for better agent coordination within multi-agent environments and train the model with a Q learning-based algorithm. We further introduce three advanced inference strategies aimed at bolstering performance during the execution phase. First, we hybridize the neural policy with single-agent expert guidance for navigating conflict-free zones. Secondly, we propose Q value-based methods for prioritized resolution of conflicts as well as deadlock situations. Finally, we introduce a robust ensemble method that can efficiently collect the best out of multiple possible solutions. We empirically evaluate EPH in complex multi-agent environments and demonstrate competitive performance against state-of-the-art neural methods for MAPF. We open-source our code at https://github.com/ai4co/eph-mapf.
<div id='section'>Paperid: <span id='pid'>77, <a href='https://arxiv.org/pdf/2403.06750.pdf' target='_blank'>https://arxiv.org/pdf/2403.06750.pdf</a></span>   <span><a href='https://github.com/proroklab/task-agnostic-comms' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Dulhan Jayalath, Steven Morad, Amanda Prorok
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.06750">Generalising Multi-Agent Cooperation through Task-Agnostic Communication</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing communication methods for multi-agent reinforcement learning (MARL) in cooperative multi-robot problems are almost exclusively task-specific, training new communication strategies for each unique task. We address this inefficiency by introducing a communication strategy applicable to any task within a given environment. We pre-train the communication strategy without task-specific reward guidance in a self-supervised manner using a set autoencoder. Our objective is to learn a fixed-size latent Markov state from a variable number of agent observations. Under mild assumptions, we prove that policies using our latent representations are guaranteed to converge, and upper bound the value error introduced by our Markov state approximation. Our method enables seamless adaptation to novel tasks without fine-tuning the communication strategy, gracefully supports scaling to more agents than present during training, and detects out-of-distribution events in an environment. Empirical results on diverse MARL scenarios validate the effectiveness of our approach, surpassing task-specific communication strategies in unseen tasks. Our implementation of this work is available at https://github.com/proroklab/task-agnostic-comms.
<div id='section'>Paperid: <span id='pid'>78, <a href='https://arxiv.org/pdf/2403.02635.pdf' target='_blank'>https://arxiv.org/pdf/2403.02635.pdf</a></span>   <span><a href='https://github.com/ColaZhang22/PPS-QMIX' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ke Zhang, DanDan Zhu, Qiuhan Xu, Hao Zhou, Ce Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.02635">PPS-QMIX: Periodically Parameter Sharing for Accelerating Convergence of Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Training for multi-agent reinforcement learning(MARL) is a time-consuming process caused by distribution shift of each agent. One drawback is that strategy of each agent in MARL is independent but actually in cooperation. Thus, a vertical issue in multi-agent reinforcement learning is how to efficiently accelerate training process. To address this problem, current research has leveraged a centralized function(CF) across multiple agents to learn contribution of the team reward for each agent. However, CF based methods introduce joint error from other agents in estimation of value network. In so doing, inspired by federated learning, we propose three simple novel approaches called Average Periodically Parameter Sharing(A-PPS), Reward-Scalability Periodically Parameter Sharing(RS-PPS) and Partial Personalized Periodically Parameter Sharing(PP-PPS) mechanism to accelerate training of MARL. Agents share Q-value network periodically during the training process. Agents which has same identity adapt collected reward as scalability and update partial neural network during period to share different parameters. We apply our approaches in classical MARL method QMIX and evaluate our approaches on various tasks in StarCraft Multi-Agent Challenge(SMAC) environment. Performance of numerical experiments yield enormous enhancement, with an average improvement of 10\%-30\%, and enable to win tasks that QMIX cannot. Our code can be downloaded from https://github.com/ColaZhang22/PPS-QMIX
<div id='section'>Paperid: <span id='pid'>79, <a href='https://arxiv.org/pdf/2401.00747.pdf' target='_blank'>https://arxiv.org/pdf/2401.00747.pdf</a></span>   <span><a href='https://github.com/shb20tsinghua/PTAS_Game/tree/main' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hongbo Sun, Chongkun Xia, Junbo Tan, Bo Yuan, Xueqian Wang, Bin Liang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.00747">Geometric Structure and Polynomial-time Algorithm of Game Equilibria</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Whether a PTAS (polynomial-time approximation scheme) exists for game equilibria has been an open question, and its absence has indications and consequences in three fields: the practicality of methods in algorithmic game theory, non-stationarity and curse of multiagency in MARL (multi-agent reinforcement learning), and the tractability of PPAD in computational complexity theory. In this paper, we formalize the game equilibrium problem as an optimization problem that splits into two subproblems with respect to policy and value function, which are solved respectively by interior point method and dynamic programming. Combining these two parts, we obtain an FPTAS (fully PTAS) for the weak approximation (approximating to an $Îµ$-equilibrium) of any perfect equilibrium of any dynamic game, implying PPAD=FP since the weak approximation problem is PPAD-complete. In addition, we introduce a geometric object called equilibrium bundle, regarding which, first, perfect equilibria of dynamic games are formalized as zero points of its canonical section, second, the hybrid iteration of dynamic programming and interior point method is formalized as a line search on it, third, it derives the existence and oddness theorems as an extension of those of Nash equilibria. In experiment, the line search process is animated, and the method is tested on 2000 randomly generated dynamic games where it converges to a perfect equilibrium in every single case.
<div id='section'>Paperid: <span id='pid'>80, <a href='https://arxiv.org/pdf/2312.16456.pdf' target='_blank'>https://arxiv.org/pdf/2312.16456.pdf</a></span>   <span><a href='https://github.com/buaawgj/TACE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Guojian Wang, Faguo Wu, Xiao Zhang, Ning Guo, Zhiming Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.16456">Adaptive trajectory-constrained exploration strategy for deep reinforcement learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep reinforcement learning (DRL) faces significant challenges in addressing the hard-exploration problems in tasks with sparse or deceptive rewards and large state spaces. These challenges severely limit the practical application of DRL. Most previous exploration methods relied on complex architectures to estimate state novelty or introduced sensitive hyperparameters, resulting in instability. To mitigate these issues, we propose an efficient adaptive trajectory-constrained exploration strategy for DRL. The proposed method guides the policy of the agent away from suboptimal solutions by leveraging incomplete offline demonstrations as references. This approach gradually expands the exploration scope of the agent and strives for optimality in a constrained optimization manner. Additionally, we introduce a novel policy-gradient-based optimization algorithm that utilizes adaptively clipped trajectory-distance rewards for both single- and multi-agent reinforcement learning. We provide a theoretical analysis of our method, including a deduction of the worst-case approximation error bounds, highlighting the validity of our approach for enhancing exploration. To evaluate the effectiveness of the proposed method, we conducted experiments on two large 2D grid world mazes and several MuJoCo tasks. The extensive experimental results demonstrate the significant advantages of our method in achieving temporally extended exploration and avoiding myopic and suboptimal behaviors in both single- and multi-agent settings. Notably, the specific metrics and quantifiable results further support these findings. The code used in the study is available at \url{https://github.com/buaawgj/TACE}.
<div id='section'>Paperid: <span id='pid'>81, <a href='https://arxiv.org/pdf/2312.15600.pdf' target='_blank'>https://arxiv.org/pdf/2312.15600.pdf</a></span>   <span><a href='https://github.com/LXXXXR/CACOM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinran Li, Jun Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.15600">Context-aware Communication for Multi-agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Effective communication protocols in multi-agent reinforcement learning (MARL) are critical to fostering cooperation and enhancing team performance. To leverage communication, many previous works have proposed to compress local information into a single message and broadcast it to all reachable agents. This simplistic messaging mechanism, however, may fail to provide adequate, critical, and relevant information to individual agents, especially in severely bandwidth-limited scenarios. This motivates us to develop context-aware communication schemes for MARL, aiming to deliver personalized messages to different agents. Our communication protocol, named CACOM, consists of two stages. In the first stage, agents exchange coarse representations in a broadcast fashion, providing context for the second stage. Following this, agents utilize attention mechanisms in the second stage to selectively generate messages personalized for the receivers. Furthermore, we employ the learned step size quantization (LSQ) technique for message quantization to reduce the communication overhead. To evaluate the effectiveness of CACOM, we integrate it with both actor-critic and value-based MARL algorithms. Empirical results on cooperative benchmark tasks demonstrate that CACOM provides evident performance gains over baselines under communication-constrained scenarios. The code is publicly available at https://github.com/LXXXXR/CACOM.
<div id='section'>Paperid: <span id='pid'>82, <a href='https://arxiv.org/pdf/2312.05166.pdf' target='_blank'>https://arxiv.org/pdf/2312.05166.pdf</a></span>   <span><a href='https://github.com/SamuelMallick/dmpcrl-concept/tree/paper-2023' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Samuel Mallick, Filippo Airaldi, Azita Dabiri, Bart De Schutter
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.05166">Multi-Agent Reinforcement Learning via Distributed MPC as a Function Approximator</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a novel approach to multi-agent reinforcement learning (RL) for linear systems with convex polytopic constraints. Existing work on RL has demonstrated the use of model predictive control (MPC) as a function approximator for the policy and value functions. The current paper is the first work to extend this idea to the multi-agent setting. We propose the use of a distributed MPC scheme as a function approximator, with a structure allowing for distributed learning and deployment. We then show that Q-learning updates can be performed distributively without introducing nonstationarity, by reconstructing a centralized learning update. The effectiveness of the approach is demonstrated on two numerical examples.
<div id='section'>Paperid: <span id='pid'>83, <a href='https://arxiv.org/pdf/2312.04819.pdf' target='_blank'>https://arxiv.org/pdf/2312.04819.pdf</a></span>   <span><a href='https://github.com/NJU-RL/ACORM](https://github.com/NJU-RL/ACORM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zican Hu, Zongzhang Zhang, Huaxiong Li, Chunlin Chen, Hongyu Ding, Zhi Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.04819">Attention-Guided Contrastive Role Representations for Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Real-world multi-agent tasks usually involve dynamic team composition with the emergence of roles, which should also be a key to efficient cooperation in multi-agent reinforcement learning (MARL). Drawing inspiration from the correlation between roles and agent's behavior patterns, we propose a novel framework of **A**ttention-guided **CO**ntrastive **R**ole representation learning for **M**ARL (**ACORM**) to promote behavior heterogeneity, knowledge transfer, and skillful coordination across agents. First, we introduce mutual information maximization to formalize role representation learning, derive a contrastive learning objective, and concisely approximate the distribution of negative pairs. Second, we leverage an attention mechanism to prompt the global state to attend to learned role representations in value decomposition, implicitly guiding agent coordination in a skillful role space to yield more expressive credit assignment. Experiments on challenging StarCraft II micromanagement and Google research football tasks demonstrate the state-of-the-art performance of our method and its advantages over existing approaches. Our code is available at [https://github.com/NJU-RL/ACORM](https://github.com/NJU-RL/ACORM).
<div id='section'>Paperid: <span id='pid'>84, <a href='https://arxiv.org/pdf/2312.01472.pdf' target='_blank'>https://arxiv.org/pdf/2312.01472.pdf</a></span>   <span><a href='https://github.com/facebookresearch/BenchMARL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Matteo Bettini, Amanda Prorok, Vincent Moens
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.01472">BenchMARL: Benchmarking Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The field of Multi-Agent Reinforcement Learning (MARL) is currently facing a reproducibility crisis. While solutions for standardized reporting have been proposed to address the issue, we still lack a benchmarking tool that enables standardization and reproducibility, while leveraging cutting-edge Reinforcement Learning (RL) implementations. In this paper, we introduce BenchMARL, the first MARL training library created to enable standardized benchmarking across different algorithms, models, and environments. BenchMARL uses TorchRL as its backend, granting it high performance and maintained state-of-the-art implementations while addressing the broad community of MARL PyTorch users. Its design enables systematic configuration and reporting, thus allowing users to create and run complex benchmarks from simple one-line inputs. BenchMARL is open-sourced on GitHub: https://github.com/facebookresearch/BenchMARL
<div id='section'>Paperid: <span id='pid'>85, <a href='https://arxiv.org/pdf/2311.04726.pdf' target='_blank'>https://arxiv.org/pdf/2311.04726.pdf</a></span>   <span><a href='https://walter0807.github.io/Social-CH/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wentao Zhu, Jason Qin, Yuke Lou, Hang Ye, Xiaoxuan Ma, Hai Ci, Yizhou Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.04726">Social Motion Prediction with Cognitive Hierarchies</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humans exhibit a remarkable capacity for anticipating the actions of others and planning their own actions accordingly. In this study, we strive to replicate this ability by addressing the social motion prediction problem. We introduce a new benchmark, a novel formulation, and a cognition-inspired framework. We present Wusi, a 3D multi-person motion dataset under the context of team sports, which features intense and strategic human interactions and diverse pose distributions. By reformulating the problem from a multi-agent reinforcement learning perspective, we incorporate behavioral cloning and generative adversarial imitation learning to boost learning efficiency and generalization. Furthermore, we take into account the cognitive aspects of the human social action planning process and develop a cognitive hierarchy framework to predict strategic human social interactions. We conduct comprehensive experiments to validate the effectiveness of our proposed dataset and approach. Code and data are available at https://walter0807.github.io/Social-CH/.
<div id='section'>Paperid: <span id='pid'>86, <a href='https://arxiv.org/pdf/2311.01753.pdf' target='_blank'>https://arxiv.org/pdf/2311.01753.pdf</a></span>   <span><a href='https://github.com/xmu-rl-3dv/RiskQ' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Siqi Shen, Chennan Ma, Chao Li, Weiquan Liu, Yongquan Fu, Songzhu Mei, Xinwang Liu, Cheng Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.01753">RiskQ: Risk-sensitive Multi-Agent Reinforcement Learning Value Factorization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent systems are characterized by environmental uncertainty, varying policies of agents, and partial observability, which result in significant risks. In the context of Multi-Agent Reinforcement Learning (MARL), learning coordinated and decentralized policies that are sensitive to risk is challenging. To formulate the coordination requirements in risk-sensitive MARL, we introduce the Risk-sensitive Individual-Global-Max (RIGM) principle as a generalization of the Individual-Global-Max (IGM) and Distributional IGM (DIGM) principles. This principle requires that the collection of risk-sensitive action selections of each agent should be equivalent to the risk-sensitive action selection of the central policy. Current MARL value factorization methods do not satisfy the RIGM principle for common risk metrics such as the Value at Risk (VaR) metric or distorted risk measurements. Therefore, we propose RiskQ to address this limitation, which models the joint return distribution by modeling quantiles of it as weighted quantile mixtures of per-agent return distribution utilities. RiskQ satisfies the RIGM principle for the VaR and distorted risk metrics. We show that RiskQ can obtain promising performance through extensive experiments. The source code of RiskQ is available in https://github.com/xmu-rl-3dv/RiskQ.
<div id='section'>Paperid: <span id='pid'>87, <a href='https://arxiv.org/pdf/2311.00865.pdf' target='_blank'>https://arxiv.org/pdf/2311.00865.pdf</a></span>   <span><a href='https://github.com/mgerstgrasser/super' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Matthias Gerstgrasser, Tom Danino, Sarah Keren
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.00865">Selectively Sharing Experiences Improves Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a novel multi-agent RL approach, Selective Multi-Agent Prioritized Experience Relay, in which agents share with other agents a limited number of transitions they observe during training. The intuition behind this is that even a small number of relevant experiences from other agents could help each agent learn. Unlike many other multi-agent RL algorithms, this approach allows for largely decentralized training, requiring only a limited communication channel between agents. We show that our approach outperforms baseline no-sharing decentralized training and state-of-the art multi-agent RL algorithms. Further, sharing only a small number of highly relevant experiences outperforms sharing all experiences between agents, and the performance uplift from selective experience sharing is robust across a range of hyperparameters and DQN variants. A reference implementation of our algorithm is available at https://github.com/mgerstgrasser/super.
<div id='section'>Paperid: <span id='pid'>88, <a href='https://arxiv.org/pdf/2310.10810.pdf' target='_blank'>https://arxiv.org/pdf/2310.10810.pdf</a></span>   <span><a href='https://github.com/abukharin3/ERNIE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Alexander Bukharin, Yan Li, Yue Yu, Qingru Zhang, Zhehui Chen, Simiao Zuo, Chao Zhang, Songan Zhang, Tuo Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.10810">Robust Multi-Agent Reinforcement Learning via Adversarial Regularization: Theoretical Foundation and Stable Algorithms</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-Agent Reinforcement Learning (MARL) has shown promising results across several domains. Despite this promise, MARL policies often lack robustness and are therefore sensitive to small changes in their environment. This presents a serious concern for the real world deployment of MARL algorithms, where the testing environment may slightly differ from the training environment. In this work we show that we can gain robustness by controlling a policy's Lipschitz constant, and under mild conditions, establish the existence of a Lipschitz and close-to-optimal policy. Based on these insights, we propose a new robust MARL framework, ERNIE, that promotes the Lipschitz continuity of the policies with respect to the state observations and actions by adversarial regularization. The ERNIE framework provides robustness against noisy observations, changing transition dynamics, and malicious actions of agents. However, ERNIE's adversarial regularization may introduce some training instability. To reduce this instability, we reformulate adversarial regularization as a Stackelberg game. We demonstrate the effectiveness of the proposed framework with extensive experiments in traffic light control and particle environments. In addition, we extend ERNIE to mean-field MARL with a formulation based on distributionally robust optimization that outperforms its non-robust counterpart and is of independent interest. Our code is available at https://github.com/abukharin3/ERNIE.
<div id='section'>Paperid: <span id='pid'>89, <a href='https://arxiv.org/pdf/2310.10701.pdf' target='_blank'>https://arxiv.org/pdf/2310.10701.pdf</a></span>   <span><a href='https://github.com/romanlee6/multi_LLM_comm' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Huao Li, Yu Quan Chong, Simon Stepputtis, Joseph Campbell, Dana Hughes, Michael Lewis, Katia Sycara
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.10701">Theory of Mind for Multi-Agent Collaboration via Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While Large Language Models (LLMs) have demonstrated impressive accomplishments in both reasoning and planning, their abilities in multi-agent collaborations remains largely unexplored. This study evaluates LLM-based agents in a multi-agent cooperative text game with Theory of Mind (ToM) inference tasks, comparing their performance with Multi-Agent Reinforcement Learning (MARL) and planning-based baselines. We observed evidence of emergent collaborative behaviors and high-order Theory of Mind capabilities among LLM-based agents. Our results reveal limitations in LLM-based agents' planning optimization due to systematic failures in managing long-horizon contexts and hallucination about the task state. We explore the use of explicit belief state representations to mitigate these issues, finding that it enhances task performance and the accuracy of ToM inferences for LLM-based agents.
<div id='section'>Paperid: <span id='pid'>90, <a href='https://arxiv.org/pdf/2310.05208.pdf' target='_blank'>https://arxiv.org/pdf/2310.05208.pdf</a></span>   <span><a href='https://github.com/sjtu-marl/ZSC-Eval' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xihuai Wang, Shao Zhang, Wenhao Zhang, Wentao Dong, Jingxiao Chen, Ying Wen, Weinan Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.05208">ZSC-Eval: An Evaluation Toolkit and Benchmark for Multi-agent Zero-shot Coordination</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Zero-shot coordination (ZSC) is a new cooperative multi-agent reinforcement learning (MARL) challenge that aims to train an ego agent to work with diverse, unseen partners during deployment. The significant difference between the deployment-time partners' distribution and the training partners' distribution determined by the training algorithm makes ZSC a unique out-of-distribution (OOD) generalization challenge. The potential distribution gap between evaluation and deployment-time partners leads to inadequate evaluation, which is exacerbated by the lack of appropriate evaluation metrics. In this paper, we present ZSC-Eval, the first evaluation toolkit and benchmark for ZSC algorithms. ZSC-Eval consists of: 1) Generation of evaluation partner candidates through behavior-preferring rewards to approximate deployment-time partners' distribution; 2) Selection of evaluation partners by Best-Response Diversity (BR-Div); 3) Measurement of generalization performance with various evaluation partners via the Best-Response Proximity (BR-Prox) metric. We use ZSC-Eval to benchmark ZSC algorithms in Overcooked and Google Research Football environments and get novel empirical findings. We also conduct a human experiment of current ZSC algorithms to verify the ZSC-Eval's consistency with human evaluation. ZSC-Eval is now available at https://github.com/sjtu-marl/ZSC-Eval.
<div id='section'>Paperid: <span id='pid'>91, <a href='https://arxiv.org/pdf/2310.04148.pdf' target='_blank'>https://arxiv.org/pdf/2310.04148.pdf</a></span>   <span><a href='https://github.com/ydchen0806/dbMiM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yinda Chen, Wei Huang, Shenglong Zhou, Qi Chen, Zhiwei Xiong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.04148">Self-Supervised Neuron Segmentation with Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The performance of existing supervised neuron segmentation methods is highly dependent on the number of accurate annotations, especially when applied to large scale electron microscopy (EM) data. By extracting semantic information from unlabeled data, self-supervised methods can improve the performance of downstream tasks, among which the mask image model (MIM) has been widely used due to its simplicity and effectiveness in recovering original information from masked images. However, due to the high degree of structural locality in EM images, as well as the existence of considerable noise, many voxels contain little discriminative information, making MIM pretraining inefficient on the neuron segmentation task. To overcome this challenge, we propose a decision-based MIM that utilizes reinforcement learning (RL) to automatically search for optimal image masking ratio and masking strategy. Due to the vast exploration space, using single-agent RL for voxel prediction is impractical. Therefore, we treat each input patch as an agent with a shared behavior policy, allowing for multi-agent collaboration. Furthermore, this multi-agent model can capture dependencies between voxels, which is beneficial for the downstream segmentation task. Experiments conducted on representative EM datasets demonstrate that our approach has a significant advantage over alternative self-supervised methods on the task of neuron segmentation. Code is available at \url{https://github.com/ydchen0806/dbMiM}.
<div id='section'>Paperid: <span id='pid'>92, <a href='https://arxiv.org/pdf/2309.05655.pdf' target='_blank'>https://arxiv.org/pdf/2309.05655.pdf</a></span>   <span><a href='https://binghao-huang.github.io/dynamic_handover/' target='_blank'>  GitHub</a></span> <span><a href='https://binghao-huang.github.io/dynamic_handover/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Binghao Huang, Yuanpei Chen, Tianyu Wang, Yuzhe Qin, Yaodong Yang, Nikolay Atanasov, Xiaolong Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.05655">Dynamic Handover: Throw and Catch with Bimanual Hands</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humans throw and catch objects all the time. However, such a seemingly common skill introduces a lot of challenges for robots to achieve: The robots need to operate such dynamic actions at high-speed, collaborate precisely, and interact with diverse objects. In this paper, we design a system with two multi-finger hands attached to robot arms to solve this problem. We train our system using Multi-Agent Reinforcement Learning in simulation and perform Sim2Real transfer to deploy on the real robots. To overcome the Sim2Real gap, we provide multiple novel algorithm designs including learning a trajectory prediction model for the object. Such a model can help the robot catcher has a real-time estimation of where the object will be heading, and then react accordingly. We conduct our experiments with multiple objects in the real-world system, and show significant improvements over multiple baselines. Our project page is available at \url{https://binghao-huang.github.io/dynamic_handover/}.
<div id='section'>Paperid: <span id='pid'>93, <a href='https://arxiv.org/pdf/2308.11842.pdf' target='_blank'>https://arxiv.org/pdf/2308.11842.pdf</a></span>   <span><a href='https://github.com/dchen48/E3AC' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Dingyang Chen, Qi Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.11842">${\rm E}(3)$-Equivariant Actor-Critic Methods for Cooperative Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Identification and analysis of symmetrical patterns in the natural world have led to significant discoveries across various scientific fields, such as the formulation of gravitational laws in physics and advancements in the study of chemical structures. In this paper, we focus on exploiting Euclidean symmetries inherent in certain cooperative multi-agent reinforcement learning (MARL) problems and prevalent in many applications. We begin by formally characterizing a subclass of Markov games with a general notion of symmetries that admits the existence of symmetric optimal values and policies. Motivated by these properties, we design neural network architectures with symmetric constraints embedded as an inductive bias for multi-agent actor-critic methods. This inductive bias results in superior performance in various cooperative MARL benchmarks and impressive generalization capabilities such as zero-shot learning and transfer learning in unseen scenarios with repeated symmetric patterns. The code is available at: https://github.com/dchen48/E3AC.
<div id='section'>Paperid: <span id='pid'>94, <a href='https://arxiv.org/pdf/2307.16212.pdf' target='_blank'>https://arxiv.org/pdf/2307.16212.pdf</a></span>   <span><a href='https://github.com/sihongho/robust_marl_with_state_uncertainty' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sihong He, Songyang Han, Sanbao Su, Shuo Han, Shaofeng Zou, Fei Miao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.16212">Robust Multi-Agent Reinforcement Learning with State Uncertainty</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In real-world multi-agent reinforcement learning (MARL) applications, agents may not have perfect state information (e.g., due to inaccurate measurement or malicious attacks), which challenges the robustness of agents' policies. Though robustness is getting important in MARL deployment, little prior work has studied state uncertainties in MARL, neither in problem formulation nor algorithm design. Motivated by this robustness issue and the lack of corresponding studies, we study the problem of MARL with state uncertainty in this work. We provide the first attempt to the theoretical and empirical analysis of this challenging problem. We first model the problem as a Markov Game with state perturbation adversaries (MG-SPA) by introducing a set of state perturbation adversaries into a Markov Game. We then introduce robust equilibrium (RE) as the solution concept of an MG-SPA. We conduct a fundamental analysis regarding MG-SPA such as giving conditions under which such a robust equilibrium exists. Then we propose a robust multi-agent Q-learning (RMAQ) algorithm to find such an equilibrium, with convergence guarantees. To handle high-dimensional state-action space, we design a robust multi-agent actor-critic (RMAAC) algorithm based on an analytical expression of the policy gradient derived in the paper. Our experiments show that the proposed RMAQ algorithm converges to the optimal value function; our RMAAC algorithm outperforms several MARL and robust MARL methods in multiple multi-agent environments when state uncertainty is present. The source code is public on \url{https://github.com/sihongho/robust_marl_with_state_uncertainty}.
<div id='section'>Paperid: <span id='pid'>95, <a href='https://arxiv.org/pdf/2307.03891.pdf' target='_blank'>https://arxiv.org/pdf/2307.03891.pdf</a></span>   <span><a href='https://shubhlohiya.github.io/MARBLER/' target='_blank'>  GitHub</a></span> <span><a href='https://shubhlohiya.github.io/MARBLER/,' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Reza Torbati, Shubham Lohiya, Shivika Singh, Meher Shashwat Nigam, Harish Ravichandar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.03891">MARBLER: An Open Platform for Standardized Evaluation of Multi-Robot Reinforcement Learning Algorithms</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-Agent Reinforcement Learning (MARL) has enjoyed significant recent progress thanks, in part, to the integration of deep learning techniques for modeling interactions in complex environments. This is naturally starting to benefit multi-robot systems (MRS) in the form of multi-robot RL (MRRL). However, existing infrastructure to train and evaluate policies predominantly focus on the challenges of coordinating virtual agents, and ignore characteristics important to robotic systems. Few platforms support realistic robot dynamics, and fewer still can evaluate Sim2Real performance of learned behavior. To address these issues, we contribute MARBLER: Multi-Agent RL Benchmark and Learning Environment for the Robotarium. MARBLER offers a robust and comprehensive evaluation platform for MRRL by marrying Georgia Tech's Robotarium (which enables rapid deployment on physical MRS) and OpenAI's Gym interface (which facilitates standardized use of modern learning algorithms). MARBLER offers a highly controllable environment with realistic dynamics, including barrier certificate-based obstacle avoidance. It allows anyone across the world to train and deploy MRRL algorithms on a physical testbed with reproducibility. Further, we introduce five novel scenarios inspired by common challenges in MRS and provide support for new custom scenarios. Finally, we use MARBLER to evaluate popular MARL algorithms and provide insights into their suitability for MRRL. In summary, MARBLER can be a valuable tool to the MRS research community by facilitating comprehensive and standardized evaluation of learning algorithms on realistic simulations and physical hardware. Links to our open-source framework and videos of real-world experiments can be found at https://shubhlohiya.github.io/MARBLER/.
<div id='section'>Paperid: <span id='pid'>96, <a href='https://arxiv.org/pdf/2306.06544.pdf' target='_blank'>https://arxiv.org/pdf/2306.06544.pdf</a></span>   <span><a href='https://github.com/andrewnash/Herds-Eye-View' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Andrew Nash, Andrew Vardy, David Churchill
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.06544">Herd's Eye View: Improving Game AI Agent Learning with Collaborative Perception</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a novel perception model named Herd's Eye View (HEV) that adopts a global perspective derived from multiple agents to boost the decision-making capabilities of reinforcement learning (RL) agents in multi-agent environments, specifically in the context of game AI. The HEV approach utilizes cooperative perception to empower RL agents with a global reasoning ability, enhancing their decision-making. We demonstrate the effectiveness of the HEV within simulated game environments and highlight its superior performance compared to traditional ego-centric perception models. This work contributes to cooperative perception and multi-agent reinforcement learning by offering a more realistic and efficient perspective for global coordination and decision-making within game environments. Moreover, our approach promotes broader AI applications beyond gaming by addressing constraints faced by AI in other fields such as robotics. The code is available at https://github.com/andrewnash/Herds-Eye-View
<div id='section'>Paperid: <span id='pid'>97, <a href='https://arxiv.org/pdf/2306.02006.pdf' target='_blank'>https://arxiv.org/pdf/2306.02006.pdf</a></span>   <span><a href='https://github.com/ustchlsong/MA2CL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Haolin Song, Mingxiao Feng, Wengang Zhou, Houqiang Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.02006">MA2CL:Masked Attentive Contrastive Learning for Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent approaches have utilized self-supervised auxiliary tasks as representation learning to improve the performance and sample efficiency of vision-based reinforcement learning algorithms in single-agent settings. However, in multi-agent reinforcement learning (MARL), these techniques face challenges because each agent only receives partial observation from an environment influenced by others, resulting in correlated observations in the agent dimension. So it is necessary to consider agent-level information in representation learning for MARL. In this paper, we propose an effective framework called \textbf{M}ulti-\textbf{A}gent \textbf{M}asked \textbf{A}ttentive \textbf{C}ontrastive \textbf{L}earning (MA2CL), which encourages learning representation to be both temporal and agent-level predictive by reconstructing the masked agent observation in latent space. Specifically, we use an attention reconstruction model for recovering and the model is trained via contrastive learning. MA2CL allows better utilization of contextual information at the agent level, facilitating the training of MARL agents for cooperation tasks. Extensive experiments demonstrate that our method significantly improves the performance and sample efficiency of different MARL algorithms and outperforms other methods in various vision-based and state-based scenarios. Our code can be found in \url{https://github.com/ustchlsong/MA2CL}
<div id='section'>Paperid: <span id='pid'>98, <a href='https://arxiv.org/pdf/2305.11465.pdf' target='_blank'>https://arxiv.org/pdf/2305.11465.pdf</a></span>   <span><a href='https://omron-sinicx.github.io/ncf2/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hikaru Asano, Ryo Yonetani, Mai Nishimura, Tadashi Kozuno
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.11465">Counterfactual Fairness Filter for Fair-Delay Multi-Robot Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-robot navigation is the task of finding trajectories for a team of robotic agents to reach their destinations as quickly as possible without collisions. In this work, we introduce a new problem: fair-delay multi-robot navigation, which aims not only to enable such efficient, safe travels but also to equalize the travel delays among agents in terms of actual trajectories as compared to the best possible trajectories. The learning of a navigation policy to achieve this objective requires resolving a nontrivial credit assignment problem with robotic agents having continuous action spaces. Hence, we developed a new algorithm called Navigation with Counterfactual Fairness Filter (NCF2). With NCF2, each agent performs counterfactual inference on whether it can advance toward its goal or should stay still to let other agents go. Doing so allows us to effectively address the aforementioned credit assignment problem and improve fairness regarding travel delays while maintaining high efficiency and safety. Our extensive experimental results in several challenging multi-robot navigation environments demonstrate the greater effectiveness of NCF2 as compared to state-of-the-art fairness-aware multi-agent reinforcement learning methods. Our demo videos and code are available on the project webpage: https://omron-sinicx.github.io/ncf2/
<div id='section'>Paperid: <span id='pid'>99, <a href='https://arxiv.org/pdf/2305.09458.pdf' target='_blank'>https://arxiv.org/pdf/2305.09458.pdf</a></span>   <span><a href='https://github.com/Shanghai-Digital-Brain-Laboratory/DB-Football' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yan Song, He Jiang, Zheng Tian, Haifeng Zhang, Yingping Zhang, Jiangcheng Zhu, Zonghong Dai, Weinan Zhang, Jun Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.09458">An Empirical Study on Google Research Football Multi-agent Scenarios</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Few multi-agent reinforcement learning (MARL) research on Google Research Football (GRF) focus on the 11v11 multi-agent full-game scenario and to the best of our knowledge, no open benchmark on this scenario has been released to the public. In this work, we fill the gap by providing a population-based MARL training pipeline and hyperparameter settings on multi-agent football scenario that outperforms the bot with difficulty 1.0 from scratch within 2 million steps. Our experiments serve as a reference for the expected performance of Independent Proximal Policy Optimization (IPPO), a state-of-the-art multi-agent reinforcement learning algorithm where each agent tries to maximize its own policy independently across various training configurations. Meanwhile, we open-source our training framework Light-MALib which extends the MALib codebase by distributed and asynchronized implementation with additional analytical tools for football games. Finally, we provide guidance for building strong football AI with population-based training and release diverse pretrained policies for benchmarking. The goal is to provide the community with a head start for whoever experiment their works on GRF and a simple-to-use population-based training framework for further improving their agents through self-play. The implementation is available at https://github.com/Shanghai-Digital-Brain-Laboratory/DB-Football.
<div id='section'>Paperid: <span id='pid'>100, <a href='https://arxiv.org/pdf/2305.06807.pdf' target='_blank'>https://arxiv.org/pdf/2305.06807.pdf</a></span>   <span><a href='https://github.com/YueLin301/InformationDesignMARL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yue Lin, Wenhao Li, Hongyuan Zha, Baoxiang Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.06807">Information Design in Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reinforcement learning (RL) is inspired by the way human infants and animals learn from the environment. The setting is somewhat idealized because, in actual tasks, other agents in the environment have their own goals and behave adaptively to the ego agent. To thrive in those environments, the agent needs to influence other agents so their actions become more helpful and less harmful. Research in computational economics distills two ways to influence others directly: by providing tangible goods (mechanism design) and by providing information (information design). This work investigates information design problems for a group of RL agents. The main challenges are two-fold. One is the information provided will immediately affect the transition of the agent trajectories, which introduces additional non-stationarity. The other is the information can be ignored, so the sender must provide information that the receiver is willing to respect. We formulate the Markov signaling game, and develop the notions of signaling gradient and the extended obedience constraints that address these challenges. Our algorithm is efficient on various mixed-motive tasks and provides further insights into computational economics. Our code is publicly available at https://github.com/YueLin301/InformationDesignMARL.
<div id='section'>Paperid: <span id='pid'>101, <a href='https://arxiv.org/pdf/2304.13004.pdf' target='_blank'>https://arxiv.org/pdf/2304.13004.pdf</a></span>   <span><a href='https://github.com/roger-creus/centralized-control-lux' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Roger Creus Castanyer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.13004">Centralized control for multi-agent RL in a complex Real-Time-Strategy game</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent Reinforcement learning (MARL) studies the behaviour of multiple learning agents that coexist in a shared environment. MARL is more challenging than single-agent RL because it involves more complex learning dynamics: the observations and rewards of each agent are functions of all other agents. In the context of MARL, Real-Time Strategy (RTS) games represent very challenging environments where multiple players interact simultaneously and control many units of different natures all at once. In fact, RTS games are so challenging for the current RL methods, that just being able to tackle them with RL is interesting. This project provides the end-to-end experience of applying RL in the Lux AI v2 Kaggle competition, where competitors design agents to control variable-sized fleets of units and tackle a multi-variable optimization, resource gathering, and allocation problem in a 1v1 scenario against other competitors. We use a centralized approach for training the RL agents, and report multiple design decisions along the process. We provide the source code of the project: https://github.com/roger-creus/centralized-control-lux.
<div id='section'>Paperid: <span id='pid'>102, <a href='https://arxiv.org/pdf/2304.12653.pdf' target='_blank'>https://arxiv.org/pdf/2304.12653.pdf</a></span>   <span><a href='https://github.com/yangmin32/GPMF' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Min Yang, Guanjun Liu, Ziyuan Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.12653">Partially Observable Mean Field Multi-Agent Reinforcement Learning Based on Graph-Attention</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Traditional multi-agent reinforcement learning algorithms are difficultly applied in a large-scale multi-agent environment. The introduction of mean field theory has enhanced the scalability of multi-agent reinforcement learning in recent years. This paper considers partially observable multi-agent reinforcement learning (MARL), where each agent can only observe other agents within a fixed range. This partial observability affects the agent's ability to assess the quality of the actions of surrounding agents. This paper focuses on developing a method to capture more effective information from local observations in order to select more effective actions. Previous work in this field employs probability distributions or weighted mean field to update the average actions of neighborhood agents, but it does not fully consider the feature information of surrounding neighbors and leads to a local optimum. In this paper, we propose a novel multi-agent reinforcement learning algorithm, Partially Observable Mean Field Multi-Agent Reinforcement Learning based on Graph-Attention (GAMFQ) to remedy this flaw. GAMFQ uses a graph attention module and a mean field module to describe how an agent is influenced by the actions of other agents at each time step. This graph attention module consists of a graph attention encoder and a differentiable attention mechanism, and this mechanism outputs a dynamic graph to represent the effectiveness of neighborhood agents against central agents. The mean-field module approximates the effect of a neighborhood agent on a central agent as the average effect of effective neighborhood agents. Experiments show that GAMFQ outperforms baselines including the state-of-the-art partially observable mean-field reinforcement learning algorithms. The code for this paper is here \url{https://github.com/yangmin32/GPMF}.
<div id='section'>Paperid: <span id='pid'>103, <a href='https://arxiv.org/pdf/2303.13935.pdf' target='_blank'>https://arxiv.org/pdf/2303.13935.pdf</a></span>   <span><a href='https://github.com/robot-perception-group/concurrent_composition' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yu Tang Liu, Aamir Ahmad
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.13935">Multi-Task Reinforcement Learning in Continuous Control with Successor Feature-Based Concurrent Composition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep reinforcement learning (DRL) frameworks are increasingly used to solve high-dimensional continuous control tasks in robotics. However, due to the lack of sample efficiency, applying DRL for online learning is still practically infeasible in the robotics domain. One reason is that DRL agents do not leverage the solution of previous tasks for new tasks. Recent work on multi-task DRL agents based on successor features (SFs) has proven to be quite promising in increasing sample efficiency. In this work, we present a new approach that unifies two prior multi-task RL frameworks, SF-GPI and value composition, and adapts them to the continuous control domain. We exploit compositional properties of successor features to compose a policy distribution from a set of primitives without training any new policy. Lastly, to demonstrate the multi-tasking mechanism, we present our proof-of-concept benchmark environments, Pointmass and Pointer, based on IsaacGym, which facilitates large-scale parallelization to accelerate the experiments. Our experimental results show that our multi-task agent has single-task performance on par with soft actor-critic (SAC), and the agent can successfully transfer to new unseen tasks. We provide our code as open-source at "https://github.com/robot-perception-group/concurrent_composition" for the benefit of the community.
<div id='section'>Paperid: <span id='pid'>104, <a href='https://arxiv.org/pdf/2303.13808.pdf' target='_blank'>https://arxiv.org/pdf/2303.13808.pdf</a></span>   <span><a href='https://github.com/kinalmehta/marl-jax' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kinal Mehta, Anuj Mahajan, Pawan Kumar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.13808">marl-jax: Multi-Agent Reinforcement Leaning Framework</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in Reinforcement Learning (RL) have led to many exciting applications. These advancements have been driven by improvements in both algorithms and engineering, which have resulted in faster training of RL agents. We present marl-jax, a multi-agent reinforcement learning software package for training and evaluating social generalization of the agents. The package is designed for training a population of agents in multi-agent environments and evaluating their ability to generalize to diverse background agents. It is built on top of DeepMind's JAX ecosystem~\cite{deepmind2020jax} and leverages the RL ecosystem developed by DeepMind. Our framework marl-jax is capable of working in cooperative and competitive, simultaneous-acting environments with multiple agents. The package offers an intuitive and user-friendly command-line interface for training a population and evaluating its generalization capabilities. In conclusion, marl-jax provides a valuable resource for researchers interested in exploring social generalization in the context of MARL. The open-source code for marl-jax is available at: \href{https://github.com/kinalmehta/marl-jax}{https://github.com/kinalmehta/marl-jax}
<div id='section'>Paperid: <span id='pid'>105, <a href='https://arxiv.org/pdf/2303.07337.pdf' target='_blank'>https://arxiv.org/pdf/2303.07337.pdf</a></span>   <span><a href='https://github.com/qihao067/PoseExaminer' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Qihao Liu, Adam Kortylewski, Alan Yuille
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.07337">PoseExaminer: Automated Testing of Out-of-Distribution Robustness in Human Pose and Shape Estimation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human pose and shape (HPS) estimation methods achieve remarkable results. However, current HPS benchmarks are mostly designed to test models in scenarios that are similar to the training data. This can lead to critical situations in real-world applications when the observed data differs significantly from the training data and hence is out-of-distribution (OOD). It is therefore important to test and improve the OOD robustness of HPS methods. To address this fundamental problem, we develop a simulator that can be controlled in a fine-grained manner using interpretable parameters to explore the manifold of images of human pose, e.g. by varying poses, shapes, and clothes. We introduce a learning-based testing method, termed PoseExaminer, that automatically diagnoses HPS algorithms by searching over the parameter space of human pose images to find the failure modes. Our strategy for exploring this high-dimensional parameter space is a multi-agent reinforcement learning system, in which the agents collaborate to explore different parts of the parameter space. We show that our PoseExaminer discovers a variety of limitations in current state-of-the-art models that are relevant in real-world scenarios but are missed by current benchmarks. For example, it finds large regions of realistic human poses that are not predicted correctly, as well as reduced performance for humans with skinny and corpulent body shapes. In addition, we show that fine-tuning HPS methods by exploiting the failure modes found by PoseExaminer improve their robustness and even their performance on standard benchmarks by a significant margin. The code are available for research purposes.
<div id='section'>Paperid: <span id='pid'>106, <a href='https://arxiv.org/pdf/2302.03322.pdf' target='_blank'>https://arxiv.org/pdf/2302.03322.pdf</a></span>   <span><a href='https://github.com/DIG-Beihang/AMI' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Simin Li, Jun Guo, Jingqiao Xiu, Yuwei Zheng, Pu Feng, Xin Yu, Aishan Liu, Yaodong Yang, Bo An, Wenjun Wu, Xianglong Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.03322">Attacking Cooperative Multi-Agent Reinforcement Learning by Adversarial Minority Influence</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This study probes the vulnerabilities of cooperative multi-agent reinforcement learning (c-MARL) under adversarial attacks, a critical determinant of c-MARL's worst-case performance prior to real-world implementation. Current observation-based attacks, constrained by white-box assumptions, overlook c-MARL's complex multi-agent interactions and cooperative objectives, resulting in impractical and limited attack capabilities. To address these shortcomes, we propose Adversarial Minority Influence (AMI), a practical and strong for c-MARL. AMI is a practical black-box attack and can be launched without knowing victim parameters. AMI is also strong by considering the complex multi-agent interaction and the cooperative goal of agents, enabling a single adversarial agent to unilaterally misleads majority victims to form targeted worst-case cooperation. This mirrors minority influence phenomena in social psychology. To achieve maximum deviation in victim policies under complex agent-wise interactions, our unilateral attack aims to characterize and maximize the impact of the adversary on the victims. This is achieved by adapting a unilateral agent-wise relation metric derived from mutual information, thereby mitigating the adverse effects of victim influence on the adversary. To lead the victims into a jointly detrimental scenario, our targeted attack deceives victims into a long-term, cooperatively harmful situation by guiding each victim towards a specific target, determined through a trial-and-error process executed by a reinforcement learning agent. Through AMI, we achieve the first successful attack against real-world robot swarms and effectively fool agents in simulated environments into collectively worst-case scenarios, including Starcraft II and Multi-agent Mujoco. The source code and demonstrations can be found at: https://github.com/DIG-Beihang/AMI.
<div id='section'>Paperid: <span id='pid'>107, <a href='https://arxiv.org/pdf/2301.05334.pdf' target='_blank'>https://arxiv.org/pdf/2301.05334.pdf</a></span>   <span><a href='https://github.com/mttga/pymarl_transformers' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Matteo Gallici, Mario Martin, Ivan Masmitja
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.05334">TransfQMix: Transformers for Leveraging the Graph Structure of Multi-Agent Reinforcement Learning Problems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Coordination is one of the most difficult aspects of multi-agent reinforcement learning (MARL). One reason is that agents normally choose their actions independently of one another. In order to see coordination strategies emerging from the combination of independent policies, the recent research has focused on the use of a centralized function (CF) that learns each agent's contribution to the team reward. However, the structure in which the environment is presented to the agents and to the CF is typically overlooked. We have observed that the features used to describe the coordination problem can be represented as vertex features of a latent graph structure. Here, we present TransfQMix, a new approach that uses transformers to leverage this latent structure and learn better coordination policies. Our transformer agents perform a graph reasoning over the state of the observable entities. Our transformer Q-mixer learns a monotonic mixing-function from a larger graph that includes the internal and external states of the agents. TransfQMix is designed to be entirely transferable, meaning that same parameters can be used to control and train larger or smaller teams of agents. This enables to deploy promising approaches to save training time and derive general policies in MARL, such as transfer learning, zero-shot transfer, and curriculum learning. We report TransfQMix's performances in the Spread and StarCraft II environments. In both settings, it outperforms state-of-the-art Q-Learning models, and it demonstrates effectiveness in solving problems that other methods can not solve.
<div id='section'>Paperid: <span id='pid'>108, <a href='https://arxiv.org/pdf/2301.03398.pdf' target='_blank'>https://arxiv.org/pdf/2301.03398.pdf</a></span>   <span><a href='https://github.com/yang-xy20/async_mappo' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chao Yu, Xinyi Yang, Jiaxuan Gao, Jiayu Chen, Yunfei Li, Jijia Liu, Yunfei Xiang, Ruixin Huang, Huazhong Yang, Yi Wu, Yu Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.03398">Asynchronous Multi-Agent Reinforcement Learning for Efficient Real-Time Multi-Robot Cooperative Exploration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We consider the problem of cooperative exploration where multiple robots need to cooperatively explore an unknown region as fast as possible. Multi-agent reinforcement learning (MARL) has recently become a trending paradigm for solving this challenge. However, existing MARL-based methods adopt action-making steps as the metric for exploration efficiency by assuming all the agents are acting in a fully synchronous manner: i.e., every single agent produces an action simultaneously and every single action is executed instantaneously at each time step. Despite its mathematical simplicity, such a synchronous MARL formulation can be problematic for real-world robotic applications. It can be typical that different robots may take slightly different wall-clock times to accomplish an atomic action or even periodically get lost due to hardware issues. Simply waiting for every robot being ready for the next action can be particularly time-inefficient. Therefore, we propose an asynchronous MARL solution, Asynchronous Coordination Explorer (ACE), to tackle this real-world challenge. We first extend a classical MARL algorithm, multi-agent PPO (MAPPO), to the asynchronous setting and additionally apply action-delay randomization to enforce the learned policy to generalize better to varying action delays in the real world. Moreover, each navigation agent is represented as a team-size-invariant CNN-based policy, which greatly benefits real-robot deployment by handling possible robot lost and allows bandwidth-efficient intra-agent communication through low-dimensional CNN features. We first validate our approach in a grid-based scenario. Both simulation and real-robot results show that ACE reduces over 10% actual exploration time compared with classical approaches. We also apply our framework to a high-fidelity visual-based environment, Habitat, achieving 28% improvement in exploration efficiency.
<div id='section'>Paperid: <span id='pid'>109, <a href='https://arxiv.org/pdf/2212.02705.pdf' target='_blank'>https://arxiv.org/pdf/2212.02705.pdf</a></span>   <span><a href='https://songyanghan.github.io/what_is_solution/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Songyang Han, Sanbao Su, Sihong He, Shuo Han, Haizhao Yang, Shaofeng Zou, Fei Miao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2212.02705">What is the Solution for State-Adversarial Multi-Agent Reinforcement Learning?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Various methods for Multi-Agent Reinforcement Learning (MARL) have been developed with the assumption that agents' policies are based on accurate state information. However, policies learned through Deep Reinforcement Learning (DRL) are susceptible to adversarial state perturbation attacks. In this work, we propose a State-Adversarial Markov Game (SAMG) and make the first attempt to investigate different solution concepts of MARL under state uncertainties. Our analysis shows that the commonly used solution concepts of optimal agent policy and robust Nash equilibrium do not always exist in SAMGs. To circumvent this difficulty, we consider a new solution concept called robust agent policy, where agents aim to maximize the worst-case expected state value. We prove the existence of robust agent policy for finite state and finite action SAMGs. Additionally, we propose a Robust Multi-Agent Adversarial Actor-Critic (RMA3C) algorithm to learn robust policies for MARL agents under state uncertainties. Our experiments demonstrate that our algorithm outperforms existing methods when faced with state perturbations and greatly improves the robustness of MARL policies. Our code is public on https://songyanghan.github.io/what_is_solution/.
<div id='section'>Paperid: <span id='pid'>110, <a href='https://arxiv.org/pdf/2211.12712.pdf' target='_blank'>https://arxiv.org/pdf/2211.12712.pdf</a></span>   <span><a href='https://github.com/liushunyu/CIA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shunyu Liu, Yihe Zhou, Jie Song, Tongya Zheng, Kaixuan Chen, Tongtian Zhu, Zunlei Feng, Mingli Song
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2211.12712">Contrastive Identity-Aware Learning for Multi-Agent Value Decomposition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Value Decomposition (VD) aims to deduce the contributions of agents for decentralized policies in the presence of only global rewards, and has recently emerged as a powerful credit assignment paradigm for tackling cooperative Multi-Agent Reinforcement Learning (MARL) problems. One of the main challenges in VD is to promote diverse behaviors among agents, while existing methods directly encourage the diversity of learned agent networks with various strategies. However, we argue that these dedicated designs for agent networks are still limited by the indistinguishable VD network, leading to homogeneous agent behaviors and thus downgrading the cooperation capability. In this paper, we propose a novel Contrastive Identity-Aware learning (CIA) method, explicitly boosting the credit-level distinguishability of the VD network to break the bottleneck of multi-agent diversity. Specifically, our approach leverages contrastive learning to maximize the mutual information between the temporal credits and identity representations of different agents, encouraging the full expressiveness of credit assignment and further the emergence of individualities. The algorithm implementation of the proposed CIA module is simple yet effective that can be readily incorporated into various VD architectures. Experiments on the SMAC benchmarks and across different VD backbones demonstrate that the proposed method yields results superior to the state-of-the-art counterparts. Our code is available at https://github.com/liushunyu/CIA.
<div id='section'>Paperid: <span id='pid'>111, <a href='https://arxiv.org/pdf/2211.02127.pdf' target='_blank'>https://arxiv.org/pdf/2211.02127.pdf</a></span>   <span><a href='https://github.com/nsidn98/InforMARL' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/nsidn98/InforMARL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Siddharth Nayak, Kenneth Choi, Wenqi Ding, Sydney Dolan, Karthik Gopalakrishnan, Hamsa Balakrishnan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2211.02127">Scalable Multi-Agent Reinforcement Learning through Intelligent Information Aggregation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We consider the problem of multi-agent navigation and collision avoidance when observations are limited to the local neighborhood of each agent. We propose InforMARL, a novel architecture for multi-agent reinforcement learning (MARL) which uses local information intelligently to compute paths for all the agents in a decentralized manner. Specifically, InforMARL aggregates information about the local neighborhood of agents for both the actor and the critic using a graph neural network and can be used in conjunction with any standard MARL algorithm. We show that (1) in training, InforMARL has better sample efficiency and performance than baseline approaches, despite using less information, and (2) in testing, it scales well to environments with arbitrary numbers of agents and obstacles. We illustrate these results using four task environments, including one with predetermined goals for each agent, and one in which the agents collectively try to cover all goals. Code available at https://github.com/nsidn98/InforMARL.
<div id='section'>Paperid: <span id='pid'>112, <a href='https://arxiv.org/pdf/2210.13708.pdf' target='_blank'>https://arxiv.org/pdf/2210.13708.pdf</a></span>   <span><a href='https://github.com/Replicable-MARL/MARLlib' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Siyi Hu, Yifan Zhong, Minquan Gao, Weixun Wang, Hao Dong, Xiaodan Liang, Zhihui Li, Xiaojun Chang, Yaodong Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2210.13708">MARLlib: A Scalable and Efficient Multi-agent Reinforcement Learning Library</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A significant challenge facing researchers in the area of multi-agent reinforcement learning (MARL) pertains to the identification of a library that can offer fast and compatible development for multi-agent tasks and algorithm combinations, while obviating the need to consider compatibility issues. In this paper, we present MARLlib, a library designed to address the aforementioned challenge by leveraging three key mechanisms: 1) a standardized multi-agent environment wrapper, 2) an agent-level algorithm implementation, and 3) a flexible policy mapping strategy. By utilizing these mechanisms, MARLlib can effectively disentangle the intertwined nature of the multi-agent task and the learning process of the algorithm, with the ability to automatically alter the training strategy based on the current task's attributes. The MARLlib library's source code is publicly accessible on GitHub: \url{https://github.com/Replicable-MARL/MARLlib}.
<div id='section'>Paperid: <span id='pid'>113, <a href='https://arxiv.org/pdf/2210.12896.pdf' target='_blank'>https://arxiv.org/pdf/2210.12896.pdf</a></span>   <span><a href='https://github.com/MR-BENjie/IDRL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shijie Han, Siyuan Li, Bo An, Wei Zhao, Peng Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2210.12896">Classifying Ambiguous Identities in Hidden-Role Stochastic Games with Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent reinforcement learning (MARL) is a prevalent learning paradigm for solving stochastic games. In most MARL studies, agents in a game are defined as teammates or enemies beforehand, and the relationships among the agents remain fixed throughout the game. However, in real-world problems, the agent relationships are commonly unknown in advance or dynamically changing. Many multi-party interactions start off by asking: who is on my team? This question arises whether it is the first day at the stock exchange or the kindergarten. Therefore, training policies for such situations in the face of imperfect information and ambiguous identities is an important problem that needs to be addressed. In this work, we develop a novel identity detection reinforcement learning (IDRL) framework that allows an agent to dynamically infer the identities of nearby agents and select an appropriate policy to accomplish the task. In the IDRL framework, a relation network is constructed to deduce the identities of other agents by observing the behaviors of the agents. A danger network is optimized to estimate the risk of false-positive identifications. Beyond that, we propose an intrinsic reward that balances the need to maximize external rewards and accurate identification. After identifying the cooperation-competition pattern among the agents, IDRL applies one of the off-the-shelf MARL methods to learn the policy. To evaluate the proposed method, we conduct experiments on Red-10 card-shedding game, and the results show that IDRL achieves superior performance over other state-of-the-art MARL methods. Impressively, the relation network has the par performance to identify the identities of agents with top human players; the danger network reasonably avoids the risk of imperfect identification. The code to reproduce all the reported results is available online at https://github.com/MR-BENjie/IDRL.
<div id='section'>Paperid: <span id='pid'>114, <a href='https://arxiv.org/pdf/2209.12681.pdf' target='_blank'>https://arxiv.org/pdf/2209.12681.pdf</a></span>   <span><a href='https://github.com/PKU-RL/FOP-DMAC-MACPF' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiangxing Wang, Deheng Ye, Zongqing Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2209.12681">More Centralized Training, Still Decentralized Execution: Multi-Agent Conditional Policy Factorization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In cooperative multi-agent reinforcement learning (MARL), combining value decomposition with actor-critic enables agents to learn stochastic policies, which are more suitable for the partially observable environment. Given the goal of learning local policies that enable decentralized execution, agents are commonly assumed to be independent of each other, even in centralized training. However, such an assumption may prohibit agents from learning the optimal joint policy. To address this problem, we explicitly take the dependency among agents into centralized training. Although this leads to the optimal joint policy, it may not be factorized for decentralized execution. Nevertheless, we theoretically show that from such a joint policy, we can always derive another joint policy that achieves the same optimality but can be factorized for decentralized execution. To this end, we propose multi-agent conditional policy factorization (MACPF), which takes more centralized training but still enables decentralized execution. We empirically verify MACPF in various cooperative MARL tasks and demonstrate that MACPF achieves better performance or faster convergence than baselines. Our code is available at https://github.com/PKU-RL/FOP-DMAC-MACPF.
<div id='section'>Paperid: <span id='pid'>115, <a href='https://arxiv.org/pdf/2209.03880.pdf' target='_blank'>https://arxiv.org/pdf/2209.03880.pdf</a></span>   <span><a href='https://github.com/ChrFabian/Learning_sparse_GMFGs' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Christian Fabian, Kai Cui, Heinz Koeppl
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2209.03880">Learning Sparse Graphon Mean Field Games</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Although the field of multi-agent reinforcement learning (MARL) has made considerable progress in the last years, solving systems with a large number of agents remains a hard challenge. Graphon mean field games (GMFGs) enable the scalable analysis of MARL problems that are otherwise intractable. By the mathematical structure of graphons, this approach is limited to dense graphs which are insufficient to describe many real-world networks such as power law graphs. Our paper introduces a novel formulation of GMFGs, called LPGMFGs, which leverages the graph theoretical concept of $L^p$ graphons and provides a machine learning tool to efficiently and accurately approximate solutions for sparse network problems. This especially includes power law networks which are empirically observed in various application areas and cannot be captured by standard graphons. We derive theoretical existence and convergence guarantees and give empirical examples that demonstrate the accuracy of our learning approach for systems with many agents. Furthermore, we extend the Online Mirror Descent (OMD) learning algorithm to our setup to accelerate learning speed, empirically show its capabilities, and conduct a theoretical analysis using the novel concept of smoothed step graphons. In general, we provide a scalable, mathematically well-founded machine learning approach to a large class of otherwise intractable problems of great relevance in numerous research fields.
<div id='section'>Paperid: <span id='pid'>116, <a href='https://arxiv.org/pdf/2207.03902.pdf' target='_blank'>https://arxiv.org/pdf/2207.03902.pdf</a></span>   <span><a href='https://github.com/liushunyu/OPT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shunyu Liu, Jie Song, Yihe Zhou, Na Yu, Kaixuan Chen, Zunlei Feng, Mingli Song
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2207.03902">Interaction Pattern Disentangling for Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep cooperative multi-agent reinforcement learning has demonstrated its remarkable success over a wide spectrum of complex control tasks. However, recent advances in multi-agent learning mainly focus on value decomposition while leaving entity interactions still intertwined, which easily leads to over-fitting on noisy interactions between entities. In this work, we introduce a novel interactiOn Pattern disenTangling (OPT) method, to disentangle the entity interactions into interaction prototypes, each of which represents an underlying interaction pattern within a subgroup of the entities. OPT facilitates filtering the noisy interactions between irrelevant entities and thus significantly improves generalizability as well as interpretability. Specifically, OPT introduces a sparse disagreement mechanism to encourage sparsity and diversity among discovered interaction prototypes. Then the model selectively restructures these prototypes into a compact interaction pattern by an aggregator with learnable weights. To alleviate the training instability issue caused by partial observability, we propose to maximize the mutual information between the aggregation weights and the history behaviors of each agent. Experiments on single-task, multi-task and zero-shot benchmarks demonstrate that the proposed method yields results superior to the state-of-the-art counterparts. Our code is available at https://github.com/liushunyu/OPT.
<div id='section'>Paperid: <span id='pid'>117, <a href='https://arxiv.org/pdf/2203.16406.pdf' target='_blank'>https://arxiv.org/pdf/2203.16406.pdf</a></span>   <span><a href='https://github.com/Netease-Games-AI-Lab-Guangzhou/PerfectDou' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Guan Yang, Minghuan Liu, Weijun Hong, Weinan Zhang, Fei Fang, Guangjun Zeng, Yue Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2203.16406">PerfectDou: Dominating DouDizhu with Perfect Information Distillation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As a challenging multi-player card game, DouDizhu has recently drawn much attention for analyzing competition and collaboration in imperfect-information games. In this paper, we propose PerfectDou, a state-of-the-art DouDizhu AI system that dominates the game, in an actor-critic framework with a proposed technique named perfect information distillation. In detail, we adopt a perfect-training-imperfect-execution framework that allows the agents to utilize the global information to guide the training of the policies as if it is a perfect information game and the trained policies can be used to play the imperfect information game during the actual gameplay. To this end, we characterize card and game features for DouDizhu to represent the perfect and imperfect information. To train our system, we adopt proximal policy optimization with generalized advantage estimation in a parallel training paradigm. In experiments we show how and why PerfectDou beats all existing AI programs, and achieves state-of-the-art performance.
<div id='section'>Paperid: <span id='pid'>118, <a href='https://arxiv.org/pdf/2202.10574.pdf' target='_blank'>https://arxiv.org/pdf/2202.10574.pdf</a></span>   <span><a href='https://github.com/RunzheStat/CausalMARL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chengchun Shi, Runzhe Wan, Ge Song, Shikai Luo, Rui Song, Hongtu Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2202.10574">A Multi-Agent Reinforcement Learning Framework for Off-Policy Evaluation in Two-sided Markets</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The two-sided markets such as ride-sharing companies often involve a group of subjects who are making sequential decisions across time and/or location. With the rapid development of smart phones and internet of things, they have substantially transformed the transportation landscape of human beings. In this paper we consider large-scale fleet management in ride-sharing companies that involve multiple units in different areas receiving sequences of products (or treatments) over time. Major technical challenges, such as policy evaluation, arise in those studies because (i) spatial and temporal proximities induce interference between locations and times; and (ii) the large number of locations results in the curse of dimensionality. To address both challenges simultaneously, we introduce a multi-agent reinforcement learning (MARL) framework for carrying policy evaluation in these studies. We propose novel estimators for mean outcomes under different products that are consistent despite the high-dimensionality of state-action space. The proposed estimator works favorably in simulation experiments. We further illustrate our method using a real dataset obtained from a two-sided marketplace company to evaluate the effects of applying different subsidizing policies. A Python implementation of our proposed method is available at https://github.com/RunzheStat/CausalMARL.
<div id='section'>Paperid: <span id='pid'>119, <a href='https://arxiv.org/pdf/2107.01460.pdf' target='_blank'>https://arxiv.org/pdf/2107.01460.pdf</a></span>   <span><a href='https://github.com/instadeepai/Mava' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruan de Kock, Omayma Mahjoub, Sasha Abramowitz, Wiem Khlifi, Callum Rhys Tilbury, Claude Formanek, Andries Smit, Arnu Pretorius
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2107.01460">Mava: a research library for distributed multi-agent reinforcement learning in JAX</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent reinforcement learning (MARL) research is inherently computationally expensive and it is often difficult to obtain a sufficient number of experiment samples to test hypotheses and make robust statistical claims. Furthermore, MARL algorithms are typically complex in their design and can be tricky to implement correctly. These aspects of MARL present a difficult challenge when it comes to creating useful software for advanced research. Our criteria for such software is that it should be simple enough to use to implement new ideas quickly, while at the same time be scalable and fast enough to test those ideas in a reasonable amount of time. In this preliminary technical report, we introduce Mava, a research library for MARL written purely in JAX, that aims to fulfill these criteria. We discuss the design and core features of Mava, and demonstrate its use and performance across a variety of environments. In particular, we show Mava's substantial speed advantage, with improvements of 10-100x compared to other popular MARL frameworks, while maintaining strong performance. This allows for researchers to test ideas in a few minutes instead of several hours. Finally, Mava forms part of an ecosystem of libraries that seamlessly integrate with each other to help facilitate advanced research in MARL. We hope Mava will benefit the community and help drive scientifically sound and statistically robust research in the field. The open-source repository for Mava is available at https://github.com/instadeepai/Mava.
<div id='section'>Paperid: <span id='pid'>120, <a href='https://arxiv.org/pdf/2105.15013.pdf' target='_blank'>https://arxiv.org/pdf/2105.15013.pdf</a></span>   <span><a href='https://github.com/hsvgbkhgbv/shapley-q-learning' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianhong Wang, Yuan Zhang, Yunjie Gu, Tae-Kyun Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2105.15013">SHAQ: Incorporating Shapley Value Theory into Multi-Agent Q-Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Value factorisation is a useful technique for multi-agent reinforcement learning (MARL) in global reward game, however its underlying mechanism is not yet fully understood. This paper studies a theoretical framework for value factorisation with interpretability via Shapley value theory. We generalise Shapley value to Markov convex game called Markov Shapley value (MSV) and apply it as a value factorisation method in global reward game, which is obtained by the equivalence between the two games. Based on the properties of MSV, we derive Shapley-Bellman optimality equation (SBOE) to evaluate the optimal MSV, which corresponds to an optimal joint deterministic policy. Furthermore, we propose Shapley-Bellman operator (SBO) that is proved to solve SBOE. With a stochastic approximation and some transformations, a new MARL algorithm called Shapley Q-learning (SHAQ) is established, the implementation of which is guided by the theoretical results of SBO and MSV. We also discuss the relationship between SHAQ and relevant value factorisation methods. In the experiments, SHAQ exhibits not only superior performances on all tasks but also the interpretability that agrees with the theoretical analysis. The implementation of this paper is on https://github.com/hsvgbkhgbv/shapley-q-learning.
<div id='section'>Paperid: <span id='pid'>121, <a href='https://arxiv.org/pdf/2103.03450.pdf' target='_blank'>https://arxiv.org/pdf/2103.03450.pdf</a></span>   <span><a href='https://github.com/LucasCJYSDL/DeepFreight' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiayu Chen, Abhishek K. Umrawal, Tian Lan, Vaneet Aggarwal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2103.03450">DeepFreight: Integrating Deep Reinforcement Learning and Mixed Integer Programming for Multi-transfer Truck Freight Delivery</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the freight delivery demands and shipping costs increasing rapidly, intelligent control of fleets to enable efficient and cost-conscious solutions becomes an important problem. In this paper, we propose DeepFreight, a model-free deep-reinforcement-learning-based algorithm for multi-transfer freight delivery, which includes two closely-collaborative components: truck-dispatch and package-matching. Specifically, a deep multi-agent reinforcement learning framework called QMIX is leveraged to learn a dispatch policy, with which we can obtain the multi-step joint vehicle dispatch decisions for the fleet with respect to the delivery requests. Then an efficient multi-transfer matching algorithm is executed to assign the delivery requests to the trucks. Also, DeepFreight is integrated with a Mixed-Integer Linear Programming optimizer for further optimization. The evaluation results show that the proposed system is highly scalable and ensures a 100\% delivery success while maintaining low delivery-time and fuel consumption. The codes are available at https://github.com/LucasCJYSDL/DeepFreight.
<div id='section'>Paperid: <span id='pid'>122, <a href='https://arxiv.org/pdf/2102.03479.pdf' target='_blank'>https://arxiv.org/pdf/2102.03479.pdf</a></span>   <span><a href='https://github.com/hijkzzz/pymarl2' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jian Hu, Siyang Jiang, Seth Austin Harding, Haibin Wu, Shih-wei Liao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2102.03479">Rethinking the Implementation Tricks and Monotonicity Constraint in Cooperative Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Many complex multi-agent systems such as robot swarms control and autonomous vehicle coordination can be modeled as Multi-Agent Reinforcement Learning (MARL) tasks. QMIX, a widely popular MARL algorithm, has been used as a baseline for the benchmark environments, e.g., Starcraft Multi-Agent Challenge (SMAC), Difficulty-Enhanced Predator-Prey (DEPP). Recent variants of QMIX target relaxing the monotonicity constraint of QMIX, allowing for performance improvement in SMAC. In this paper, we investigate the code-level optimizations of these variants and the monotonicity constraint. (1) We find that such improvements of the variants are significantly affected by various code-level optimizations. (2) The experiment results show that QMIX with normalized optimizations outperforms other works in SMAC; (3) beyond the common wisdom from these works, the monotonicity constraint can improve sample efficiency in SMAC and DEPP. We also discuss why monotonicity constraints work well in purely cooperative tasks with a theoretical analysis. We open-source the code at \url{https://github.com/hijkzzz/pymarl2}.
<div id='section'>Paperid: <span id='pid'>123, <a href='https://arxiv.org/pdf/2505.24378.pdf' target='_blank'>https://arxiv.org/pdf/2505.24378.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yilun Kong, Guozheng Ma, Qi Zhao, Haoyu Wang, Li Shen, Xueqian Wang, Dacheng Tao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.24378">Mastering Massive Multi-Task Reinforcement Learning via Mixture-of-Expert Decision Transformer</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite recent advancements in offline multi-task reinforcement learning (MTRL) have harnessed the powerful capabilities of the Transformer architecture, most approaches focus on a limited number of tasks, with scaling to extremely massive tasks remaining a formidable challenge. In this paper, we first revisit the key impact of task numbers on current MTRL method, and further reveal that naively expanding the parameters proves insufficient to counteract the performance degradation as the number of tasks escalates. Building upon these insights, we propose M3DT, a novel mixture-of-experts (MoE) framework that tackles task scalability by further unlocking the model's parameter scalability. Specifically, we enhance both the architecture and the optimization of the agent, where we strengthen the Decision Transformer (DT) backbone with MoE to reduce task load on parameter subsets, and introduce a three-stage training mechanism to facilitate efficient training with optimal performance. Experimental results show that, by increasing the number of experts, M3DT not only consistently enhances its performance as model expansion on the fixed task numbers, but also exhibits remarkable task scalability, successfully extending to 160 tasks with superior performance.
<div id='section'>Paperid: <span id='pid'>124, <a href='https://arxiv.org/pdf/2504.08134.pdf' target='_blank'>https://arxiv.org/pdf/2504.08134.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Minrui Xu, Dusit Niyato, Jiawen Kang, Zehui Xiong, Mingzhe Chen, Dong In Kim, Xuemin, Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.08134">Hybrid Reinforcement Learning-based Sustainable Multi-User Computation Offloading for Mobile Edge-Quantum Computing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Exploiting quantum computing at the mobile edge holds immense potential for facilitating large-scale network design, processing multimodal data, optimizing resource management, and enhancing network security. In this paper, we propose a pioneering paradigm of mobile edge quantum computing (MEQC) that integrates quantum computing capabilities into classical edge computing servers that are proximate to mobile devices. To conceptualize the MEQC, we first design an MEQC system, where mobile devices can offload classical and quantum computation tasks to edge servers equipped with classical and quantum computers. We then formulate the hybrid classical-quantum computation offloading problem whose goal is to minimize system cost in terms of latency and energy consumption. To solve the offloading problem efficiently, we propose a hybrid discrete-continuous multi-agent reinforcement learning algorithm to learn long-term sustainable offloading and partitioning strategies. Finally, numerical results demonstrate that the proposed algorithm can reduce the MEQC system cost by up to 30% compared to existing baselines.
<div id='section'>Paperid: <span id='pid'>125, <a href='https://arxiv.org/pdf/2411.01146.pdf' target='_blank'>https://arxiv.org/pdf/2411.01146.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziqing Fan, Shengchao Hu, Yuhang Zhou, Li Shen, Ya Zhang, Yanfeng Wang, Dacheng Tao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.01146">Task-Aware Harmony Multi-Task Decision Transformer for Offline Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The purpose of offline multi-task reinforcement learning (MTRL) is to develop a unified policy applicable to diverse tasks without the need for online environmental interaction. Recent advancements approach this through sequence modeling, leveraging the Transformer architecture's scalability and the benefits of parameter sharing to exploit task similarities. However, variations in task content and complexity pose significant challenges in policy formulation, necessitating judicious parameter sharing and management of conflicting gradients for optimal policy performance. Furthermore, identifying the optimal parameter subspace for each task often necessitates prior knowledge of the task identifier during inference, limiting applicability in real-world scenarios with variable task content and unknown current tasks. In this work, we introduce the Harmony Multi-Task Decision Transformer (HarmoDT), a novel solution designed to identify an optimal harmony subspace of parameters for each task. We formulate this as a bi-level optimization problem within a meta-learning framework, where the upper level learns masks to define the harmony subspace, while the inner level focuses on updating parameters to improve the overall performance of the unified policy. To eliminate the need for task identifiers, we further design a group-wise variant (G-HarmoDT) that clusters tasks into coherent groups based on gradient information, and utilizes a gating network to determine task identifiers during inference. Empirical evaluations across various benchmarks highlight the superiority of our approach, demonstrating its effectiveness in the multi-task context with specific improvements of 8% gain in task-provided settings, 5% in task-agnostic settings, and 10% in unseen settings.
<div id='section'>Paperid: <span id='pid'>126, <a href='https://arxiv.org/pdf/2405.18080.pdf' target='_blank'>https://arxiv.org/pdf/2405.18080.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shengchao Hu, Ziqing Fan, Li Shen, Ya Zhang, Yanfeng Wang, Dacheng Tao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.18080">HarmoDT: Harmony Multi-Task Decision Transformer for Offline Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The purpose of offline multi-task reinforcement learning (MTRL) is to develop a unified policy applicable to diverse tasks without the need for online environmental interaction. Recent advancements approach this through sequence modeling, leveraging the Transformer architecture's scalability and the benefits of parameter sharing to exploit task similarities. However, variations in task content and complexity pose significant challenges in policy formulation, necessitating judicious parameter sharing and management of conflicting gradients for optimal policy performance. In this work, we introduce the Harmony Multi-Task Decision Transformer (HarmoDT), a novel solution designed to identify an optimal harmony subspace of parameters for each task. We approach this as a bi-level optimization problem, employing a meta-learning framework that leverages gradient-based techniques. The upper level of this framework is dedicated to learning a task-specific mask that delineates the harmony subspace, while the inner level focuses on updating parameters to enhance the overall performance of the unified policy. Empirical evaluations on a series of benchmarks demonstrate the superiority of HarmoDT, verifying the effectiveness of our approach.
<div id='section'>Paperid: <span id='pid'>127, <a href='https://arxiv.org/pdf/2303.10692.pdf' target='_blank'>https://arxiv.org/pdf/2303.10692.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chaofan Ma, Qisen Xu, Xiangfeng Wang, Bo Jin, Xiaoyun Zhang, Yanfeng Wang, Ya Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.10692">Boundary-aware Supervoxel-level Iteratively Refined Interactive 3D Image Segmentation with Multi-agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Interactive segmentation has recently been explored to effectively and efficiently harvest high-quality segmentation masks by iteratively incorporating user hints. While iterative in nature, most existing interactive segmentation methods tend to ignore the dynamics of successive interactions and take each interaction independently. We here propose to model iterative interactive image segmentation with a Markov decision process (MDP) and solve it with reinforcement learning (RL) where each voxel is treated as an agent. Considering the large exploration space for voxel-wise prediction and the dependence among neighboring voxels for the segmentation tasks, multi-agent reinforcement learning is adopted, where the voxel-level policy is shared among agents. Considering that boundary voxels are more important for segmentation, we further introduce a boundary-aware reward, which consists of a global reward in the form of relative cross-entropy gain, to update the policy in a constrained direction, and a boundary reward in the form of relative weight, to emphasize the correctness of boundary predictions. To combine the advantages of different types of interactions, i.e., simple and efficient for point-clicking, and stable and robust for scribbles, we propose a supervoxel-clicking based interaction design. Experimental results on four benchmark datasets have shown that the proposed method significantly outperforms the state-of-the-arts, with the advantage of fewer interactions, higher accuracy, and enhanced robustness.
<div id='section'>Paperid: <span id='pid'>128, <a href='https://arxiv.org/pdf/2307.02827.pdf' target='_blank'>https://arxiv.org/pdf/2307.02827.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhilong Liu, Jiayi Zhang, Ziheng Liu, Hongyang Du, Zhe Wang, Dusit Niyato, Mohsen Guizani, Bo Ai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.02827">Cell-Free XL-MIMO Meets Multi-Agent Reinforcement Learning: Architectures, Challenges, and Future Directions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cell-free massive multiple-input multiple-output (mMIMO) and extremely large-scale MIMO (XL-MIMO) are regarded as promising innovations for the forthcoming generation of wireless communication systems. Their significant advantages in augmenting the number of degrees of freedom have garnered considerable interest. In this article, we first review the essential opportunities and challenges induced by XL-MIMO systems. We then propose the enhanced paradigm of cell-free XL-MIMO, which incorporates multi-agent reinforcement learning (MARL) to provide a distributed strategy for tackling the problem of high-dimension signal processing and costly energy consumption. Based on the unique near-field characteristics, we propose two categories of the low-complexity design, i.e., antenna selection and power control, to adapt to different cell-free XL-MIMO scenarios and achieve the maximum data rate. For inspiration, several critical future research directions pertaining to green cell-free XL-MIMO systems are presented.
<div id='section'>Paperid: <span id='pid'>129, <a href='https://arxiv.org/pdf/2502.13569.pdf' target='_blank'>https://arxiv.org/pdf/2502.13569.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yan Yu, Wengang Zhou, Yaodong Yang, Wanxuan Lu, Yingyan Hou, Houqiang Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.13569">Model Evolution Framework with Genetic Algorithm for Multi-Task Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-task reinforcement learning employs a single policy to complete various tasks, aiming to develop an agent with generalizability across different scenarios. Given the shared characteristics of tasks, the agent's learning efficiency can be enhanced through parameter sharing. Existing approaches typically use a routing network to generate specific routes for each task and reconstruct a set of modules into diverse models to complete multiple tasks simultaneously. However, due to the inherent difference between tasks, it is crucial to allocate resources based on task difficulty, which is constrained by the model's structure. To this end, we propose a Model Evolution framework with Genetic Algorithm (MEGA), which enables the model to evolve during training according to the difficulty of the tasks. When the current model is insufficient for certain tasks, the framework will automatically incorporate additional modules, enhancing the model's capabilities. Moreover, to adapt to our model evolution framework, we introduce a genotype module-level model, using binary sequences as genotype policies for model reconstruction, while leveraging a non-gradient genetic algorithm to optimize these genotype policies. Unlike routing networks with fixed output dimensions, our approach allows for the dynamic adjustment of the genotype policy length, enabling it to accommodate models with a varying number of modules. We conducted experiments on various robotics manipulation tasks in the Meta-World benchmark. Our state-of-the-art performance demonstrated the effectiveness of the MEGA framework. We will release our source code to the public.
<div id='section'>Paperid: <span id='pid'>130, <a href='https://arxiv.org/pdf/2301.10574.pdf' target='_blank'>https://arxiv.org/pdf/2301.10574.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xunhan Hu, Jian Zhao, Wengang Zhou, Ruili Feng, Houqiang Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.10574">DIFFER: Decomposing Individual Reward for Fair Experience Replay in Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cooperative multi-agent reinforcement learning (MARL) is a challenging task, as agents must learn complex and diverse individual strategies from a shared team reward. However, existing methods struggle to distinguish and exploit important individual experiences, as they lack an effective way to decompose the team reward into individual rewards. To address this challenge, we propose DIFFER, a powerful theoretical framework for decomposing individual rewards to enable fair experience replay in MARL. By enforcing the invariance of network gradients, we establish a partial differential equation whose solution yields the underlying individual reward function. The individual TD-error can then be computed from the solved closed-form individual rewards, indicating the importance of each piece of experience in the learning task and guiding the training process. Our method elegantly achieves an equivalence to the original learning framework when individual experiences are homogeneous, while also adapting to achieve more muscular efficiency and fairness when diversity is observed.Our extensive experiments on popular benchmarks validate the effectiveness of our theory and method, demonstrating significant improvements in learning efficiency and fairness.
<div id='section'>Paperid: <span id='pid'>131, <a href='https://arxiv.org/pdf/2501.15228.pdf' target='_blank'>https://arxiv.org/pdf/2501.15228.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiqun Chen, Lingyong Yan, Weiwei Sun, Xinyu Ma, Yi Zhang, Shuaiqiang Wang, Dawei Yin, Yiming Yang, Jiaxin Mao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.15228">Improving Retrieval-Augmented Generation through Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Retrieval-augmented generation (RAG) is widely utilized to incorporate external knowledge into large language models, thereby enhancing factuality and reducing hallucinations in question-answering (QA) tasks. A standard RAG pipeline consists of several components, such as query rewriting, document retrieval, document filtering, and answer generation. However, these components are typically optimized separately through supervised fine-tuning, which can lead to misalignments between the objectives of individual components and the overarching aim of generating accurate answers. Although recent efforts have explored using reinforcement learning (RL) to optimize specific RAG components, these approaches often focus on simple pipelines with only two components or do not adequately address the complex interdependencies and collaborative interactions among the modules. To overcome these limitations, we propose treating the complex RAG pipeline with multiple components as a multi-agent cooperative task, in which each component can be regarded as an RL agent. Specifically, we present MMOA-RAG, Multi-Module joint Optimization Algorithm for RAG, which employs multi-agent reinforcement learning to harmonize all agents' goals toward a unified reward, such as the F1 score of the final answer. Experiments conducted on various QA benchmarks demonstrate that MMOA-RAG effectively boost the overall performance of the pipeline and outperforms existing baselines. Furthermore, comprehensive ablation studies validate the contributions of individual components and demonstrate MMOA-RAG can be adapted to different RAG pipelines and benchmarks.
<div id='section'>Paperid: <span id='pid'>132, <a href='https://arxiv.org/pdf/2501.15228.pdf' target='_blank'>https://arxiv.org/pdf/2501.15228.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiqun Chen, Lingyong Yan, Weiwei Sun, Xinyu Ma, Yi Zhang, Shuaiqiang Wang, Dawei Yin, Yiming Yang, Jiaxin Mao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.15228">Improving Retrieval-Augmented Generation through Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Retrieval-augmented generation (RAG) is widely utilized to incorporate external knowledge into large language models, thereby enhancing factuality and reducing hallucinations in question-answering (QA) tasks. A standard RAG pipeline consists of several components, such as query rewriting, document retrieval, document filtering, and answer generation. However, these components are typically optimized separately through supervised fine-tuning, which can lead to misalignments between the objectives of individual components and the overarching aim of generating accurate answers. Although recent efforts have explored using reinforcement learning (RL) to optimize specific RAG components, these approaches often focus on simple pipelines with only two components or do not adequately address the complex interdependencies and collaborative interactions among the modules. To overcome these limitations, we propose treating the complex RAG pipeline with multiple components as a multi-agent cooperative task, in which each component can be regarded as an RL agent. Specifically, we present MMOA-RAG, Multi-Module joint Optimization Algorithm for RAG, which employs multi-agent reinforcement learning to harmonize all agents' goals toward a unified reward, such as the F1 score of the final answer. Experiments conducted on various QA benchmarks demonstrate that MMOA-RAG effectively boost the overall performance of the pipeline and outperforms existing baselines. Furthermore, comprehensive ablation studies validate the contributions of individual components and demonstrate MMOA-RAG can be adapted to different RAG pipelines and benchmarks.
<div id='section'>Paperid: <span id='pid'>133, <a href='https://arxiv.org/pdf/2504.13554.pdf' target='_blank'>https://arxiv.org/pdf/2504.13554.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xin Tang, Qian Chen, Wenjie Weng, Chao Jin, Zhang Liu, Jiacheng Wang, Geng Sun, Xiaohuan Li, Dusit Niyato
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.13554">Task Assignment and Exploration Optimization for Low Altitude UAV Rescue via Generative AI Enhanced Multi-agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The integration of emerging uncrewed aerial vehicles (UAVs) with artificial intelligence (AI) and ground-embedded robots (GERs) has transformed emergency rescue operations in unknown environments. However, the high computational demands often exceed a single UAV's capacity, making it difficult to continuously provide stable high-level services. To address this, this paper proposes a cooperation framework involving UAVs, GERs, and airships. The framework enables resource pooling through UAV-to-GER (U2G) and UAV-to-airship (U2A) links, offering computing services for offloaded tasks. Specifically, we formulate the multi-objective problem of task assignment and exploration as a dynamic long-term optimization problem aiming to minimize task completion time and energy use while ensuring stability. Using Lyapunov optimization, we transform it into a per-slot deterministic problem and propose HG-MADDPG, which combines the Hungarian algorithm with a GDM-based multi-agent deep deterministic policy gradient. Simulations demonstrate significant improvements in offloading efficiency, latency, and system stability over baselines.
<div id='section'>Paperid: <span id='pid'>134, <a href='https://arxiv.org/pdf/2404.17780.pdf' target='_blank'>https://arxiv.org/pdf/2404.17780.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dapeng Li, Hang Dong, Lu Wang, Bo Qiao, Si Qin, Qingwei Lin, Dongmei Zhang, Qi Zhang, Zhiwei Xu, Bin Zhang, Guoliang Fan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.17780">Verco: Learning Coordinated Verbal Communication for Multi-agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, multi-agent reinforcement learning algorithms have made significant advancements in diverse gaming environments, leading to increased interest in the broader application of such techniques. To address the prevalent challenge of partial observability, communication-based algorithms have improved cooperative performance through the sharing of numerical embedding between agents. However, the understanding of the formation of collaborative mechanisms is still very limited, making designing a human-understandable communication mechanism a valuable problem to address. In this paper, we propose a novel multi-agent reinforcement learning algorithm that embeds large language models into agents, endowing them with the ability to generate human-understandable verbal communication. The entire framework has a message module and an action module. The message module is responsible for generating and sending verbal messages to other agents, effectively enhancing information sharing among agents. To further enhance the message module, we employ a teacher model to generate message labels from the global view and update the student model through Supervised Fine-Tuning (SFT). The action module receives messages from other agents and selects actions based on current local observations and received messages. Experiments conducted on the Overcooked game demonstrate our method significantly enhances the learning efficiency and performance of existing methods, while also providing an interpretable tool for humans to understand the process of multi-agent cooperation.
<div id='section'>Paperid: <span id='pid'>135, <a href='https://arxiv.org/pdf/2502.14496.pdf' target='_blank'>https://arxiv.org/pdf/2502.14496.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhitao He, Zijun Liu, Peng Li, Yi R. Fung, Ming Yan, Ji Zhang, Fei Huang, Yang Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.14496">Advancing Language Multi-Agent Learning with Credit Re-Assignment for Interactive Environment Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>LLM-based agents have made significant advancements in interactive environments, such as mobile operations and web browsing, and other domains beyond computer using. Current multi-agent systems universally excel in performance, compared to single agents, but struggle with generalization across environments due to predefined roles and inadequate strategies for generalizing language agents. The challenge of achieving both strong performance and good generalization has hindered the progress of multi-agent systems for interactive environments. To address these issues, we propose CollabUIAgents, a multi-agent reinforcement learning framework with a novel multi-agent credit re-assignment (CR) strategy, assigning process rewards with LLMs rather than environment-specific rewards and learning with synthesized preference data, in order to foster generalizable, collaborative behaviors among the role-free agents' policies. Empirical results show that our framework improves both performance and cross-environment generalizability of multi-agent systems. Moreover, our 7B-parameter system achieves results on par with or exceed strong closed-source models, and the LLM that guides the CR. We also provide insights in using granular CR rewards effectively for environment generalization, and accommodating trained LLMs in multi-agent systems.
<div id='section'>Paperid: <span id='pid'>136, <a href='https://arxiv.org/pdf/2510.04935.pdf' target='_blank'>https://arxiv.org/pdf/2510.04935.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Guoxin Chen, Zile Qiao, Wenqing Wang, Donglei Yu, Xuanzhong Chen, Hao Sun, Minpeng Liao, Kai Fan, Yong Jiang, Penguin Xie, Wayne Xin Zhao, Ruihua Song, Fei Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.04935">MARS: Optimizing Dual-System Deep Research via Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large Reasoning Models (LRMs) often exhibit a tendency for overanalysis in simple tasks, where the models excessively utilize System 2-type, deliberate reasoning, leading to inefficient token generation. Furthermore, these models face challenges in adapting their reasoning capabilities to rapidly changing environments due to the static nature of their pretraining data. To address these issues, advancing Large Language Models (LLMs) for complex reasoning tasks requires innovative approaches that bridge intuitive and deliberate cognitive processes, akin to human cognition's dual-system dynamic. This paper introduces a Multi-Agent System for Deep ReSearch (MARS) enabling seamless integration of System 1's fast, intuitive thinking with System 2's deliberate reasoning within LLMs. MARS strategically integrates multiple external tools, such as Google Search, Google Scholar, and Python Interpreter, to access up-to-date information and execute complex computations, while creating a specialized division of labor where System 1 efficiently processes and summarizes high-volume external information, providing distilled insights that expand System 2's reasoning context without overwhelming its capacity. Furthermore, we propose a multi-agent reinforcement learning framework extending Group Relative Policy Optimization to simultaneously optimize both systems with multi-turn tool interactions, bin-packing optimization, and sample balancing strategies that enhance collaborative efficiency. Extensive experiments demonstrate MARS achieves substantial improvements of 3.86% on the challenging Humanity's Last Exam (HLE) benchmark and an average gain of 8.9% across 7 knowledge-intensive tasks, validating the effectiveness of our dual-system paradigm for complex reasoning in dynamic information environments.
<div id='section'>Paperid: <span id='pid'>137, <a href='https://arxiv.org/pdf/2510.04935.pdf' target='_blank'>https://arxiv.org/pdf/2510.04935.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Guoxin Chen, Zile Qiao, Wenqing Wang, Donglei Yu, Xuanzhong Chen, Hao Sun, Minpeng Liao, Kai Fan, Yong Jiang, Penguin Xie, Wayne Xin Zhao, Ruihua Song, Fei Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.04935">MARS: Optimizing Dual-System Deep Research via Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large Reasoning Models (LRMs) often exhibit a tendency for overanalysis in simple tasks, where the models excessively utilize System 2-type, deliberate reasoning, leading to inefficient token generation. Furthermore, these models face challenges in adapting their reasoning capabilities to rapidly changing environments due to the static nature of their pretraining data. To address these issues, advancing Large Language Models (LLMs) for complex reasoning tasks requires innovative approaches that bridge intuitive and deliberate cognitive processes, akin to human cognition's dual-system dynamic. This paper introduces a Multi-Agent System for Deep ReSearch (MARS) enabling seamless integration of System 1's fast, intuitive thinking with System 2's deliberate reasoning within LLMs. MARS strategically integrates multiple external tools, such as Google Search, Google Scholar, and Python Interpreter, to access up-to-date information and execute complex computations, while creating a specialized division of labor where System 1 efficiently processes and summarizes high-volume external information, providing distilled insights that expand System 2's reasoning context without overwhelming its capacity. Furthermore, we propose a multi-agent reinforcement learning framework extending Group Relative Policy Optimization to simultaneously optimize both systems with multi-turn tool interactions, bin-packing optimization, and sample balancing strategies that enhance collaborative efficiency. Extensive experiments demonstrate MARS achieves substantial improvements of 3.86% on the challenging Humanity's Last Exam (HLE) benchmark and an average gain of 8.9% across 7 knowledge-intensive tasks, validating the effectiveness of our dual-system paradigm for complex reasoning in dynamic information environments.
<div id='section'>Paperid: <span id='pid'>138, <a href='https://arxiv.org/pdf/2112.10374.pdf' target='_blank'>https://arxiv.org/pdf/2112.10374.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qi Tian, Kun Kuang, Baoxiang Wang, Furui Liu, Fei Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2112.10374">CGIBNet: Bandwidth-constrained Communication with Graph Information Bottleneck in Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Communication is one of the core components for cooperative multi-agent reinforcement learning (MARL). The communication bandwidth, in many real applications, is always subject to certain constraints. To improve communication efficiency, in this article, we propose to simultaneously optimize whom to communicate with and what to communicate for each agent in MARL. By initiating the communication between agents with a directed complete graph, we propose a novel communication model, named Communicative Graph Information Bottleneck Network (CGIBNet), to simultaneously compress the graph structure and the node information with the graph information bottleneck principle. The graph structure compression is designed to cut the redundant edges for determining whom to communicate with. The node information compression aims to address the problem of what to communicate via learning compact node representations. Moreover, CGIBNet is the first universal module for bandwidth-constrained communication, which can be applied to various training frameworks (i.e., policy-based and value-based MARL frameworks) and communication modes (i.e., single-round and multi-round communication). Extensive experiments are conducted in Traffic Control and StarCraft II environments. The results indicate that our method can achieve better performance in bandwidth-constrained settings compared with state-of-the-art algorithms.
<div id='section'>Paperid: <span id='pid'>139, <a href='https://arxiv.org/pdf/2302.11457.pdf' target='_blank'>https://arxiv.org/pdf/2302.11457.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ismail Lotfi, Dusit Niyato, Sumei Sun, Dong In Kim, Xuemin Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.11457">Semantic Information Marketing in The Metaverse: A Learning-Based Contract Theory Framework</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we address the problem of designing incentive mechanisms by a virtual service provider (VSP) to hire sensing IoT devices to sell their sensing data to help creating and rendering the digital copy of the physical world in the Metaverse. Due to the limited bandwidth, we propose to use semantic extraction algorithms to reduce the delivered data by the sensing IoT devices. Nevertheless, mechanisms to hire sensing IoT devices to share their data with the VSP and then deliver the constructed digital twin to the Metaverse users are vulnerable to adverse selection problem. The adverse selection problem, which is caused by information asymmetry between the system entities, becomes harder to solve when the private information of the different entities are multi-dimensional. We propose a novel iterative contract design and use a new variant of multi-agent reinforcement learning (MARL) to solve the modelled multi-dimensional contract problem. To demonstrate the effectiveness of our algorithm, we conduct extensive simulations and measure several key performance metrics of the contract for the Metaverse. Our results show that our designed iterative contract is able to incentivize the participants to interact truthfully, which maximizes the profit of the VSP with minimal individual rationality (IR) and incentive compatibility (IC) violation rates. Furthermore, the proposed learning-based iterative contract framework has limited access to the private information of the participants, which is to the best of our knowledge, the first of its kind in addressing the problem of adverse selection in incentive mechanisms.
<div id='section'>Paperid: <span id='pid'>140, <a href='https://arxiv.org/pdf/2410.24152.pdf' target='_blank'>https://arxiv.org/pdf/2410.24152.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiaqi Liu, Chengkai Xu, Peng Hang, Jian Sun, Wei Zhan, Masayoshi Tomizuka, Mingyu Ding
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.24152">Language-Driven Policy Distillation for Cooperative Driving in Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The cooperative driving technology of Connected and Autonomous Vehicles (CAVs) is crucial for improving the efficiency and safety of transportation systems. Learning-based methods, such as Multi-Agent Reinforcement Learning (MARL), have demonstrated strong capabilities in cooperative decision-making tasks. However, existing MARL approaches still face challenges in terms of learning efficiency and performance. In recent years, Large Language Models (LLMs) have rapidly advanced and shown remarkable abilities in various sequential decision-making tasks. To enhance the learning capabilities of cooperative agents while ensuring decision-making efficiency and cost-effectiveness, we propose LDPD, a language-driven policy distillation method for guiding MARL exploration. In this framework, a teacher agent based on LLM trains smaller student agents to achieve cooperative decision-making through its own decision-making demonstrations. The teacher agent enhances the observation information of CAVs and utilizes LLMs to perform complex cooperative decision-making reasoning, which also leverages carefully designed decision-making tools to achieve expert-level decisions, providing high-quality teaching experiences. The student agent then refines the teacher's prior knowledge into its own model through gradient policy updates. The experiments demonstrate that the students can rapidly improve their capabilities with minimal guidance from the teacher and eventually surpass the teacher's performance. Extensive experiments show that our approach demonstrates better performance and learning efficiency compared to baseline methods.
<div id='section'>Paperid: <span id='pid'>141, <a href='https://arxiv.org/pdf/2310.07218.pdf' target='_blank'>https://arxiv.org/pdf/2310.07218.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuxin Chen, Chen Tang, Ran Tian, Chenran Li, Jinning Li, Masayoshi Tomizuka, Wei Zhan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.07218">Quantifying Agent Interaction in Multi-agent Reinforcement Learning for Cost-efficient Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generalization poses a significant challenge in Multi-agent Reinforcement Learning (MARL). The extent to which an agent is influenced by unseen co-players depends on the agent's policy and the specific scenario. A quantitative examination of this relationship sheds light on effectively training agents for diverse scenarios. In this study, we present the Level of Influence (LoI), a metric quantifying the interaction intensity among agents within a given scenario and environment. We observe that, generally, a more diverse set of co-play agents during training enhances the generalization performance of the ego agent; however, this improvement varies across distinct scenarios and environments. LoI proves effective in predicting these improvement disparities within specific scenarios. Furthermore, we introduce a LoI-guided resource allocation method tailored to train a set of policies for diverse scenarios under a constrained budget. Our results demonstrate that strategic resource allocation based on LoI can achieve higher performance than uniform allocation under the same computation budget.
<div id='section'>Paperid: <span id='pid'>142, <a href='https://arxiv.org/pdf/2310.03303.pdf' target='_blank'>https://arxiv.org/pdf/2310.03303.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jintao Xue, Dongkun Zhang, Rong Xiong, Yue Wang, Eryun Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.03303">A Two-stage Based Social Preference Recognition in Multi-Agent Autonomous Driving System</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-Agent Reinforcement Learning (MARL) has become a promising solution for constructing a multi-agent autonomous driving system (MADS) in complex and dense scenarios. But most methods consider agents acting selfishly, which leads to conflict behaviors. Some existing works incorporate the concept of social value orientation (SVO) to promote coordination, but they lack the knowledge of other agents' SVOs, resulting in conservative maneuvers. In this paper, we aim to tackle the mentioned problem by enabling the agents to understand other agents' SVOs. To accomplish this, we propose a two-stage system framework. Firstly, we train a policy by allowing the agents to share their ground truth SVOs to establish a coordinated traffic flow. Secondly, we develop a recognition network that estimates agents' SVOs and integrates it with the policy trained in the first stage. Experiments demonstrate that our developed method significantly improves the performance of the driving policy in MADS compared to two state-of-the-art MARL algorithms.
<div id='section'>Paperid: <span id='pid'>143, <a href='https://arxiv.org/pdf/2502.01932.pdf' target='_blank'>https://arxiv.org/pdf/2502.01932.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zelai Xu, Ruize Zhang, Chao Yu, Huining Yuan, Xiangmin Yi, Shilong Ji, Chuqi Wang, Wenhao Tang, Feng Gao, Wenbo Ding, Xinlei Chen, Yu Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.01932">VolleyBots: A Testbed for Multi-Drone Volleyball Game Combining Motion Control and Strategic Play</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robot sports, characterized by well-defined objectives, explicit rules, and dynamic interactions, present ideal scenarios for demonstrating embodied intelligence. In this paper, we present VolleyBots, a novel robot sports testbed where multiple drones cooperate and compete in the sport of volleyball under physical dynamics. VolleyBots integrates three features within a unified platform: competitive and cooperative gameplay, turn-based interaction structure, and agile 3D maneuvering. Competitive and cooperative gameplay challenges each drone to coordinate with its teammates while anticipating and countering opposing teams' tactics. Turn-based interaction demands precise timing, accurate state prediction, and management of long-horizon temporal dependencies. Agile 3D maneuvering requires rapid accelerations, sharp turns, and precise 3D positioning despite the quadrotor's underactuated dynamics. These intertwined features yield a complex problem combining motion control and strategic play, with no available expert demonstrations. We provide a comprehensive suite of tasks ranging from single-drone drills to multi-drone cooperative and competitive tasks, accompanied by baseline evaluations of representative multi-agent reinforcement learning (MARL) and game-theoretic algorithms. Simulation results show that on-policy reinforcement learning (RL) methods outperform off-policy methods in single-agent tasks, but both approaches struggle in complex tasks that combine motion control and strategic play. We additionally design a hierarchical policy which achieves a 69.5% percent win rate against the strongest baseline in the 3 vs 3 task, underscoring its potential as an effective solution for tackling the complex interplay between low-level control and high-level strategy. The project page is at https://sites.google.com/view/thu-volleybots.
<div id='section'>Paperid: <span id='pid'>144, <a href='https://arxiv.org/pdf/2409.15866.pdf' target='_blank'>https://arxiv.org/pdf/2409.15866.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiayu Chen, Chao Yu, Guosheng Li, Wenhao Tang, Shilong Ji, Xinyi Yang, Botian Xu, Huazhong Yang, Yu Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.15866">Online Planning for Multi-UAV Pursuit-Evasion in Unknown Environments Using Deep Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-UAV pursuit-evasion, where pursuers aim to capture evaders, poses a key challenge for UAV swarm intelligence. Multi-agent reinforcement learning (MARL) has demonstrated potential in modeling cooperative behaviors, but most RL-based approaches remain constrained to simplified simulations with limited dynamics or fixed scenarios. Previous attempts to deploy RL policy to real-world pursuit-evasion are largely restricted to two-dimensional scenarios, such as ground vehicles or UAVs at fixed altitudes. In this paper, we address multi-UAV pursuit-evasion by considering UAV dynamics and physical constraints. We introduce an evader prediction-enhanced network to tackle partial observability in cooperative strategy learning. Additionally, we propose an adaptive environment generator within MARL training, enabling higher exploration efficiency and better policy generalization across diverse scenarios. Simulations show our method significantly outperforms all baselines in challenging scenarios, generalizing to unseen scenarios with a 100% capture rate. Finally, we derive a feasible policy via a two-stage reward refinement and deploy the policy on real quadrotors in a zero-shot manner. To our knowledge, this is the first work to derive and deploy an RL-based policy using collective thrust and body rates control commands for multi-UAV pursuit-evasion in unknown environments. The open-source code and videos are available at https://sites.google.com/view/pursuit-evasion-rl.
<div id='section'>Paperid: <span id='pid'>145, <a href='https://arxiv.org/pdf/2408.01072.pdf' target='_blank'>https://arxiv.org/pdf/2408.01072.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruize Zhang, Zelai Xu, Chengdong Ma, Chao Yu, Wei-Wei Tu, Wenhao Tang, Shiyu Huang, Deheng Ye, Wenbo Ding, Yaodong Yang, Yu Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.01072">A Survey on Self-play Methods in Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Self-play, characterized by agents' interactions with copies or past versions of themselves, has recently gained prominence in reinforcement learning (RL). This paper first clarifies the preliminaries of self-play, including the multi-agent reinforcement learning framework and basic game theory concepts. Then, it provides a unified framework and classifies existing self-play algorithms within this framework. Moreover, the paper bridges the gap between the algorithms and their practical implications by illustrating the role of self-play in different scenarios. Finally, the survey highlights open challenges and future research directions in self-play. This paper is an essential guide map for understanding the multifaceted landscape of self-play in RL.
<div id='section'>Paperid: <span id='pid'>146, <a href='https://arxiv.org/pdf/2310.04796.pdf' target='_blank'>https://arxiv.org/pdf/2310.04796.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiayu Chen, Zelai Xu, Yunfei Li, Chao Yu, Jiaming Song, Huazhong Yang, Fei Fang, Yu Wang, Yi Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.04796">Accelerate Multi-Agent Reinforcement Learning in Zero-Sum Games with Subgame Curriculum Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Learning Nash equilibrium (NE) in complex zero-sum games with multi-agent reinforcement learning (MARL) can be extremely computationally expensive. Curriculum learning is an effective way to accelerate learning, but an under-explored dimension for generating a curriculum is the difficulty-to-learn of the subgames -- games induced by starting from a specific state. In this work, we present a novel subgame curriculum learning framework for zero-sum games. It adopts an adaptive initial state distribution by resetting agents to some previously visited states where they can quickly learn to improve performance. Building upon this framework, we derive a subgame selection metric that approximates the squared distance to NE values and further adopt a particle-based state sampler for subgame generation. Integrating these techniques leads to our new algorithm, Subgame Automatic Curriculum Learning (SACL), which is a realization of the subgame curriculum learning framework. SACL can be combined with any MARL algorithm such as MAPPO. Experiments in the particle-world environment and Google Research Football environment show SACL produces much stronger policies than baselines. In the challenging hide-and-seek quadrant environment, SACL produces all four emergent stages and uses only half the samples of MAPPO with self-play. The project website is at https://sites.google.com/view/sacl-rl.
<div id='section'>Paperid: <span id='pid'>147, <a href='https://arxiv.org/pdf/2310.03354.pdf' target='_blank'>https://arxiv.org/pdf/2310.03354.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zelai Xu, Yancheng Liang, Chao Yu, Yu Wang, Yi Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.03354">Fictitious Cross-Play: Learning Global Nash Equilibrium in Mixed Cooperative-Competitive Games</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Self-play (SP) is a popular multi-agent reinforcement learning (MARL) framework for solving competitive games, where each agent optimizes policy by treating others as part of the environment. Despite the empirical successes, the theoretical properties of SP-based methods are limited to two-player zero-sum games. However, for mixed cooperative-competitive games where agents on the same team need to cooperate with each other, we can show a simple counter-example where SP-based methods cannot converge to a global Nash equilibrium (NE) with high probability. Alternatively, Policy-Space Response Oracles (PSRO) is an iterative framework for learning NE, where the best responses w.r.t. previous policies are learned in each iteration. PSRO can be directly extended to mixed cooperative-competitive settings by jointly learning team best responses with all convergence properties unchanged. However, PSRO requires repeatedly training joint policies from scratch till convergence, which makes it hard to scale to complex games. In this work, we develop a novel algorithm, Fictitious Cross-Play (FXP), which inherits the benefits from both frameworks. FXP simultaneously trains an SP-based main policy and a counter population of best response policies. The main policy is trained by fictitious self-play and cross-play against the counter population, while the counter policies are trained as the best responses to the main policy's past versions. We validate our method in matrix games and show that FXP converges to global NEs while SP methods fail. We also conduct experiments in a gridworld domain, where FXP achieves higher Elo ratings and lower exploitabilities than baselines, and a more challenging football game, where FXP defeats SOTA models with over 94% win rate.
<div id='section'>Paperid: <span id='pid'>148, <a href='https://arxiv.org/pdf/2302.04094.pdf' target='_blank'>https://arxiv.org/pdf/2302.04094.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinyi Yang, Shiyu Huang, Yiwen Sun, Yuxiang Yang, Chao Yu, Wei-Wei Tu, Huazhong Yang, Yu Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.04094">Learning Graph-Enhanced Commander-Executor for Multi-Agent Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper investigates the multi-agent navigation problem, which requires multiple agents to reach the target goals in a limited time. Multi-agent reinforcement learning (MARL) has shown promising results for solving this issue. However, it is inefficient for MARL to directly explore the (nearly) optimal policy in the large search space, which is exacerbated as the agent number increases (e.g., 10+ agents) or the environment is more complex (e.g., 3D simulator). Goal-conditioned hierarchical reinforcement learning (HRL) provides a promising direction to tackle this challenge by introducing a hierarchical structure to decompose the search space, where the low-level policy predicts primitive actions in the guidance of the goals derived from the high-level policy. In this paper, we propose Multi-Agent Graph-Enhanced Commander-Executor (MAGE-X), a graph-based goal-conditioned hierarchical method for multi-agent navigation tasks. MAGE-X comprises a high-level Goal Commander and a low-level Action Executor. The Goal Commander predicts the probability distribution of goals and leverages them to assign each agent the most appropriate final target. The Action Executor utilizes graph neural networks (GNN) to construct a subgraph for each agent that only contains crucial partners to improve cooperation. Additionally, the Goal Encoder in the Action Executor captures the relationship between the agent and the designated goal to encourage the agent to reach the final target. The results show that MAGE-X outperforms the state-of-the-art MARL baselines with a 100% success rate with only 3 million training steps in multi-agent particle environments (MPE) with 50 agents, and at least a 12% higher success rate and 2x higher data efficiency in a more complicated quadrotor 3D navigation task.
<div id='section'>Paperid: <span id='pid'>149, <a href='https://arxiv.org/pdf/2302.01605.pdf' target='_blank'>https://arxiv.org/pdf/2302.01605.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chao Yu, Jiaxuan Gao, Weilin Liu, Botian Xu, Hao Tang, Jiaqi Yang, Yu Wang, Yi Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.01605">Learning Zero-Shot Cooperation with Humans, Assuming Humans Are Biased</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>There is a recent trend of applying multi-agent reinforcement learning (MARL) to train an agent that can cooperate with humans in a zero-shot fashion without using any human data. The typical workflow is to first repeatedly run self-play (SP) to build a policy pool and then train the final adaptive policy against this pool. A crucial limitation of this framework is that every policy in the pool is optimized w.r.t. the environment reward function, which implicitly assumes that the testing partners of the adaptive policy will be precisely optimizing the same reward function as well. However, human objectives are often substantially biased according to their own preferences, which can differ greatly from the environment reward. We propose a more general framework, Hidden-Utility Self-Play (HSP), which explicitly models human biases as hidden reward functions in the self-play objective. By approximating the reward space as linear functions, HSP adopts an effective technique to generate an augmented policy pool with biased policies. We evaluate HSP on the Overcooked benchmark. Empirical results show that our HSP method produces higher rewards than baselines when cooperating with learned human models, manually scripted policies, and real humans. The HSP policy is also rated as the most assistive policy based on human feedback.
<div id='section'>Paperid: <span id='pid'>150, <a href='https://arxiv.org/pdf/2504.14824.pdf' target='_blank'>https://arxiv.org/pdf/2504.14824.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wei Wang, Nan Cheng, Conghao Zhou, Haixia Peng, Haibo Zhou, Zhou Su, Xuemin, Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.14824">An Enhanced Dual-Currency VCG Auction Mechanism for Resource Allocation in IoV: A Value of Information Perspective</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The Internet of Vehicles (IoV) is undergoing a transformative evolution, enabled by advancements in future 6G network technologies, to support intelligent, highly reliable, and low-latency vehicular services. However, the enhanced capabilities of loV have heightened the demands for efficient network resource allocation while simultaneously giving rise to diverse vehicular service requirements. For network service providers (NSPs), meeting the customized resource-slicing requirements of vehicle service providers (VSPs) while maximizing social welfare has become a significant challenge. This paper proposes an innovative solution by integrating a mean-field multi-agent reinforcement learning (MFMARL) framework with an enhanced Vickrey-Clarke-Groves (VCG) auction mechanism to address the problem of social welfare maximization under the condition of unknown VSP utility functions. The core of this solution is introducing the ``value of information" as a novel monetary metric to estimate the expected benefits of VSPs, thereby ensuring the effective execution of the VCG auction mechanism. MFMARL is employed to optimize resource allocation for social welfare maximization while adapting to the intelligent and dynamic requirements of IoV. The proposed enhanced VCG auction mechanism not only protects the privacy of VSPs but also reduces the likelihood of collusion among VSPs, and it is theoretically proven to be dominant-strategy incentive compatible (DSIC). The simulation results demonstrate that, compared to the VCG mechanism implemented using quantization methods, the proposed mechanism exhibits significant advantages in convergence speed, social welfare maximization, and resistance to collusion, providing new insights into resource allocation in intelligent 6G networks.
<div id='section'>Paperid: <span id='pid'>151, <a href='https://arxiv.org/pdf/2502.05812.pdf' target='_blank'>https://arxiv.org/pdf/2502.05812.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiayi Zhang, Ziheng Liu, Yiyang Zhu, Enyu Shi, Bokai Xu, Chau Yuen, Dusit Niyato, MÃ©rouane Debbah, Shi Jin, Bo Ai, Xuemin, Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.05812">Multi-Agent Reinforcement Learning in Wireless Distributed Networks for 6G</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The introduction of intelligent interconnectivity between the physical and human worlds has attracted great attention for future sixth-generation (6G) networks, emphasizing massive capacity, ultra-low latency, and unparalleled reliability. Wireless distributed networks and multi-agent reinforcement learning (MARL), both of which have evolved from centralized paradigms, are two promising solutions for the great attention. Given their distinct capabilities, such as decentralization and collaborative mechanisms, integrating these two paradigms holds great promise for unleashing the full power of 6G, attracting significant research and development attention. This paper provides a comprehensive study on MARL-assisted wireless distributed networks for 6G. In particular, we introduce the basic mathematical background and evolution of wireless distributed networks and MARL, as well as demonstrate their interrelationships. Subsequently, we analyze different structures of wireless distributed networks from the perspectives of homogeneous and heterogeneous. Furthermore, we introduce the basic concepts of MARL and discuss two typical categories, including model-based and model-free. We then present critical challenges faced by MARL-assisted wireless distributed networks, providing important guidance and insights for actual implementation. We also explore an interplay between MARL-assisted wireless distributed networks and emerging techniques, such as information bottleneck and mirror learning, delivering in-depth analyses and application scenarios. Finally, we outline several compelling research directions for future MARL-assisted wireless distributed networks.
<div id='section'>Paperid: <span id='pid'>152, <a href='https://arxiv.org/pdf/2410.20151.pdf' target='_blank'>https://arxiv.org/pdf/2410.20151.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shanshan Song, Bingwen Huangfu, Jiani Guo, Jun Liu, Junhong Cui, Xuemin, Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.20151">A Digital Twin-based Intelligent Network Architecture for Underwater Acoustic Sensor Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Underwater acoustic sensor networks (UASNs) drive toward strong environmental adaptability, intelligence, and multifunctionality. However, due to unique UASN characteristics, such as long propagation delay, dynamic channel quality, and high attenuation, existing studies present untimeliness, inefficiency, and inflexibility in real practice. Digital twin (DT) technology is promising for UASNs to break the above bottlenecks by providing high-fidelity status prediction and exploring optimal schemes. In this article, we propose a Digital Twin-based Network Architecture (DTNA), enhancing UASNs' environmental adaptability, intelligence, and multifunctionality. By extracting real UASN information from local (node) and global (network) levels, we first design a layered architecture to improve the DT replica fidelity and UASN control flexibility. In local DT, we develop a resource allocation paradigm (RAPD), which rapidly perceives performance variations and iteratively optimizes allocation schemes to improve real-time environmental adaptability of resource allocation algorithms. In global DT, we aggregate decentralized local DT data and propose a collaborative Multi-agent reinforcement learning framework (CMFD) and a task-oriented network slicing (TNSD). CMFD patches scarce real data and provides extensive DT data to accelerate AI model training. TNSD unifies heterogeneous tasks' demand extraction and efficiently provides comprehensive network status, improving the flexibility of multi-task scheduling algorithms. Finally, practical and simulation experiments verify the high fidelity of DT. Compared with the original UASN architecture, experiment results demonstrate that DTNA can: (i) improve the timeliness and robustness of resource allocation; (ii) greatly reduce the training time of AI algorithms; (iii) more rapidly obtain network status for multi-task scheduling at a low cost.
<div id='section'>Paperid: <span id='pid'>153, <a href='https://arxiv.org/pdf/2404.13158.pdf' target='_blank'>https://arxiv.org/pdf/2404.13158.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mingcheng He, Huaqing Wu, Conghao Zhou, Xuemin, Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.13158">Resource Slicing with Cross-Cell Coordination in Satellite-Terrestrial Integrated Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Satellite-terrestrial integrated networks (STIN) are envisioned as a promising architecture for ubiquitous network connections to support diversified services. In this paper, we propose a novel resource slicing scheme with cross-cell coordination in STIN to satisfy distinct service delay requirements and efficient resource usage. To address the challenges posed by spatiotemporal dynamics in service demands and satellite mobility, we formulate the resource slicing problem into a long-term optimization problem and propose a distributed resource slicing (DRS) scheme for scalable and flexible resource management across different cells. Specifically, a hybrid data-model co-driven approach is developed, including an asynchronous multi-agent reinforcement learning-based algorithm to determine the optimal satellite set serving each cell and a distributed optimization-based algorithm to make the resource reservation decisions for each slice. Simulation results demonstrate that the proposed scheme outperforms benchmark methods in terms of resource usage and delay performance.
<div id='section'>Paperid: <span id='pid'>154, <a href='https://arxiv.org/pdf/2404.04898.pdf' target='_blank'>https://arxiv.org/pdf/2404.04898.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziheng Liu, Jiayi Zhang, Enyu Shi, Zhilong Liu, Dusit Niyato, Bo Ai, Xuemin, Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.04898">Graph Neural Network Meets Multi-Agent Reinforcement Learning: Fundamentals, Applications, and Future Directions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent reinforcement learning (MARL) has become a fundamental component of next-generation wireless communication systems. Theoretically, although MARL has the advantages of low computational complexity and fast convergence rate, there exist several challenges including partial observability, non-stationary, and scalability. In this article, we investigate a novel MARL with graph neural network-aided communication (GNNComm-MARL) to address the aforementioned challenges by making use of graph attention networks to effectively sample neighborhoods and selectively aggregate messages. Furthermore, we thoroughly study the architecture of GNNComm-MARL and present a systematic design solution. We then present the typical applications of GNNComm-MARL from two aspects: resource allocation and mobility management. The results obtained unveil that GNNComm-MARL can achieve better performance with lower communication overhead compared to conventional communication schemes. Finally, several important research directions regarding GNNComm-MARL are presented to facilitate further investigation.
<div id='section'>Paperid: <span id='pid'>155, <a href='https://arxiv.org/pdf/2412.13551.pdf' target='_blank'>https://arxiv.org/pdf/2412.13551.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuhan Zuo, Minghao Wang, Tianqing Zhu, Shui Yu, Wanlei Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.13551">Large Language Model Federated Learning with Blockchain and Unlearning for Cross-Organizational Collaboration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large language models (LLMs) have transformed the way computers understand and process human language, but using them effectively across different organizations remains still difficult. When organizations work together to improve LLMs, they face several main challenges. First, organizations hesitate to share their valuable data with others. Second, competition between organizations creates trust problems during collaboration. Third, new privacy laws require organizations to be able to delete specific data when requested, which is especially difficult when multiple organizations are learning from shared data. Traditional federated learning approaches do not address these interconnected challenges, particularly in scenarios where participants cannot fully trust each other or the central aggregator. To overcome these limitations, we propose a hybrid blockchain-based federated learning framework that uniquely combines public and private blockchain architectures with multi-agent reinforcement learning. Our framework enables transparent sharing of model update through the public blockchain while protecting sensitive computations in private chains. Each organization operates as an intelligent agent, using Q-learning to optimize its participation strategy and resource allocation, thus aligning individual incentives with collective goals. Notably, we introduce an efficient unlearning mechanism based on Low-Rank Adaptation (LoRA) that enables selective removal of specific data contributions without compromising the model's overall performance. Through extensive experimentation on real-world datasets, we demonstrate that our framework effectively balances privacy protection, trust establishment, and regulatory compliance while maintaining high model performance.
<div id='section'>Paperid: <span id='pid'>156, <a href='https://arxiv.org/pdf/2305.18459.pdf' target='_blank'>https://arxiv.org/pdf/2305.18459.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoran He, Chenjia Bai, Kang Xu, Zhuoran Yang, Weinan Zhang, Dong Wang, Bin Zhao, Xuelong Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.18459">Diffusion Model is an Effective Planner and Data Synthesizer for Multi-Task Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Diffusion models have demonstrated highly-expressive generative capabilities in vision and NLP. Recent studies in reinforcement learning (RL) have shown that diffusion models are also powerful in modeling complex policies or trajectories in offline datasets. However, these works have been limited to single-task settings where a generalist agent capable of addressing multi-task predicaments is absent. In this paper, we aim to investigate the effectiveness of a single diffusion model in modeling large-scale multi-task offline data, which can be challenging due to diverse and multimodal data distribution. Specifically, we propose Multi-Task Diffusion Model (\textsc{MTDiff}), a diffusion-based method that incorporates Transformer backbones and prompt learning for generative planning and data synthesis in multi-task offline settings. \textsc{MTDiff} leverages vast amounts of knowledge available in multi-task data and performs implicit knowledge sharing among tasks. For generative planning, we find \textsc{MTDiff} outperforms state-of-the-art algorithms across 50 tasks on Meta-World and 8 maps on Maze2D. For data synthesis, \textsc{MTDiff} generates high-quality data for testing tasks given a single demonstration as a prompt, which enhances the low-quality datasets for even unseen tasks.
<div id='section'>Paperid: <span id='pid'>157, <a href='https://arxiv.org/pdf/2408.09501.pdf' target='_blank'>https://arxiv.org/pdf/2408.09501.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiwei Xu, Hangyu Mao, Nianmin Zhang, Xin Xin, Pengjie Ren, Dapeng Li, Bin Zhang, Guoliang Fan, Zhumin Chen, Changwei Wang, Jiangjin Yin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.09501">Beyond Local Views: Global State Inference with Diffusion Models for Cooperative Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In partially observable multi-agent systems, agents typically only have access to local observations. This severely hinders their ability to make precise decisions, particularly during decentralized execution. To alleviate this problem and inspired by image outpainting, we propose State Inference with Diffusion Models (SIDIFF), which uses diffusion models to reconstruct the original global state based solely on local observations. SIDIFF consists of a state generator and a state extractor, which allow agents to choose suitable actions by considering both the reconstructed global state and local observations. In addition, SIDIFF can be effortlessly incorporated into current multi-agent reinforcement learning algorithms to improve their performance. Finally, we evaluated SIDIFF on different experimental platforms, including Multi-Agent Battle City (MABC), a novel and flexible multi-agent reinforcement learning environment we developed. SIDIFF achieved desirable results and outperformed other popular algorithms.
<div id='section'>Paperid: <span id='pid'>158, <a href='https://arxiv.org/pdf/2301.00912.pdf' target='_blank'>https://arxiv.org/pdf/2301.00912.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yahao Ding, Zhaohui Yang, Quoc-Viet Pham, Zhaoyang Zhang, Mohammad Shikh-Bahaei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.00912">Distributed Machine Learning for UAV Swarms: Computing, Sensing, and Semantics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Unmanned aerial vehicle (UAV) swarms are considered as a promising technique for next-generation communication networks due to their flexibility, mobility, low cost, and the ability to collaboratively and autonomously provide services. Distributed learning (DL) enables UAV swarms to intelligently provide communication services, multi-directional remote surveillance, and target tracking. In this survey, we first introduce several popular DL algorithms such as federated learning (FL), multi-agent Reinforcement Learning (MARL), distributed inference, and split learning, and present a comprehensive overview of their applications for UAV swarms, such as trajectory design, power control, wireless resource allocation, user assignment, perception, and satellite communications. Then, we present several state-of-the-art applications of UAV swarms in wireless communication systems, such us reconfigurable intelligent surface (RIS), virtual reality (VR), semantic communications, and discuss the problems and challenges that DL-enabled UAV swarms can solve in these applications. Finally, we describe open problems of using DL in UAV swarms and future research directions of DL enabled UAV swarms. In summary, this survey provides a comprehensive survey of various DL applications for UAV swarms in extensive scenarios.
<div id='section'>Paperid: <span id='pid'>159, <a href='https://arxiv.org/pdf/2502.19675.pdf' target='_blank'>https://arxiv.org/pdf/2502.19675.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiyang Zhu, Jiayi Zhang, Enyu Shi, Ziheng Liu, Chau Yuen, Bo Ai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.19675">Joint Power Allocation and Phase Shift Design for Stacked Intelligent Metasurfaces-aided Cell-Free Massive MIMO Systems with MARL</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cell-free (CF) massive multiple-input multiple-output (mMIMO) systems offer high spectral efficiency (SE) through multiple distributed access points (APs). However, the large number of antennas increases power consumption. We propose incorporating stacked intelligent metasurfaces (SIM) into CF mMIMO systems as a cost-effective, energy-efficient solution. This paper focuses on optimizing the joint power allocation of APs and the phase shift of SIMs to maximize the sum SE. To address this complex problem, we introduce a fully distributed multi-agent reinforcement learning (MARL) algorithm. Our novel algorithm, the noisy value method with a recurrent policy in multi-agent policy optimization (NVR-MAPPO), enhances performance by encouraging diverse exploration under centralized training and decentralized execution. Simulations demonstrate that NVR-MAPPO significantly improves sum SE and robustness across various scenarios.
<div id='section'>Paperid: <span id='pid'>160, <a href='https://arxiv.org/pdf/2412.02581.pdf' target='_blank'>https://arxiv.org/pdf/2412.02581.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziheng Liu, Jiayi Zhang, Yiyang Zhu, Enyu Shi, Bo Ai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.02581">Mobile Cell-Free Massive MIMO with Multi-Agent Reinforcement Learning: A Scalable Framework</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cell-free massive multiple-input multiple-output (mMIMO) offers significant advantages in mobility scenarios, mainly due to the elimination of cell boundaries and strong macro diversity. In this paper, we examine the downlink performance of cell-free mMIMO systems equipped with mobile-APs utilizing the concept of unmanned aerial vehicles, where mobility and power control are jointly considered to effectively enhance coverage and suppress interference. However, the high computational complexity, poor collaboration, limited scalability, and uneven reward distribution of conventional optimization schemes lead to serious performance degradation and instability. These factors complicate the provision of consistent and high-quality service across all user equipments in downlink cell-free mMIMO systems. Consequently, we propose a novel scalable framework enhanced by multi-agent reinforcement learning (MARL) to tackle these challenges. The established framework incorporates a graph neural network (GNN)-aided communication mechanism to facilitate effective collaboration among agents, a permutation architecture to improve scalability, and a directional decoupling architecture to accurately distinguish contributions. In the numerical results, we present comparisons of different optimization schemes and network architectures, which reveal that the proposed scheme can effectively enhance system performance compared to conventional schemes due to the adoption of advanced technologies. In particular, appropriately compressing the observation space of agents is beneficial for achieving a better balance between performance and convergence.
<div id='section'>Paperid: <span id='pid'>161, <a href='https://arxiv.org/pdf/2411.11070.pdf' target='_blank'>https://arxiv.org/pdf/2411.11070.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Enyu Shi, Jiayi Zhang, Ziheng Liu, Yiyang Zhu, Chau Yuen, Derrick Wing Kwan Ng, Marco Di Renzo, Bo Ai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.11070">Joint Precoding and AP Selection for Energy Efficient RIS-aided Cell-Free Massive MIMO Using Multi-agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cell-free (CF) massive multiple-input multiple-output (mMIMO) and reconfigurable intelligent surface (RIS) are two advanced transceiver technologies for realizing future sixth-generation (6G) networks. In this paper, we investigate the joint precoding and access point (AP) selection for energy efficient RIS-aided CF mMIMO system. To address the associated computational complexity and communication power consumption, we advocate for user-centric dynamic networks in which each user is served by a subset of APs rather than by all of them. Based on the user-centric network, we formulate a joint precoding and AP selection problem to maximize the energy efficiency (EE) of the considered system. To solve this complex nonconvex problem, we propose an innovative double-layer multi-agent reinforcement learning (MARL)-based scheme. Moreover, we propose an adaptive power threshold-based AP selection scheme to further enhance the EE of the considered system. To reduce the computational complexity of the RIS-aided CF mMIMO system, we introduce a fuzzy logic (FL) strategy into the MARL scheme to accelerate convergence. The simulation results show that the proposed FL-based MARL cooperative architecture effectively improves EE performance, offering a 85\% enhancement over the zero-forcing (ZF) method, and achieves faster convergence speed compared with MARL. It is important to note that increasing the transmission power of the APs or the number of RIS elements can effectively enhance the spectral efficiency (SE) performance, which also leads to an increase in power consumption, resulting in a non-trivial trade-off between the quality of service and EE performance.
<div id='section'>Paperid: <span id='pid'>162, <a href='https://arxiv.org/pdf/2410.06506.pdf' target='_blank'>https://arxiv.org/pdf/2410.06506.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziheng Liu, Jiayi Zhang, Enyu Shi, Yiyang Zhu, Derrick Wing Kwan Ng, Bo Ai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.06506">Cooperative Multi-Target Positioning for Cell-Free Massive MIMO with Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cell-free massive multiple-input multiple-output (mMIMO) is a promising technology to empower next-generation mobile communication networks. In this paper, to address the computational complexity associated with conventional fingerprint positioning, we consider a novel cooperative positioning architecture that involves certain relevant access points (APs) to establish positioning similarity coefficients. Then, we propose an innovative joint positioning and correction framework employing multi-agent reinforcement learning (MARL) to tackle the challenges of high-dimensional sophisticated signal processing, which mainly leverages on the received signal strength information for preliminary positioning, supplemented by the angle of arrival information to refine the initial position estimation. Moreover, to mitigate the bias effects originating from remote APs, we design a cooperative weighted K-nearest neighbor (Co-WKNN)-based estimation scheme to select APs with a high correlation to participate in user positioning. In the numerical results, we present comparisons of various user positioning schemes, which reveal that the proposed MARL-based positioning scheme with Co-WKNN can effectively improve positioning performance. It is important to note that the cooperative positioning architecture is a critical element in striking a balance between positioning performance and computational complexity.
<div id='section'>Paperid: <span id='pid'>163, <a href='https://arxiv.org/pdf/2410.04871.pdf' target='_blank'>https://arxiv.org/pdf/2410.04871.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziheng Liu, Jiayi Zhang, Enyu Shi, Yiyang Zhu, Derrick Wing Kwan Ng, Bo Ai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.04871">Distributed Collaborative User Positioning for Cell-Free Massive MIMO with Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we investigate a cell-free massive multiple-input multiple-output system, which exhibits great potential in enhancing the capabilities of next-generation mobile communication networks. We first study the distributed positioning problem to lay the groundwork for solving resource allocation and interference management issues. Instead of relying on computationally and spatially complex fingerprint positioning methods, we propose a novel two-stage distributed collaborative positioning architecture with multi-agent reinforcement learning (MARL) network, consisting of a received signal strength-based preliminary positioning network and an angle of arrival-based auxiliary correction network. Our experimental results demonstrate that the two-stage distributed collaborative user positioning architecture can outperform conventional fingerprint positioning methods in terms of positioning accuracy.
<div id='section'>Paperid: <span id='pid'>164, <a href='https://arxiv.org/pdf/2406.05481.pdf' target='_blank'>https://arxiv.org/pdf/2406.05481.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziheng Liu, Jiayi Zhang, Zhilong Liu, Derrick Wing Kwan Ng, Bo Ai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.05481">Joint Cooperative Clustering and Power Control for Energy-Efficient Cell-Free XL-MIMO with Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we investigate the amalgamation of cell-free (CF) and extremely large-scale multiple-input multiple-output (XL-MIMO) technologies, referred to as a CF XL-MIMO, as a promising advancement for enabling future mobile networks. To address the computational complexity and communication power consumption associated with conventional centralized optimization, we focus on user-centric dynamic networks in which each user is served by an adaptive subset of access points (AP) rather than all of them. We begin our research by analyzing a joint resource allocation problem for energy-efficient CF XL-MIMO systems, encompassing cooperative clustering and power control design, where all clusters are adaptively adjustable. Then, we propose an innovative double-layer multi-agent reinforcement learning (MARL)-based scheme, which offers an effective strategy to tackle the challenges of high-dimensional signal processing. In the section of numerical results, we compare various algorithms with different network architectures. These comparisons reveal that the proposed MARL-based cooperative architecture can effectively strike a balance between system performance and communication overhead, thereby improving energy efficiency performance. It is important to note that increasing the number of user equipments participating in information sharing can effectively enhance SE performance, which also leads to an increase in power consumption, resulting in a non-trivial trade-off between the number of participants and EE performance.
<div id='section'>Paperid: <span id='pid'>165, <a href='https://arxiv.org/pdf/2404.14092.pdf' target='_blank'>https://arxiv.org/pdf/2404.14092.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiyang Zhu, Enyu Shi, Ziheng Liu, Jiayi Zhang, Bo Ai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.14092">Multi-agent Reinforcement Learning-based Joint Precoding and Phase Shift Optimization for RIS-aided Cell-Free Massive MIMO Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cell-free (CF) massive multiple-input multiple-output (mMIMO) is a promising technique for achieving high spectral efficiency (SE) using multiple distributed access points (APs). However, harsh propagation environments often lead to significant communication performance degradation due to high penetration loss. To overcome this issue, we introduce the reconfigurable intelligent surface (RIS) into the CF mMIMO system as a low-cost and power-efficient solution. In this paper, we focus on optimizing the joint precoding design of the RIS-aided CF mMIMO system to maximize the sum SE. This involves optimizing the precoding matrix at the APs and the reflection coefficients at the RIS. To tackle this problem, we propose a fully distributed multi-agent reinforcement learning (MARL) algorithm that incorporates fuzzy logic (FL). Unlike conventional approaches that rely on alternating optimization techniques, our FL-based MARL algorithm only requires local channel state information, which reduces the need for high backhaul capacity. Simulation results demonstrate that our proposed FL-MARL algorithm effectively reduces computational complexity while achieving similar performance as conventional MARL methods.
<div id='section'>Paperid: <span id='pid'>166, <a href='https://arxiv.org/pdf/2309.17079.pdf' target='_blank'>https://arxiv.org/pdf/2309.17079.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziheng Liu, Jiayi Zhang, Zhilong Liu, Huahua Xiao, Bo Ai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.17079">Double-Layer Power Control for Mobile Cell-Free XL-MIMO with Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cell-free (CF) extremely large-scale multiple-input multiple-output (XL-MIMO) is regarded as a promising technology for enabling future wireless communication systems. Significant attention has been generated by its considerable advantages in augmenting degrees of freedom. In this paper, we first investigate a CF XL-MIMO system with base stations equipped with XL-MIMO panels under a dynamic environment. Then, we propose an innovative multi-agent reinforcement learning (MARL)-based power control algorithm that incorporates predictive management and distributed optimization architecture, which provides a dynamic strategy for addressing high-dimension signal processing problems. Specifically, we compare various MARL-based algorithms, which shows that the proposed MARL-based algorithm effectively strikes a balance between spectral efficiency (SE) performance and convergence time. Moreover, we consider a double-layer power control architecture based on the large-scale fading coefficients between antennas to suppress interference within dynamic systems. Compared to the single-layer architecture, the results obtained unveil that the proposed double-layer architecture has a nearly24% SE performance improvement, especially with massive antennas and smaller antenna spacing.
<div id='section'>Paperid: <span id='pid'>167, <a href='https://arxiv.org/pdf/2302.09290.pdf' target='_blank'>https://arxiv.org/pdf/2302.09290.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziheng Liu, Zhilong Liu, Jiayi Zhang, Huahua Xiao, Bo Ai, Derrick Wing Kwan Ng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.09290">Uplink Power Control for Extremely Large-Scale MIMO with Multi-Agent Reinforcement Learning and Fuzzy Logic</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we investigate the uplink transmit power optimization problem in cell-free (CF) extremely large-scale multiple-input multiple-output (XL-MIMO) systems. Instead of applying the traditional methods, we propose two signal processing architectures: the centralized training and centralized execution with fuzzy logic as well as the centralized training and decentralized execution with fuzzy logic, respectively, which adopt the amalgamation of multi-agent reinforcement learning (MARL) and fuzzy logic to solve the design problem of power control for the maximization of the system spectral efficiency (SE). Furthermore, the uplink performance of the system adopting maximum ratio (MR) combining and local minimum mean-squared error (L-MMSE) combining is evaluated. Our results show that the proposed methods with fuzzy logic outperform the conventional MARL-based method and signal processing methods in terms of computational complexity. Also, the SE performance under MR combining is even better than that of the conventional MARL-based method.
<div id='section'>Paperid: <span id='pid'>168, <a href='https://arxiv.org/pdf/2503.20507.pdf' target='_blank'>https://arxiv.org/pdf/2503.20507.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rakesh Nadig, Vamanan Arulchelvan, Rahul Bera, Taha Shahroodi, Gagandeep Singh, Andreas Kakolyris, Mohammad Sadrosadati, Jisung Park, Onur Mutlu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.20507">Harmonia: A Multi-Agent Reinforcement Learning Approach to Data Placement and Migration in Hybrid Storage Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Hybrid storage systems (HSS) integrate multiple storage devices with diverse characteristics to deliver high performance and capacity at low cost. The performance of an HSS highly depends on the effectiveness of two key policies: (1) the data-placement policy, which determines the best-fit storage device for incoming data, and (2) the data-migration policy, which dynamically rearranges stored data (i.e., prefetches hot data and evicts cold data) across the devices to sustain high HSS performance. Prior works optimize either data placement or data migration in isolation, which leads to suboptimal HSS performance. Unfortunately, no prior work tries to optimize both policies together. Our goal is to design a holistic data-management technique that optimizes both data-placement and data-migration policies to fully exploit the potential of an HSS, and thus significantly improve system performance. We propose Harmonia, a multi-agent reinforcement learning (RL)-based data-management technique that employs two lightweight autonomous RL agents, a data-placement agent and a data-migration agent, that adapt their policies for the current workload and HSS configuration while coordinating with each other to improve overall HSS performance. We evaluate Harmonia on real HSS configurations with up to four heterogeneous storage devices and seventeen data-intensive workloads. On performance-optimized (cost-optimized) HSS with two storage devices, Harmonia outperforms the best-performing prior approach by 49.5% (31.7%) on average. On an HSS with three (four) devices, Harmonia outperforms the best-performing prior work by 37.0% (42.0%) on average. Harmonia's performance benefits come with low latency (240ns for inference) and storage overheads (206 KiB in DRAM for both RL agents combined). We will open-source Harmonia's implementation to aid future research on HSS.
<div id='section'>Paperid: <span id='pid'>169, <a href='https://arxiv.org/pdf/2503.20507.pdf' target='_blank'>https://arxiv.org/pdf/2503.20507.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rakesh Nadig, Vamanan Arulchelvan, Rahul Bera, Taha Shahroodi, Gagandeep Singh, Andreas Kakolyris, Mohammad Sadrosadati, Jisung Park, Onur Mutlu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.20507">Harmonia: A Multi-Agent Reinforcement Learning Approach to Data Placement and Migration in Hybrid Storage Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Hybrid storage systems (HSS) integrate multiple storage devices with diverse characteristics to deliver high performance and capacity at low cost. The performance of an HSS highly depends on the effectiveness of two key policies: (1) the data-placement policy, which determines the best-fit storage device for incoming data, and (2) the data-migration policy, which dynamically rearranges stored data (i.e., prefetches hot data and evicts cold data) across the devices to sustain high HSS performance. Prior works optimize either data placement or data migration in isolation, which leads to suboptimal HSS performance. Unfortunately, no prior work tries to optimize both policies together. Our goal is to design a holistic data-management technique that optimizes both data-placement and data-migration policies to fully exploit the potential of an HSS, and thus significantly improve system performance. We propose Harmonia, a multi-agent reinforcement learning (RL)-based data-management technique that employs two lightweight autonomous RL agents, a data-placement agent and a data-migration agent, that adapt their policies for the current workload and HSS configuration while coordinating with each other to improve overall HSS performance. We evaluate Harmonia on real HSS configurations with up to four heterogeneous storage devices and seventeen data-intensive workloads. On performance-optimized (cost-optimized) HSS with two storage devices, Harmonia outperforms the best-performing prior approach by 49.5% (31.7%) on average. On an HSS with three (four) devices, Harmonia outperforms the best-performing prior work by 37.0% (42.0%) on average. Harmonia's performance benefits come with low latency (240ns for inference) and storage overheads (206 KiB in DRAM for both RL agents combined). We will open-source Harmonia's implementation to aid future research on HSS.
<div id='section'>Paperid: <span id='pid'>170, <a href='https://arxiv.org/pdf/2505.24155.pdf' target='_blank'>https://arxiv.org/pdf/2505.24155.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ehtesamul Azim, Dongjie Wang, Tae Hyun Hwang, Yanjie Fu, Wei Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.24155">Biological Pathway Guided Gene Selection Through Collaborative Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Gene selection in high-dimensional genomic data is essential for understanding disease mechanisms and improving therapeutic outcomes. Traditional feature selection methods effectively identify predictive genes but often ignore complex biological pathways and regulatory networks, leading to unstable and biologically irrelevant signatures. Prior approaches, such as Lasso-based methods and statistical filtering, either focus solely on individual gene-outcome associations or fail to capture pathway-level interactions, presenting a key challenge: how to integrate biological pathway knowledge while maintaining statistical rigor in gene selection? To address this gap, we propose a novel two-stage framework that integrates statistical selection with biological pathway knowledge using multi-agent reinforcement learning (MARL). First, we introduce a pathway-guided pre-filtering strategy that leverages multiple statistical methods alongside KEGG pathway information for initial dimensionality reduction. Next, for refined selection, we model genes as collaborative agents in a MARL framework, where each agent optimizes both predictive power and biological relevance. Our framework incorporates pathway knowledge through Graph Neural Network-based state representations, a reward mechanism combining prediction performance with gene centrality and pathway coverage, and collaborative learning strategies using shared memory and a centralized critic component. Extensive experiments on multiple gene expression datasets demonstrate that our approach significantly improves both prediction accuracy and biological interpretability compared to traditional methods.
<div id='section'>Paperid: <span id='pid'>171, <a href='https://arxiv.org/pdf/2504.17355.pdf' target='_blank'>https://arxiv.org/pdf/2504.17355.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaohan Huang, Dongjie Wang, Zhiyuan Ning, Ziyue Qiao, Qingqing Long, Haowei Zhu, Yi Du, Min Wu, Yuanchun Zhou, Meng Xiao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.17355">Collaborative Multi-Agent Reinforcement Learning for Automated Feature Transformation with Graph-Driven Path Optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Feature transformation methods aim to find an optimal mathematical feature-feature crossing process that generates high-value features and improves the performance of downstream machine learning tasks. Existing frameworks, though designed to mitigate manual costs, often treat feature transformations as isolated operations, ignoring dynamic dependencies between transformation steps. To address the limitations, we propose TCTO, a collaborative multi-agent reinforcement learning framework that automates feature engineering through graph-driven path optimization. The framework's core innovation lies in an evolving interaction graph that models features as nodes and transformations as edges. Through graph pruning and backtracking, it dynamically eliminates low-impact edges, reduces redundant operations, and enhances exploration stability. This graph also provides full traceability to empower TCTO to reuse high-utility subgraphs from historical transformations. To demonstrate the efficacy and adaptability of our approach, we conduct comprehensive experiments and case studies, which show superior performance across a range of datasets.
<div id='section'>Paperid: <span id='pid'>172, <a href='https://arxiv.org/pdf/2305.12872.pdf' target='_blank'>https://arxiv.org/pdf/2305.12872.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Simin Li, Jun Guo, Jingqiao Xiu, Ruixiao Xu, Xin Yu, Jiakai Wang, Aishan Liu, Yaodong Yang, Xianglong Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.12872">Byzantine Robust Cooperative Multi-Agent Reinforcement Learning as a Bayesian Game</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this study, we explore the robustness of cooperative multi-agent reinforcement learning (c-MARL) against Byzantine failures, where any agent can enact arbitrary, worst-case actions due to malfunction or adversarial attack. To address the uncertainty that any agent can be adversarial, we propose a Bayesian Adversarial Robust Dec-POMDP (BARDec-POMDP) framework, which views Byzantine adversaries as nature-dictated types, represented by a separate transition. This allows agents to learn policies grounded on their posterior beliefs about the type of other agents, fostering collaboration with identified allies and minimizing vulnerability to adversarial manipulation. We define the optimal solution to the BARDec-POMDP as an ex post robust Bayesian Markov perfect equilibrium, which we proof to exist and weakly dominates the equilibrium of previous robust MARL approaches. To realize this equilibrium, we put forward a two-timescale actor-critic algorithm with almost sure convergence under specific conditions. Experimentation on matrix games, level-based foraging and StarCraft II indicate that, even under worst-case perturbations, our method successfully acquires intricate micromanagement skills and adaptively aligns with allies, demonstrating resilience against non-oblivious adversaries, random allies, observation-based attacks, and transfer-based attacks.
<div id='section'>Paperid: <span id='pid'>173, <a href='https://arxiv.org/pdf/2510.00477.pdf' target='_blank'>https://arxiv.org/pdf/2510.00477.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chengzhen Li, Likun Zhang, Chuang Zhang, Jiahui Li, Changyuan Zhao, Ruichen Zhang, Geng Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.00477">Wireless Laser Power Transfer for Low-altitude Uncrewed Aerial Vehicle-assisted Internet of Things: Paradigms, Challenges, and Solutions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Low-altitude uncrewed aerial vehicles (UAVs) have become integral enablers for the Internet of Things (IoT) by offering enhanced coverage, improved connectivity and access to remote areas. A critical challenge limiting their operational capacity lies in the energy constraints of both aerial platforms and ground-based sensors. This paper explores WLPT as a transformative solution for sustainable energy provisioning in UAV-assisted IoT networks. We first systematically investigate the fundamental principles of WLPT and analysis the comparative advantages. Then, we introduce three operational paradigms for system integration, identify key challenges, and discuss corresponding potential solutions. In case study, we propose a multi-agent reinforcement learning framework to address the coordination and optimization challenges in WLPT-enabled UAV-assisted IoT data collection. Simulation results demonstrate that our framework significantly improves energy sustainability and data freshness. Finally, we discuss some future directions.
<div id='section'>Paperid: <span id='pid'>174, <a href='https://arxiv.org/pdf/2510.00477.pdf' target='_blank'>https://arxiv.org/pdf/2510.00477.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chengzhen Li, Likun Zhang, Chuang Zhang, Jiahui Li, Changyuan Zhao, Ruichen Zhang, Geng Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.00477">Wireless Laser Power Transfer for Low-altitude Uncrewed Aerial Vehicle-assisted Internet of Things: Paradigms, Challenges, and Solutions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Low-altitude uncrewed aerial vehicles (UAVs) have become integral enablers for the Internet of Things (IoT) by offering enhanced coverage, improved connectivity and access to remote areas. A critical challenge limiting their operational capacity lies in the energy constraints of both aerial platforms and ground-based sensors. This paper explores WLPT as a transformative solution for sustainable energy provisioning in UAV-assisted IoT networks. We first systematically investigate the fundamental principles of WLPT and analysis the comparative advantages. Then, we introduce three operational paradigms for system integration, identify key challenges, and discuss corresponding potential solutions. In case study, we propose a multi-agent reinforcement learning framework to address the coordination and optimization challenges in WLPT-enabled UAV-assisted IoT data collection. Simulation results demonstrate that our framework significantly improves energy sustainability and data freshness. Finally, we discuss some future directions.
<div id='section'>Paperid: <span id='pid'>175, <a href='https://arxiv.org/pdf/2505.04339.pdf' target='_blank'>https://arxiv.org/pdf/2505.04339.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao Peng, Xiang Huang, Shuo Sun, Ruitong Zhang, Philip S. Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.04339">Adaptive and Robust DBSCAN with Multi-agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>DBSCAN, a well-known density-based clustering algorithm, has gained widespread popularity and usage due to its effectiveness in identifying clusters of arbitrary shapes and handling noisy data. However, it encounters challenges in producing satisfactory cluster results when confronted with datasets of varying density scales, a common scenario in real-world applications. In this paper, we propose a novel Adaptive and Robust DBSCAN with Multi-agent Reinforcement Learning cluster framework, namely AR-DBSCAN. First, we model the initial dataset as a two-level encoding tree and categorize the data vertices into distinct density partitions according to the information uncertainty determined in the encoding tree. Each partition is then assigned to an agent to find the best clustering parameters without manual assistance. The allocation is density-adaptive, enabling AR-DBSCAN to effectively handle diverse density distributions within the dataset by utilizing distinct agents for different partitions. Second, a multi-agent deep reinforcement learning guided automatic parameter searching process is designed. The process of adjusting the parameter search direction by perceiving the clustering environment is modeled as a Markov decision process. Using a weakly-supervised reward training policy network, each agent adaptively learns the optimal clustering parameters by interacting with the clusters. Third, a recursive search mechanism adaptable to the data's scale is presented, enabling efficient and controlled exploration of large parameter spaces. Extensive experiments are conducted on nine artificial datasets and a real-world dataset. The results of offline and online tasks show that AR-DBSCAN not only improves clustering accuracy by up to 144.1% and 175.3% in the NMI and ARI metrics, respectively, but also is capable of robustly finding dominant parameters.
<div id='section'>Paperid: <span id='pid'>176, <a href='https://arxiv.org/pdf/2409.14177.pdf' target='_blank'>https://arxiv.org/pdf/2409.14177.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhihao Lin, Wei Ma, Mingyi Zhou, Yanjie Zhao, Haoyu Wang, Yang Liu, Jun Wang, Li Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.14177">PathSeeker: Exploring LLM Security Vulnerabilities with a Reinforcement Learning-Based Jailbreak Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, Large Language Models (LLMs) have gained widespread use, raising concerns about their security. Traditional jailbreak attacks, which often rely on the model internal information or have limitations when exploring the unsafe behavior of the victim model, limiting their reducing their general applicability. In this paper, we introduce PathSeeker, a novel black-box jailbreak method, which is inspired by the game of rats escaping a maze. We think that each LLM has its unique "security maze", and attackers attempt to find the exit learning from the received feedback and their accumulated experience to compromise the target LLM's security defences. Our approach leverages multi-agent reinforcement learning, where smaller models collaborate to guide the main LLM in performing mutation operations to achieve the attack objectives. By progressively modifying inputs based on the model's feedback, our system induces richer, harmful responses. During our manual attempts to perform jailbreak attacks, we found that the vocabulary of the response of the target model gradually became richer and eventually produced harmful responses. Based on the observation, we also introduce a reward mechanism that exploits the expansion of vocabulary richness in LLM responses to weaken security constraints. Our method outperforms five state-of-the-art attack techniques when tested across 13 commercial and open-source LLMs, achieving high attack success rates, especially in strongly aligned commercial models like GPT-4o-mini, Claude-3.5, and GLM-4-air with strong safety alignment. This study aims to improve the understanding of LLM security vulnerabilities and we hope that this sturdy can contribute to the development of more robust defenses.
<div id='section'>Paperid: <span id='pid'>177, <a href='https://arxiv.org/pdf/2311.07127.pdf' target='_blank'>https://arxiv.org/pdf/2311.07127.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shijie Wang, Wenqi Fan, Xiao-yong Wei, Xiaowei Mei, Shanru Lin, Qing Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.07127">Multi-agent Attacks for Black-box Social Recommendations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rise of online social networks has facilitated the evolution of social recommender systems, which incorporate social relations to enhance users' decision-making process. With the great success of Graph Neural Networks (GNNs) in learning node representations, GNN-based social recommendations have been widely studied to model user-item interactions and user-user social relations simultaneously. Despite their great successes, recent studies have shown that these advanced recommender systems are highly vulnerable to adversarial attacks, in which attackers can inject well-designed fake user profiles to disrupt recommendation performances. While most existing studies mainly focus on argeted attacks to promote target items on vanilla recommender systems, untargeted attacks to degrade the overall prediction performance are less explored on social recommendations under a black-box scenario. To perform untargeted attacks on social recommender systems, attackers can construct malicious social relationships for fake users to enhance the attack performance. However, the coordination of social relations and item profiles is challenging for attacking black-box social recommendations. To address this limitation, we first conduct several preliminary studies to demonstrate the effectiveness of cross-community connections and cold-start items in degrading recommendations performance. Specifically, we propose a novel framework MultiAttack based on multi-agent reinforcement learning to coordinate the generation of cold-start item profiles and cross-community social relations for conducting untargeted attacks on black-box social recommendations. Comprehensive experiments on various real-world datasets demonstrate the effectiveness of our proposed attacking framework under the black-box setting.
<div id='section'>Paperid: <span id='pid'>178, <a href='https://arxiv.org/pdf/2310.06243.pdf' target='_blank'>https://arxiv.org/pdf/2310.06243.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nuoya Xiong, Zhihan Liu, Zhaoran Wang, Zhuoran Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.06243">Sample-Efficient Multi-Agent RL: An Optimization Perspective</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We study multi-agent reinforcement learning (MARL) for the general-sum Markov Games (MGs) under the general function approximation. In order to find the minimum assumption for sample-efficient learning, we introduce a novel complexity measure called the Multi-Agent Decoupling Coefficient (MADC) for general-sum MGs. Using this measure, we propose the first unified algorithmic framework that ensures sample efficiency in learning Nash Equilibrium, Coarse Correlated Equilibrium, and Correlated Equilibrium for both model-based and model-free MARL problems with low MADC. We also show that our algorithm provides comparable sublinear regret to the existing works. Moreover, our algorithm combines an equilibrium-solving oracle with a single objective optimization subprocedure that solves for the regularized payoff of each deterministic joint policy, which avoids solving constrained optimization problems within data-dependent constraints (Jin et al. 2020; Wang et al. 2023) or executing sampling procedures with complex multi-objective optimization problems (Foster et al. 2023), thus being more amenable to empirical implementation.
<div id='section'>Paperid: <span id='pid'>179, <a href='https://arxiv.org/pdf/2306.00212.pdf' target='_blank'>https://arxiv.org/pdf/2306.00212.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dongsheng Ding, Xiaohan Wei, Zhuoran Yang, Zhaoran Wang, Mihailo R. JovanoviÄ
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.00212">Provably Efficient Generalized Lagrangian Policy Optimization for Safe Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We examine online safe multi-agent reinforcement learning using constrained Markov games in which agents compete by maximizing their expected total rewards under a constraint on expected total utilities. Our focus is confined to an episodic two-player zero-sum constrained Markov game with independent transition functions that are unknown to agents, adversarial reward functions, and stochastic utility functions. For such a Markov game, we employ an approach based on the occupancy measure to formulate it as an online constrained saddle-point problem with an explicit constraint. We extend the Lagrange multiplier method in constrained optimization to handle the constraint by creating a generalized Lagrangian with minimax decision primal variables and a dual variable. Next, we develop an upper confidence reinforcement learning algorithm to solve this Lagrangian problem while balancing exploration and exploitation. Our algorithm updates the minimax decision primal variables via online mirror descent and the dual variable via projected gradient step and we prove that it enjoys sublinear rate $ O((|X|+|Y|) L \sqrt{T(|A|+|B|)}))$ for both regret and constraint violation after playing $T$ episodes of the game. Here, $L$ is the horizon of each episode, $(|X|,|A|)$ and $(|Y|,|B|)$ are the state/action space sizes of the min-player and the max-player, respectively. To the best of our knowledge, we provide the first provably efficient online safe reinforcement learning algorithm in constrained Markov games.
<div id='section'>Paperid: <span id='pid'>180, <a href='https://arxiv.org/pdf/2305.04819.pdf' target='_blank'>https://arxiv.org/pdf/2305.04819.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yulai Zhao, Zhuoran Yang, Zhaoran Wang, Jason D. Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.04819">Local Optimization Achieves Global Optimality in Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Policy optimization methods with function approximation are widely used in multi-agent reinforcement learning. However, it remains elusive how to design such algorithms with statistical guarantees. Leveraging a multi-agent performance difference lemma that characterizes the landscape of multi-agent policy optimization, we find that the localized action value function serves as an ideal descent direction for each local policy. Motivated by the observation, we present a multi-agent PPO algorithm in which the local policy of each agent is updated similarly to vanilla PPO. We prove that with standard regularity conditions on the Markov game and problem-dependent quantities, our algorithm converges to the globally optimal policy at a sublinear rate. We extend our algorithm to the off-policy setting and introduce pessimism to policy evaluation, which aligns with experiments. To our knowledge, this is the first provably convergent multi-agent PPO algorithm in cooperative Markov games.
<div id='section'>Paperid: <span id='pid'>181, <a href='https://arxiv.org/pdf/2507.19050.pdf' target='_blank'>https://arxiv.org/pdf/2507.19050.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qiong Wu, Yu Xie, Pingyi Fan, Dong Qin, Kezhi Wang, Nan Cheng, Khaled B. Letaief
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.19050">Large Language Model-Based Task Offloading and Resource Allocation for Digital Twin Edge Computing Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we propose a general digital twin edge computing network comprising multiple vehicles and a server. Each vehicle generates multiple computing tasks within a time slot, leading to queuing challenges when offloading tasks to the server. The study investigates task offloading strategies, queue stability, and resource allocation. Lyapunov optimization is employed to transform long-term constraints into tractable short-term decisions. To solve the resulting problem, an in-context learning approach based on large language model (LLM) is adopted, replacing the conventional multi-agent reinforcement learning (MARL) framework. Experimental results demonstrate that the LLM-based method achieves comparable or even superior performance to MARL.
<div id='section'>Paperid: <span id='pid'>182, <a href='https://arxiv.org/pdf/2404.12090.pdf' target='_blank'>https://arxiv.org/pdf/2404.12090.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoyuan Jiang, Ziyue Li, Hua Wei, Xuantang Xiong, Jingqing Ruan, Jiaming Lu, Hangyu Mao, Rui Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.12090">X-Light: Cross-City Traffic Signal Control Using Transformer on Transformer as Meta Multi-Agent Reinforcement Learner</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The effectiveness of traffic light control has been significantly improved by current reinforcement learning-based approaches via better cooperation among multiple traffic lights. However, a persisting issue remains: how to obtain a multi-agent traffic signal control algorithm with remarkable transferability across diverse cities? In this paper, we propose a Transformer on Transformer (TonT) model for cross-city meta multi-agent traffic signal control, named as X-Light: We input the full Markov Decision Process trajectories, and the Lower Transformer aggregates the states, actions, rewards among the target intersection and its neighbors within a city, and the Upper Transformer learns the general decision trajectories across different cities. This dual-level approach bolsters the model's robust generalization and transferability. Notably, when directly transferring to unseen scenarios, ours surpasses all baseline methods with +7.91% on average, and even +16.3% in some cases, yielding the best results.
<div id='section'>Paperid: <span id='pid'>183, <a href='https://arxiv.org/pdf/2309.12951.pdf' target='_blank'>https://arxiv.org/pdf/2309.12951.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yan Song, He Jiang, Haifeng Zhang, Zheng Tian, Weinan Zhang, Jun Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.12951">Boosting Studies of Multi-Agent Reinforcement Learning on Google Research Football Environment: the Past, Present, and Future</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Even though Google Research Football (GRF) was initially benchmarked and studied as a single-agent environment in its original paper, recent years have witnessed an increasing focus on its multi-agent nature by researchers utilizing it as a testbed for Multi-Agent Reinforcement Learning (MARL). However, the absence of standardized environment settings and unified evaluation metrics for multi-agent scenarios hampers the consistent understanding of various studies. Furthermore, the challenging 5-vs-5 and 11-vs-11 full-game scenarios have received limited thorough examination due to their substantial training complexities. To address these gaps, this paper extends the original environment by not only standardizing the environment settings and benchmarking cooperative learning algorithms across different scenarios, including the most challenging full-game scenarios, but also by discussing approaches to enhance football AI from diverse perspectives and introducing related research tools. Specifically, we provide a distributed and asynchronous population-based self-play framework with diverse pre-trained policies for faster training, two football-specific analytical tools for deeper investigation, and an online leaderboard for broader evaluation. The overall expectation of this work is to advance the study of Multi-Agent Reinforcement Learning on Google Research Football environment, with the ultimate goal of benefiting real-world sports beyond virtual games.
<div id='section'>Paperid: <span id='pid'>184, <a href='https://arxiv.org/pdf/2305.17352.pdf' target='_blank'>https://arxiv.org/pdf/2305.17352.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yihe Zhou, Shunyu Liu, Yunpeng Qing, Kaixuan Chen, Tongya Zheng, Jie Song, Mingli Song
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.17352">Is Centralized Training with Decentralized Execution Framework Centralized Enough for MARL?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Centralized Training with Decentralized Execution (CTDE) has recently emerged as a popular framework for cooperative Multi-Agent Reinforcement Learning (MARL), where agents can use additional global state information to guide training in a centralized way and make their own decisions only based on decentralized local policies. Despite the encouraging results achieved, CTDE makes an independence assumption on agent policies, which limits agents to adopt global cooperative information from each other during centralized training. Therefore, we argue that existing CTDE methods cannot fully utilize global information for training, leading to an inefficient joint-policy exploration and even suboptimal results. In this paper, we introduce a novel Centralized Advising and Decentralized Pruning (CADP) framework for multi-agent reinforcement learning, that not only enables an efficacious message exchange among agents during training but also guarantees the independent policies for execution. Firstly, CADP endows agents the explicit communication channel to seek and take advices from different agents for more centralized training. To further ensure the decentralized execution, we propose a smooth model pruning mechanism to progressively constraint the agent communication into a closed one without degradation in agent cooperation capability. Empirical evaluations on StarCraft II micromanagement and Google Research Football benchmarks demonstrate that the proposed framework achieves superior performance compared with the state-of-the-art counterparts. Our code will be made publicly available.
<div id='section'>Paperid: <span id='pid'>185, <a href='https://arxiv.org/pdf/2503.20685.pdf' target='_blank'>https://arxiv.org/pdf/2503.20685.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuhao Huang, Ao Chang, Haoran Dou, Xing Tao, Xinrui Zhou, Yan Cao, Ruobing Huang, Alejandro F Frangi, Lingyun Bao, Xin Yang, Dong Ni
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.20685">Flip Learning: Weakly Supervised Erase to Segment Nodules in Breast Ultrasound</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate segmentation of nodules in both 2D breast ultrasound (BUS) and 3D automated breast ultrasound (ABUS) is crucial for clinical diagnosis and treatment planning. Therefore, developing an automated system for nodule segmentation can enhance user independence and expedite clinical analysis. Unlike fully-supervised learning, weakly-supervised segmentation (WSS) can streamline the laborious and intricate annotation process. However, current WSS methods face challenges in achieving precise nodule segmentation, as many of them depend on inaccurate activation maps or inefficient pseudo-mask generation algorithms. In this study, we introduce a novel multi-agent reinforcement learning-based WSS framework called Flip Learning, which relies solely on 2D/3D boxes for accurate segmentation. Specifically, multiple agents are employed to erase the target from the box to facilitate classification tag flipping, with the erased region serving as the predicted segmentation mask. The key contributions of this research are as follows: (1) Adoption of a superpixel/supervoxel-based approach to encode the standardized environment, capturing boundary priors and expediting the learning process. (2) Introduction of three meticulously designed rewards, comprising a classification score reward and two intensity distribution rewards, to steer the agents' erasing process precisely, thereby avoiding both under- and over-segmentation. (3) Implementation of a progressive curriculum learning strategy to enable agents to interact with the environment in a progressively challenging manner, thereby enhancing learning efficiency. Extensively validated on the large in-house BUS and ABUS datasets, our Flip Learning method outperforms state-of-the-art WSS methods and foundation models, and achieves comparable performance as fully-supervised learning algorithms.
<div id='section'>Paperid: <span id='pid'>186, <a href='https://arxiv.org/pdf/2503.20685.pdf' target='_blank'>https://arxiv.org/pdf/2503.20685.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuhao Huang, Ao Chang, Haoran Dou, Xing Tao, Xinrui Zhou, Yan Cao, Ruobing Huang, Alejandro F Frangi, Lingyun Bao, Xin Yang, Dong Ni
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.20685">Flip Learning: Weakly Supervised Erase to Segment Nodules in Breast Ultrasound</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate segmentation of nodules in both 2D breast ultrasound (BUS) and 3D automated breast ultrasound (ABUS) is crucial for clinical diagnosis and treatment planning. Therefore, developing an automated system for nodule segmentation can enhance user independence and expedite clinical analysis. Unlike fully-supervised learning, weakly-supervised segmentation (WSS) can streamline the laborious and intricate annotation process. However, current WSS methods face challenges in achieving precise nodule segmentation, as many of them depend on inaccurate activation maps or inefficient pseudo-mask generation algorithms. In this study, we introduce a novel multi-agent reinforcement learning-based WSS framework called Flip Learning, which relies solely on 2D/3D boxes for accurate segmentation. Specifically, multiple agents are employed to erase the target from the box to facilitate classification tag flipping, with the erased region serving as the predicted segmentation mask. The key contributions of this research are as follows: (1) Adoption of a superpixel/supervoxel-based approach to encode the standardized environment, capturing boundary priors and expediting the learning process. (2) Introduction of three meticulously designed rewards, comprising a classification score reward and two intensity distribution rewards, to steer the agents' erasing process precisely, thereby avoiding both under- and over-segmentation. (3) Implementation of a progressive curriculum learning strategy to enable agents to interact with the environment in a progressively challenging manner, thereby enhancing learning efficiency. Extensively validated on the large in-house BUS and ABUS datasets, our Flip Learning method outperforms state-of-the-art WSS methods and foundation models, and achieves comparable performance as fully-supervised learning algorithms.
<div id='section'>Paperid: <span id='pid'>187, <a href='https://arxiv.org/pdf/2510.01586.pdf' target='_blank'>https://arxiv.org/pdf/2510.01586.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhenyu Pan, Yiting Zhang, Zhuo Liu, Yolo Yunlong Tang, Zeliang Zhang, Haozheng Luo, Yuwei Han, Jianshu Zhang, Dennis Wu, Hong-Yu Chen, Haoran Lu, Haoyang Fang, Manling Li, Chenliang Xu, Philip S. Yu, Han Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.01586">AdvEvo-MARL: Shaping Internalized Safety through Adversarial Co-Evolution in Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>LLM-based multi-agent systems excel at planning, tool use, and role coordination, but their openness and interaction complexity also expose them to jailbreak, prompt-injection, and adversarial collaboration. Existing defenses fall into two lines: (i) self-verification that asks each agent to pre-filter unsafe instructions before execution, and (ii) external guard modules that police behaviors. The former often underperforms because a standalone agent lacks sufficient capacity to detect cross-agent unsafe chains and delegation-induced risks; the latter increases system overhead and creates a single-point-of-failure-once compromised, system-wide safety collapses, and adding more guards worsens cost and complexity. To solve these challenges, we propose AdvEvo-MARL, a co-evolutionary multi-agent reinforcement learning framework that internalizes safety into task agents. Rather than relying on external guards, AdvEvo-MARL jointly optimizes attackers (which synthesize evolving jailbreak prompts) and defenders (task agents trained to both accomplish their duties and resist attacks) in adversarial learning environments. To stabilize learning and foster cooperation, we introduce a public baseline for advantage estimation: agents within the same functional group share a group-level mean-return baseline, enabling lower-variance updates and stronger intra-group coordination. Across representative attack scenarios, AdvEvo-MARL consistently keeps attack-success rate (ASR) below 20%, whereas baselines reach up to 38.33%, while preserving-and sometimes improving-task accuracy (up to +3.67% on reasoning tasks). These results show that safety and utility can be jointly improved without relying on extra guard agents or added system overhead.
<div id='section'>Paperid: <span id='pid'>188, <a href='https://arxiv.org/pdf/2510.01586.pdf' target='_blank'>https://arxiv.org/pdf/2510.01586.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhenyu Pan, Yiting Zhang, Zhuo Liu, Yolo Yunlong Tang, Zeliang Zhang, Haozheng Luo, Yuwei Han, Jianshu Zhang, Dennis Wu, Hong-Yu Chen, Haoran Lu, Haoyang Fang, Manling Li, Chenliang Xu, Philip S. Yu, Han Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.01586">AdvEvo-MARL: Shaping Internalized Safety through Adversarial Co-Evolution in Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>LLM-based multi-agent systems excel at planning, tool use, and role coordination, but their openness and interaction complexity also expose them to jailbreak, prompt-injection, and adversarial collaboration. Existing defenses fall into two lines: (i) self-verification that asks each agent to pre-filter unsafe instructions before execution, and (ii) external guard modules that police behaviors. The former often underperforms because a standalone agent lacks sufficient capacity to detect cross-agent unsafe chains and delegation-induced risks; the latter increases system overhead and creates a single-point-of-failure-once compromised, system-wide safety collapses, and adding more guards worsens cost and complexity. To solve these challenges, we propose AdvEvo-MARL, a co-evolutionary multi-agent reinforcement learning framework that internalizes safety into task agents. Rather than relying on external guards, AdvEvo-MARL jointly optimizes attackers (which synthesize evolving jailbreak prompts) and defenders (task agents trained to both accomplish their duties and resist attacks) in adversarial learning environments. To stabilize learning and foster cooperation, we introduce a public baseline for advantage estimation: agents within the same functional group share a group-level mean-return baseline, enabling lower-variance updates and stronger intra-group coordination. Across representative attack scenarios, AdvEvo-MARL consistently keeps attack-success rate (ASR) below 20%, whereas baselines reach up to 38.33%, while preserving-and sometimes improving-task accuracy (up to +3.67% on reasoning tasks). These results show that safety and utility can be jointly improved without relying on extra guard agents or added system overhead.
<div id='section'>Paperid: <span id='pid'>189, <a href='https://arxiv.org/pdf/2506.02718.pdf' target='_blank'>https://arxiv.org/pdf/2506.02718.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Guanzhong Chen, Shaoxiong Yang, Chao Li, Wei Liu, Jian Luan, Zenglin Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.02718">Heterogeneous Group-Based Reinforcement Learning for LLM-based Multi-Agent Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large Language Models (LLMs) have achieved remarkable success across diverse natural language processing tasks, yet their deployment in real-world applications is hindered by fixed knowledge cutoffs and difficulties in generating controllable, accurate outputs in a single inference. Multi-agent systems (MAS) built from specialized LLM agents offer a promising solution, enabling dynamic collaboration and iterative reasoning. However, optimizing these systems remains a challenge, as conventional methods such as prompt engineering and supervised fine-tuning entail high engineering overhead and limited adaptability. Reinforcement learning (RL), particularly multi-agent reinforcement learning (MARL), provides a scalable framework by refining agent policies based on system-level feedback. Nevertheless, existing MARL algorithms, such as Multi-Agent Proximal Policy Optimization (MAPPO), rely on Critic networks, which can cause training instability and increase computational burden. To address these limitations and target the prototypical Multi-Agent Search System (MASS), we propose Multi-Agent Heterogeneous Group Policy Optimization (MHGPO), a novel Critic-free algorithm that guides policy updates by estimating relative reward advantages across heterogeneous groups of rollouts. MHGPO eliminates the need for Critic networks, enhancing stability and reducing computational overhead. Additionally, we introduce three group rollout sampling strategies that trade off between efficiency and effectiveness. Experiments on a multi-agent LLM-based search system demonstrate that MHGPO consistently outperforms MAPPO in both task performance and computational efficiency, without requiring warm-up, underscoring its potential for stable and scalable optimization of complex LLM-based MAS.
<div id='section'>Paperid: <span id='pid'>190, <a href='https://arxiv.org/pdf/2505.06706.pdf' target='_blank'>https://arxiv.org/pdf/2505.06706.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuxuan Zheng, Yihe Zhou, Feiyang Xu, Mingli Song, Shunyu Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.06706">Bi-level Mean Field: Dynamic Grouping for Large-Scale MARL</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large-scale Multi-Agent Reinforcement Learning (MARL) often suffers from the curse of dimensionality, as the exponential growth in agent interactions significantly increases computational complexity and impedes learning efficiency. To mitigate this, existing efforts that rely on Mean Field (MF) simplify the interaction landscape by approximating neighboring agents as a single mean agent, thus reducing overall complexity to pairwise interactions. However, these MF methods inevitably fail to account for individual differences, leading to aggregation noise caused by inaccurate iterative updates during MF learning. In this paper, we propose a Bi-level Mean Field (BMF) method to capture agent diversity with dynamic grouping in large-scale MARL, which can alleviate aggregation noise via bi-level interaction. Specifically, BMF introduces a dynamic group assignment module, which employs a Variational AutoEncoder (VAE) to learn the representations of agents, facilitating their dynamic grouping over time. Furthermore, we propose a bi-level interaction module to model both inter- and intra-group interactions for effective neighboring aggregation. Experiments across various tasks demonstrate that the proposed BMF yields results superior to the state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>191, <a href='https://arxiv.org/pdf/2501.01266.pdf' target='_blank'>https://arxiv.org/pdf/2501.01266.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Michael KÃ¶lle, Johannes Tochtermann, Julian SchÃ¶nberger, Gerhard Stenzel, Philipp Altmann, Claudia Linnhoff-Popien
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.01266">PIMAEX: Multi-Agent Exploration through Peer Incentivization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While exploration in single-agent reinforcement learning has been studied extensively in recent years, considerably less work has focused on its counterpart in multi-agent reinforcement learning. To address this issue, this work proposes a peer-incentivized reward function inspired by previous research on intrinsic curiosity and influence-based rewards. The \textit{PIMAEX} reward, short for Peer-Incentivized Multi-Agent Exploration, aims to improve exploration in the multi-agent setting by encouraging agents to exert influence over each other to increase the likelihood of encountering novel states. We evaluate the \textit{PIMAEX} reward in conjunction with \textit{PIMAEX-Communication}, a multi-agent training algorithm that employs a communication channel for agents to influence one another. The evaluation is conducted in the \textit{Consume/Explore} environment, a partially observable environment with deceptive rewards, specifically designed to challenge the exploration vs.\ exploitation dilemma and the credit-assignment problem. The results empirically demonstrate that agents using the \textit{PIMAEX} reward with \textit{PIMAEX-Communication} outperform those that do not.
<div id='section'>Paperid: <span id='pid'>192, <a href='https://arxiv.org/pdf/2407.20739.pdf' target='_blank'>https://arxiv.org/pdf/2407.20739.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Michael KÃ¶lle, Karola Schneider, Sabrina Egger, Felix Topp, Thomy Phan, Philipp Altmann, Jonas NÃ¼Ãlein, Claudia Linnhoff-Popien
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.20739">Architectural Influence on Variational Quantum Circuits in Multi-Agent Reinforcement Learning: Evolutionary Strategies for Optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, Multi-Agent Reinforcement Learning (MARL) has found application in numerous areas of science and industry, such as autonomous driving, telecommunications, and global health. Nevertheless, MARL suffers from, for instance, an exponential growth of dimensions. Inherent properties of quantum mechanics help to overcome these limitations, e.g., by significantly reducing the number of trainable parameters. Previous studies have developed an approach that uses gradient-free quantum Reinforcement Learning and evolutionary optimization for variational quantum circuits (VQCs) to reduce the trainable parameters and avoid barren plateaus as well as vanishing gradients. This leads to a significantly better performance of VQCs compared to classical neural networks with a similar number of trainable parameters and a reduction in the number of parameters by more than 97 \% compared to similarly good neural networks. We extend an approach of KÃ¶lle et al. by proposing a Gate-Based, a Layer-Based, and a Prototype-Based concept to mutate and recombine VQCs. Our results show the best performance for mutation-only strategies and the Gate-Based approach. In particular, we observe a significantly better score, higher total and own collected coins, as well as a superior own coin rate for the best agent when evaluated in the Coin Game environment.
<div id='section'>Paperid: <span id='pid'>193, <a href='https://arxiv.org/pdf/2401.07056.pdf' target='_blank'>https://arxiv.org/pdf/2401.07056.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Michael KÃ¶lle, Yannick Erpelding, Fabian Ritz, Thomy Phan, Steffen Illium, Claudia Linnhoff-Popien
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.07056">Aquarium: A Comprehensive Framework for Exploring Predator-Prey Dynamics through Multi-Agent Reinforcement Learning Algorithms</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in Multi-Agent Reinforcement Learning have prompted the modeling of intricate interactions between agents in simulated environments. In particular, the predator-prey dynamics have captured substantial interest and various simulations been tailored to unique requirements. To prevent further time-intensive developments, we introduce Aquarium, a comprehensive Multi-Agent Reinforcement Learning environment for predator-prey interaction, enabling the study of emergent behavior. Aquarium is open source and offers a seamless integration of the PettingZoo framework, allowing a quick start with proven algorithm implementations. It features physics-based agent movement on a two-dimensional, edge-wrapping plane. The agent-environment interaction (observations, actions, rewards) and the environment settings (agent speed, prey reproduction, predator starvation, and others) are fully customizable. Besides a resource-efficient visualization, Aquarium supports to record video files, providing a visual comprehension of agent behavior. To demonstrate the environment's capabilities, we conduct preliminary studies which use PPO to train multiple prey agents to evade a predator. In accordance to the literature, we find Individual Learning to result in worse performance than Parameter Sharing, which significantly improves coordination and sample-efficiency.
<div id='section'>Paperid: <span id='pid'>194, <a href='https://arxiv.org/pdf/2401.03504.pdf' target='_blank'>https://arxiv.org/pdf/2401.03504.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Robert MÃ¼ller, Hasan Turalic, Thomy Phan, Michael KÃ¶lle, Jonas NÃ¼Ãlein, Claudia Linnhoff-Popien
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.03504">ClusterComm: Discrete Communication in Decentralized MARL using Internal Representation Clustering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the realm of Multi-Agent Reinforcement Learning (MARL), prevailing approaches exhibit shortcomings in aligning with human learning, robustness, and scalability. Addressing this, we introduce ClusterComm, a fully decentralized MARL framework where agents communicate discretely without a central control unit. ClusterComm utilizes Mini-Batch-K-Means clustering on the last hidden layer's activations of an agent's policy network, translating them into discrete messages. This approach outperforms no communication and competes favorably with unbounded, continuous communication and hence poses a simple yet effective strategy for enhancing collaborative task-solving in MARL.
<div id='section'>Paperid: <span id='pid'>195, <a href='https://arxiv.org/pdf/2311.05546.pdf' target='_blank'>https://arxiv.org/pdf/2311.05546.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Michael KÃ¶lle, Felix Topp, Thomy Phan, Philipp Altmann, Jonas NÃ¼Ãlein, Claudia Linnhoff-Popien
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.05546">Multi-Agent Quantum Reinforcement Learning using Evolutionary Optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-Agent Reinforcement Learning is becoming increasingly more important in times of autonomous driving and other smart industrial applications. Simultaneously a promising new approach to Reinforcement Learning arises using the inherent properties of quantum mechanics, reducing the trainable parameters of a model significantly. However, gradient-based Multi-Agent Quantum Reinforcement Learning methods often have to struggle with barren plateaus, holding them back from matching the performance of classical approaches. While gradient free Quantum Reinforcement Learning methods may alleviate some of these challenges, they too are not immune to the difficulties posed by barren plateaus. We build upon an existing approach for gradient free Quantum Reinforcement Learning and propose three genetic variations with Variational Quantum Circuits for Multi-Agent Reinforcement Learning using evolutionary optimization. We evaluate our genetic variations in the Coin Game environment and also compare them to classical approaches. We showed that our Variational Quantum Circuit approaches perform significantly better compared to a neural network with a similar amount of trainable parameters. Compared to the larger neural network, our approaches archive similar results using $97.88\%$ less parameters.
<div id='section'>Paperid: <span id='pid'>196, <a href='https://arxiv.org/pdf/2301.01649.pdf' target='_blank'>https://arxiv.org/pdf/2301.01649.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Thomy Phan, Fabian Ritz, Philipp Altmann, Maximilian Zorn, Jonas NÃ¼Ãlein, Michael KÃ¶lle, Thomas Gabor, Claudia Linnhoff-Popien
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.01649">Attention-Based Recurrence for Multi-Agent Reinforcement Learning under Stochastic Partial Observability</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Stochastic partial observability poses a major challenge for decentralized coordination in multi-agent reinforcement learning but is largely neglected in state-of-the-art research due to a strong focus on state-based centralized training for decentralized execution (CTDE) and benchmarks that lack sufficient stochasticity like StarCraft Multi-Agent Challenge (SMAC). In this paper, we propose Attention-based Embeddings of Recurrence In multi-Agent Learning (AERIAL) to approximate value functions under stochastic partial observability. AERIAL replaces the true state with a learned representation of multi-agent recurrence, considering more accurate information about decentralized agent decisions than state-based CTDE. We then introduce MessySMAC, a modified version of SMAC with stochastic observations and higher variance in initial states, to provide a more general and configurable benchmark regarding stochastic partial observability. We evaluate AERIAL in Dec-Tiger as well as in a variety of SMAC and MessySMAC maps, and compare the results with state-based CTDE. Furthermore, we evaluate the robustness of AERIAL and state-based CTDE against various stochasticity configurations in MessySMAC.
<div id='section'>Paperid: <span id='pid'>197, <a href='https://arxiv.org/pdf/2506.07468.pdf' target='_blank'>https://arxiv.org/pdf/2506.07468.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mickel Liu, Liwei Jiang, Yancheng Liang, Simon Shaolei Du, Yejin Choi, Tim Althoff, Natasha Jaques
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.07468">Chasing Moving Targets with Online Self-Play Reinforcement Learning for Safer Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Conventional language model (LM) safety alignment relies on a reactive, disjoint procedure: attackers exploit a static model, followed by defensive fine-tuning to patch exposed vulnerabilities. This sequential approach creates a mismatch -- attackers overfit to obsolete defenses, while defenders perpetually lag behind emerging threats. To address this, we propose Self-RedTeam, an online self-play reinforcement learning algorithm where an attacker and defender agent co-evolve through continuous interaction. We cast safety alignment as a two-player zero-sum game, where a single model alternates between attacker and defender roles -- generating adversarial prompts and safeguarding against them -- while a reward LM adjudicates outcomes. This enables dynamic co-adaptation. Grounded in the game-theoretic framework of zero-sum games, we establish a theoretical safety guarantee which motivates the design of our method: if self-play converges to a Nash Equilibrium, the defender will reliably produce safe responses to any adversarial input. Empirically, Self-RedTeam uncovers more diverse attacks (+21.8% SBERT) compared to attackers trained against static defenders and achieves higher robustness on safety benchmarks (e.g., +65.5% on WildJailBreak) than defenders trained against static attackers. We further propose hidden Chain-of-Thought, allowing agents to plan privately, which boosts adversarial diversity and reduces over-refusals. Our results motivate a shift from reactive patching to proactive co-evolution in LM safety training, enabling scalable, autonomous, and robust self-improvement of LMs via multi-agent reinforcement learning (MARL).
<div id='section'>Paperid: <span id='pid'>198, <a href='https://arxiv.org/pdf/2506.07468.pdf' target='_blank'>https://arxiv.org/pdf/2506.07468.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mickel Liu, Liwei Jiang, Yancheng Liang, Simon Shaolei Du, Yejin Choi, Tim Althoff, Natasha Jaques
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.07468">Chasing Moving Targets with Online Self-Play Reinforcement Learning for Safer Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Conventional language model (LM) safety alignment relies on a reactive, disjoint procedure: attackers exploit a static model, followed by defensive fine-tuning to patch exposed vulnerabilities. This sequential approach creates a mismatch -- attackers overfit to obsolete defenses, while defenders perpetually lag behind emerging threats. To address this, we propose Self-RedTeam, an online self-play reinforcement learning algorithm where an attacker and defender agent co-evolve through continuous interaction. We cast safety alignment as a two-player zero-sum game, where a single model alternates between attacker and defender roles -- generating adversarial prompts and safeguarding against them -- while a reward LM adjudicates outcomes. This enables dynamic co-adaptation. Grounded in the game-theoretic framework of zero-sum games, we establish a theoretical safety guarantee which motivates the design of our method: if self-play converges to a Nash Equilibrium, the defender will reliably produce safe responses to any adversarial input. Empirically, Self-RedTeam uncovers more diverse attacks (+21.8% SBERT) compared to attackers trained against static defenders and achieves higher robustness on safety benchmarks (e.g., +65.5% on WildJailBreak) than defenders trained against static attackers. We further propose hidden Chain-of-Thought, allowing agents to plan privately, which boosts adversarial diversity and reduces over-refusals. Our results motivate a shift from reactive patching to proactive co-evolution in LM safety training, enabling scalable, autonomous, and robust self-improvement of LMs via multi-agent reinforcement learning (MARL).
<div id='section'>Paperid: <span id='pid'>199, <a href='https://arxiv.org/pdf/2506.07468.pdf' target='_blank'>https://arxiv.org/pdf/2506.07468.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mickel Liu, Liwei Jiang, Yancheng Liang, Simon Shaolei Du, Yejin Choi, Tim Althoff, Natasha Jaques
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.07468">Chasing Moving Targets with Online Self-Play Reinforcement Learning for Safer Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Conventional language model (LM) safety alignment relies on a reactive, disjoint procedure: attackers exploit a static model, followed by defensive fine-tuning to patch exposed vulnerabilities. This sequential approach creates a mismatch -- attackers overfit to obsolete defenses, while defenders perpetually lag behind emerging threats. To address this, we propose Self-RedTeam, an online self-play reinforcement learning algorithm where an attacker and defender agent co-evolve through continuous interaction. We cast safety alignment as a two-player zero-sum game, where a single model alternates between attacker and defender roles -- generating adversarial prompts and safeguarding against them -- while a reward LM adjudicates outcomes. This enables dynamic co-adaptation. Grounded in the game-theoretic framework of zero-sum games, we establish a theoretical safety guarantee which motivates the design of our method: if self-play converges to a Nash Equilibrium, the defender will reliably produce safe responses to any adversarial input. Empirically, Self-RedTeam uncovers more diverse attacks (+21.8% SBERT) compared to attackers trained against static defenders and achieves higher robustness on safety benchmarks (e.g., +65.5% on WildJailBreak) than defenders trained against static attackers. We further propose hidden Chain-of-Thought, allowing agents to plan privately, which boosts adversarial diversity and reduces over-refusals. Our results motivate a shift from reactive patching to proactive co-evolution in LM safety training, enabling scalable, autonomous, and robust self-improvement of LMs via multi-agent reinforcement learning (MARL).
<div id='section'>Paperid: <span id='pid'>200, <a href='https://arxiv.org/pdf/2311.01075.pdf' target='_blank'>https://arxiv.org/pdf/2311.01075.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Siming Lan, Rui Zhang, Qi Yi, Jiaming Guo, Shaohui Peng, Yunkai Gao, Fan Wu, Ruizhi Chen, Zidong Du, Xing Hu, Xishan Zhang, Ling Li, Yunji Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.01075">Contrastive Modules with Temporal Attention for Multi-Task Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the field of multi-task reinforcement learning, the modular principle, which involves specializing functionalities into different modules and combining them appropriately, has been widely adopted as a promising approach to prevent the negative transfer problem that performance degradation due to conflicts between tasks. However, most of the existing multi-task RL methods only combine shared modules at the task level, ignoring that there may be conflicts within the task. In addition, these methods do not take into account that without constraints, some modules may learn similar functions, resulting in restricting the model's expressiveness and generalization capability of modular methods. In this paper, we propose the Contrastive Modules with Temporal Attention(CMTA) method to address these limitations. CMTA constrains the modules to be different from each other by contrastive learning and combining shared modules at a finer granularity than the task level with temporal attention, alleviating the negative transfer within the task and improving the generalization ability and the performance for multi-task RL. We conducted the experiment on Meta-World, a multi-task RL benchmark containing various robotics manipulation tasks. Experimental results show that CMTA outperforms learning each task individually for the first time and achieves substantial performance improvements over the baselines.
<div id='section'>Paperid: <span id='pid'>201, <a href='https://arxiv.org/pdf/2304.07948.pdf' target='_blank'>https://arxiv.org/pdf/2304.07948.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Siyue Zhang, Minrui Xu, Wei Yang Bryan Lim, Dusit Niyato
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.07948">Sustainable AIGC Workload Scheduling of Geo-Distributed Data Centers: A Multi-Agent Reinforcement Learning Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent breakthroughs in generative artificial intelligence have triggered a surge in demand for machine learning training, which poses significant cost burdens and environmental challenges due to its substantial energy consumption. Scheduling training jobs among geographically distributed cloud data centers unveils the opportunity to optimize the usage of computing capacity powered by inexpensive and low-carbon energy and address the issue of workload imbalance. To tackle the challenge of multi-objective scheduling, i.e., maximizing GPU utilization while reducing operational costs, we propose an algorithm based on multi-agent reinforcement learning and actor-critic methods to learn the optimal collaborative scheduling strategy through interacting with a cloud system built with real-life workload patterns, energy prices, and carbon intensities. Compared with other algorithms, our proposed method improves the system utility by up to 28.6% attributable to higher GPU utilization, lower energy cost, and less carbon emission.
<div id='section'>Paperid: <span id='pid'>202, <a href='https://arxiv.org/pdf/2508.16037.pdf' target='_blank'>https://arxiv.org/pdf/2508.16037.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Renxuan Tan, Rongpeng Li, Xiaoxue Yu, Xianfu Chen, Xing Xu, Zhifeng Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.16037">Pareto Actor-Critic for Communication and Computation Co-Optimization in Non-Cooperative Federated Learning Services</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Federated learning (FL) in multi-service provider (SP) ecosystems is fundamentally hampered by non-cooperative dynamics, where privacy constraints and competing interests preclude the centralized optimization of multi-SP communication and computation resources. In this paper, we introduce PAC-MCoFL, a game-theoretic multi-agent reinforcement learning (MARL) framework where SPs act as agents to jointly optimize client assignment, adaptive quantization, and resource allocation. Within the framework, we integrate Pareto Actor-Critic (PAC) principles with expectile regression, enabling agents to conjecture optimal joint policies to achieve Pareto-optimal equilibria while modeling heterogeneous risk profiles. To manage the high-dimensional action space, we devise a ternary Cartesian decomposition (TCAD) mechanism that facilitates fine-grained control. Further, we develop PAC-MCoFL-p, a scalable variant featuring a parameterized conjecture generator that substantially reduces computational complexity with a provably bounded error. Alongside theoretical convergence guarantees, our framework's superiority is validated through extensive simulations -- PAC-MCoFL achieves approximately 5.8% and 4.2% improvements in total reward and hypervolume indicator (HVI), respectively, over the latest MARL solutions. The results also demonstrate that our method can more effectively balance individual SP and system performance in scaled deployments and under diverse data heterogeneity.
<div id='section'>Paperid: <span id='pid'>203, <a href='https://arxiv.org/pdf/2507.01378.pdf' target='_blank'>https://arxiv.org/pdf/2507.01378.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziyao Wang, Rongpeng Li, Sizhao Li, Yuming Xiang, Haiping Wang, Zhifeng Zhao, Honggang Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.01378">RALLY: Role-Adaptive LLM-Driven Yoked Navigation for Agentic UAV Swarms</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Intelligent control of Unmanned Aerial Vehicles (UAVs) swarms has emerged as a critical research focus, and it typically requires the swarm to navigate effectively while avoiding obstacles and achieving continuous coverage over multiple mission targets. Although traditional Multi-Agent Reinforcement Learning (MARL) approaches offer dynamic adaptability, they are hindered by the semantic gap in numerical communication and the rigidity of homogeneous role structures, resulting in poor generalization and limited task scalability. Recent advances in Large Language Model (LLM)-based control frameworks demonstrate strong semantic reasoning capabilities by leveraging extensive prior knowledge. However, due to the lack of online learning and over-reliance on static priors, these works often struggle with effective exploration, leading to reduced individual potential and overall system performance. To address these limitations, we propose a Role-Adaptive LLM-Driven Yoked navigation algorithm RALLY. Specifically, we first develop an LLM-driven semantic decision framework that uses structured natural language for efficient semantic communication and collaborative reasoning. Afterward, we introduce a dynamic role-heterogeneity mechanism for adaptive role switching and personalized decision-making. Furthermore, we propose a Role-value Mixing Network (RMIX)-based assignment strategy that integrates LLM offline priors with MARL online policies to enable semi-offline training of role selection strategies. Experiments in the Multi-Agent Particle Environment (MPE) environment and a Software-In-The-Loop (SITL) platform demonstrate that RALLY outperforms conventional approaches in terms of task coverage, convergence speed, and generalization, highlighting its strong potential for collaborative navigation in agentic multi-UAV systems.
<div id='section'>Paperid: <span id='pid'>204, <a href='https://arxiv.org/pdf/2507.01378.pdf' target='_blank'>https://arxiv.org/pdf/2507.01378.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziyao Wang, Rongpeng Li, Sizhao Li, Yuming Xiang, Haiping Wang, Zhifeng Zhao, Honggang Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.01378">RALLY: Role-Adaptive LLM-Driven Yoked Navigation for Agentic UAV Swarms</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Intelligent control of Unmanned Aerial Vehicles (UAVs) swarms has emerged as a critical research focus, and it typically requires the swarm to navigate effectively while avoiding obstacles and achieving continuous coverage over multiple mission targets. Although traditional Multi-Agent Reinforcement Learning (MARL) approaches offer dynamic adaptability, they are hindered by the semantic gap in numerical communication and the rigidity of homogeneous role structures, resulting in poor generalization and limited task scalability. Recent advances in Large Language Model (LLM)-based control frameworks demonstrate strong semantic reasoning capabilities by leveraging extensive prior knowledge. However, due to the lack of online learning and over-reliance on static priors, these works often struggle with effective exploration, leading to reduced individual potential and overall system performance. To address these limitations, we propose a Role-Adaptive LLM-Driven Yoked navigation algorithm RALLY. Specifically, we first develop an LLM-driven semantic decision framework that uses structured natural language for efficient semantic communication and collaborative reasoning. Afterward, we introduce a dynamic role-heterogeneity mechanism for adaptive role switching and personalized decision-making. Furthermore, we propose a Role-value Mixing Network (RMIX)-based assignment strategy that integrates LLM offline priors with MARL online policies to enable semi-offline training of role selection strategies. Experiments in the Multi-Agent Particle Environment (MPE) environment and a Software-In-The-Loop (SITL) platform demonstrate that RALLY outperforms conventional approaches in terms of task coverage, convergence speed, and generalization, highlighting its strong potential for collaborative navigation in agentic multi-UAV systems.
<div id='section'>Paperid: <span id='pid'>205, <a href='https://arxiv.org/pdf/2506.12453.pdf' target='_blank'>https://arxiv.org/pdf/2506.12453.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rongpeng Li, Jianhang Zhu, Jiahao Huang, Zhifeng Zhao, Honggang Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.12453">Topology-Assisted Spatio-Temporal Pattern Disentangling for Scalable MARL in Large-scale Autonomous Traffic Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Intelligent Transportation Systems (ITSs) have emerged as a promising solution towards ameliorating urban traffic congestion, with Traffic Signal Control (TSC) identified as a critical component. Although Multi-Agent Reinforcement Learning (MARL) algorithms have shown potential in optimizing TSC through real-time decision-making, their scalability and effectiveness often suffer from large-scale and complex environments. Typically, these limitations primarily stem from a fundamental mismatch between the exponential growth of the state space driven by the environmental heterogeneities and the limited modeling capacity of current solutions. To address these issues, this paper introduces a novel MARL framework that integrates Dynamic Graph Neural Networks (DGNNs) and Topological Data Analysis (TDA), aiming to enhance the expressiveness of environmental representations and improve agent coordination. Furthermore, inspired by the Mixture of Experts (MoE) architecture in Large Language Models (LLMs), a topology-assisted spatial pattern disentangling (TSD)-enhanced MoE is proposed, which leverages topological signatures to decouple graph features for specialized processing, thus improving the model's ability to characterize dynamic and heterogeneous local observations. The TSD module is also integrated into the policy and value networks of the Multi-agent Proximal Policy Optimization (MAPPO) algorithm, further improving decision-making efficiency and robustness. Extensive experiments conducted on real-world traffic scenarios, together with comprehensive theoretical analysis, validate the superior performance of the proposed framework, highlighting the model's scalability and effectiveness in addressing the complexities of large-scale TSC tasks.
<div id='section'>Paperid: <span id='pid'>206, <a href='https://arxiv.org/pdf/2505.13834.pdf' target='_blank'>https://arxiv.org/pdf/2505.13834.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhi Su, Yuman Gao, Emily Lukas, Yunfei Li, Jiaze Cai, Faris Tulbah, Fei Gao, Chao Yu, Zhongyu Li, Yi Wu, Koushil Sreenath
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.13834">Toward Real-World Cooperative and Competitive Soccer with Quadrupedal Robot Teams</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Achieving coordinated teamwork among legged robots requires both fine-grained locomotion control and long-horizon strategic decision-making. Robot soccer offers a compelling testbed for this challenge, combining dynamic, competitive, and multi-agent interactions. In this work, we present a hierarchical multi-agent reinforcement learning (MARL) framework that enables fully autonomous and decentralized quadruped robot soccer. First, a set of highly dynamic low-level skills is trained for legged locomotion and ball manipulation, such as walking, dribbling, and kicking. On top of these, a high-level strategic planning policy is trained with Multi-Agent Proximal Policy Optimization (MAPPO) via Fictitious Self-Play (FSP). This learning framework allows agents to adapt to diverse opponent strategies and gives rise to sophisticated team behaviors, including coordinated passing, interception, and dynamic role allocation. With an extensive ablation study, the proposed learning method shows significant advantages in the cooperative and competitive multi-agent soccer game. We deploy the learned policies to real quadruped robots relying solely on onboard proprioception and decentralized localization, with the resulting system supporting autonomous robot-robot and robot-human soccer matches on indoor and outdoor soccer courts.
<div id='section'>Paperid: <span id='pid'>207, <a href='https://arxiv.org/pdf/2505.13834.pdf' target='_blank'>https://arxiv.org/pdf/2505.13834.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhi Su, Yuman Gao, Emily Lukas, Yunfei Li, Jiaze Cai, Faris Tulbah, Fei Gao, Chao Yu, Zhongyu Li, Yi Wu, Koushil Sreenath
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.13834">Toward Real-World Cooperative and Competitive Soccer with Quadrupedal Robot Teams</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Achieving coordinated teamwork among legged robots requires both fine-grained locomotion control and long-horizon strategic decision-making. Robot soccer offers a compelling testbed for this challenge, combining dynamic, competitive, and multi-agent interactions. In this work, we present a hierarchical multi-agent reinforcement learning (MARL) framework that enables fully autonomous and decentralized quadruped robot soccer. First, a set of highly dynamic low-level skills is trained for legged locomotion and ball manipulation, such as walking, dribbling, and kicking. On top of these, a high-level strategic planning policy is trained with Multi-Agent Proximal Policy Optimization (MAPPO) via Fictitious Self-Play (FSP). This learning framework allows agents to adapt to diverse opponent strategies and gives rise to sophisticated team behaviors, including coordinated passing, interception, and dynamic role allocation. With an extensive ablation study, the proposed learning method shows significant advantages in the cooperative and competitive multi-agent soccer game. We deploy the learned policies to real quadruped robots relying solely on onboard proprioception and decentralized localization, with the resulting system supporting autonomous robot-robot and robot-human soccer matches on indoor and outdoor soccer courts.
<div id='section'>Paperid: <span id='pid'>208, <a href='https://arxiv.org/pdf/2503.10049.pdf' target='_blank'>https://arxiv.org/pdf/2503.10049.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziqi Jia, Junjie Li, Xiaoyang Qu, Jianzong Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.10049">Enhancing Multi-Agent Systems via Reinforcement Learning with LLM-based Planner and Graph-based Policy</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent systems (MAS) have shown great potential in executing complex tasks, but coordination and safety remain significant challenges. Multi-Agent Reinforcement Learning (MARL) offers a promising framework for agent collaboration, but it faces difficulties in handling complex tasks and designing reward functions. The introduction of Large Language Models (LLMs) has brought stronger reasoning and cognitive abilities to MAS, but existing LLM-based systems struggle to respond quickly and accurately in dynamic environments. To address these challenges, we propose LLM-based Graph Collaboration MARL (LGC-MARL), a framework that efficiently combines LLMs and MARL. This framework decomposes complex tasks into executable subtasks and achieves efficient collaboration among multiple agents through graph-based coordination. Specifically, LGC-MARL consists of two main components: an LLM planner and a graph-based collaboration meta policy. The LLM planner transforms complex task instructions into a series of executable subtasks, evaluates the rationality of these subtasks using a critic model, and generates an action dependency graph. The graph-based collaboration meta policy facilitates communication and collaboration among agents based on the action dependency graph, and adapts to new task environments through meta-learning. Experimental results on the AI2-THOR simulation platform demonstrate the superior performance and scalability of LGC-MARL in completing various complex tasks.
<div id='section'>Paperid: <span id='pid'>209, <a href='https://arxiv.org/pdf/2502.09846.pdf' target='_blank'>https://arxiv.org/pdf/2502.09846.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziqiong Wang, Xiaoxue Yu, Rongpeng Li, Zhifeng Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.09846">Robust Event-Triggered Integrated Communication and Control with Graph Information Bottleneck Optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Integrated communication and control serves as a critical ingredient in Multi-Agent Reinforcement Learning. However, partial observability limitations will impair collaboration effectiveness, and a potential solution is to establish consensus through well-calibrated latent variables obtained from neighboring agents. Nevertheless, the rigid transmission of less informative content can still result in redundant information exchanges. Therefore, we propose a Consensus-Driven Event-Based Graph Information Bottleneck (CDE-GIB) method, which integrates the communication graph and information flow through a GIB regularizer to extract more concise message representations while avoiding the high computational complexity of inner-loop operations. To further minimize the communication volume required for establishing consensus during interactions, we also develop a variable-threshold event-triggering mechanism. By simultaneously considering historical data and current observations, this mechanism capably evaluates the importance of information to determine whether an event should be triggered. Experimental results demonstrate that our proposed method outperforms existing state-of-the-art methods in terms of both efficiency and adaptability.
<div id='section'>Paperid: <span id='pid'>210, <a href='https://arxiv.org/pdf/2501.00950.pdf' target='_blank'>https://arxiv.org/pdf/2501.00950.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Cleverson Nahum, Salvatore D'Oro, Pedro Batista, Cristiano Both, Kleber Cardoso, Aldebaro Klautau, Tommaso Melodia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.00950">Intent-based Radio Scheduler for RAN Slicing: Learning to deal with different network scenarios</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The future mobile network has the complex mission of distributing available radio resources among various applications with different requirements. The radio access network slicing enables the creation of different logical networks by isolating and using dedicated resources for each group of applications. In this scenario, the radio resource scheduling (RRS) is responsible for distributing the radio resources available among the slices to fulfill their service-level agreement (SLA) requirements, prioritizing critical slices while minimizing the number of intent violations. Moreover, ensuring that the RRS can deal with a high diversity of network scenarios is essential. Several recent papers present advances in machine learning-based RRS. However, the scenarios and slice variety are restricted, which inhibits solid conclusions about the generalization capabilities of the models after deployment in real networks. This paper proposes an intent-based RRS using multi-agent reinforcement learning in a radio access network (RAN) slicing context. The proposed method protects high-priority slices when the available radio resources cannot fulfill all the slices. It uses transfer learning to reduce the number of training steps required. The proposed method and baselines are evaluated in different network scenarios that comprehend combinations of different slice types, channel trajectories, number of active slices and users' equipment (UEs), and UE characteristics. The proposed method outperformed the baselines in protecting slices with higher priority, obtaining an improvement of 40% and, when considering all the slices, obtaining an improvement of 20% in relation to the baselines. The results show that by using transfer learning, the required number of training steps could be reduced by a factor of eight without hurting performance.
<div id='section'>Paperid: <span id='pid'>211, <a href='https://arxiv.org/pdf/2501.00950.pdf' target='_blank'>https://arxiv.org/pdf/2501.00950.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Cleverson Nahum, Salvatore D'Oro, Pedro Batista, Cristiano Both, Kleber Cardoso, Aldebaro Klautau, Tommaso Melodia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.00950">Intent-based Radio Scheduler for RAN Slicing: Learning to deal with different network scenarios</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The future mobile network has the complex mission of distributing available radio resources among various applications with different requirements. The radio access network slicing enables the creation of different logical networks by isolating and using dedicated resources for each group of applications. In this scenario, the radio resource scheduling (RRS) is responsible for distributing the radio resources available among the slices to fulfill their service-level agreement (SLA) requirements, prioritizing critical slices while minimizing the number of intent violations. Moreover, ensuring that the RRS can deal with a high diversity of network scenarios is essential. Several recent papers present advances in machine learning-based RRS. However, the scenarios and slice variety are restricted, which inhibits solid conclusions about the generalization capabilities of the models after deployment in real networks. This paper proposes an intent-based RRS using multi-agent reinforcement learning in a radio access network (RAN) slicing context. The proposed method protects high-priority slices when the available radio resources cannot fulfill all the slices. It uses transfer learning to reduce the number of training steps required. The proposed method and baselines are evaluated in different network scenarios that comprehend combinations of different slice types, channel trajectories, number of active slices and users' equipment (UEs), and UE characteristics. The proposed method outperformed the baselines in protecting slices with higher priority, obtaining an improvement of 40% and, when considering all the slices, obtaining an improvement of 20% in relation to the baselines. The results show that by using transfer learning, the required number of training steps could be reduced by a factor of eight without hurting performance.
<div id='section'>Paperid: <span id='pid'>212, <a href='https://arxiv.org/pdf/2501.00950.pdf' target='_blank'>https://arxiv.org/pdf/2501.00950.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Cleverson Nahum, Salvatore D'Oro, Pedro Batista, Cristiano Both, Kleber Cardoso, Aldebaro Klautau, Tommaso Melodia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.00950">Intent-based Radio Scheduler for RAN Slicing: Learning to deal with different network scenarios</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The future mobile network has the complex mission of distributing available radio resources among various applications with different requirements. The radio access network slicing enables the creation of different logical networks by isolating and using dedicated resources for each group of applications. In this scenario, the radio resource scheduling (RRS) is responsible for distributing the radio resources available among the slices to fulfill their service-level agreement (SLA) requirements, prioritizing critical slices while minimizing the number of intent violations. Moreover, ensuring that the RRS can deal with a high diversity of network scenarios is essential. Several recent papers present advances in machine learning-based RRS. However, the scenarios and slice variety are restricted, which inhibits solid conclusions about the generalization capabilities of the models after deployment in real networks. This paper proposes an intent-based RRS using multi-agent reinforcement learning in a radio access network (RAN) slicing context. The proposed method protects high-priority slices when the available radio resources cannot fulfill all the slices. It uses transfer learning to reduce the number of training steps required. The proposed method and baselines are evaluated in different network scenarios that comprehend combinations of different slice types, channel trajectories, number of active slices and users' equipment (UEs), and UE characteristics. The proposed method outperformed the baselines in protecting slices with higher priority, obtaining an improvement of 40% and, when considering all the slices, obtaining an improvement of 20% in relation to the baselines. The results show that by using transfer learning, the required number of training steps could be reduced by a factor of eight without hurting performance.
<div id='section'>Paperid: <span id='pid'>213, <a href='https://arxiv.org/pdf/2501.00950.pdf' target='_blank'>https://arxiv.org/pdf/2501.00950.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Cleverson Nahum, Salvatore D'Oro, Pedro Batista, Cristiano Both, Kleber Cardoso, Aldebaro Klautau, Tommaso Melodia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.00950">Intent-based Radio Scheduler for RAN Slicing: Learning to deal with different network scenarios</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The future mobile network has the complex mission of distributing available radio resources among various applications with different requirements. The radio access network slicing enables the creation of different logical networks by isolating and using dedicated resources for each group of applications. In this scenario, the radio resource scheduling (RRS) is responsible for distributing the radio resources available among the slices to fulfill their service-level agreement (SLA) requirements, prioritizing critical slices while minimizing the number of intent violations. Moreover, ensuring that the RRS can deal with a high diversity of network scenarios is essential. Several recent papers present advances in machine learning-based RRS. However, the scenarios and slice variety are restricted, which inhibits solid conclusions about the generalization capabilities of the models after deployment in real networks. This paper proposes an intent-based RRS using multi-agent reinforcement learning in a radio access network (RAN) slicing context. The proposed method protects high-priority slices when the available radio resources cannot fulfill all the slices. It uses transfer learning to reduce the number of training steps required. The proposed method and baselines are evaluated in different network scenarios that comprehend combinations of different slice types, channel trajectories, number of active slices and users' equipment (UEs), and UE characteristics. The proposed method outperformed the baselines in protecting slices with higher priority, obtaining an improvement of 40% and, when considering all the slices, obtaining an improvement of 20% in relation to the baselines. The results show that by using transfer learning, the required number of training steps could be reduced by a factor of eight without hurting performance.
<div id='section'>Paperid: <span id='pid'>214, <a href='https://arxiv.org/pdf/2501.00950.pdf' target='_blank'>https://arxiv.org/pdf/2501.00950.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Cleverson Nahum, Salvatore D'Oro, Pedro Batista, Cristiano Both, Kleber Cardoso, Aldebaro Klautau, Tommaso Melodia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.00950">Intent-based Radio Scheduler for RAN Slicing: Learning to deal with different network scenarios</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The future mobile network has the complex mission of distributing available radio resources among various applications with different requirements. The radio access network slicing enables the creation of different logical networks by isolating and using dedicated resources for each group of applications. In this scenario, the radio resource scheduling (RRS) is responsible for distributing the radio resources available among the slices to fulfill their service-level agreement (SLA) requirements, prioritizing critical slices while minimizing the number of intent violations. Moreover, ensuring that the RRS can deal with a high diversity of network scenarios is essential. Several recent papers present advances in machine learning-based RRS. However, the scenarios and slice variety are restricted, which inhibits solid conclusions about the generalization capabilities of the models after deployment in real networks. This paper proposes an intent-based RRS using multi-agent reinforcement learning in a radio access network (RAN) slicing context. The proposed method protects high-priority slices when the available radio resources cannot fulfill all the slices. It uses transfer learning to reduce the number of training steps required. The proposed method and baselines are evaluated in different network scenarios that comprehend combinations of different slice types, channel trajectories, number of active slices and users' equipment (UEs), and UE characteristics. The proposed method outperformed the baselines in protecting slices with higher priority, obtaining an improvement of 40% and, when considering all the slices, obtaining an improvement of 20% in relation to the baselines. The results show that by using transfer learning, the required number of training steps could be reduced by a factor of eight without hurting performance.
<div id='section'>Paperid: <span id='pid'>215, <a href='https://arxiv.org/pdf/2312.10123.pdf' target='_blank'>https://arxiv.org/pdf/2312.10123.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaoxue Yu, Rongpeng Li, Chengchao Liang, Zhifeng Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.10123">Communication-Efficient Soft Actor-Critic Policy Collaboration via Regulated Segment Mixture</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-Agent Reinforcement Learning (MARL) has emerged as a foundational approach for addressing diverse, intelligent control tasks in various scenarios like the Internet of Vehicles, Internet of Things, and Unmanned Aerial Vehicles. However, the widely assumed existence of a central node for centralized, federated learning-assisted MARL might be impractical in highly dynamic environments. This can lead to excessive communication overhead, potentially overwhelming the system. To address these challenges, we design a novel communication-efficient, fully distributed algorithm for collaborative MARL under the frameworks of Soft Actor-Critic (SAC) and Decentralized Federated Learning (DFL), named RSM-MASAC. In particular, RSM-MASAC enhances multi-agent collaboration and prioritizes higher communication efficiency in dynamic systems by incorporating the concept of segmented aggregation in DFL and augmenting multiple model replicas from received neighboring policy segments, which are subsequently employed as reconstructed referential policies for mixing. Distinctively diverging from traditional RL approaches, RSM-MASAC introduces new bounds under the framework of Maximum Entropy Reinforcement Learning (MERL). Correspondingly, it adopts a theory-guided mixture metric to regulate the selection of contributive referential policies, thus guaranteeing soft policy improvement during the communication-assisted mixing phase. Finally, the extensive simulations in mixed-autonomy traffic control scenarios verify the effectiveness and superiority of our algorithm.
<div id='section'>Paperid: <span id='pid'>216, <a href='https://arxiv.org/pdf/2312.07025.pdf' target='_blank'>https://arxiv.org/pdf/2312.07025.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wei Geng, Baidi Xiao, Rongpeng Li, Ning Wei, Dong Wang, Zhifeng Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.07025">Noise Distribution Decomposition based Multi-Agent Distributional Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generally, Reinforcement Learning (RL) agent updates its policy by repetitively interacting with the environment, contingent on the received rewards to observed states and undertaken actions. However, the environmental disturbance, commonly leading to noisy observations (e.g., rewards and states), could significantly shape the performance of agent. Furthermore, the learning performance of Multi-Agent Reinforcement Learning (MARL) is more susceptible to noise due to the interference among intelligent agents. Therefore, it becomes imperative to revolutionize the design of MARL, so as to capably ameliorate the annoying impact of noisy rewards. In this paper, we propose a novel decomposition-based multi-agent distributional RL method by approximating the globally shared noisy reward by a Gaussian mixture model (GMM) and decomposing it into the combination of individual distributional local rewards, with which each agent can be updated locally through distributional RL. Moreover, a diffusion model (DM) is leveraged for reward generation in order to mitigate the issue of costly interaction expenditure for learning distributions. Furthermore, the optimality of the distribution decomposition is theoretically validated, while the design of loss function is carefully calibrated to avoid the decomposition ambiguity. We also verify the effectiveness of the proposed method through extensive simulation experiments with noisy rewards. Besides, different risk-sensitive policies are evaluated in order to demonstrate the superiority of distributional RL in different MARL tasks.
<div id='section'>Paperid: <span id='pid'>217, <a href='https://arxiv.org/pdf/2309.07108.pdf' target='_blank'>https://arxiv.org/pdf/2309.07108.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Samuel Wiggins, Yuan Meng, Rajgopal Kannan, Viktor Prasanna
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.07108">Characterizing Speed Performance of Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-Agent Reinforcement Learning (MARL) has achieved significant success in large-scale AI systems and big-data applications such as smart grids, surveillance, etc. Existing advancements in MARL algorithms focus on improving the rewards obtained by introducing various mechanisms for inter-agent cooperation. However, these optimizations are usually compute- and memory-intensive, thus leading to suboptimal speed performance in end-to-end training time. In this work, we analyze the speed performance (i.e., latency-bounded throughput) as the key metric in MARL implementations. Specifically, we first introduce a taxonomy of MARL algorithms from an acceleration perspective categorized by (1) training scheme and (2) communication method. Using our taxonomy, we identify three state-of-the-art MARL algorithms - Multi-Agent Deep Deterministic Policy Gradient (MADDPG), Target-oriented Multi-agent Communication and Cooperation (ToM2C), and Networked Multi-Agent RL (NeurComm) - as target benchmark algorithms, and provide a systematic analysis of their performance bottlenecks on a homogeneous multi-core CPU platform. We justify the need for MARL latency-bounded throughput to be a key performance metric in future literature while also addressing opportunities for parallelization and acceleration.
<div id='section'>Paperid: <span id='pid'>218, <a href='https://arxiv.org/pdf/2308.04198.pdf' target='_blank'>https://arxiv.org/pdf/2308.04198.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaoxue Yu, Rongpeng Li, Fei Wang, Chenghui Peng, Chengchao Liang, Zhifeng Zhao, Honggang Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.04198">Communication-Efficient Cooperative Multi-Agent PPO via Regulated Segment Mixture in Internet of Vehicles</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-Agent Reinforcement Learning (MARL) has become a classic paradigm to solve diverse, intelligent control tasks like autonomous driving in Internet of Vehicles (IoV). However, the widely assumed existence of a central node to implement centralized federated learning-assisted MARL might be impractical in highly dynamic scenarios, and the excessive communication overheads possibly overwhelm the IoV system. Therefore, in this paper, we design a communication efficient cooperative MARL algorithm, named RSM-MAPPO, to reduce the communication overheads in a fully distributed architecture. In particular, RSM-MAPPO enhances the multi-agent Proximal Policy Optimization (PPO) by incorporating the idea of segment mixture and augmenting multiple model replicas from received neighboring policy segments. Afterwards, RSM-MAPPO adopts a theory-guided metric to regulate the selection of contributive replicas to guarantee the policy improvement. Finally, extensive simulations in a mixed-autonomy traffic control scenario verify the effectiveness of the RSM-MAPPO algorithm.
<div id='section'>Paperid: <span id='pid'>219, <a href='https://arxiv.org/pdf/2307.12287.pdf' target='_blank'>https://arxiv.org/pdf/2307.12287.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuming Xiang, Sizhao Li, Rongpeng Li, Zhifeng Zhao, Honggang Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.12287">Decentralized Adaptive Formation via Consensus-Oriented Multi-Agent Communication</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Adaptive multi-agent formation control, which requires the formation to flexibly adjust along with the quantity variations of agents in a decentralized manner, belongs to one of the most challenging issues in multi-agent systems, especially under communication-limited constraints. In this paper, we propose a novel Consensus-based Decentralized Adaptive Formation (Cons-DecAF) framework. Specifically, we develop a novel multi-agent reinforcement learning method, Consensus-oriented Multi-Agent Communication (ConsMAC), to enable agents to perceive global information and establish the consensus from local states by effectively aggregating neighbor messages. Afterwards, we leverage policy distillation to accomplish the adaptive formation adjustment. Meanwhile, instead of pre-assigning specific positions of agents, we employ a displacement-based formation by Hausdorff distance to significantly improve the formation efficiency. The experimental results through extensive simulations validate that the proposed method has achieved outstanding performance in terms of both speed and stability.
<div id='section'>Paperid: <span id='pid'>220, <a href='https://arxiv.org/pdf/2304.09870.pdf' target='_blank'>https://arxiv.org/pdf/2304.09870.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yifan Zhong, Jakub Grudzien Kuba, Xidong Feng, Siyi Hu, Jiaming Ji, Yaodong Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.09870">Heterogeneous-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The necessity for cooperation among intelligent machines has popularised cooperative multi-agent reinforcement learning (MARL) in AI research. However, many research endeavours heavily rely on parameter sharing among agents, which confines them to only homogeneous-agent setting and leads to training instability and lack of convergence guarantees. To achieve effective cooperation in the general heterogeneous-agent setting, we propose Heterogeneous-Agent Reinforcement Learning (HARL) algorithms that resolve the aforementioned issues. Central to our findings are the multi-agent advantage decomposition lemma and the sequential update scheme. Based on these, we develop the provably correct Heterogeneous-Agent Trust Region Learning (HATRL), and derive HATRPO and HAPPO by tractable approximations. Furthermore, we discover a novel framework named Heterogeneous-Agent Mirror Learning (HAML), which strengthens theoretical guarantees for HATRPO and HAPPO and provides a general template for cooperative MARL algorithmic designs. We prove that all algorithms derived from HAML inherently enjoy monotonic improvement of joint return and convergence to Nash Equilibrium. As its natural outcome, HAML validates more novel algorithms in addition to HATRPO and HAPPO, including HAA2C, HADDPG, and HATD3, which generally outperform their existing MA-counterparts. We comprehensively test HARL algorithms on six challenging benchmarks and demonstrate their superior effectiveness and stability for coordinating heterogeneous agents compared to strong baselines such as MAPPO and QMIX.
<div id='section'>Paperid: <span id='pid'>221, <a href='https://arxiv.org/pdf/2303.13213.pdf' target='_blank'>https://arxiv.org/pdf/2303.13213.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Baidi Xiao, Rongpeng Li, Fei Wang, Chenghui Peng, Jianjun Wu, Zhifeng Zhao, Honggang Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.13213">Stochastic Graph Neural Network-based Value Decomposition for MARL in Internet of Vehicles</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autonomous driving has witnessed incredible advances in the past several decades, while Multi-Agent Reinforcement Learning (MARL) promises to satisfy the essential need of autonomous vehicle control in a wireless connected vehicle networks. In MARL, how to effectively decompose a global feedback into the relative contributions of individual agents belongs to one of the most fundamental problems. However, the environment volatility due to vehicle movement and wireless disturbance could significantly shape time-varying topological relationships among agents, thus making the Value Decomposition (VD) challenging. Therefore, in order to cope with this annoying volatility, it becomes imperative to design a dynamic VD framework. Hence, in this paper, we propose a novel Stochastic VMIX (SVMIX) methodology by taking account of dynamic topological features during the VD and incorporating the corresponding components into a multi-agent actor-critic architecture. In particular, Stochastic Graph Neural Network (SGNN) is leveraged to effectively capture underlying dynamics in topological features and improve the flexibility of VD against the environment volatility. Finally, the superiority of SVMIX is verified through extensive simulations.
<div id='section'>Paperid: <span id='pid'>222, <a href='https://arxiv.org/pdf/2103.13026.pdf' target='_blank'>https://arxiv.org/pdf/2103.13026.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xing Xu, Rongpeng Li, Zhifeng Zhao, Honggang Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2103.13026">The Gradient Convergence Bound of Federated Multi-Agent Reinforcement Learning with Efficient Communication</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The paper considers independent reinforcement learning (IRL) for multi-agent collaborative decision-making in the paradigm of federated learning (FL). However, FL generates excessive communication overheads between agents and a remote central server, especially when it involves a large number of agents or iterations. Besides, due to the heterogeneity of independent learning environments, multiple agents may undergo asynchronous Markov decision processes (MDPs), which will affect the training samples and the model's convergence performance. On top of the variation-aware periodic averaging (VPA) method and the policy-based deep reinforcement learning (DRL) algorithm (i.e., proximal policy optimization (PPO)), this paper proposes two advanced optimization schemes orienting to stochastic gradient descent (SGD): 1) A decay-based scheme gradually decays the weights of a model's local gradients with the progress of successive local updates, and 2) By representing the agents as a graph, a consensus-based scheme studies the impact of exchanging a model's local gradients among nearby agents from an algebraic connectivity perspective. This paper also provides novel convergence guarantees for both developed schemes, and demonstrates their superior effectiveness and efficiency in improving the system's utility value through theoretical analyses and simulation results.
<div id='section'>Paperid: <span id='pid'>223, <a href='https://arxiv.org/pdf/2203.08553.pdf' target='_blank'>https://arxiv.org/pdf/2203.08553.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pengyi Li, Hongyao Tang, Tianpei Yang, Xiaotian Hao, Tong Sang, Yan Zheng, Jianye Hao, Matthew E. Taylor, Wenyuan Tao, Zhen Wang, Fazl Barez
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2203.08553">PMIC: Improving Multi-Agent Reinforcement Learning with Progressive Mutual Information Collaboration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Learning to collaborate is critical in Multi-Agent Reinforcement Learning (MARL). Previous works promote collaboration by maximizing the correlation of agents' behaviors, which is typically characterized by Mutual Information (MI) in different forms. However, we reveal sub-optimal collaborative behaviors also emerge with strong correlations, and simply maximizing the MI can, surprisingly, hinder the learning towards better collaboration. To address this issue, we propose a novel MARL framework, called Progressive Mutual Information Collaboration (PMIC), for more effective MI-driven collaboration. PMIC uses a new collaboration criterion measured by the MI between global states and joint actions. Based on this criterion, the key idea of PMIC is maximizing the MI associated with superior collaborative behaviors and minimizing the MI associated with inferior ones. The two MI objectives play complementary roles by facilitating better collaborations while avoiding falling into sub-optimal ones. Experiments on a wide range of MARL benchmarks show the superior performance of PMIC compared with other algorithms.
<div id='section'>Paperid: <span id='pid'>224, <a href='https://arxiv.org/pdf/2502.03723.pdf' target='_blank'>https://arxiv.org/pdf/2502.03723.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Muhan Lin, Shuyang Shi, Yue Guo, Vaishnav Tadiparthi, Behdad Chalaki, Ehsan Moradi Pari, Simon Stepputtis, Woojun Kim, Joseph Campbell, Katia Sycara
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.03723">Speaking the Language of Teamwork: LLM-Guided Credit Assignment in Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Credit assignment, the process of attributing credit or blame to individual agents for their contributions to a team's success or failure, remains a fundamental challenge in multi-agent reinforcement learning (MARL), particularly in environments with sparse rewards. Commonly-used approaches such as value decomposition often lead to suboptimal policies in these settings, and designing dense reward functions that align with human intuition can be complex and labor-intensive. In this work, we propose a novel framework where a large language model (LLM) generates dense, agent-specific rewards based on a natural language description of the task and the overall team goal. By learning a potential-based reward function over multiple queries, our method reduces the impact of ranking errors while allowing the LLM to evaluate each agent's contribution to the overall task. Through extensive experiments, we demonstrate that our approach achieves faster convergence and higher policy returns compared to state-of-the-art MARL baselines.
<div id='section'>Paperid: <span id='pid'>225, <a href='https://arxiv.org/pdf/2406.01377.pdf' target='_blank'>https://arxiv.org/pdf/2406.01377.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Weihao Zeng, Joseph Campbell, Simon Stepputtis, Katia Sycara
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.01377">Multi-Agent Transfer Learning via Temporal Contrastive Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces a novel transfer learning framework for deep multi-agent reinforcement learning. The approach automatically combines goal-conditioned policies with temporal contrastive learning to discover meaningful sub-goals. The approach involves pre-training a goal-conditioned agent, finetuning it on the target domain, and using contrastive learning to construct a planning graph that guides the agent via sub-goals. Experiments on multi-agent coordination Overcooked tasks demonstrate improved sample efficiency, the ability to solve sparse-reward and long-horizon problems, and enhanced interpretability compared to baselines. The results highlight the effectiveness of integrating goal-conditioned policies with unsupervised temporal abstraction learning for complex multi-agent transfer learning. Compared to state-of-the-art baselines, our method achieves the same or better performances while requiring only 21.7% of the training samples.
<div id='section'>Paperid: <span id='pid'>226, <a href='https://arxiv.org/pdf/2307.01158.pdf' target='_blank'>https://arxiv.org/pdf/2307.01158.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ini Oguntola, Joseph Campbell, Simon Stepputtis, Katia Sycara
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.01158">Theory of Mind as Intrinsic Motivation for Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The ability to model the mental states of others is crucial to human social intelligence, and can offer similar benefits to artificial agents with respect to the social dynamics induced in multi-agent settings. We present a method of grounding semantically meaningful, human-interpretable beliefs within policies modeled by deep networks. We then consider the task of 2nd-order belief prediction. We propose that ability of each agent to predict the beliefs of the other agents can be used as an intrinsic reward signal for multi-agent reinforcement learning. Finally, we present preliminary empirical results in a mixed cooperative-competitive environment.
<div id='section'>Paperid: <span id='pid'>227, <a href='https://arxiv.org/pdf/2302.12232.pdf' target='_blank'>https://arxiv.org/pdf/2302.12232.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Renos Zabounidis, Joseph Campbell, Simon Stepputtis, Dana Hughes, Katia Sycara
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.12232">Concept Learning for Interpretable Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent robotic systems are increasingly operating in real-world environments in close proximity to humans, yet are largely controlled by policy models with inscrutable deep neural network representations. We introduce a method for incorporating interpretable concepts from a domain expert into models trained through multi-agent reinforcement learning, by requiring the model to first predict such concepts then utilize them for decision making. This allows an expert to both reason about the resulting concept policy models in terms of these high-level concepts at run-time, as well as intervene and correct mispredictions to improve performance. We show that this yields improved interpretability and training stability, with benefits to policy performance and sample efficiency in a simulated and real-world cooperative-competitive multi-agent game.
<div id='section'>Paperid: <span id='pid'>228, <a href='https://arxiv.org/pdf/2211.07882.pdf' target='_blank'>https://arxiv.org/pdf/2211.07882.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yue Guo, Joseph Campbell, Simon Stepputtis, Ruiyu Li, Dana Hughes, Fei Fang, Katia Sycara
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2211.07882">Explainable Action Advising for Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Action advising is a knowledge transfer technique for reinforcement learning based on the teacher-student paradigm. An expert teacher provides advice to a student during training in order to improve the student's sample efficiency and policy performance. Such advice is commonly given in the form of state-action pairs. However, it makes it difficult for the student to reason with and apply to novel states. We introduce Explainable Action Advising, in which the teacher provides action advice as well as associated explanations indicating why the action was chosen. This allows the student to self-reflect on what it has learned, enabling advice generalization and leading to improved sample efficiency and learning performance - even in environments where the teacher is sub-optimal. We empirically show that our framework is effective in both single-agent and multi-agent scenarios, yielding improved policy returns and convergence rates when compared to state-of-the-art methods
<div id='section'>Paperid: <span id='pid'>229, <a href='https://arxiv.org/pdf/2411.07104.pdf' target='_blank'>https://arxiv.org/pdf/2411.07104.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuming Feng, Chuye Hong, Yaru Niu, Shiqi Liu, Yuxiang Yang, Wenhao Yu, Tingnan Zhang, Jie Tan, Ding Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.07104">Learning Multi-Agent Loco-Manipulation for Long-Horizon Quadrupedal Pushing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, quadrupedal locomotion has achieved significant success, but their manipulation capabilities, particularly in handling large objects, remain limited, restricting their usefulness in demanding real-world applications such as search and rescue, construction, industrial automation, and room organization. This paper tackles the task of obstacle-aware, long-horizon pushing by multiple quadrupedal robots. We propose a hierarchical multi-agent reinforcement learning framework with three levels of control. The high-level controller integrates an RRT planner and a centralized adaptive policy to generate subgoals, while the mid-level controller uses a decentralized goal-conditioned policy to guide the robots toward these sub-goals. A pre-trained low-level locomotion policy executes the movement commands. We evaluate our method against several baselines in simulation, demonstrating significant improvements over baseline approaches, with 36.0% higher success rates and 24.5% reduction in completion time than the best baseline. Our framework successfully enables long-horizon, obstacle-aware manipulation tasks like Push-Cuboid and Push-T on Go1 robots in the real world.
<div id='section'>Paperid: <span id='pid'>230, <a href='https://arxiv.org/pdf/2404.14319.pdf' target='_blank'>https://arxiv.org/pdf/2404.14319.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>David R. Nickel, Anindya Bijoy Das, David J. Love, Christopher G. Brinton
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.14319">Multi-Agent Hybrid SAC for Joint SS-DSA in CRNs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Opportunistic spectrum access has the potential to increase the efficiency of spectrum utilization in cognitive radio networks (CRNs). In CRNs, both spectrum sensing and resource allocation (SSRA) are critical to maximizing system throughput while minimizing collisions of secondary users with the primary network. However, many works in dynamic spectrum access do not consider the impact of imperfect sensing information such as mis-detected channels, which the additional information available in joint SSRA can help remediate. In this work, we examine joint SSRA as an optimization which seeks to maximize a CRN's net communication rate subject to constraints on channel sensing, channel access, and transmit power. Given the non-trivial nature of the problem, we leverage multi-agent reinforcement learning to enable a network of secondary users to dynamically access unoccupied spectrum via only local test statistics, formulated under the energy detection paradigm of spectrum sensing. In doing so, we develop a novel multi-agent implementation of hybrid soft actor critic, MHSAC, based on the QMIX mixing scheme. Through experiments, we find that our SSRA algorithm, HySSRA, is successful in maximizing the CRN's utilization of spectrum resources while also limiting its interference with the primary network, and outperforms the current state-of-the-art by a wide margin. We also explore the impact of wireless variations such as coherence time on the efficacy of the system.
<div id='section'>Paperid: <span id='pid'>231, <a href='https://arxiv.org/pdf/2404.09715.pdf' target='_blank'>https://arxiv.org/pdf/2404.09715.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Linjie Xu, Zichuan Liu, Alexander Dockhorn, Diego Perez-Liebana, Jinyu Wang, Lei Song, Jiang Bian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.09715">Higher Replay Ratio Empowers Sample-Efficient Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>One of the notorious issues for Reinforcement Learning (RL) is poor sample efficiency. Compared to single agent RL, the sample efficiency for Multi-Agent Reinforcement Learning (MARL) is more challenging because of its inherent partial observability, non-stationary training, and enormous strategy space. Although much effort has been devoted to developing new methods and enhancing sample efficiency, we look at the widely used episodic training mechanism. In each training step, tens of frames are collected, but only one gradient step is made. We argue that this episodic training could be a source of poor sample efficiency. To better exploit the data already collected, we propose to increase the frequency of the gradient updates per environment interaction (a.k.a. Replay Ratio or Update-To-Data ratio). To show its generality, we evaluate $3$ MARL methods on $6$ SMAC tasks. The empirical results validate that a higher replay ratio significantly improves the sample efficiency for MARL algorithms. The codes to reimplement the results presented in this paper are open-sourced at https://anonymous.4open.science/r/rr_for_MARL-0D83/.
<div id='section'>Paperid: <span id='pid'>232, <a href='https://arxiv.org/pdf/2311.03756.pdf' target='_blank'>https://arxiv.org/pdf/2311.03756.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yao Zhang, Zhiwen Yu, Jun Zhang, Liang Wang, Tom H. Luan, Bin Guo, Chau Yuen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.03756">Learning Decentralized Traffic Signal Controllers with Multi-Agent Graph Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper considers optimal traffic signal control in smart cities, which has been taken as a complex networked system control problem. Given the interacting dynamics among traffic lights and road networks, attaining controller adaptivity and scalability stands out as a primary challenge. Capturing the spatial-temporal correlation among traffic lights under the framework of Multi-Agent Reinforcement Learning (MARL) is a promising solution. Nevertheless, existing MARL algorithms ignore effective information aggregation which is fundamental for improving the learning capacity of decentralized agents. In this paper, we design a new decentralized control architecture with improved environmental observability to capture the spatial-temporal correlation. Specifically, we first develop a topology-aware information aggregation strategy to extract correlation-related information from unstructured data gathered in the road network. Particularly, we transfer the road network topology into a graph shift operator by forming a diffusion process on the topology, which subsequently facilitates the construction of graph signals. A diffusion convolution module is developed, forming a new MARL algorithm, which endows agents with the capabilities of graph learning. Extensive experiments based on both synthetic and real-world datasets verify that our proposal outperforms existing decentralized algorithms.
<div id='section'>Paperid: <span id='pid'>233, <a href='https://arxiv.org/pdf/2311.03150.pdf' target='_blank'>https://arxiv.org/pdf/2311.03150.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhuoya Zhao, Feifei Zhao, Shiwen Wang, Yinqian Sun, Yi Zeng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.03150">A Brain-inspired Theory of Collective Mind Model for Efficient Social Cooperation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Social intelligence manifests the capability, often referred to as the Theory of Mind (ToM), to discern others' behavioral intentions, beliefs, and other mental states. ToM is especially important in multi-agent and human-machine interaction environments because each agent needs to understand the mental states of other agents in order to better respond, interact, and collaborate. Recent research indicates that the ToM model possesses the capability to infer beliefs, intentions, and anticipate future observations and actions; nonetheless, its deployment in tackling intricate tasks remains notably limited. The challenges arise when the number of agents increases, the environment becomes more complex, and interacting with the environment and predicting the mental state of each other becomes difficult and time consuming. To overcome such limits, we take inspiration from the Theory of Collective Mind (ToCM) mechanism, predicting observations of all other agents into a unified but plural representation and discerning how our own actions affect this mental state representation. Based on this foundation, we construct an imaginative space to simulate the multi-agent interaction process, thus improving the efficiency of cooperation among multiple agents in complex decision-making environments. In various cooperative tasks with different numbers of agents, the experimental results highlight the superior cooperative efficiency and performance of our approach compared to the Multi-Agent Reinforcement Learning (MARL) baselines. We achieve consistent boost on SNN- and DNN-based decision networks, and demonstrate that ToCM's inferences about others' mental states can be transferred to new tasks for quickly and flexible adaptation.
<div id='section'>Paperid: <span id='pid'>234, <a href='https://arxiv.org/pdf/2309.16307.pdf' target='_blank'>https://arxiv.org/pdf/2309.16307.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qirui Mi, Siyu Xia, Yan Song, Haifeng Zhang, Shenghao Zhu, Jun Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.16307">TaxAI: A Dynamic Economic Simulator and Benchmark for Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Taxation and government spending are crucial tools for governments to promote economic growth and maintain social equity. However, the difficulty in accurately predicting the dynamic strategies of diverse self-interested households presents a challenge for governments to implement effective tax policies. Given its proficiency in modeling other agents in partially observable environments and adaptively learning to find optimal policies, Multi-Agent Reinforcement Learning (MARL) is highly suitable for solving dynamic games between the government and numerous households. Although MARL shows more potential than traditional methods such as the genetic algorithm and dynamic programming, there is a lack of large-scale multi-agent reinforcement learning economic simulators. Therefore, we propose a MARL environment, named \textbf{TaxAI}, for dynamic games involving $N$ households, government, firms, and financial intermediaries based on the Bewley-Aiyagari economic model. Our study benchmarks 2 traditional economic methods with 7 MARL methods on TaxAI, demonstrating the effectiveness and superiority of MARL algorithms. Moreover, TaxAI's scalability in simulating dynamic interactions between the government and 10,000 households, coupled with real-data calibration, grants it a substantial improvement in scale and reality over existing simulators. Therefore, TaxAI is the most realistic economic simulator for optimal tax policy, which aims to generate feasible recommendations for governments and individuals.
<div id='section'>Paperid: <span id='pid'>235, <a href='https://arxiv.org/pdf/2308.02603.pdf' target='_blank'>https://arxiv.org/pdf/2308.02603.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruijin Sun, Xiao Yang, Nan Cheng, Xiucheng Wang, Changle Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.02603">Knowledge-Driven Multi-Agent Reinforcement Learning for Computation Offloading in Cybertwin-Enabled Internet of Vehicles</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>By offloading computation-intensive tasks of vehicles to roadside units (RSUs), mobile edge computing (MEC) in the Internet of Vehicles (IoV) can relieve the onboard computation burden. However, existing model-based task offloading methods suffer from heavy computational complexity with the increase of vehicles and data-driven methods lack interpretability. To address these challenges, in this paper, we propose a knowledge-driven multi-agent reinforcement learning (KMARL) approach to reduce the latency of task offloading in cybertwin-enabled IoV. Specifically, in the considered scenario, the cybertwin serves as a communication agent for each vehicle to exchange information and make offloading decisions in the virtual space. To reduce the latency of task offloading, a KMARL approach is proposed to select the optimal offloading option for each vehicle, where graph neural networks are employed by leveraging domain knowledge concerning graph-structure communication topology and permutation invariance into neural networks. Numerical results show that our proposed KMARL yields higher rewards and demonstrates improved scalability compared with other methods, benefitting from the integration of domain knowledge.
<div id='section'>Paperid: <span id='pid'>236, <a href='https://arxiv.org/pdf/2306.07542.pdf' target='_blank'>https://arxiv.org/pdf/2306.07542.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xianliang Yang, Zhihao Liu, Wei Jiang, Chuheng Zhang, Li Zhao, Lei Song, Jiang Bian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.07542">A Versatile Multi-Agent Reinforcement Learning Benchmark for Inventory Management</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent reinforcement learning (MARL) models multiple agents that interact and learn within a shared environment. This paradigm is applicable to various industrial scenarios such as autonomous driving, quantitative trading, and inventory management. However, applying MARL to these real-world scenarios is impeded by many challenges such as scaling up, complex agent interactions, and non-stationary dynamics. To incentivize the research of MARL on these challenges, we develop MABIM (Multi-Agent Benchmark for Inventory Management) which is a multi-echelon, multi-commodity inventory management simulator that can generate versatile tasks with these different challenging properties. Based on MABIM, we evaluate the performance of classic operations research (OR) methods and popular MARL algorithms on these challenging tasks to highlight their weaknesses and potential.
<div id='section'>Paperid: <span id='pid'>237, <a href='https://arxiv.org/pdf/2502.08119.pdf' target='_blank'>https://arxiv.org/pdf/2502.08119.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiahao You, Ziye Jia, Chao Dong, Qihui Wu, Zhu Han
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.08119">Generative AI-Enhanced Cooperative MEC of UAVs and Ground Stations for Unmanned Surface Vehicles</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The increasing deployment of unmanned surface vehicles (USVs) require computational support and coverage in applications such as maritime search and rescue. Unmanned aerial vehicles (UAVs) can offer low-cost, flexible aerial services, and ground stations (GSs) can provide powerful supports, which can cooperate to help the USVs in complex scenarios. However, the collaboration between UAVs and GSs for USVs faces challenges of task uncertainties, USVs trajectory uncertainties, heterogeneities, and limited computational resources. To address these issues, we propose a cooperative UAV and GS based robust multi-access edge computing framework to assist USVs in completing computational tasks. Specifically, we formulate the optimization problem of joint task offloading and UAV trajectory to minimize the total execution time, which is in the form of mixed integer nonlinear programming and NP-hard to tackle. Therefore, we propose the algorithm of generative artificial intelligence-enhanced heterogeneous agent proximal policy optimization (GAI-HAPPO). The proposed algorithm integrates GAI models to enhance the actor network ability to model complex environments and extract high-level features, thereby allowing the algorithm to predict uncertainties and adapt to dynamic conditions. Additionally, GAI stabilizes the critic network, addressing the instability of multi-agent reinforcement learning approaches. Finally, extensive simulations demonstrate that the proposed algorithm outperforms the existing benchmark methods, thus highlighting the potentials in tackling intricate, cross-domain issues in the considered scenarios.
<div id='section'>Paperid: <span id='pid'>238, <a href='https://arxiv.org/pdf/2410.21616.pdf' target='_blank'>https://arxiv.org/pdf/2410.21616.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiwen Qiu, Yujia Zheng, Kun Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.21616">Identifying Selections for Unsupervised Subtask Discovery</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>When solving long-horizon tasks, it is intriguing to decompose the high-level task into subtasks. Decomposing experiences into reusable subtasks can improve data efficiency, accelerate policy generalization, and in general provide promising solutions to multi-task reinforcement learning and imitation learning problems. However, the concept of subtasks is not sufficiently understood and modeled yet, and existing works often overlook the true structure of the data generation process: subtasks are the results of a $\textit{selection}$ mechanism on actions, rather than possible underlying confounders or intermediates. Specifically, we provide a theory to identify, and experiments to verify the existence of selection variables in such data. These selections serve as subgoals that indicate subtasks and guide policy. In light of this idea, we develop a sequential non-negative matrix factorization (seq- NMF) method to learn these subgoals and extract meaningful behavior patterns as subtasks. Our empirical results on a challenging Kitchen environment demonstrate that the learned subtasks effectively enhance the generalization to new tasks in multi-task imitation learning scenarios. The codes are provided at https://anonymous.4open.science/r/Identifying\_Selections\_for\_Unsupervised\_Subtask\_Discovery/README.md.
<div id='section'>Paperid: <span id='pid'>239, <a href='https://arxiv.org/pdf/2509.23154.pdf' target='_blank'>https://arxiv.org/pdf/2509.23154.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinzhe Pan, Jingqing Wang, Yuehui Ouyang, Wenchi Cheng, Wei Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.23154">AI-Enhanced Distributed Channel Access for Collision Avoidance in Future Wi-Fi 8</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The exponential growth of wireless devices and stringent reliability requirements of emerging applications demand fundamental improvements in distributed channel access mechanisms for unlicensed bands. Current Wi-Fi systems, which rely on binary exponential backoff (BEB), suffer from suboptimal collision resolution in dense deployments and persistent fairness challenges due to inherent randomness. This paper introduces a multi-agent reinforcement learning framework that integrates artificial intelligence (AI) optimization with legacy device coexistence. We first develop a dynamic backoff selection mechanism that adapts to real-time channel conditions through access deferral events while maintaining full compatibility with conventional CSMA/CA operations. Second, we introduce a fairness quantification metric aligned with enhanced distributed channel access (EDCA) principles to ensure equitable medium access opportunities. Finally, we propose a centralized training decentralized execution (CTDE) architecture incorporating neighborhood activity patterns as observational inputs, optimized via constrained multi-agent proximal policy optimization (MAPPO) to jointly minimize collisions and guarantee fairness. Experimental results demonstrate that our solution significantly reduces collision probability compared to conventional BEB while preserving backward compatibility with commercial Wi-Fi devices. The proposed fairness metric effectively eliminates starvation risks in heterogeneous scenarios.
<div id='section'>Paperid: <span id='pid'>240, <a href='https://arxiv.org/pdf/2509.23154.pdf' target='_blank'>https://arxiv.org/pdf/2509.23154.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinzhe Pan, Jingqing Wang, Yuehui Ouyang, Wenchi Cheng, Wei Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.23154">AI-Enhanced Distributed Channel Access for Collision Avoidance in Future Wi-Fi 8</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The exponential growth of wireless devices and stringent reliability requirements of emerging applications demand fundamental improvements in distributed channel access mechanisms for unlicensed bands. Current Wi-Fi systems, which rely on binary exponential backoff (BEB), suffer from suboptimal collision resolution in dense deployments and persistent fairness challenges due to inherent randomness. This paper introduces a multi-agent reinforcement learning framework that integrates artificial intelligence (AI) optimization with legacy device coexistence. We first develop a dynamic backoff selection mechanism that adapts to real-time channel conditions through access deferral events while maintaining full compatibility with conventional CSMA/CA operations. Second, we introduce a fairness quantification metric aligned with enhanced distributed channel access (EDCA) principles to ensure equitable medium access opportunities. Finally, we propose a centralized training decentralized execution (CTDE) architecture incorporating neighborhood activity patterns as observational inputs, optimized via constrained multi-agent proximal policy optimization (MAPPO) to jointly minimize collisions and guarantee fairness. Experimental results demonstrate that our solution significantly reduces collision probability compared to conventional BEB while preserving backward compatibility with commercial Wi-Fi devices. The proposed fairness metric effectively eliminates starvation risks in heterogeneous scenarios.
<div id='section'>Paperid: <span id='pid'>241, <a href='https://arxiv.org/pdf/2501.00226.pdf' target='_blank'>https://arxiv.org/pdf/2501.00226.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tadahiro Taniguchi, Ryo Ueda, Tomoaki Nakamura, Masahiro Suzuki, Akira Taniguchi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.00226">Generative Emergent Communication: Large Language Model is a Collective World Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large Language Models (LLMs) have demonstrated a remarkable ability to capture extensive world knowledge, yet how this is achieved without direct sensorimotor experience remains a fundamental puzzle. This study proposes a novel theoretical solution by introducing the Collective World Model hypothesis. We argue that an LLM does not learn a world model from scratch; instead, it learns a statistical approximation of a collective world model that is already implicitly encoded in human language through a society-wide process of embodied, interactive sense-making. To formalize this process, we introduce generative emergent communication (Generative EmCom), a framework built on the Collective Predictive Coding (CPC). This framework models the emergence of language as a process of decentralized Bayesian inference over the internal states of multiple agents. We argue that this process effectively creates an encoder-decoder structure at a societal scale: human society collectively encodes its grounded, internal representations into language, and an LLM subsequently decodes these symbols to reconstruct a latent space that mirrors the structure of the original collective representations. This perspective provides a principled, mathematical explanation for how LLMs acquire their capabilities. The main contributions of this paper are: 1) the formalization of the Generative EmCom framework, clarifying its connection to world models and multi-agent reinforcement learning, and 2) its application to interpret LLMs, explaining phenomena such as distributional semantics as a natural consequence of representation reconstruction. This work provides a unified theory that bridges individual cognitive development, collective language evolution, and the foundations of large-scale AI.
<div id='section'>Paperid: <span id='pid'>242, <a href='https://arxiv.org/pdf/2408.02148.pdf' target='_blank'>https://arxiv.org/pdf/2408.02148.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mustafa Yasir, Andrew Howes, Vasilios Mavroudis, Chris Hicks
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.02148">Environment Complexity and Nash Equilibria in a Sequential Social Dilemma</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent reinforcement learning (MARL) methods, while effective in zero-sum or positive-sum games, often yield suboptimal outcomes in general-sum games where cooperation is essential for achieving globally optimal outcomes. Matrix game social dilemmas, which abstract key aspects of general-sum interactions, such as cooperation, risk, and trust, fail to model the temporal and spatial dynamics characteristic of real-world scenarios. In response, our study extends matrix game social dilemmas into more complex, higher-dimensional MARL environments. We adapt a gridworld implementation of the Stag Hunt dilemma to more closely match the decision-space of a one-shot matrix game while also introducing variable environment complexity. Our findings indicate that as complexity increases, MARL agents trained in these environments converge to suboptimal strategies, consistent with the risk-dominant Nash equilibria strategies found in matrix games. Our work highlights the impact of environment complexity on achieving optimal outcomes in higher-dimensional game-theoretic MARL environments.
<div id='section'>Paperid: <span id='pid'>243, <a href='https://arxiv.org/pdf/2307.05004.pdf' target='_blank'>https://arxiv.org/pdf/2307.05004.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tomoaki Nakamura, Akira Taniguchi, Tadahiro Taniguchi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.05004">Control as Probabilistic Inference as an Emergent Communication Mechanism in Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper proposes a generative probabilistic model integrating emergent communication and multi-agent reinforcement learning. The agents plan their actions by probabilistic inference, called control as inference, and communicate using messages that are latent variables and estimated based on the planned actions. Through these messages, each agent can send information about its actions and know information about the actions of another agent. Therefore, the agents change their actions according to the estimated messages to achieve cooperative tasks. This inference of messages can be considered as communication, and this procedure can be formulated by the Metropolis-Hasting naming game. Through experiments in the grid world environment, we show that the proposed PGM can infer meaningful messages to achieve the cooperative task.
<div id='section'>Paperid: <span id='pid'>244, <a href='https://arxiv.org/pdf/2302.12515.pdf' target='_blank'>https://arxiv.org/pdf/2302.12515.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuefeng Wang, Xinran Li, Jiawei Shao, Jun Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.12515">AC2C: Adaptively Controlled Two-Hop Communication for Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Learning communication strategies in cooperative multi-agent reinforcement learning (MARL) has recently attracted intensive attention. Early studies typically assumed a fully-connected communication topology among agents, which induces high communication costs and may not be feasible. Some recent works have developed adaptive communication strategies to reduce communication overhead, but these methods cannot effectively obtain valuable information from agents that are beyond the communication range. In this paper, we consider a realistic communication model where each agent has a limited communication range, and the communication topology dynamically changes. To facilitate effective agent communication, we propose a novel communication protocol called Adaptively Controlled Two-Hop Communication (AC2C). After an initial local communication round, AC2C employs an adaptive two-hop communication strategy to enable long-range information exchange among agents to boost performance, which is implemented by a communication controller. This controller determines whether each agent should ask for two-hop messages and thus helps to reduce the communication overhead during distributed execution. We evaluate AC2C on three cooperative multi-agent tasks, and the experimental results show that it outperforms relevant baselines with lower communication costs.
<div id='section'>Paperid: <span id='pid'>245, <a href='https://arxiv.org/pdf/2505.19714.pdf' target='_blank'>https://arxiv.org/pdf/2505.19714.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhaopeng Feng, Yupu Liang, Shaosheng Cao, Jiayuan Su, Jiahan Ren, Zhe Xu, Yao Hu, Wenxuan Huang, Jian Wu, Zuozhu Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.19714">MT$^{3}$: Scaling MLLM-based Text Image Machine Translation via Multi-Task Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text Image Machine Translation (TIMT)-the task of translating textual content embedded in images-is critical for applications in accessibility, cross-lingual information access, and real-world document understanding. However, TIMT remains a complex challenge due to the need for accurate optical character recognition (OCR), robust visual-text reasoning, and high-quality translation, often requiring cascading multi-stage pipelines. Recent advances in large-scale Reinforcement Learning (RL) have improved reasoning in Large Language Models (LLMs) and Multimodal LLMs (MLLMs), but their application to end-to-end TIMT is still underexplored. To bridge this gap, we introduce MT$^{3}$, the first framework to apply Multi-Task RL to MLLMs for end-to-end TIMT. MT$^{3}$ adopts a multi-task optimization paradigm targeting three key sub-skills: text recognition, context-aware reasoning, and translation. It is trained using a novel multi-mixed reward mechanism that adapts rule-based RL strategies to TIMT's intricacies, offering fine-grained, non-binary feedback across tasks. Furthermore, to facilitate the evaluation of TIMT in authentic cross-cultural and real-world social media contexts, we introduced XHSPost, the first social media TIMT benchmark. Our MT$^{3}$-7B-Zero achieves state-of-the-art results on the latest in-domain MIT-10M benchmark, outperforming strong baselines such as Qwen2.5-VL-72B and InternVL2.5-78B by notable margins across multiple metrics. Additionally, the model shows strong generalization to out-of-distribution language pairs and datasets. In-depth analyses reveal how multi-task synergy, reinforcement learning initialization, curriculum design, and reward formulation contribute to advancing MLLM-driven TIMT.
<div id='section'>Paperid: <span id='pid'>246, <a href='https://arxiv.org/pdf/2505.03558.pdf' target='_blank'>https://arxiv.org/pdf/2505.03558.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Giacomo Avanzi, Marco Giordani, Michele Zorzi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.03558">Multi-Agent Reinforcement Learning Scheduling to Support Low Latency in Teleoperated Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The teleoperated driving (TD) scenario comes with stringent Quality of Service (QoS) communication constraints, especially in terms of end-to-end (E2E) latency and reliability. In this context, Predictive Quality of Service (PQoS), possibly combined with Reinforcement Learning (RL) techniques, is a powerful tool to estimate QoS degradation and react accordingly. For example, an intelligent agent can be trained to select the optimal compression configuration for automotive data, and reduce the file size whenever QoS conditions deteriorate. However, compression may inevitably compromise data quality, with negative implications for the TD application. An alternative strategy involves operating at the Radio Access Network (RAN) level to optimize radio parameters based on current network conditions, while preserving data quality. In this paper, we propose Multi-Agent Reinforcement Learning (MARL) scheduling algorithms, based on Proximal Policy Optimization (PPO), to dynamically and intelligently allocate radio resources to minimize E2E latency in a TD scenario. We evaluate two training paradigms, i.e., decentralized learning with local observations (IPPO) vs. centralized aggregation (MAPPO), in conjunction with two resource allocation strategies, i.e., proportional allocation (PA) and greedy allocation (GA). We prove via ns-3 simulations that MAPPO, combined with GA, achieves the best results in terms of latency, especially as the number of vehicles increases.
<div id='section'>Paperid: <span id='pid'>247, <a href='https://arxiv.org/pdf/2409.05712.pdf' target='_blank'>https://arxiv.org/pdf/2409.05712.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiaqi Liu, Peng Hang, Xiaoxiang Na, Chao Huang, Jian Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.05712">Cooperative Decision-Making for CAVs at Unsignalized Intersections: A MARL Approach with Attention and Hierarchical Game Priors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The development of autonomous vehicles has shown great potential to enhance the efficiency and safety of transportation systems. However, the decision-making issue in complex human-machine mixed traffic scenarios, such as unsignalized intersections, remains a challenge for autonomous vehicles. While reinforcement learning (RL) has been used to solve complex decision-making problems, existing RL methods still have limitations in dealing with cooperative decision-making of multiple connected autonomous vehicles (CAVs), ensuring safety during exploration, and simulating realistic human driver behaviors. In this paper, a novel and efficient algorithm, Multi-Agent Game-prior Attention Deep Deterministic Policy Gradient (MA-GA-DDPG), is proposed to address these limitations. Our proposed algorithm formulates the decision-making problem of CAVs at unsignalized intersections as a decentralized multi-agent reinforcement learning problem and incorporates an attention mechanism to capture interaction dependencies between ego CAV and other agents. The attention weights between the ego vehicle and other agents are then used to screen interaction objects and obtain prior hierarchical game relations, based on which a safety inspector module is designed to improve the traffic safety. Furthermore, both simulation and hardware-in-the-loop experiments were conducted, demonstrating that our method outperforms other baseline approaches in terms of driving safety, efficiency, and comfort.
<div id='section'>Paperid: <span id='pid'>248, <a href='https://arxiv.org/pdf/2404.19292.pdf' target='_blank'>https://arxiv.org/pdf/2404.19292.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qiaosheng Zhang, Chenjia Bai, Shuyue Hu, Zhen Wang, Xuelong Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.19292">Provably Efficient Information-Directed Sampling Algorithms for Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work designs and analyzes a novel set of algorithms for multi-agent reinforcement learning (MARL) based on the principle of information-directed sampling (IDS). These algorithms draw inspiration from foundational concepts in information theory, and are proven to be sample efficient in MARL settings such as two-player zero-sum Markov games (MGs) and multi-player general-sum MGs. For episodic two-player zero-sum MGs, we present three sample-efficient algorithms for learning Nash equilibrium. The basic algorithm, referred to as MAIDS, employs an asymmetric learning structure where the max-player first solves a minimax optimization problem based on the joint information ratio of the joint policy, and the min-player then minimizes the marginal information ratio with the max-player's policy fixed. Theoretical analyses show that it achieves a Bayesian regret of tilde{O}(sqrt{K}) for K episodes. To reduce the computational load of MAIDS, we develop an improved algorithm called Reg-MAIDS, which has the same Bayesian regret bound while enjoying less computational complexity. Moreover, by leveraging the flexibility of IDS principle in choosing the learning target, we propose two methods for constructing compressed environments based on rate-distortion theory, upon which we develop an algorithm Compressed-MAIDS wherein the learning target is a compressed environment. Finally, we extend Reg-MAIDS to multi-player general-sum MGs and prove that it can learn either the Nash equilibrium or coarse correlated equilibrium in a sample efficient manner.
<div id='section'>Paperid: <span id='pid'>249, <a href='https://arxiv.org/pdf/2404.15696.pdf' target='_blank'>https://arxiv.org/pdf/2404.15696.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiaqi Liu, Ziran Wang, Peng Hang, Jian Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.15696">Delay-Aware Multi-Agent Reinforcement Learning for Cooperative Adaptive Cruise Control with Model-based Stability Enhancement</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cooperative Adaptive Cruise Control (CACC) represents a quintessential control strategy for orchestrating vehicular platoon movement within Connected and Automated Vehicle (CAV) systems, significantly enhancing traffic efficiency and reducing energy consumption. In recent years, the data-driven methods, such as reinforcement learning (RL), have been employed to address this task due to their significant advantages in terms of efficiency and flexibility. However, the delay issue, which often arises in real-world CACC systems, is rarely taken into account by current RL-based approaches. To tackle this problem, we propose a Delay-Aware Multi-Agent Reinforcement Learning (DAMARL) framework aimed at achieving safe and stable control for CACC. We model the entire decision-making process using a Multi-Agent Delay-Aware Markov Decision Process (MADA-MDP) and develop a centralized training with decentralized execution (CTDE) MARL framework for distributed control of CACC platoons. An attention mechanism-integrated policy network is introduced to enhance the performance of CAV communication and decision-making. Additionally, a velocity optimization model-based action filter is incorporated to further ensure the stability of the platoon. Experimental results across various delay conditions and platoon sizes demonstrate that our approach consistently outperforms baseline methods in terms of platoon safety, stability and overall performance.
<div id='section'>Paperid: <span id='pid'>250, <a href='https://arxiv.org/pdf/2308.02345.pdf' target='_blank'>https://arxiv.org/pdf/2308.02345.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dong Chen, Kaixiang Zhang, Yongqiang Wang, Xunyuan Yin, Zhaojian Li, Dimitar Filev
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.02345">Communication-Efficient Decentralized Multi-Agent Reinforcement Learning for Cooperative Adaptive Cruise Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Connected and autonomous vehicles (CAVs) promise next-gen transportation systems with enhanced safety, energy efficiency, and sustainability. One typical control strategy for CAVs is the so-called cooperative adaptive cruise control (CACC) where vehicles drive in platoons and cooperate to achieve safe and efficient transportation. In this study, we formulate CACC as a multi-agent reinforcement learning (MARL) problem. Diverging from existing MARL methods that use centralized training and decentralized execution which require not only a centralized communication mechanism but also dense inter-agent communication during training and online adaptation, we propose a fully decentralized MARL framework for enhanced efficiency and scalability. In addition, a quantization-based communication scheme is proposed to reduce the communication overhead without significantly degrading the control performance. This is achieved by employing randomized rounding numbers to quantize each piece of communicated information and only communicating non-zero components after quantization. Extensive experimentation in two distinct CACC settings reveals that the proposed MARL framework consistently achieves superior performance over several contemporary benchmarks in terms of both communication efficiency and control efficacy. In the appendix, we show that our proposed framework's applicability extends beyond CACC, showing promise for broader intelligent transportation systems with intricate action and state spaces.
<div id='section'>Paperid: <span id='pid'>251, <a href='https://arxiv.org/pdf/2304.02005.pdf' target='_blank'>https://arxiv.org/pdf/2304.02005.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Abdullah Al Maruf, Luyao Niu, Bhaskar Ramasubramanian, Andrew Clark, Radha Poovendran
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.02005">Risk-Aware Distributed Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autonomous cyber and cyber-physical systems need to perform decision-making, learning, and control in unknown environments. Such decision-making can be sensitive to multiple factors, including modeling errors, changes in costs, and impacts of events in the tails of probability distributions. Although multi-agent reinforcement learning (MARL) provides a framework for learning behaviors through repeated interactions with the environment by minimizing an average cost, it will not be adequate to overcome the above challenges. In this paper, we develop a distributed MARL approach to solve decision-making problems in unknown environments by learning risk-aware actions. We use the conditional value-at-risk (CVaR) to characterize the cost function that is being minimized, and define a Bellman operator to characterize the value function associated to a given state-action pair. We prove that this operator satisfies a contraction property, and that it converges to the optimal value function. We then propose a distributed MARL algorithm called the CVaR QD-Learning algorithm, and establish that value functions of individual agents reaches consensus. We identify several challenges that arise in the implementation of the CVaR QD-Learning algorithm, and present solutions to overcome these. We evaluate the CVaR QD-Learning algorithm through simulations, and demonstrate the effect of a risk parameter on value functions at consensus.
<div id='section'>Paperid: <span id='pid'>252, <a href='https://arxiv.org/pdf/2504.21278.pdf' target='_blank'>https://arxiv.org/pdf/2504.21278.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuyan Ma, Yawen Wang, Junjie Wang, Xiaofei Xie, Boyu Wu, Shoubin Li, Fanjiang Xu, Qing Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.21278">Robust Multi-agent Communication Based on Decentralization-Oriented Adversarial Training</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In typical multi-agent reinforcement learning (MARL) problems, communication is important for agents to share information and make the right decisions. However, due to the complexity of training multi-agent communication, existing methods often fall into the dilemma of local optimization, which leads to the concentration of communication in a limited number of channels and presents an unbalanced structure. Such unbalanced communication policy are vulnerable to abnormal conditions, where the damage of critical communication channels can trigger the crash of the entire system. Inspired by decentralization theory in sociology, we propose DMAC, which enhances the robustness of multi-agent communication policies by retraining them into decentralized patterns. Specifically, we train an adversary DMAC\_Adv which can dynamically identify and mask the critical communication channels, and then apply the adversarial samples generated by DMAC\_Adv to the adversarial learning of the communication policy to force the policy in exploring other potential communication schemes and transition to a decentralized structure. As a training method to improve robustness, DMAC can be fused with any learnable communication policy algorithm. The experimental results in two communication policies and four multi-agent tasks demonstrate that DMAC achieves higher improvement on robustness and performance of communication policy compared with two state-of-the-art and commonly-used baselines. Also, the results demonstrate that DMAC can achieve decentralized communication structure with acceptable communication cost.
<div id='section'>Paperid: <span id='pid'>253, <a href='https://arxiv.org/pdf/2402.16499.pdf' target='_blank'>https://arxiv.org/pdf/2402.16499.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junzhe Chen, Xuming Hu, Shuodi Liu, Shiyu Huang, Wei-Wei Tu, Zhaofeng He, Lijie Wen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.16499">LLMArena: Assessing Capabilities of Large Language Models in Dynamic Multi-Agent Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in large language models (LLMs) have revealed their potential for achieving autonomous agents possessing human-level intelligence. However, existing benchmarks for evaluating LLM Agents either use static datasets, potentially leading to data leakage or focus only on single-agent scenarios, overlooking the complexities of multi-agent interactions. There is a lack of a benchmark that evaluates the diverse capabilities of LLM agents in multi-agent, dynamic environments. To this end, we introduce LLMArena, a novel and easily extensible framework for evaluating the diverse capabilities of LLM in multi-agent dynamic environments. LLMArena encompasses seven distinct gaming environments, employing Trueskill scoring to assess crucial abilities in LLM agents, including spatial reasoning, strategic planning, numerical reasoning, risk assessment, communication, opponent modeling, and team collaboration. We conduct an extensive experiment and human evaluation among different sizes and types of LLMs, showing that LLMs still have a significant journey ahead in their development towards becoming fully autonomous agents, especially in opponent modeling and team collaboration. We hope LLMArena could guide future research towards enhancing these capabilities in LLMs, ultimately leading to more sophisticated and practical applications in dynamic, multi-agent settings. The code and data will be available.
<div id='section'>Paperid: <span id='pid'>254, <a href='https://arxiv.org/pdf/2111.04613.pdf' target='_blank'>https://arxiv.org/pdf/2111.04613.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiayu Chen, Yuanxin Zhang, Yuanfan Xu, Huimin Ma, Huazhong Yang, Jiaming Song, Yu Wang, Yi Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2111.04613">Variational Automatic Curriculum Learning for Sparse-Reward Cooperative Multi-Agent Problems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce a curriculum learning algorithm, Variational Automatic Curriculum Learning (VACL), for solving challenging goal-conditioned cooperative multi-agent reinforcement learning problems. We motivate our paradigm through a variational perspective, where the learning objective can be decomposed into two terms: task learning on the current task distribution, and curriculum update to a new task distribution. Local optimization over the second term suggests that the curriculum should gradually expand the training tasks from easy to hard. Our VACL algorithm implements this variational paradigm with two practical components, task expansion and entity progression, which produces training curricula over both the task configurations as well as the number of entities in the task. Experiment results show that VACL solves a collection of sparse-reward problems with a large number of agents. Particularly, using a single desktop machine, VACL achieves 98% coverage rate with 100 agents in the simple-spread benchmark and reproduces the ramp-use behavior originally shown in OpenAI's hide-and-seek project. Our project website is at https://sites.google.com/view/vacl-neurips-2021.
<div id='section'>Paperid: <span id='pid'>255, <a href='https://arxiv.org/pdf/2509.14680.pdf' target='_blank'>https://arxiv.org/pdf/2509.14680.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianyang Duan, Zongyuan Zhang, Songxiao Guo, Dong Huang, Yuanye Zhao, Zheng Lin, Zihan Fang, Dianxin Luan, Heming Cui, Yong Cui
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.14680">LEED: A Highly Efficient and Scalable LLM-Empowered Expert Demonstrations Framework for Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent reinforcement learning (MARL) holds substantial promise for intelligent decision-making in complex environments. However, it suffers from a coordination and scalability bottleneck as the number of agents increases. To address these issues, we propose the LLM-empowered expert demonstrations framework for multi-agent reinforcement learning (LEED). LEED consists of two components: a demonstration generation (DG) module and a policy optimization (PO) module. Specifically, the DG module leverages large language models to generate instructions for interacting with the environment, thereby producing high-quality demonstrations. The PO module adopts a decentralized training paradigm, where each agent utilizes the generated demonstrations to construct an expert policy loss, which is then integrated with its own policy loss. This enables each agent to effectively personalize and optimize its local policy based on both expert knowledge and individual experience. Experimental results show that LEED achieves superior sample efficiency, time efficiency, and robust scalability compared to state-of-the-art baselines.
<div id='section'>Paperid: <span id='pid'>256, <a href='https://arxiv.org/pdf/2509.14680.pdf' target='_blank'>https://arxiv.org/pdf/2509.14680.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianyang Duan, Zongyuan Zhang, Songxiao Guo, Dong Huang, Yuanye Zhao, Zheng Lin, Zihan Fang, Dianxin Luan, Heming Cui, Yong Cui
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.14680">LEED: A Highly Efficient and Scalable LLM-Empowered Expert Demonstrations Framework for Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent reinforcement learning (MARL) holds substantial promise for intelligent decision-making in complex environments. However, it suffers from a coordination and scalability bottleneck as the number of agents increases. To address these issues, we propose the LLM-empowered expert demonstrations framework for multi-agent reinforcement learning (LEED). LEED consists of two components: a demonstration generation (DG) module and a policy optimization (PO) module. Specifically, the DG module leverages large language models to generate instructions for interacting with the environment, thereby producing high-quality demonstrations. The PO module adopts a decentralized training paradigm, where each agent utilizes the generated demonstrations to construct an expert policy loss, which is then integrated with its own policy loss. This enables each agent to effectively personalize and optimize its local policy based on both expert knowledge and individual experience. Experimental results show that LEED achieves superior sample efficiency, time efficiency, and robust scalability compared to state-of-the-art baselines.
<div id='section'>Paperid: <span id='pid'>257, <a href='https://arxiv.org/pdf/2507.13370.pdf' target='_blank'>https://arxiv.org/pdf/2507.13370.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shijun Guo, Haoran Xu, Yaming Yang, Ziyu Guan, Wei Zhao, Xinyi Zhang, Yishan Song, Jiwei Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.13370">H-NeiFi: Non-Invasive and Consensus-Efficient Multi-Agent Opinion Guidance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The openness of social media enables the free exchange of opinions, but it also presents challenges in guiding opinion evolution towards global consensus. Existing methods often directly modify user views or enforce cross-group connections. These intrusive interventions undermine user autonomy, provoke psychological resistance, and reduce the efficiency of global consensus. Additionally, due to the lack of a long-term perspective, promoting local consensus often exacerbates divisions at the macro level. To address these issues, we propose the hierarchical, non-intrusive opinion guidance framework, H-NeiFi. It first establishes a two-layer dynamic model based on social roles, considering the behavioral characteristics of both experts and non-experts. Additionally, we introduce a non-intrusive neighbor filtering method that adaptively controls user communication channels. Using multi-agent reinforcement learning (MARL), we optimize information propagation paths through a long-term reward function, avoiding direct interference with user interactions. Experiments show that H-NeiFi increases consensus speed by 22.0% to 30.7% and maintains global convergence even in the absence of experts. This approach enables natural and efficient consensus guidance by protecting user interaction autonomy, offering a new paradigm for social network governance.
<div id='section'>Paperid: <span id='pid'>258, <a href='https://arxiv.org/pdf/2503.18816.pdf' target='_blank'>https://arxiv.org/pdf/2503.18816.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chak Lam Shek, Amrit Singh Bedi, Anjon Basak, Ellen Novoseller, Nick Waytowich, Priya Narayanan, Dinesh Manocha, Pratap Tokekar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.18816">Learning Multi-Robot Coordination through Locality-Based Factorized Multi-Agent Actor-Critic Algorithm</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this work, we present a novel cooperative multi-agent reinforcement learning method called \textbf{Loc}ality based \textbf{Fac}torized \textbf{M}ulti-Agent \textbf{A}ctor-\textbf{C}ritic (Loc-FACMAC). Existing state-of-the-art algorithms, such as FACMAC, rely on global reward information, which may not accurately reflect the quality of individual robots' actions in decentralized systems. We integrate the concept of locality into critic learning, where strongly related robots form partitions during training. Robots within the same partition have a greater impact on each other, leading to more precise policy evaluation. Additionally, we construct a dependency graph to capture the relationships between robots, facilitating the partitioning process. This approach mitigates the curse of dimensionality and prevents robots from using irrelevant information. Our method improves existing algorithms by focusing on local rewards and leveraging partition-based learning to enhance training efficiency and performance. We evaluate the performance of Loc-FACMAC in three environments: Hallway, Multi-cartpole, and Bounded-Cooperative-Navigation. We explore the impact of partition sizes on the performance and compare the result with baseline MARL algorithms such as LOMAQ, FACMAC, and QMIX. The experiments reveal that, if the locality structure is defined properly, Loc-FACMAC outperforms these baseline algorithms up to 108\%, indicating that exploiting the locality structure in the actor-critic framework improves the MARL performance.
<div id='section'>Paperid: <span id='pid'>259, <a href='https://arxiv.org/pdf/2412.03072.pdf' target='_blank'>https://arxiv.org/pdf/2412.03072.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinyu Qiao, Yudong Hu, Congying Han, Weiyan Wu, Tiande Guo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.03072">Preference-based opponent shaping in differentiable games</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Strategy learning in game environments with multi-agent is a challenging problem. Since each agent's reward is determined by the joint strategy, a greedy learning strategy that aims to maximize its own reward may fall into a local optimum. Recent studies have proposed the opponent modeling and shaping methods for game environments. These methods enhance the efficiency of strategy learning by modeling the strategies and updating processes of other agents. However, these methods often rely on simple predictions of opponent strategy changes. Due to the lack of modeling behavioral preferences such as cooperation and competition, they are usually applicable only to predefined scenarios and lack generalization capabilities. In this paper, we propose a novel Preference-based Opponent Shaping (PBOS) method to enhance the strategy learning process by shaping agents' preferences towards cooperation. We introduce the preference parameter, which is incorporated into the agent's loss function, thus allowing the agent to directly consider the opponent's loss function when updating the strategy. We update the preference parameters concurrently with strategy learning to ensure that agents can adapt to any cooperative or competitive game environment. Through a series of experiments, we verify the performance of PBOS algorithm in a variety of differentiable games. The experimental results show that the PBOS algorithm can guide the agent to learn the appropriate preference parameters, so as to achieve better reward distribution in multiple game environments.
<div id='section'>Paperid: <span id='pid'>260, <a href='https://arxiv.org/pdf/2312.15412.pdf' target='_blank'>https://arxiv.org/pdf/2312.15412.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuchen Shi, Congying Han, Tiande Guo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.15412">CARSS: Cooperative Attention-guided Reinforcement Subpath Synthesis for Solving Traveling Salesman Problem</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces CARSS (Cooperative Attention-guided Reinforcement Subpath Synthesis), a novel approach to address the Traveling Salesman Problem (TSP) by leveraging cooperative Multi-Agent Reinforcement Learning (MARL). CARSS decomposes the TSP solving process into two distinct yet synergistic steps: "subpath generation" and "subpath merging." In the former, a cooperative MARL framework is employed to iteratively generate subpaths using multiple agents. In the latter, these subpaths are progressively merged to form a complete cycle. The algorithm's primary objective is to enhance efficiency in terms of training memory consumption, testing time, and scalability, through the adoption of a multi-agent divide and conquer paradigm. Notably, attention mechanisms play a pivotal role in feature embedding and parameterization strategies within CARSS. The training of the model is facilitated by the independent REINFORCE algorithm. Empirical experiments reveal CARSS's superiority compared to single-agent alternatives: it demonstrates reduced GPU memory utilization, accommodates training graphs nearly 2.5 times larger, and exhibits the potential for scaling to even more extensive problem sizes. Furthermore, CARSS substantially reduces testing time and optimization gaps by approximately 50% for TSP instances of up to 1000 vertices, when compared to standard decoding methods.
<div id='section'>Paperid: <span id='pid'>261, <a href='https://arxiv.org/pdf/2311.04241.pdf' target='_blank'>https://arxiv.org/pdf/2311.04241.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Li-Hsiang Shen, Kai-Ten Feng, Ta-Sung Lee, Yuan-Chun Lin, Shih-Cheng Lin, Chia-Chan Chang, Sheng-Fuh Chang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.04241">AI-Enabled Unmanned Vehicle-Assisted Reconfigurable Intelligent Surfaces: Deployment, Prototyping, Experiments, and Opportunities</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The requirement of wireless data demands is increasingly high as the sixth-generation (6G) technology evolves. Reconfigurable intelligent surface (RIS) is promisingly deemed to be one of 6G techniques for extending service coverage, reducing power consumption, and enhancing spectral efficiency. In this article, we have provided some fundamentals of RIS deployment in theory and hardware perspectives as well as utilization of artificial intelligence (AI) and machine learning. We conducted an intelligent deployment of RIS (i-Dris) prototype, including dual-band auto-guided vehicle (AGV) assisted RISs associated with an mmWave base station (BS) and a receiver. The RISs are deployed on the AGV with configured incident/reflection angles. While, both the mmWave BS and receiver are associated with an edge server monitoring downlink packets for obtaining system throughput. We have designed a federated multi-agent reinforcement learning scheme associated with several AGV-RIS agents and sub-agents per AGV-RIS consisting of the deployment of position, height, orientation and elevation angles. The experimental results presented the stationary measurement in different aspects and scenarios. The i-Dris can reach up to 980 Mbps transmission throughput under a bandwidth of 100 MHz with comparably low complexity as well as rapid deployment, which outperforms the other existing works. At last, we highlight some opportunities and future issues in leveraging RIS-empowered wireless communication networks.
<div id='section'>Paperid: <span id='pid'>262, <a href='https://arxiv.org/pdf/2306.06236.pdf' target='_blank'>https://arxiv.org/pdf/2306.06236.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiyang Wu, Rohan Chandra, Tianrui Guan, Amrit Singh Bedi, Dinesh Manocha
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.06236">iPLAN: Intent-Aware Planning in Heterogeneous Traffic via Distributed Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Navigating safely and efficiently in dense and heterogeneous traffic scenarios is challenging for autonomous vehicles (AVs) due to their inability to infer the behaviors or intentions of nearby drivers. In this work, we introduce a distributed multi-agent reinforcement learning (MARL) algorithm that can predict trajectories and intents in dense and heterogeneous traffic scenarios. Our approach for intent-aware planning, iPLAN, allows agents to infer nearby drivers' intents solely from their local observations. We model two distinct incentives for agents' strategies: Behavioral Incentive for high-level decision-making based on their driving behavior or personality and Instant Incentive for motion planning for collision avoidance based on the current traffic state. Our approach enables agents to infer their opponents' behavior incentives and integrate this inferred information into their decision-making and motion-planning processes. We perform experiments on two simulation environments, Non-Cooperative Navigation and Heterogeneous Highway. In Heterogeneous Highway, results show that, compared with centralized training decentralized execution (CTDE) MARL baselines such as QMIX and MAPPO, our method yields a 4.3% and 38.4% higher episodic reward in mild and chaotic traffic, with 48.1% higher success rate and 80.6% longer survival time in chaotic traffic. We also compare with a decentralized training decentralized execution (DTDE) baseline IPPO and demonstrate a higher episodic reward of 12.7% and 6.3% in mild traffic and chaotic traffic, 25.3% higher success rate, and 13.7% longer survival time.
<div id='section'>Paperid: <span id='pid'>263, <a href='https://arxiv.org/pdf/2212.02723.pdf' target='_blank'>https://arxiv.org/pdf/2212.02723.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yudong Hu, Congying Han, Tiande Guo, Hao Xiao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2212.02723">Applying Opponent Modeling for Automatic Bidding in Online Repeated Auctions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Online auction scenarios, such as bidding searches on advertising platforms, often require bidders to participate repeatedly in auctions for identical or similar items. Most previous studies have only considered the process by which the seller learns the prior-dependent optimal mechanism in a repeated auction. However, in this paper, we define a multiagent reinforcement learning environment in which strategic bidders and the seller learn their strategies simultaneously and design an automatic bidding algorithm that updates the strategy of bidders through online interactions. We propose Bid Net to replace the linear shading function as a representation of the strategic bidders' strategy, which effectively improves the utility of strategy learned by bidders. We apply and revise the opponent modeling methods to design the PG (pseudo-gradient) algorithm, which allows bidders to learn optimal bidding strategies with predictions of the other agents' strategy transition. We prove that when a bidder uses the PG algorithm, it can learn the best response to static opponents. When all bidders adopt the PG algorithm, the system will converge to the equilibrium of the game induced by the auction. In experiments with diverse environmental settings and varying opponent strategies, the PG algorithm maximizes the utility of bidders. We hope that this article will inspire research on automatic bidding strategies for strategic bidders.
<div id='section'>Paperid: <span id='pid'>264, <a href='https://arxiv.org/pdf/2209.05738.pdf' target='_blank'>https://arxiv.org/pdf/2209.05738.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aakriti Agrawal, Amrit Singh Bedi, Dinesh Manocha
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2209.05738">RTAW: An Attention Inspired Reinforcement Learning Method for Multi-Robot Task Allocation in Warehouse Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a novel reinforcement learning based algorithm for multi-robot task allocation problem in warehouse environments. We formulate it as a Markov Decision Process and solve via a novel deep multi-agent reinforcement learning method (called RTAW) with attention inspired policy architecture. Hence, our proposed policy network uses global embeddings that are independent of the number of robots/tasks. We utilize proximal policy optimization algorithm for training and use a carefully designed reward to obtain a converged policy. The converged policy ensures cooperation among different robots to minimize total travel delay (TTD) which ultimately improves the makespan for a sufficiently large task-list. In our extensive experiments, we compare the performance of our RTAW algorithm to state of the art methods such as myopic pickup distance minimization (greedy) and regret based baselines on different navigation schemes. We show an improvement of upto 14% (25-1000 seconds) in TTD on scenarios with hundreds or thousands of tasks for different challenging warehouse layouts and task generation schemes. We also demonstrate the scalability of our approach by showing performance with up to $1000$ robots in simulations.
<div id='section'>Paperid: <span id='pid'>265, <a href='https://arxiv.org/pdf/2506.14990.pdf' target='_blank'>https://arxiv.org/pdf/2506.14990.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tristan Tomilin, Luka van den Boogaard, Samuel Garcin, Bram Grooten, Meng Fang, Yali Du, Mykola Pechenizkiy
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.14990">MEAL: A Benchmark for Continual Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Benchmarks play a crucial role in the development and analysis of reinforcement learning (RL) algorithms, with environment availability strongly impacting research. One particularly underexplored intersection is continual learning (CL) in cooperative multi-agent settings. To remedy this, we introduce MEAL (Multi-agent Environments for Adaptive Learning), the first benchmark tailored for continual multi-agent reinforcement learning (CMARL). Existing CL benchmarks run environments on the CPU, leading to computational bottlenecks and limiting the length of task sequences. MEAL leverages JAX for GPU acceleration, enabling continual learning across sequences of 100 tasks on a standard desktop PC in a few hours. We show that naively combining popular CL and MARL methods yields strong performance on simple environments, but fails to scale to more complex settings requiring sustained coordination and adaptation. Our ablation study identifies architectural and algorithmic features critical for CMARL on MEAL.
<div id='section'>Paperid: <span id='pid'>266, <a href='https://arxiv.org/pdf/2506.14990.pdf' target='_blank'>https://arxiv.org/pdf/2506.14990.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tristan Tomilin, Luka van den Boogaard, Samuel Garcin, Bram Grooten, Meng Fang, Yali Du, Mykola Pechenizkiy
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.14990">MEAL: A Benchmark for Continual Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Benchmarks play a crucial role in the development and analysis of reinforcement learning (RL) algorithms, with environment availability strongly impacting research. One particularly underexplored intersection is continual learning (CL) in cooperative multi-agent settings. To remedy this, we introduce MEAL (Multi-agent Environments for Adaptive Learning), the first benchmark tailored for continual multi-agent reinforcement learning (CMARL). Existing CL benchmarks run environments on the CPU, leading to computational bottlenecks and limiting the length of task sequences. MEAL leverages JAX for GPU acceleration, enabling continual learning across sequences of 100 tasks on a standard desktop PC in a few hours. We show that naively combining popular CL and MARL methods yields strong performance on simple environments, but fails to scale to more complex settings requiring sustained coordination and adaptation. Our ablation study identifies architectural and algorithmic features critical for CMARL on MEAL.
<div id='section'>Paperid: <span id='pid'>267, <a href='https://arxiv.org/pdf/2502.20217.pdf' target='_blank'>https://arxiv.org/pdf/2502.20217.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jimmy Chiun, Shizhe Zhang, Yizhuo Wang, Yuhong Cao, Guillaume Sartoretti
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.20217">MARVEL: Multi-Agent Reinforcement Learning for constrained field-of-View multi-robot Exploration in Large-scale environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In multi-robot exploration, a team of mobile robot is tasked with efficiently mapping an unknown environments. While most exploration planners assume omnidirectional sensors like LiDAR, this is impractical for small robots such as drones, where lightweight, directional sensors like cameras may be the only option due to payload constraints. These sensors have a constrained field-of-view (FoV), which adds complexity to the exploration problem, requiring not only optimal robot positioning but also sensor orientation during movement. In this work, we propose MARVEL, a neural framework that leverages graph attention networks, together with novel frontiers and orientation features fusion technique, to develop a collaborative, decentralized policy using multi-agent reinforcement learning (MARL) for robots with constrained FoV. To handle the large action space of viewpoints planning, we further introduce a novel information-driven action pruning strategy. MARVEL improves multi-robot coordination and decision-making in challenging large-scale indoor environments, while adapting to various team sizes and sensor configurations (i.e., FoV and sensor range) without additional training. Our extensive evaluation shows that MARVEL's learned policies exhibit effective coordinated behaviors, outperforming state-of-the-art exploration planners across multiple metrics. We experimentally demonstrate MARVEL's generalizability in large-scale environments, of up to 90m by 90m, and validate its practical applicability through successful deployment on a team of real drone hardware.
<div id='section'>Paperid: <span id='pid'>268, <a href='https://arxiv.org/pdf/2412.10700.pdf' target='_blank'>https://arxiv.org/pdf/2412.10700.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiying Wang, Gang Sun, Yuhui Wang, Hongfang Yu, Dusit Niyato
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.10700">Cluster-Based Multi-Agent Task Scheduling for Space-Air-Ground Integrated Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The Space-Air-Ground Integrated Network (SAGIN) framework is a crucial foundation for future networks, where satellites and aerial nodes assist in computational task offloading. The low-altitude economy, leveraging the flexibility and multifunctionality of Unmanned Aerial Vehicles (UAVs) in SAGIN, holds significant potential for development in areas such as communication and sensing. However, effective coordination is needed to streamline information exchange and enable efficient system resource allocation. In this paper, we propose a Clustering-based Multi-agent Deep Deterministic Policy Gradient (CMADDPG) algorithm to address the multi-UAV cooperative task scheduling challenges in SAGIN. The CMADDPG algorithm leverages dynamic UAV clustering to partition UAVs into clusters, each managed by a Cluster Head (CH) UAV, facilitating a distributed-centralized control approach. Within each cluster, UAVs delegate offloading decisions to the CH UAV, reducing intra-cluster communication costs and decision conflicts, thereby enhancing task scheduling efficiency. Additionally, by employing a multi-agent reinforcement learning framework, the algorithm leverages the extensive coverage of satellites to achieve centralized training and distributed execution of multi-agent tasks, while maximizing overall system profit through optimized task offloading decision-making. Simulation results reveal that the CMADDPG algorithm effectively optimizes resource allocation, minimizes queue delays, maintains balanced load distribution, and surpasses existing methods by achieving at least a 25\% improvement in system profit, showcasing its robustness and adaptability across diverse scenarios.
<div id='section'>Paperid: <span id='pid'>269, <a href='https://arxiv.org/pdf/2412.10442.pdf' target='_blank'>https://arxiv.org/pdf/2412.10442.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ching-Chun Chang, Isao Echizen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.10442">Steganography in Game Actions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The exchange of messages has always carried with it the timeless challenge of secrecy. From whispers in shadows to the enigmatic notes written in the margins of history, humanity has long sought ways to convey thoughts that remain imperceptible to all but the chosen few. The challenge of subliminal communication has been addressed in various forms of steganography. However, the field faces a fundamental paradox: as the art of concealment advances, so too does the science of revelation, leading to an ongoing evolutionary interplay. This study seeks to extend the boundaries of what is considered a viable steganographic medium. We explore a steganographic paradigm, in which hidden information is communicated through the episodes of multiple agents interacting with an environment. Each agent, acting as an encoder, learns a policy to disguise the very existence of hidden messages within actions seemingly directed toward innocent objectives. Meanwhile, an observer, serving as a decoder, learns to associate behavioural patterns with their respective agents despite their dynamic nature, thereby unveiling the hidden messages. The interactions of agents are governed by the framework of multi-agent reinforcement learning and shaped by feedback from the observer. This framework encapsulates a game-theoretic dilemma, wherein agents face decisions between cooperating to create distinguishable behavioural patterns or defecting to pursue individually optimal yet potentially overlapping episodic actions. As a proof of concept, we exemplify action steganography through the game of labyrinth, a navigation task where subliminal communication is concealed within the act of steering toward a destination, and systematically validate the stego-system in terms of distortion, capacity, secrecy and robustness when subjected to simulated passive and active adversaries.
<div id='section'>Paperid: <span id='pid'>270, <a href='https://arxiv.org/pdf/2310.04579.pdf' target='_blank'>https://arxiv.org/pdf/2310.04579.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tao Li, Juan Guevara, Xinhong Xie, Quanyan Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.04579">Self-Confirming Transformer for Belief-Conditioned Adaptation in Offline Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Offline reinforcement learning (RL) suffers from the distribution shift between the offline dataset and the online environment. In multi-agent RL (MARL), this distribution shift may arise from the nonstationary opponents in the online testing who display distinct behaviors from those recorded in the offline dataset. Hence, the key to the broader deployment of offline MARL is the online adaptation to nonstationary opponents. Recent advances in foundation models, e.g., large language models, have demonstrated the generalization ability of the transformer, an emerging neural network architecture, in sequence modeling, of which offline RL is a special case. One naturally wonders \textit{whether offline-trained transformer-based RL policies adapt to nonstationary opponents online}. We propose a novel auto-regressive training to equip transformer agents with online adaptability based on the idea of self-augmented pre-conditioning. The transformer agent first learns offline to predict the opponent's action based on past observations. When deployed online, such a fictitious opponent play, referred to as the belief, is fed back to the transformer, together with other environmental feedback, to generate future actions conditional on the belief. Motivated by self-confirming equilibrium in game theory, the training loss consists of belief consistency loss, requiring the beliefs to match the opponent's actual actions and best response loss, mandating the agent to behave optimally under the belief. We evaluate the online adaptability of the proposed self-confirming transformer (SCT) in a structured environment, iterated prisoner's dilemma games, to demonstrate SCT's belief consistency and equilibrium behaviors as well as more involved multi-particle environments to showcase its superior performance against nonstationary opponents over prior transformers and offline MARL baselines.
<div id='section'>Paperid: <span id='pid'>271, <a href='https://arxiv.org/pdf/2306.05353.pdf' target='_blank'>https://arxiv.org/pdf/2306.05353.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junjie Sheng, Wenhao Li, Bo Jin, Hongyuan Zha, Jun Wang, Xiangfeng Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.05353">Negotiated Reasoning: On Provably Addressing Relative Over-Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Over-generalization is a thorny issue in cognitive science, where people may become overly cautious due to past experiences. Agents in multi-agent reinforcement learning (MARL) also have been found to suffer relative over-generalization (RO) as people do and stuck to sub-optimal cooperation. Recent methods have shown that assigning reasoning ability to agents can mitigate RO algorithmically and empirically, but there has been a lack of theoretical understanding of RO, let alone designing provably RO-free methods. This paper first proves that RO can be avoided when the MARL method satisfies a consistent reasoning requirement under certain conditions. Then we introduce a novel reasoning framework, called negotiated reasoning, that first builds the connection between reasoning and RO with theoretical justifications. After that, we propose an instantiated algorithm, Stein variational negotiated reasoning (SVNR), which uses Stein variational gradient descent to derive a negotiation policy that provably avoids RO in MARL under maximum entropy policy iteration. The method is further parameterized with neural networks for amortized learning, making computation efficient. Numerical experiments on many RO-challenged environments demonstrate the superiority and efficiency of SVNR compared to state-of-the-art methods in addressing RO.
<div id='section'>Paperid: <span id='pid'>272, <a href='https://arxiv.org/pdf/2305.10865.pdf' target='_blank'>https://arxiv.org/pdf/2305.10865.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenhao Li, Dan Qiao, Baoxiang Wang, Xiangfeng Wang, Bo Jin, Hongyuan Zha
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.10865">Semantically Aligned Task Decomposition in Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The difficulty of appropriately assigning credit is particularly heightened in cooperative MARL with sparse reward, due to the concurrent time and structural scales involved. Automatic subgoal generation (ASG) has recently emerged as a viable MARL approach inspired by utilizing subgoals in intrinsically motivated reinforcement learning. However, end-to-end learning of complex task planning from sparse rewards without prior knowledge, undoubtedly requires massive training samples. Moreover, the diversity-promoting nature of existing ASG methods can lead to the "over-representation" of subgoals, generating numerous spurious subgoals of limited relevance to the actual task reward and thus decreasing the sample efficiency of the algorithm. To address this problem and inspired by the disentangled representation learning, we propose a novel "disentangled" decision-making method, Semantically Aligned task decomposition in MARL (SAMA), that prompts pretrained language models with chain-of-thought that can suggest potential goals, provide suitable goal decomposition and subgoal allocation as well as self-reflection-based replanning. Additionally, SAMA incorporates language-grounded RL to train each agent's subgoal-conditioned policy. SAMA demonstrates considerable advantages in sample efficiency compared to state-of-the-art ASG methods, as evidenced by its performance on two challenging sparse-reward tasks, Overcooked and MiniRTS.
<div id='section'>Paperid: <span id='pid'>273, <a href='https://arxiv.org/pdf/2305.06227.pdf' target='_blank'>https://arxiv.org/pdf/2305.06227.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yun Hua, Shang Gao, Wenhao Li, Bo Jin, Xiangfeng Wang, Hongyuan Zha
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.06227">Learning Optimal "Pigovian Tax" in Sequential Social Dilemmas</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In multi-agent reinforcement learning, each agent acts to maximize its individual accumulated rewards. Nevertheless, individual accumulated rewards could not fully reflect how others perceive them, resulting in selfish behaviors that undermine global performance. The externality theory, defined as ``the activities of one economic actor affect the activities of another in ways that are not reflected in market transactions,'' is applicable to analyze the social dilemmas in MARL. One of its most profound non-market solutions, ``Pigovian Tax'', which internalizes externalities by taxing those who create negative externalities and subsidizing those who create positive externalities, could aid in developing a mechanism to resolve MARL's social dilemmas. The purpose of this paper is to apply externality theory to analyze social dilemmas in MARL. To internalize the externalities in MARL, the \textbf{L}earning \textbf{O}ptimal \textbf{P}igovian \textbf{T}ax method (LOPT), is proposed, where an additional agent is introduced to learn the tax/allowance allocation policy so as to approximate the optimal ``Pigovian Tax'' which accurately reflects the externalities for all agents. Furthermore, a reward shaping mechanism based on the approximated optimal ``Pigovian Tax'' is applied to reduce the social cost of each agent and tries to alleviate the social dilemmas. Compared with existing state-of-the-art methods, the proposed LOPT leads to higher collective social welfare in both the Escape Room and the Cleanup environments, which shows the superiority of our method in solving social dilemmas.
<div id='section'>Paperid: <span id='pid'>274, <a href='https://arxiv.org/pdf/2301.13812.pdf' target='_blank'>https://arxiv.org/pdf/2301.13812.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenhao Li, Xiangfeng Wang, Bo Jin, Jingyi Lu, Hongyuan Zha
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.13812">Learning Roles with Emergent Social Value Orientations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Social dilemmas can be considered situations where individual rationality leads to collective irrationality. The multi-agent reinforcement learning community has leveraged ideas from social science, such as social value orientations (SVO), to solve social dilemmas in complex cooperative tasks. In this paper, by first introducing the typical "division of labor or roles" mechanism in human society, we provide a promising solution for intertemporal social dilemmas (ISD) with SVOs. A novel learning framework, called Learning Roles with Emergent SVOs (RESVO), is proposed to transform the learning of roles into the social value orientation emergence, which is symmetrically solved by endowing agents with altruism to share rewards with other agents. An SVO-based role embedding space is then constructed by individual conditioning policies on roles with a novel rank regularizer and mutual information maximizer. Experiments show that RESVO achieves a stable division of labor and cooperation in ISDs with different complexity.
<div id='section'>Paperid: <span id='pid'>275, <a href='https://arxiv.org/pdf/2209.01054.pdf' target='_blank'>https://arxiv.org/pdf/2209.01054.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Taher Jafferjee, Juliusz Ziomek, Tianpei Yang, Zipeng Dai, Jianhong Wang, Matthew Taylor, Kun Shao, Jun Wang, David Mguni
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2209.01054">Taming Multi-Agent Reinforcement Learning with Estimator Variance Reduction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Centralised training with decentralised execution (CT-DE) serves as the foundation of many leading multi-agent reinforcement learning (MARL) algorithms. Despite its popularity, it suffers from a critical drawback due to its reliance on learning from a single sample of the joint-action at a given state. As agents explore and update their policies during training, these single samples may poorly represent the actual joint-policy of the system of agents leading to high variance gradient estimates that hinder learning. To address this problem, we propose an enhancement tool that accommodates any actor-critic MARL method. Our framework, Performance Enhancing Reinforcement Learning Apparatus (PERLA), introduces a sampling technique of the agents' joint-policy into the critics while the agents train. This leads to TD updates that closely approximate the true expected value under the current joint-policy rather than estimates from a single sample of the joint-action at a given state. This produces low variance and precise estimates of expected returns, minimising the variance in the critic estimators which typically hinders learning. Moreover, as we demonstrate, by eliminating much of the critic variance from the single sampling of the joint policy, PERLA enables CT-DE methods to scale more efficiently with the number of agents. Theoretically, we prove that PERLA reduces variance in value estimates similar to that of decentralised training while maintaining the benefits of centralised training. Empirically, we demonstrate PERLA's superior performance and ability to reduce estimator variance in a range of benchmarks including Multi-agent Mujoco, and StarCraft II Multi-agent Challenge.
<div id='section'>Paperid: <span id='pid'>276, <a href='https://arxiv.org/pdf/2204.13070.pdf' target='_blank'>https://arxiv.org/pdf/2204.13070.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rishabh Saumil Thakkar, Aryaman Singh Samyal, David Fridovich-Keil, Zhe Xu, Ufuk Topcu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2204.13070">Hierarchical Control for Cooperative Teams in Competitive Autonomous Racing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We investigate the problem of autonomous racing among teams of cooperative agents that are subject to realistic racing rules. Our work extends previous research on hierarchical control in head-to-head autonomous racing by considering a generalized version of the problem while maintaining the two-level hierarchical control structure. A high-level tactical planner constructs a discrete game that encodes the complex rules using simplified dynamics to produce a sequence of target waypoints. The low-level path planner uses these waypoints as a reference trajectory and computes high-resolution control inputs by solving a simplified formulation of a racing game with a simplified representation of the realistic racing rules. We explore two approaches for the low-level path planner: training a multi-agent reinforcement learning (MARL) policy and solving a linear-quadratic Nash game (LQNG) approximation. We evaluate our controllers on simple and complex tracks against three baselines: an end-to-end MARL controller, a MARL controller tracking a fixed racing line, and an LQNG controller tracking a fixed racing line. Quantitative results show our hierarchical methods outperform the baselines in terms of race wins, overall team performance, and compliance with the rules. Qualitatively, we observe the hierarchical controllers mimic actions performed by expert human drivers such as coordinated overtaking, defending against multiple opponents, and long-term planning for delayed advantages.
<div id='section'>Paperid: <span id='pid'>277, <a href='https://arxiv.org/pdf/2202.12861.pdf' target='_blank'>https://arxiv.org/pdf/2202.12861.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rishabh Saumil Thakkar, Aryaman Singh Samyal, David Fridovich-Keil, Zhe Xu, Ufuk Topcu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2202.12861">Hierarchical Control for Head-to-Head Autonomous Racing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We develop a hierarchical controller for head-to-head autonomous racing. We first introduce a formulation of a racing game with realistic safety and fairness rules. A high-level planner approximates the original formulation as a discrete game with simplified state, control, and dynamics to easily encode the complex safety and fairness rules and calculates a series of target waypoints. The low-level controller takes the resulting waypoints as a reference trajectory and computes high-resolution control inputs by solving an alternative formulation approximation with simplified objectives and constraints. We consider two approaches for the low-level planner, constructing two hierarchical controllers. One approach uses multi-agent reinforcement learning (MARL), and the other solves a linear-quadratic Nash game (LQNG) to produce control inputs. The controllers are compared against three baselines: an end-to-end MARL controller, a MARL controller tracking a fixed racing line, and an LQNG controller tracking a fixed racing line. Quantitative results show that the proposed hierarchical methods outperform their respective baseline methods in terms of head-to-head race wins and abiding by the rules. The hierarchical controller using MARL for low-level control consistently outperformed all other methods by winning over 90% of head-to-head races and more consistently adhered to the complex racing rules. Qualitatively, we observe the proposed controllers mimicking actions performed by expert human drivers such as shielding/blocking, overtaking, and long-term planning for delayed advantages. We show that hierarchical planning for game-theoretic reasoning produces competitive behavior even when challenged with complex rules and constraints.
<div id='section'>Paperid: <span id='pid'>278, <a href='https://arxiv.org/pdf/2004.11145.pdf' target='_blank'>https://arxiv.org/pdf/2004.11145.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenhao Li, Bo Jin, Xiangfeng Wang, Junchi Yan, Hongyuan Zha
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2004.11145">F2A2: Flexible Fully-decentralized Approximate Actor-critic for Cooperative Multi-agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Traditional centralized multi-agent reinforcement learning (MARL) algorithms are sometimes unpractical in complicated applications, due to non-interactivity between agents, curse of dimensionality and computation complexity. Hence, several decentralized MARL algorithms are motivated. However, existing decentralized methods only handle the fully cooperative setting where massive information needs to be transmitted in training. The block coordinate gradient descent scheme they used for successive independent actor and critic steps can simplify the calculation, but it causes serious bias. In this paper, we propose a flexible fully decentralized actor-critic MARL framework, which can combine most of actor-critic methods, and handle large-scale general cooperative multi-agent setting. A primal-dual hybrid gradient descent type algorithm framework is designed to learn individual agents separately for decentralization. From the perspective of each agent, policy improvement and value evaluation are jointly optimized, which can stabilize multi-agent policy learning. Furthermore, our framework can achieve scalability and stability for large-scale environment and reduce information transmission, by the parameter sharing mechanism and a novel modeling-other-agents methods based on theory-of-mind and online supervised learning. Sufficient experiments in cooperative Multi-agent Particle Environment and StarCraft II show that our decentralized MARL instantiation algorithms perform competitively against conventional centralized and decentralized methods.
<div id='section'>Paperid: <span id='pid'>279, <a href='https://arxiv.org/pdf/2510.04862.pdf' target='_blank'>https://arxiv.org/pdf/2510.04862.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sam Earle, Zehua Jiang, Eugene Vinitsky, Julian Togelius
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.04862">Video Game Level Design as a Multi-Agent Reinforcement Learning Problem</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Procedural Content Generation via Reinforcement Learning (PCGRL) offers a method for training controllable level designer agents without the need for human datasets, using metrics that serve as proxies for level quality as rewards. Existing PCGRL research focuses on single generator agents, but are bottlenecked by the need to frequently recalculate heuristics of level quality and the agent's need to navigate around potentially large maps. By framing level generation as a multi-agent problem, we mitigate the efficiency bottleneck of single-agent PCGRL by reducing the number of reward calculations relative to the number of agent actions. We also find that multi-agent level generators are better able to generalize to out-of-distribution map shapes, which we argue is due to the generators' learning more local, modular design policies. We conclude that treating content generation as a distributed, multi-agent task is beneficial for generating functional artifacts at scale.
<div id='section'>Paperid: <span id='pid'>280, <a href='https://arxiv.org/pdf/2510.04862.pdf' target='_blank'>https://arxiv.org/pdf/2510.04862.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sam Earle, Zehua Jiang, Eugene Vinitsky, Julian Togelius
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.04862">Video Game Level Design as a Multi-Agent Reinforcement Learning Problem</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Procedural Content Generation via Reinforcement Learning (PCGRL) offers a method for training controllable level designer agents without the need for human datasets, using metrics that serve as proxies for level quality as rewards. Existing PCGRL research focuses on single generator agents, but are bottlenecked by the need to frequently recalculate heuristics of level quality and the agent's need to navigate around potentially large maps. By framing level generation as a multi-agent problem, we mitigate the efficiency bottleneck of single-agent PCGRL by reducing the number of reward calculations relative to the number of agent actions. We also find that multi-agent level generators are better able to generalize to out-of-distribution map shapes, which we argue is due to the generators' learning more local, modular design policies. We conclude that treating content generation as a distributed, multi-agent task is beneficial for generating functional artifacts at scale.
<div id='section'>Paperid: <span id='pid'>281, <a href='https://arxiv.org/pdf/2508.03864.pdf' target='_blank'>https://arxiv.org/pdf/2508.03864.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhenyu Pan, Yiting Zhang, Yutong Zhang, Jianshu Zhang, Haozheng Luo, Yuwei Han, Dennis Wu, Hong-Yu Chen, Philip S. Yu, Manling Li, Han Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.03864">Evo-MARL: Co-Evolutionary Multi-Agent Reinforcement Learning for Internalized Safety</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent systems (MAS) built on multimodal large language models exhibit strong collaboration and performance. However, their growing openness and interaction complexity pose serious risks, notably jailbreak and adversarial attacks. Existing defenses typically rely on external guard modules, such as dedicated safety agents, to handle unsafe behaviors. Unfortunately, this paradigm faces two challenges: (1) standalone agents offer limited protection, and (2) their independence leads to single-point failure-if compromised, system-wide safety collapses. Naively increasing the number of guard agents further raises cost and complexity. To address these challenges, we propose Evo-MARL, a novel multi-agent reinforcement learning (MARL) framework that enables all task agents to jointly acquire defensive capabilities. Rather than relying on external safety modules, Evo-MARL trains each agent to simultaneously perform its primary function and resist adversarial threats, ensuring robustness without increasing system overhead or single-node failure. Furthermore, Evo-MARL integrates evolutionary search with parameter-sharing reinforcement learning to co-evolve attackers and defenders. This adversarial training paradigm internalizes safety mechanisms and continually enhances MAS performance under co-evolving threats. Experiments show that Evo-MARL reduces attack success rates by up to 22% while boosting accuracy by up to 5% on reasoning tasks-demonstrating that safety and utility can be jointly improved.
<div id='section'>Paperid: <span id='pid'>282, <a href='https://arxiv.org/pdf/2508.03864.pdf' target='_blank'>https://arxiv.org/pdf/2508.03864.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhenyu Pan, Yiting Zhang, Yutong Zhang, Jianshu Zhang, Haozheng Luo, Yuwei Han, Dennis Wu, Hong-Yu Chen, Philip S. Yu, Manling Li, Han Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.03864">Evo-MARL: Co-Evolutionary Multi-Agent Reinforcement Learning for Internalized Safety</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent systems (MAS) built on multimodal large language models exhibit strong collaboration and performance. However, their growing openness and interaction complexity pose serious risks, notably jailbreak and adversarial attacks. Existing defenses typically rely on external guard modules, such as dedicated safety agents, to handle unsafe behaviors. Unfortunately, this paradigm faces two challenges: (1) standalone agents offer limited protection, and (2) their independence leads to single-point failure-if compromised, system-wide safety collapses. Naively increasing the number of guard agents further raises cost and complexity. To address these challenges, we propose Evo-MARL, a novel multi-agent reinforcement learning (MARL) framework that enables all task agents to jointly acquire defensive capabilities. Rather than relying on external safety modules, Evo-MARL trains each agent to simultaneously perform its primary function and resist adversarial threats, ensuring robustness without increasing system overhead or single-node failure. Furthermore, Evo-MARL integrates evolutionary search with parameter-sharing reinforcement learning to co-evolve attackers and defenders. This adversarial training paradigm internalizes safety mechanisms and continually enhances MAS performance under co-evolving threats. Experiments show that Evo-MARL reduces attack success rates by up to 22% while boosting accuracy by up to 5% on reasoning tasks-demonstrating that safety and utility can be jointly improved.
<div id='section'>Paperid: <span id='pid'>283, <a href='https://arxiv.org/pdf/2506.14187.pdf' target='_blank'>https://arxiv.org/pdf/2506.14187.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiaming Yu, Le Liang, Hao Ye, Shi Jin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.14187">Hierarchical Multi-Agent Reinforcement Learning-based Coordinated Spatial Reuse for Next Generation WLANs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>High-density Wi-Fi deployments often result in significant co-channel interference, which degrades overall network performance. To address this issue, coordination of multi access points (APs) has been considered to enable coordinated spatial reuse (CSR) in next generation wireless local area networks. This paper tackles the challenge of downlink spatial reuse in Wi-Fi networks, specifically in scenarios involving overlapping basic service sets, by employing hierarchical multi-agent reinforcement learning (HMARL). We decompose the CSR process into two phases, i.e., a polling phase and a decision phase, and introduce the HMARL algorithm to enable efficient CSR. To enhance training efficiency, the proposed HMARL algorithm employs a hierarchical structure, where station selection and power control are determined by a high- and low-level policy network, respectively. Simulation results demonstrate that this approach consistently outperforms baseline methods in terms of throughput and latency across various network topologies. Moreover, the algorithm exhibits robust performance when coexisting with legacy APs. Additional experiments in a representative topology further reveal that the carefully designed reward function not only maximizes the overall network throughput, but also improves fairness in transmission opportunities for APs in high-interference regions.
<div id='section'>Paperid: <span id='pid'>284, <a href='https://arxiv.org/pdf/2505.03533.pdf' target='_blank'>https://arxiv.org/pdf/2505.03533.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiacheng Wang, Le Liang, Hao Ye, Chongtao Guo, Shi Jin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.03533">Small-Scale-Fading-Aware Resource Allocation in Wireless Federated Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Judicious resource allocation can effectively enhance federated learning (FL) training performance in wireless networks by addressing both system and statistical heterogeneity. However, existing strategies typically rely on block fading assumptions, which overlooks rapid channel fluctuations within each round of FL gradient uploading, leading to a degradation in FL training performance. Therefore, this paper proposes a small-scale-fading-aware resource allocation strategy using a multi-agent reinforcement learning (MARL) framework. Specifically, we establish a one-step convergence bound of the FL algorithm and formulate the resource allocation problem as a decentralized partially observable Markov decision process (Dec-POMDP), which is subsequently solved using the QMIX algorithm. In our framework, each client serves as an agent that dynamically determines spectrum and power allocations within each coherence time slot, based on local observations and a reward derived from the convergence analysis. The MARL setting reduces the dimensionality of the action space and facilitates decentralized decision-making, enhancing the scalability and practicality of the solution. Experimental results demonstrate that our QMIX-based resource allocation strategy significantly outperforms baseline methods across various degrees of statistical heterogeneity. Additionally, ablation studies validate the critical importance of incorporating small-scale fading dynamics, highlighting its role in optimizing FL performance.
<div id='section'>Paperid: <span id='pid'>285, <a href='https://arxiv.org/pdf/2412.14218.pdf' target='_blank'>https://arxiv.org/pdf/2412.14218.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiaming Yu, Le Liang, Chongtao Guo, Ziyang Guo, Shi Jin, Geoffrey Ye Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.14218">Heterogeneous Multi-Agent Reinforcement Learning for Distributed Channel Access in WLANs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper investigates the use of multi-agent reinforcement learning (MARL) to address distributed channel access in wireless local area networks. In particular, we consider the challenging yet more practical case where the agents heterogeneously adopt value-based or policy-based reinforcement learning algorithms to train the model. We propose a heterogeneous MARL training framework, named QPMIX, which adopts a centralized training with distributed execution paradigm to enable heterogeneous agents to collaborate. Moreover, we theoretically prove the convergence of the proposed heterogeneous MARL method when using the linear value function approximation. Our method maximizes the network throughput and ensures fairness among stations, therefore, enhancing the overall network performance. Simulation results demonstrate that the proposed QPMIX algorithm improves throughput, mean delay, delay jitter, and collision rates compared with conventional carrier-sense multiple access with collision avoidance (CSMA/CA) mechanism in the saturated traffic scenario. Furthermore, the QPMIX algorithm is robust in unsaturated and delay-sensitive traffic scenarios. It coexists well with the conventional CSMA/CA mechanism and promotes cooperation among heterogeneous agents.
<div id='section'>Paperid: <span id='pid'>286, <a href='https://arxiv.org/pdf/2409.06366.pdf' target='_blank'>https://arxiv.org/pdf/2409.06366.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nico Bohlinger, Grzegorz Czechmanowski, Maciej Krupka, Piotr Kicki, Krzysztof Walas, Jan Peters, Davide Tateo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.06366">One Policy to Run Them All: an End-to-end Learning Approach to Multi-Embodiment Locomotion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep Reinforcement Learning techniques are achieving state-of-the-art results in robust legged locomotion. While there exists a wide variety of legged platforms such as quadruped, humanoids, and hexapods, the field is still missing a single learning framework that can control all these different embodiments easily and effectively and possibly transfer, zero or few-shot, to unseen robot embodiments. We introduce URMA, the Unified Robot Morphology Architecture, to close this gap. Our framework brings the end-to-end Multi-Task Reinforcement Learning approach to the realm of legged robots, enabling the learned policy to control any type of robot morphology. The key idea of our method is to allow the network to learn an abstract locomotion controller that can be seamlessly shared between embodiments thanks to our morphology-agnostic encoders and decoders. This flexible architecture can be seen as a potential first step in building a foundation model for legged robot locomotion. Our experiments show that URMA can learn a locomotion policy on multiple embodiments that can be easily transferred to unseen robot platforms in simulation and the real world.
<div id='section'>Paperid: <span id='pid'>287, <a href='https://arxiv.org/pdf/2409.06366.pdf' target='_blank'>https://arxiv.org/pdf/2409.06366.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nico Bohlinger, Grzegorz Czechmanowski, Maciej Krupka, Piotr Kicki, Krzysztof Walas, Jan Peters, Davide Tateo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.06366">One Policy to Run Them All: an End-to-end Learning Approach to Multi-Embodiment Locomotion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep Reinforcement Learning techniques are achieving state-of-the-art results in robust legged locomotion. While there exists a wide variety of legged platforms such as quadruped, humanoids, and hexapods, the field is still missing a single learning framework that can control all these different embodiments easily and effectively and possibly transfer, zero or few-shot, to unseen robot embodiments. We introduce URMA, the Unified Robot Morphology Architecture, to close this gap. Our framework brings the end-to-end Multi-Task Reinforcement Learning approach to the realm of legged robots, enabling the learned policy to control any type of robot morphology. The key idea of our method is to allow the network to learn an abstract locomotion controller that can be seamlessly shared between embodiments thanks to our morphology-agnostic encoders and decoders. This flexible architecture can be seen as a potential first step in building a foundation model for legged robot locomotion. Our experiments show that URMA can learn a locomotion policy on multiple embodiments that can be easily transferred to unseen robot platforms in simulation and the real world.
<div id='section'>Paperid: <span id='pid'>288, <a href='https://arxiv.org/pdf/2409.06366.pdf' target='_blank'>https://arxiv.org/pdf/2409.06366.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nico Bohlinger, Grzegorz Czechmanowski, Maciej Krupka, Piotr Kicki, Krzysztof Walas, Jan Peters, Davide Tateo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.06366">One Policy to Run Them All: an End-to-end Learning Approach to Multi-Embodiment Locomotion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep Reinforcement Learning techniques are achieving state-of-the-art results in robust legged locomotion. While there exists a wide variety of legged platforms such as quadruped, humanoids, and hexapods, the field is still missing a single learning framework that can control all these different embodiments easily and effectively and possibly transfer, zero or few-shot, to unseen robot embodiments. We introduce URMA, the Unified Robot Morphology Architecture, to close this gap. Our framework brings the end-to-end Multi-Task Reinforcement Learning approach to the realm of legged robots, enabling the learned policy to control any type of robot morphology. The key idea of our method is to allow the network to learn an abstract locomotion controller that can be seamlessly shared between embodiments thanks to our morphology-agnostic encoders and decoders. This flexible architecture can be seen as a potential first step in building a foundation model for legged robot locomotion. Our experiments show that URMA can learn a locomotion policy on multiple embodiments that can be easily transferred to unseen robot platforms in simulation and the real world.
<div id='section'>Paperid: <span id='pid'>289, <a href='https://arxiv.org/pdf/2401.09561.pdf' target='_blank'>https://arxiv.org/pdf/2401.09561.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Carlo D'Eramo, Davide Tateo, Andrea Bonarini, Marcello Restelli, Jan Peters
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.09561">Sharing Knowledge in Multi-Task Deep Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We study the benefit of sharing representations among tasks to enable the effective use of deep neural networks in Multi-Task Reinforcement Learning. We leverage the assumption that learning from different tasks, sharing common properties, is helpful to generalize the knowledge of them resulting in a more effective feature extraction compared to learning a single task. Intuitively, the resulting set of features offers performance benefits when used by Reinforcement Learning algorithms. We prove this by providing theoretical guarantees that highlight the conditions for which is convenient to share representations among tasks, extending the well-known finite-time bounds of Approximate Value-Iteration to the multi-task setting. In addition, we complement our analysis by proposing multi-task extensions of three Reinforcement Learning algorithms that we empirically evaluate on widely used Reinforcement Learning benchmarks showing significant improvements over the single-task counterparts in terms of sample efficiency and performance.
<div id='section'>Paperid: <span id='pid'>290, <a href='https://arxiv.org/pdf/2311.08708.pdf' target='_blank'>https://arxiv.org/pdf/2311.08708.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yu Min Park, Yan Kyaw Tun, Choong Seon Hong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.08708">Joint User Pairing and Beamforming Design of Multi-STAR-RISs-Aided NOMA in the Indoor Environment via Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The development of 6G/B5G wireless networks, which have requirements that go beyond current 5G networks, is gaining interest from academia and industry. However, to increase 6G/B5G network quality, conventional cellular networks that rely on terrestrial base stations are constrained geographically and economically. Meanwhile, NOMA allows multiple users to share the same resources, which improves the spectral efficiency of the system and has the advantage of supporting a larger number of users. Additionally, by intelligently manipulating the phase and amplitude of both the reflected and transmitted signals, STAR-RISs can achieve improved coverage, increased spectral efficiency, and enhanced communication reliability. However, STAR-RISs must simultaneously optimize the amplitude and phase shift corresponding to reflection and transmission, which makes the existing terrestrial networks more complicated and is considered a major challenging issue. Motivated by the above, we study the joint user pairing for NOMA and beamforming design of Multi-STAR-RISs in an indoor environment. Then, we formulate the optimization problem with the objective of maximizing the total throughput of MUs by jointly optimizing the decoding order, user pairing, active beamforming, and passive beamforming. However, the formulated problem is a MINLP. To address this challenge, we first introduce the decoding order for NOMA networks. Next, we decompose the original problem into two subproblems, namely: 1) MU pairing and 2) Beamforming optimization under the optimal decoding order. For the first subproblem, we employ correlation-based K-means clustering to solve the user pairing problem. Then, to jointly deal with beamforming vector optimizations, we propose MAPPO, which can make quick decisions in the given environment owing to its low complexity.
<div id='section'>Paperid: <span id='pid'>291, <a href='https://arxiv.org/pdf/2210.07184.pdf' target='_blank'>https://arxiv.org/pdf/2210.07184.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nelson Vadori, Leo Ardon, Sumitra Ganesh, Thomas Spooner, Selim Amrouni, Jared Vann, Mengda Xu, Zeyu Zheng, Tucker Balch, Manuela Veloso
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2210.07184">Towards Multi-Agent Reinforcement Learning driven Over-The-Counter Market Simulations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We study a game between liquidity provider and liquidity taker agents interacting in an over-the-counter market, for which the typical example is foreign exchange. We show how a suitable design of parameterized families of reward functions coupled with shared policy learning constitutes an efficient solution to this problem. By playing against each other, our deep-reinforcement-learning-driven agents learn emergent behaviors relative to a wide spectrum of objectives encompassing profit-and-loss, optimal execution and market share. In particular, we find that liquidity providers naturally learn to balance hedging and skewing, where skewing refers to setting their buy and sell prices asymmetrically as a function of their inventory. We further introduce a novel RL-based calibration algorithm which we found performed well at imposing constraints on the game equilibrium. On the theoretical side, we are able to show convergence rates for our multi-agent policy gradient algorithm under a transitivity assumption, closely related to generalized ordinal potential games.
<div id='section'>Paperid: <span id='pid'>292, <a href='https://arxiv.org/pdf/2503.18221.pdf' target='_blank'>https://arxiv.org/pdf/2503.18221.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wen-Tse Chen, Minh Nguyen, Zhongyu Li, Guo Ning Sue, Koushil Sreenath
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.18221">Decentralized Navigation of a Cable-Towed Load using Quadrupedal Robot Team via MARL</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work addresses the challenge of enabling a team of quadrupedal robots to collaboratively tow a cable-connected load through cluttered and unstructured environments while avoiding obstacles. Leveraging cables allows the multi-robot system to navigate narrow spaces by maintaining slack when necessary. However, this introduces hybrid physical interactions due to alternating taut and slack states, with computational complexity that scales exponentially as the number of agents increases. To tackle these challenges, we developed a scalable and decentralized system capable of dynamically coordinating a variable number of quadrupedal robots while managing the hybrid physical interactions inherent in the load-towing task. At the core of this system is a novel multi-agent reinforcement learning (MARL)-based planner, designed for decentralized coordination. The MARL-based planner is trained using a centralized training with decentralized execution (CTDE) framework, enabling each robot to make decisions autonomously using only local (ego) observations. To accelerate learning and ensure effective collaboration across varying team sizes, we introduce a tailored training curriculum for MARL. Experimental results highlight the flexibility and scalability of the framework, demonstrating successful deployment with one to four robots in real-world scenarios and up to twelve robots in simulation. The decentralized planner maintains consistent inference times, regardless of the team size. Additionally, the proposed system demonstrates robustness to environment perturbations and adaptability to varying load weights. This work represents a step forward in achieving flexible and efficient multi-legged robotic collaboration in complex and real-world environments.
<div id='section'>Paperid: <span id='pid'>293, <a href='https://arxiv.org/pdf/2412.20075.pdf' target='_blank'>https://arxiv.org/pdf/2412.20075.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gabriele Calzolari, Vidya Sumathy, Christoforos Kanellakis, George Nikolakopoulos
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.20075">Investigating the Impact of Communication-Induced Action Space on Exploration of Unknown Environments with Decentralized Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces a novel enhancement to the Decentralized Multi-Agent Reinforcement Learning (D-MARL) exploration by proposing communication-induced action space to improve the mapping efficiency of unknown environments using homogeneous agents. Efficient exploration of large environments relies heavily on inter-agent communication as real-world scenarios are often constrained by data transmission limits, such as signal latency and bandwidth. Our proposed method optimizes each agent's policy using the heterogeneous-agent proximal policy optimization algorithm, allowing agents to autonomously decide whether to communicate or to explore, that is whether to share the locally collected maps or continue the exploration. We propose and compare multiple novel reward functions that integrate inter-agent communication and exploration, enhance mapping efficiency and robustness, and minimize exploration overlap. This article presents a framework developed in ROS2 to evaluate and validate the investigated architecture. Specifically, four TurtleBot3 Burgers have been deployed in a Gazebo-designed environment filled with obstacles to evaluate the efficacy of the trained policies in mapping the exploration arena.
<div id='section'>Paperid: <span id='pid'>294, <a href='https://arxiv.org/pdf/2407.20983.pdf' target='_blank'>https://arxiv.org/pdf/2407.20983.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Faisal Haque Bappy, Tariqul Islam, Kamrul Hasan, Md Sajidul Islam Sajid, Mir Mehedi Ahsan Pritom
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.20983">Securing Proof of Stake Blockchains: Leveraging Multi-Agent Reinforcement Learning for Detecting and Mitigating Malicious Nodes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Proof of Stake (PoS) blockchains offer promising alternatives to traditional Proof of Work (PoW) systems, providing scalability and energy efficiency. However, blockchains operate in a decentralized manner and the network is composed of diverse users. This openness creates the potential for malicious nodes to disrupt the network in various ways. Therefore, it is crucial to embed a mechanism within the blockchain network to constantly monitor, identify, and eliminate these malicious nodes without involving any central authority. In this paper, we propose MRL-PoS+, a novel consensus algorithm to enhance the security of PoS blockchains by leveraging Multi-agent Reinforcement Learning (MRL) techniques. Our proposed consensus algorithm introduces a penalty-reward scheme for detecting and eliminating malicious nodes. This approach involves the detection of behaviors that can lead to potential attacks in a blockchain network and hence penalizes the malicious nodes, restricting them from performing certain actions. Our developed Proof of Concept demonstrates effectiveness in eliminating malicious nodes for six types of major attacks. Experimental results demonstrate that MRL-PoS+ significantly improves the attack resilience of PoS blockchains compared to the traditional schemes without incurring additional computation overhead.
<div id='section'>Paperid: <span id='pid'>295, <a href='https://arxiv.org/pdf/2312.09123.pdf' target='_blank'>https://arxiv.org/pdf/2312.09123.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tariqul Islam, Faisal Haque Bappy, Tarannum Shaila Zaman, Md Sajidul Islam Sajid, Mir Mehedi Ahsan Pritom
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.09123">MRL-PoS: A Multi-agent Reinforcement Learning based Proof of Stake Consensus Algorithm for Blockchain</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The core of a blockchain network is its consensus algorithm. Starting with the Proof-of-Work, there have been various versions of consensus algorithms, such as Proof-of-Stake (PoS), Proof-of-Authority (PoA), and Practical Byzantine Fault Tolerance (PBFT). Each of these algorithms focuses on different aspects to ensure efficient and reliable processing of transactions. Blockchain operates in a decentralized manner where there is no central authority and the network is composed of diverse users. This openness creates the potential for malicious nodes to disrupt the network in various ways. Therefore, it is crucial to embed a mechanism within the blockchain network to constantly monitor, identify, and eliminate these malicious nodes. However, there is no one-size-fits-all mechanism to identify all malicious nodes. Hence, the dynamic adaptability of the blockchain network is important to maintain security and reliability at all times. This paper introduces MRL-PoS, a Proof-of-Stake consensus algorithm based on multi-agent reinforcement learning. MRL-PoS employs reinforcement learning for dynamically adjusting to the behavior of all users. It incorporates a system of rewards and penalties to eliminate malicious nodes and incentivize honest ones. Additionally, MRL-PoS has the capability to learn and respond to new malicious tactics by continually training its agents.
<div id='section'>Paperid: <span id='pid'>296, <a href='https://arxiv.org/pdf/2506.00439.pdf' target='_blank'>https://arxiv.org/pdf/2506.00439.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuqian Fu, Yuanheng Zhu, Jiajun Chai, Guojun Yin, Wei Lin, Qichao Zhang, Dongbin Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.00439">RLAE: Reinforcement Learning-Assisted Ensemble for LLMs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Ensembling large language models (LLMs) can effectively combine diverse strengths of different models, offering a promising approach to enhance performance across various tasks. However, existing methods typically rely on fixed weighting strategies that fail to adapt to the dynamic, context-dependent characteristics of LLM capabilities. In this work, we propose Reinforcement Learning-Assisted Ensemble for LLMs (RLAE), a novel framework that reformulates LLM ensemble through the lens of a Markov Decision Process (MDP). Our approach introduces a RL agent that dynamically adjusts ensemble weights by considering both input context and intermediate generation states, with the agent being trained using rewards that directly correspond to the quality of final outputs. We implement RLAE using both single-agent and multi-agent reinforcement learning algorithms ($\text{RLAE}_\text{PPO}$ and $\text{RLAE}_\text{MAPPO}$ ), demonstrating substantial improvements over conventional ensemble methods. Extensive evaluations on a diverse set of tasks show that RLAE outperforms existing approaches by up to $3.3\%$ accuracy points, offering a more effective framework for LLM ensembling. Furthermore, our method exhibits superior generalization capabilities across different tasks without the need for retraining, while simultaneously achieving lower time latency.
<div id='section'>Paperid: <span id='pid'>297, <a href='https://arxiv.org/pdf/2402.03741.pdf' target='_blank'>https://arxiv.org/pdf/2402.03741.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Oubo Ma, Yuwen Pu, Linkang Du, Yang Dai, Ruo Wang, Xiaolei Liu, Yingcai Wu, Shouling Ji
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.03741">SUB-PLAY: Adversarial Policies against Partially Observed Multi-Agent Reinforcement Learning Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in multi-agent reinforcement learning (MARL) have opened up vast application prospects, such as swarm control of drones, collaborative manipulation by robotic arms, and multi-target encirclement. However, potential security threats during the MARL deployment need more attention and thorough investigation. Recent research reveals that attackers can rapidly exploit the victim's vulnerabilities, generating adversarial policies that result in the failure of specific tasks. For instance, reducing the winning rate of a superhuman-level Go AI to around 20%. Existing studies predominantly focus on two-player competitive environments, assuming attackers possess complete global state observation.
  In this study, we unveil, for the first time, the capability of attackers to generate adversarial policies even when restricted to partial observations of the victims in multi-agent competitive environments. Specifically, we propose a novel black-box attack (SUB-PLAY) that incorporates the concept of constructing multiple subgames to mitigate the impact of partial observability and suggests sharing transitions among subpolicies to improve attackers' exploitative ability. Extensive evaluations demonstrate the effectiveness of SUB-PLAY under three typical partial observability limitations. Visualization results indicate that adversarial policies induce significantly different activations of the victims' policy networks. Furthermore, we evaluate three potential defenses aimed at exploring ways to mitigate security threats posed by adversarial policies, providing constructive recommendations for deploying MARL in competitive environments.
<div id='section'>Paperid: <span id='pid'>298, <a href='https://arxiv.org/pdf/2310.17485.pdf' target='_blank'>https://arxiv.org/pdf/2310.17485.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Stephen Mak, Liming Xu, Tim Pearce, Michael Ostroumov, Alexandra Brintrup
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.17485">Fair collaborative vehicle routing: A deep multi-agent reinforcement learning approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Collaborative vehicle routing occurs when carriers collaborate through sharing their transportation requests and performing transportation requests on behalf of each other. This achieves economies of scale, thus reducing cost, greenhouse gas emissions and road congestion. But which carrier should partner with whom, and how much should each carrier be compensated? Traditional game theoretic solution concepts are expensive to calculate as the characteristic function scales exponentially with the number of agents. This would require solving the vehicle routing problem (NP-hard) an exponential number of times. We therefore propose to model this problem as a coalitional bargaining game solved using deep multi-agent reinforcement learning, where - crucially - agents are not given access to the characteristic function. Instead, we implicitly reason about the characteristic function; thus, when deployed in production, we only need to evaluate the expensive post-collaboration vehicle routing problem once. Our contribution is that we are the first to consider both the route allocation problem and gain sharing problem simultaneously - without access to the expensive characteristic function. Through decentralised machine learning, our agents bargain with each other and agree to outcomes that correlate well with the Shapley value - a fair profit allocation mechanism. Importantly, we are able to achieve a reduction in run-time of 88%.
<div id='section'>Paperid: <span id='pid'>299, <a href='https://arxiv.org/pdf/2310.14526.pdf' target='_blank'>https://arxiv.org/pdf/2310.14526.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yunfan Zhao, Nikhil Behari, Edward Hughes, Edwin Zhang, Dheeraj Nagaraj, Karl Tuyls, Aparna Taneja, Milind Tambe
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.14526">Towards a Pretrained Model for Restless Bandits via Multi-arm Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Restless multi-arm bandits (RMABs), a class of resource allocation problems with broad application in areas such as healthcare, online advertising, and anti-poaching, have recently been studied from a multi-agent reinforcement learning perspective. Prior RMAB research suffers from several limitations, e.g., it fails to adequately address continuous states, and requires retraining from scratch when arms opt-in and opt-out over time, a common challenge in many real world applications. We address these limitations by developing a neural network-based pre-trained model (PreFeRMAB) that has general zero-shot ability on a wide range of previously unseen RMABs, and which can be fine-tuned on specific instances in a more sample-efficient way than retraining from scratch. Our model also accommodates general multi-action settings and discrete or continuous state spaces. To enable fast generalization, we learn a novel single policy network model that utilizes feature information and employs a training procedure in which arms opt-in and out over time. We derive a new update rule for a crucial $Î»$-network with theoretical convergence guarantees and empirically demonstrate the advantages of our approach on several challenging, real-world inspired problems.
<div id='section'>Paperid: <span id='pid'>300, <a href='https://arxiv.org/pdf/2305.05911.pdf' target='_blank'>https://arxiv.org/pdf/2305.05911.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziqian Zhang, Lei Yuan, Lihe Li, Ke Xue, Chengxing Jia, Cong Guan, Chao Qian, Yang Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.05911">Fast Teammate Adaptation in the Presence of Sudden Policy Change</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In cooperative multi-agent reinforcement learning (MARL), where an agent coordinates with teammate(s) for a shared goal, it may sustain non-stationary caused by the policy change of teammates. Prior works mainly concentrate on the policy change during the training phase or teammates altering cross episodes, ignoring the fact that teammates may suffer from policy change suddenly within an episode, which might lead to miscoordination and poor performance as a result. We formulate the problem as an open Dec-POMDP, where we control some agents to coordinate with uncontrolled teammates, whose policies could be changed within one episode. Then we develop a new framework, fast teammates adaptation (Fastap), to address the problem. Concretely, we first train versatile teammates' policies and assign them to different clusters via the Chinese Restaurant Process (CRP). Then, we train the controlled agent(s) to coordinate with the sampled uncontrolled teammates by capturing their identifications as context for fast adaptation. Finally, each agent applies its local information to anticipate the teammates' context for decision-making accordingly. This process proceeds alternately, leading to a robust policy that can adapt to any teammates during the decentralized execution phase. We show in multiple multi-agent benchmarks that Fastap can achieve superior performance than multiple baselines in stationary and non-stationary scenarios.
<div id='section'>Paperid: <span id='pid'>301, <a href='https://arxiv.org/pdf/2305.05909.pdf' target='_blank'>https://arxiv.org/pdf/2305.05909.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lei Yuan, Zi-Qian Zhang, Ke Xue, Hao Yin, Feng Chen, Cong Guan, Li-He Li, Chao Qian, Yang Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.05909">Robust multi-agent coordination via evolutionary generation of auxiliary adversarial attackers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cooperative multi-agent reinforcement learning (CMARL) has shown to be promising for many real-world applications. Previous works mainly focus on improving coordination ability via solving MARL-specific challenges (e.g., non-stationarity, credit assignment, scalability), but ignore the policy perturbation issue when testing in a different environment. This issue hasn't been considered in problem formulation or efficient algorithm design. To address this issue, we firstly model the problem as a limited policy adversary Dec-POMDP (LPA-Dec-POMDP), where some coordinators from a team might accidentally and unpredictably encounter a limited number of malicious action attacks, but the regular coordinators still strive for the intended goal. Then, we propose Robust Multi-Agent Coordination via Evolutionary Generation of Auxiliary Adversarial Attackers (ROMANCE), which enables the trained policy to encounter diversified and strong auxiliary adversarial attacks during training, thus achieving high robustness under various policy perturbations. Concretely, to avoid the ego-system overfitting to a specific attacker, we maintain a set of attackers, which is optimized to guarantee the attackers high attacking quality and behavior diversity. The goal of quality is to minimize the ego-system coordination effect, and a novel diversity regularizer based on sparse action is applied to diversify the behaviors among attackers. The ego-system is then paired with a population of attackers selected from the maintained attacker set, and alternately trained against the constantly evolving attackers. Extensive experiments on multiple scenarios from SMAC indicate our ROMANCE provides comparable or better robustness and generalization ability than other baselines.
<div id='section'>Paperid: <span id='pid'>302, <a href='https://arxiv.org/pdf/2208.04957.pdf' target='_blank'>https://arxiv.org/pdf/2208.04957.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ke Xue, Yutong Wang, Cong Guan, Lei Yuan, Haobo Fu, Qiang Fu, Chao Qian, Yang Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2208.04957">Heterogeneous Multi-agent Zero-Shot Coordination by Coevolution</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating agents that can achieve zero-shot coordination (ZSC) with unseen partners is a new challenge in cooperative multi-agent reinforcement learning (MARL). Recently, some studies have made progress in ZSC by exposing the agents to diverse partners during the training process. They usually involve self-play when training the partners, implicitly assuming that the tasks are homogeneous. However, many real-world tasks are heterogeneous, and hence previous methods may be inefficient. In this paper, we study the heterogeneous ZSC problem for the first time and propose a general method based on coevolution, which coevolves two populations of agents and partners through three sub-processes: pairing, updating and selection. Experimental results on various heterogeneous tasks highlight the necessity of considering the heterogeneous setting and demonstrate that our proposed method is a promising solution for heterogeneous ZSC tasks.
<div id='section'>Paperid: <span id='pid'>303, <a href='https://arxiv.org/pdf/2507.23698.pdf' target='_blank'>https://arxiv.org/pdf/2507.23698.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shaofei Cai, Zhancun Mu, Haiwen Xia, Bowei Zhang, Anji Liu, Yitao Liang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.23698">Scalable Multi-Task Reinforcement Learning for Generalizable Spatial Intelligence in Visuomotor Agents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While Reinforcement Learning (RL) has achieved remarkable success in language modeling, its triumph hasn't yet fully translated to visuomotor agents. A primary challenge in RL models is their tendency to overfit specific tasks or environments, thereby hindering the acquisition of generalizable behaviors across diverse settings. This paper provides a preliminary answer to this challenge by demonstrating that RL-finetuned visuomotor agents in Minecraft can achieve zero-shot generalization to unseen worlds. Specifically, we explore RL's potential to enhance generalizable spatial reasoning and interaction capabilities in 3D worlds. To address challenges in multi-task RL representation, we analyze and establish cross-view goal specification as a unified multi-task goal space for visuomotor policies. Furthermore, to overcome the significant bottleneck of manual task design, we propose automated task synthesis within the highly customizable Minecraft environment for large-scale multi-task RL training, and we construct an efficient distributed RL framework to support this. Experimental results show RL significantly boosts interaction success rates by $4\times$ and enables zero-shot generalization of spatial reasoning across diverse environments, including real-world settings. Our findings underscore the immense potential of RL training in 3D simulated environments, especially those amenable to large-scale task generation, for significantly advancing visuomotor agents' spatial reasoning.
<div id='section'>Paperid: <span id='pid'>304, <a href='https://arxiv.org/pdf/2411.14264.pdf' target='_blank'>https://arxiv.org/pdf/2411.14264.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pedro Enrique Iturria-Rivera, Raimundas Gaigalas, Medhat Elsayed, Majid Bavand, Yigit Ozcan, Melike Erol-Kantarci
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.14264">Explainable Multi-Agent Reinforcement Learning for Extended Reality Codec Adaptation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Extended Reality (XR) services are set to transform applications over 5th and 6th generation wireless networks, delivering immersive experiences. Concurrently, Artificial Intelligence (AI) advancements have expanded their role in wireless networks, however, trust and transparency in AI remain to be strengthened. Thus, providing explanations for AI-enabled systems can enhance trust. We introduce Value Function Factorization (VFF)-based Explainable (X) Multi-Agent Reinforcement Learning (MARL) algorithms, explaining reward design in XR codec adaptation through reward decomposition. We contribute four enhancements to XMARL algorithms. Firstly, we detail architectural modifications to enable reward decomposition in VFF-based MARL algorithms: Value Decomposition Networks (VDN), Mixture of Q-Values (QMIX), and Q-Transformation (Q-TRAN). Secondly, inspired by multi-task learning, we reduce the overhead of vanilla XMARL algorithms. Thirdly, we propose a new explainability metric, Reward Difference Fluctuation Explanation (RDFX), suitable for problems with adjustable parameters. Lastly, we propose adaptive XMARL, leveraging network gradients and reward decomposition for improved action selection. Simulation results indicate that, in XR codec adaptation, the Packet Delivery Ratio reward is the primary contributor to optimal performance compared to the initial composite reward, which included delay and Data Rate Ratio components. Modifications to VFF-based XMARL algorithms, incorporating multi-headed structures and adaptive loss functions, enable the best-performing algorithm, Multi-Headed Adaptive (MHA)-QMIX, to achieve significant average gains over the Adjust Packet Size baseline up to 10.7%, 41.4%, 33.3%, and 67.9% in XR index, jitter, delay, and Packet Loss Ratio (PLR), respectively.
<div id='section'>Paperid: <span id='pid'>305, <a href='https://arxiv.org/pdf/2410.05127.pdf' target='_blank'>https://arxiv.org/pdf/2410.05127.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Noboru Isobe, Kenshi Abe, Kaito Ariu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.05127">Last Iterate Convergence in Monotone Mean Field Games</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Mean Field Game (MFG) is a framework for modeling and approximating the behavior of large numbers of agents. Computing equilibria in MFG has been of interest in multi-agent reinforcement learning. The theoretical guarantee that the last updated policy converges to an equilibrium has been limited. We propose the use of a simple, proximal-point (PP) type method to compute equilibria for MFGs. We then provide the first last-iterate convergence (LIC) guarantee under the Lasry--Lions-type monotonicity condition. We also propose an approximation of the update rule of PP ($\mathtt{APP}$) based on the observation that it is equivalent to solving the regularized MFG, which can be solved by mirror descent. We further establish that the regularized mirror descent achieves LIC at an exponential rate. Our numerical experiment demonstrates that $\mathtt{APP}$ efficiently computes the equilibrium.
<div id='section'>Paperid: <span id='pid'>306, <a href='https://arxiv.org/pdf/2405.15872.pdf' target='_blank'>https://arxiv.org/pdf/2405.15872.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pedro Enrique Iturria-Rivera, Raimundas Gaigalas, Medhat Elsayed, Majid Bavand, Yigit Ozcan, Melike Erol-Kantarci
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.15872">Extended Reality (XR) Codec Adaptation in 5G using Multi-Agent Reinforcement Learning with Attention Action Selection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Extended Reality (XR) services will revolutionize applications over 5th and 6th generation wireless networks by providing seamless virtual and augmented reality experiences. These applications impose significant challenges on network infrastructure, which can be addressed by machine learning algorithms due to their adaptability. This paper presents a Multi- Agent Reinforcement Learning (MARL) solution for optimizing codec parameters of XR traffic, comparing it to the Adjust Packet Size (APS) algorithm. Our cooperative multi-agent system uses an Optimistic Mixture of Q-Values (oQMIX) approach for handling Cloud Gaming (CG), Augmented Reality (AR), and Virtual Reality (VR) traffic. Enhancements include an attention mechanism and slate-Markov Decision Process (MDP) for improved action selection. Simulations show our solution outperforms APS with average gains of 30.1%, 15.6%, 16.5% 50.3% in XR index, jitter, delay, and Packet Loss Ratio (PLR), respectively. APS tends to increase throughput but also packet losses, whereas oQMIX reduces PLR, delay, and jitter while maintaining goodput.
<div id='section'>Paperid: <span id='pid'>307, <a href='https://arxiv.org/pdf/2310.16730.pdf' target='_blank'>https://arxiv.org/pdf/2310.16730.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dong-Ki Kim, Sungryull Sohn, Lajanugen Logeswaran, Dongsub Shim, Honglak Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.16730">MultiPrompter: Cooperative Prompt Optimization with Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, there has been an increasing interest in automated prompt optimization based on reinforcement learning (RL). This approach offers important advantages, such as generating interpretable prompts and being compatible with black-box foundation models. However, the substantial prompt space size poses challenges for RL-based methods, often leading to suboptimal policy convergence. This paper introduces MultiPrompter, a new framework that views prompt optimization as a cooperative game between prompters which take turns composing a prompt together. Our cooperative prompt optimization effectively reduces the problem size and helps prompters learn optimal prompts. We test our method on the text-to-image task and show its ability to generate higher-quality images than baselines.
<div id='section'>Paperid: <span id='pid'>308, <a href='https://arxiv.org/pdf/2305.07182.pdf' target='_blank'>https://arxiv.org/pdf/2305.07182.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qingpeng Zhao, Yuanyang Zhu, Zichuan Liu, Zhi Wang, Chunlin Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.07182">Boosting Value Decomposition via Unit-Wise Attentive State Representation for Cooperative Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In cooperative multi-agent reinforcement learning (MARL), the environmental stochasticity and uncertainties will increase exponentially when the number of agents increases, which puts hard pressure on how to come up with a compact latent representation from partial observation for boosting value decomposition. To tackle these issues, we propose a simple yet powerful method that alleviates partial observability and efficiently promotes coordination by introducing the UNit-wise attentive State Representation (UNSR). In UNSR, each agent learns a compact and disentangled unit-wise state representation outputted from transformer blocks, and produces its local action-value function. The proposed UNSR is used to boost the value decomposition with a multi-head attention mechanism for producing efficient credit assignment in the mixing network, providing an efficient reasoning path between the individual value function and joint value function. Experimental results demonstrate that our method achieves superior performance and data efficiency compared to solid baselines on the StarCraft II micromanagement challenge. Additional ablation experiments also help identify the key factors contributing to the performance of UNSR.
<div id='section'>Paperid: <span id='pid'>309, <a href='https://arxiv.org/pdf/2301.08491.pdf' target='_blank'>https://arxiv.org/pdf/2301.08491.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Elizaveta Tennant, Stephen Hailes, Mirco Musolesi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.08491">Modeling Moral Choices in Social Dilemmas with Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Practical uses of Artificial Intelligence (AI) in the real world have demonstrated the importance of embedding moral choices into intelligent agents. They have also highlighted that defining top-down ethical constraints on AI according to any one type of morality is extremely challenging and can pose risks. A bottom-up learning approach may be more appropriate for studying and developing ethical behavior in AI agents. In particular, we believe that an interesting and insightful starting point is the analysis of emergent behavior of Reinforcement Learning (RL) agents that act according to a predefined set of moral rewards in social dilemmas.
  In this work, we present a systematic analysis of the choices made by intrinsically-motivated RL agents whose rewards are based on moral theories. We aim to design reward structures that are simplified yet representative of a set of key ethical systems. Therefore, we first define moral reward functions that distinguish between consequence- and norm-based agents, between morality based on societal norms or internal virtues, and between single- and mixed-virtue (e.g., multi-objective) methodologies. Then, we evaluate our approach by modeling repeated dyadic interactions between learning moral agents in three iterated social dilemma games (Prisoner's Dilemma, Volunteer's Dilemma and Stag Hunt). We analyze the impact of different types of morality on the emergence of cooperation, defection or exploitation, and the corresponding social outcomes. Finally, we discuss the implications of these findings for the development of moral agents in artificial and mixed human-AI societies.
<div id='section'>Paperid: <span id='pid'>310, <a href='https://arxiv.org/pdf/2209.07225.pdf' target='_blank'>https://arxiv.org/pdf/2209.07225.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zichuan Liu, Yuanyang Zhu, Zhi Wang, Yang Gao, Chunlin Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2209.07225">MIXRTs: Toward Interpretable Multi-Agent Reinforcement Learning via Mixing Recurrent Soft Decision Trees</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While achieving tremendous success in various fields, existing multi-agent reinforcement learning (MARL) with a black-box neural network makes decisions in an opaque manner that hinders humans from understanding the learned knowledge and how input observations influence decisions. In contrast, existing interpretable approaches usually suffer from weak expressivity and low performance. To bridge this gap, we propose MIXing Recurrent soft decision Trees (MIXRTs), a novel interpretable architecture that can represent explicit decision processes via the root-to-leaf path and reflect each agent's contribution to the team. Specifically, we construct a novel soft decision tree using a recurrent structure and demonstrate which features influence the decision-making process. Then, based on the value decomposition framework, we linearly assign credit to each agent by explicitly mixing individual action values to estimate the joint action value using only local observations, providing new insights into interpreting the cooperation mechanism. Theoretical analysis confirms that MIXRTs guarantee additivity and monotonicity in the factorization of joint action values. Evaluations on complex tasks like Spread and StarCraft II demonstrate that MIXRTs compete with existing methods while providing clear explanations, paving the way for interpretable and high-performing MARL systems.
<div id='section'>Paperid: <span id='pid'>311, <a href='https://arxiv.org/pdf/2205.12880.pdf' target='_blank'>https://arxiv.org/pdf/2205.12880.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ho Long Fung, Victor-Alexandru Darvariu, Stephen Hailes, Mirco Musolesi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2205.12880">Trust-based Consensus in Multi-Agent Reinforcement Learning Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>An often neglected issue in multi-agent reinforcement learning (MARL) is the potential presence of unreliable agents in the environment whose deviations from expected behavior can prevent a system from accomplishing its intended tasks. In particular, consensus is a fundamental underpinning problem of cooperative distributed multi-agent systems. Consensus requires different agents, situated in a decentralized communication network, to reach an agreement out of a set of initial proposals that they put forward. Learning-based agents should adopt a protocol that allows them to reach consensus despite having one or more unreliable agents in the system. This paper investigates the problem of unreliable agents in MARL, considering consensus as a case study. Echoing established results in the distributed systems literature, our experiments show that even a moderate fraction of such agents can greatly impact the ability of reaching consensus in a networked environment. We propose Reinforcement Learning-based Trusted Consensus (RLTC), a decentralized trust mechanism, in which agents can independently decide which neighbors to communicate with. We empirically demonstrate that our trust mechanism is able to handle unreliable agents effectively, as evidenced by higher consensus success rates.
<div id='section'>Paperid: <span id='pid'>312, <a href='https://arxiv.org/pdf/2509.20648.pdf' target='_blank'>https://arxiv.org/pdf/2509.20648.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiyuan Pan, Zhe Liu, Hesheng Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.20648">Wonder Wins Ways: Curiosity-Driven Exploration through Multi-Agent Contextual Calibration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autonomous exploration in complex multi-agent reinforcement learning (MARL) with sparse rewards critically depends on providing agents with effective intrinsic motivation. While artificial curiosity offers a powerful self-supervised signal, it often confuses environmental stochasticity with meaningful novelty. Moreover, existing curiosity mechanisms exhibit a uniform novelty bias, treating all unexpected observations equally. However, peer behavior novelty, which encode latent task dynamics, are often overlooked, resulting in suboptimal exploration in decentralized, communication-free MARL settings. To this end, inspired by how human children adaptively calibrate their own exploratory behaviors via observing peers, we propose a novel approach to enhance multi-agent exploration. We introduce CERMIC, a principled framework that empowers agents to robustly filter noisy surprise signals and guide exploration by dynamically calibrating their intrinsic curiosity with inferred multi-agent context. Additionally, CERMIC generates theoretically-grounded intrinsic rewards, encouraging agents to explore state transitions with high information gain. We evaluate CERMIC on benchmark suites including VMAS, Meltingpot, and SMACv2. Empirical results demonstrate that exploration with CERMIC significantly outperforms SoTA algorithms in sparse-reward environments.
<div id='section'>Paperid: <span id='pid'>313, <a href='https://arxiv.org/pdf/2509.20648.pdf' target='_blank'>https://arxiv.org/pdf/2509.20648.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiyuan Pan, Zhe Liu, Hesheng Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.20648">Wonder Wins Ways: Curiosity-Driven Exploration through Multi-Agent Contextual Calibration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autonomous exploration in complex multi-agent reinforcement learning (MARL) with sparse rewards critically depends on providing agents with effective intrinsic motivation. While artificial curiosity offers a powerful self-supervised signal, it often confuses environmental stochasticity with meaningful novelty. Moreover, existing curiosity mechanisms exhibit a uniform novelty bias, treating all unexpected observations equally. However, peer behavior novelty, which encode latent task dynamics, are often overlooked, resulting in suboptimal exploration in decentralized, communication-free MARL settings. To this end, inspired by how human children adaptively calibrate their own exploratory behaviors via observing peers, we propose a novel approach to enhance multi-agent exploration. We introduce CERMIC, a principled framework that empowers agents to robustly filter noisy surprise signals and guide exploration by dynamically calibrating their intrinsic curiosity with inferred multi-agent context. Additionally, CERMIC generates theoretically-grounded intrinsic rewards, encouraging agents to explore state transitions with high information gain. We evaluate CERMIC on benchmark suites including VMAS, Meltingpot, and SMACv2. Empirical results demonstrate that exploration with CERMIC significantly outperforms SoTA algorithms in sparse-reward environments.
<div id='section'>Paperid: <span id='pid'>314, <a href='https://arxiv.org/pdf/2507.07671.pdf' target='_blank'>https://arxiv.org/pdf/2507.07671.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jovan Prodanov, BlaÅ¾ BertalaniÄ, Carolina Fortuna, Shih-Kai Chou, MatjaÅ¾ Branko JuriÄ, Ramon Sanchez-Iborra, Jernej Hribar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.07671">Multi-agent Reinforcement Learning-based In-place Scaling Engine for Edge-cloud Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Modern edge-cloud systems face challenges in efficiently scaling resources to handle dynamic and unpredictable workloads. Traditional scaling approaches typically rely on static thresholds and predefined rules, which are often inadequate for optimizing resource utilization and maintaining performance in distributed and dynamic environments. This inefficiency hinders the adaptability and performance required in edge-cloud infrastructures, which can only be achieved through the newly proposed in-place scaling. To address this problem, we propose the Multi-Agent Reinforcement Learning-based In-place Scaling Engine (MARLISE) that enables seamless, dynamic, reactive control with in-place resource scaling. We develop our solution using two Deep Reinforcement Learning algorithms: Deep Q-Network (DQN), and Proximal Policy Optimization (PPO). We analyze each version of the proposed MARLISE solution using dynamic workloads, demonstrating their ability to ensure low response times of microservices and scalability. Our results show that MARLISE-based approaches outperform heuristic method in managing resource elasticity while maintaining microservice response times and achieving higher resource efficiency.
<div id='section'>Paperid: <span id='pid'>315, <a href='https://arxiv.org/pdf/2507.07671.pdf' target='_blank'>https://arxiv.org/pdf/2507.07671.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jovan Prodanov, BlaÅ¾ BertalaniÄ, Carolina Fortuna, Shih-Kai Chou, MatjaÅ¾ Branko JuriÄ, Ramon Sanchez-Iborra, Jernej Hribar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.07671">Multi-agent Reinforcement Learning-based In-place Scaling Engine for Edge-cloud Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Modern edge-cloud systems face challenges in efficiently scaling resources to handle dynamic and unpredictable workloads. Traditional scaling approaches typically rely on static thresholds and predefined rules, which are often inadequate for optimizing resource utilization and maintaining performance in distributed and dynamic environments. This inefficiency hinders the adaptability and performance required in edge-cloud infrastructures, which can only be achieved through the newly proposed in-place scaling. To address this problem, we propose the Multi-Agent Reinforcement Learning-based In-place Scaling Engine (MARLISE) that enables seamless, dynamic, reactive control with in-place resource scaling. We develop our solution using two Deep Reinforcement Learning algorithms: Deep Q-Network (DQN), and Proximal Policy Optimization (PPO). We analyze each version of the proposed MARLISE solution using dynamic workloads, demonstrating their ability to ensure low response times of microservices and scalability. Our results show that MARLISE-based approaches outperform heuristic method in managing resource elasticity while maintaining microservice response times and achieving higher resource efficiency.
<div id='section'>Paperid: <span id='pid'>316, <a href='https://arxiv.org/pdf/2504.14520.pdf' target='_blank'>https://arxiv.org/pdf/2504.14520.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ahsan Bilal, Muhammad Ahmed Mohsin, Muhammad Umer, Muhammad Awais Khan Bangash, Muhammad Ali Jamshed
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.14520">Meta-Thinking in LLMs via Multi-Agent Reinforcement Learning: A Survey</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This survey explores the development of meta-thinking capabilities in Large Language Models (LLMs) from a Multi-Agent Reinforcement Learning (MARL) perspective. Meta-thinking self-reflection, assessment, and control of thinking processes is an important next step in enhancing LLM reliability, flexibility, and performance, particularly for complex or high-stakes tasks. The survey begins by analyzing current LLM limitations, such as hallucinations and the lack of internal self-assessment mechanisms. It then talks about newer methods, including RL from human feedback (RLHF), self-distillation, and chain-of-thought prompting, and each of their limitations. The crux of the survey is to talk about how multi-agent architectures, namely supervisor-agent hierarchies, agent debates, and theory of mind frameworks, can emulate human-like introspective behavior and enhance LLM robustness. By exploring reward mechanisms, self-play, and continuous learning methods in MARL, this survey gives a comprehensive roadmap to building introspective, adaptive, and trustworthy LLMs. Evaluation metrics, datasets, and future research avenues, including neuroscience-inspired architectures and hybrid symbolic reasoning, are also discussed.
<div id='section'>Paperid: <span id='pid'>317, <a href='https://arxiv.org/pdf/2407.12532.pdf' target='_blank'>https://arxiv.org/pdf/2407.12532.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xihe Qiu, Haoyu Wang, Xiaoyu Tan, Chao Qu, Yujie Xiong, Yuan Cheng, Yinghui Xu, Wei Chu, Yuan Qi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.12532">Towards Collaborative Intelligence: Propagating Intentions and Reasoning for Multi-Agent Coordination with Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Effective collaboration in multi-agent systems requires communicating goals and intentions between agents. Current agent frameworks often suffer from dependencies on single-agent execution and lack robust inter-module communication, frequently leading to suboptimal multi-agent reinforcement learning (MARL) policies and inadequate task coordination. To address these challenges, we present a framework for training large language models (LLMs) as collaborative agents to enable coordinated behaviors in cooperative MARL. Each agent maintains a private intention consisting of its current goal and associated sub-tasks. Agents broadcast their intentions periodically, allowing other agents to infer coordination tasks. A propagation network transforms broadcast intentions into teammate-specific communication messages, sharing relevant goals with designated teammates. The architecture of our framework is structured into planning, grounding, and execution modules. During execution, multiple agents interact in a downstream environment and communicate intentions to enable coordinated behaviors. The grounding module dynamically adapts comprehension strategies based on emerging coordination patterns, while feedback from execution agents influnces the planning module, enabling the dynamic re-planning of sub-tasks. Results in collaborative environment simulation demonstrate intention propagation reduces miscoordination errors by aligning sub-task dependencies between agents. Agents learn when to communicate intentions and which teammates require task details, resulting in emergent coordinated behaviors. This demonstrates the efficacy of intention sharing for cooperative multi-agent RL based on LLMs.
<div id='section'>Paperid: <span id='pid'>318, <a href='https://arxiv.org/pdf/2403.10996.pdf' target='_blank'>https://arxiv.org/pdf/2403.10996.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chinmay Vilas Samak, Tanmay Vilas Samak, Venkat Narayan Krovi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.10996">Mixed-Reality Digital Twins: Leveraging the Physical and Virtual Worlds for Hybrid Sim2Real Transition of Multi-Agent Reinforcement Learning Policies</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent reinforcement learning (MARL) for cyber-physical vehicle systems usually requires a significantly long training time due to their inherent complexity. Furthermore, deploying the trained policies in the real world demands a feature-rich environment along with multiple physical embodied agents, which may not be feasible due to monetary, physical, energy, or safety constraints. This work seeks to address these pain points by presenting a mixed-reality (MR) digital twin (DT) framework capable of: (i) boosting training speeds by selectively scaling parallelized simulation workloads on-demand, and (ii) immersing the MARL policies across hybrid simulation-to-reality (sim2real) experiments. The viability and performance of the proposed framework are highlighted through two representative use cases, which cover cooperative as well as competitive classes of MARL problems. We study the effect of: (i) agent and environment parallelization on training time, and (ii) systematic domain randomization on zero-shot sim2real transfer, across both case studies. Results indicate up to 76.3% reduction in training time with the proposed parallelization scheme and sim2real gap as low as 2.9% using the proposed deployment method.
<div id='section'>Paperid: <span id='pid'>319, <a href='https://arxiv.org/pdf/2312.12861.pdf' target='_blank'>https://arxiv.org/pdf/2312.12861.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Murad Dawood, Sicong Pan, Nils Dengler, Siqi Zhou, Angela P. Schoellig, Maren Bennewitz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.12861">Safe Multi-Agent Reinforcement Learning for Behavior-Based Cooperative Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we address the problem of behavior-based cooperative navigation of mobile robots using safe multi-agent reinforcement learning~(MARL). Our work is the first to focus on cooperative navigation without individual reference targets for the robots, using a single target for the formation's centroid. This eliminates the complexities involved in having several path planners to control a team of robots. To ensure safety, our MARL framework uses model predictive control (MPC) to prevent actions that could lead to collisions during training and execution. We demonstrate the effectiveness of our method in simulation and on real robots, achieving safe behavior-based cooperative navigation without using individual reference targets, with zero collisions, and faster target reaching compared to baselines. Finally, we study the impact of MPC safety filters on the learning process, revealing that we achieve faster convergence during training and we show that our approach can be safely deployed on real robots, even during early stages of the training.
<div id='section'>Paperid: <span id='pid'>320, <a href='https://arxiv.org/pdf/2309.10007.pdf' target='_blank'>https://arxiv.org/pdf/2309.10007.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tanmay Vilas Samak, Chinmay Vilas Samak, Venkat Krovi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.10007">Multi-Agent Deep Reinforcement Learning for Cooperative and Competitive Autonomous Vehicles using AutoDRIVE Ecosystem</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work presents a modular and parallelizable multi-agent deep reinforcement learning framework for imbibing cooperative as well as competitive behaviors within autonomous vehicles. We introduce AutoDRIVE Ecosystem as an enabler to develop physically accurate and graphically realistic digital twins of Nigel and F1TENTH, two scaled autonomous vehicle platforms with unique qualities and capabilities, and leverage this ecosystem to train and deploy multi-agent reinforcement learning policies. We first investigate an intersection traversal problem using a set of cooperative vehicles (Nigel) that share limited state information with each other in single as well as multi-agent learning settings using a common policy approach. We then investigate an adversarial head-to-head autonomous racing problem using a different set of vehicles (F1TENTH) in a multi-agent learning setting using an individual policy approach. In either set of experiments, a decentralized learning architecture was adopted, which allowed robust training and testing of the approaches in stochastic environments, since the agents were mutually independent and exhibited asynchronous motion behavior. The problems were further aggravated by providing the agents with sparse observation spaces and requiring them to sample control commands that implicitly satisfied the imposed kinodynamic as well as safety constraints. The experimental results for both problem statements are reported in terms of quantitative metrics and qualitative remarks for training as well as deployment phases.
<div id='section'>Paperid: <span id='pid'>321, <a href='https://arxiv.org/pdf/2305.06446.pdf' target='_blank'>https://arxiv.org/pdf/2305.06446.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yifei Min, Jiafan He, Tianhao Wang, Quanquan Gu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.06446">Cooperative Multi-Agent Reinforcement Learning: Asynchronous Communication and Linear Function Approximation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We study multi-agent reinforcement learning in the setting of episodic Markov decision processes, where multiple agents cooperate via communication through a central server. We propose a provably efficient algorithm based on value iteration that enable asynchronous communication while ensuring the advantage of cooperation with low communication overhead. With linear function approximation, we prove that our algorithm enjoys an $\tilde{\mathcal{O}}(d^{3/2}H^2\sqrt{K})$ regret with $\tilde{\mathcal{O}}(dHM^2)$ communication complexity, where $d$ is the feature dimension, $H$ is the horizon length, $M$ is the total number of agents, and $K$ is the total number of episodes. We also provide a lower bound showing that a minimal $Î©(dM)$ communication complexity is required to improve the performance through collaboration.
<div id='section'>Paperid: <span id='pid'>322, <a href='https://arxiv.org/pdf/2302.14399.pdf' target='_blank'>https://arxiv.org/pdf/2302.14399.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Federico Mason, Federico Chiariotti, Andrea Zanella, Petar Popovski
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.14399">Multi-Agent Reinforcement Learning for Pragmatic Communication and Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The automation of factories and manufacturing processes has been accelerating over the past few years, boosted by the Industry 4.0 paradigm, including diverse scenarios with mobile, flexible agents. Efficient coordination between mobile robots requires reliable wireless transmission in highly dynamic environments, often with strict timing requirements. Goal-oriented communication is a possible solution for this problem: communication decisions should be optimized for the target control task, providing the information that is most relevant to decide which action to take. From the control perspective, networked control design takes the communication impairments into account in its optmization of physical actions. In this work, we propose a joint design that combines goal-oriented communication and networked control into a single optimization model, an extension of a multiagent POMDP which we call Cyber-Physical POMDP (CP-POMDP). The model is flexible enough to represent several swarm and cooperative scenarios, and we illustrate its potential with two simple reference scenarios with a single agent and a set of supporting sensors. Joint training of the communication and control systems can significantly improve the overall performance, particularly if communication is severely constrained, and can even lead to implicit coordination of communication actions.
<div id='section'>Paperid: <span id='pid'>323, <a href='https://arxiv.org/pdf/2509.23960.pdf' target='_blank'>https://arxiv.org/pdf/2509.23960.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Manan Tayal, Aditya Singh, Shishir Kolathaya, Somil Bansal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.23960">MAD-PINN: A Decentralized Physics-Informed Machine Learning Framework for Safe and Optimal Multi-Agent Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Co-optimizing safety and performance in large-scale multi-agent systems remains a fundamental challenge. Existing approaches based on multi-agent reinforcement learning (MARL), safety filtering, or Model Predictive Control (MPC) either lack strict safety guarantees, suffer from conservatism, or fail to scale effectively. We propose MAD-PINN, a decentralized physics-informed machine learning framework for solving the multi-agent state-constrained optimal control problem (MASC-OCP). Our method leverages an epigraph-based reformulation of SC-OCP to simultaneously capture performance and safety, and approximates its solution via a physics-informed neural network. Scalability is achieved by training the SC-OCP value function on reduced-agent systems and deploying them in a decentralized fashion, where each agent relies only on local observations of its neighbours for decision-making. To further enhance safety and efficiency, we introduce an Hamilton-Jacobi (HJ) reachability-based neighbour selection strategy to prioritize safety-critical interactions, and a receding-horizon policy execution scheme that adapts to dynamic interactions while reducing computational burden. Experiments on multi-agent navigation tasks demonstrate that MAD-PINN achieves superior safety-performance trade-offs, maintains scalability as the number of agents grows, and consistently outperforms state-of-the-art baselines.
<div id='section'>Paperid: <span id='pid'>324, <a href='https://arxiv.org/pdf/2509.23960.pdf' target='_blank'>https://arxiv.org/pdf/2509.23960.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Manan Tayal, Aditya Singh, Shishir Kolathaya, Somil Bansal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.23960">MAD-PINN: A Decentralized Physics-Informed Machine Learning Framework for Safe and Optimal Multi-Agent Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Co-optimizing safety and performance in large-scale multi-agent systems remains a fundamental challenge. Existing approaches based on multi-agent reinforcement learning (MARL), safety filtering, or Model Predictive Control (MPC) either lack strict safety guarantees, suffer from conservatism, or fail to scale effectively. We propose MAD-PINN, a decentralized physics-informed machine learning framework for solving the multi-agent state-constrained optimal control problem (MASC-OCP). Our method leverages an epigraph-based reformulation of SC-OCP to simultaneously capture performance and safety, and approximates its solution via a physics-informed neural network. Scalability is achieved by training the SC-OCP value function on reduced-agent systems and deploying them in a decentralized fashion, where each agent relies only on local observations of its neighbours for decision-making. To further enhance safety and efficiency, we introduce an Hamilton-Jacobi (HJ) reachability-based neighbour selection strategy to prioritize safety-critical interactions, and a receding-horizon policy execution scheme that adapts to dynamic interactions while reducing computational burden. Experiments on multi-agent navigation tasks demonstrate that MAD-PINN achieves superior safety-performance trade-offs, maintains scalability as the number of agents grows, and consistently outperforms state-of-the-art baselines.
<div id='section'>Paperid: <span id='pid'>325, <a href='https://arxiv.org/pdf/2502.16496.pdf' target='_blank'>https://arxiv.org/pdf/2502.16496.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kun Hu, Muning Wen, Xihuai Wang, Shao Zhang, Yiwei Shi, Minne Li, Minglong Li, Ying Wen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.16496">PMAT: Optimizing Action Generation Order in Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent reinforcement learning (MARL) faces challenges in coordinating agents due to complex interdependencies within multi-agent systems. Most MARL algorithms use the simultaneous decision-making paradigm but ignore the action-level dependencies among agents, which reduces coordination efficiency. In contrast, the sequential decision-making paradigm provides finer-grained supervision for agent decision order, presenting the potential for handling dependencies via better decision order management. However, determining the optimal decision order remains a challenge. In this paper, we introduce Action Generation with Plackett-Luce Sampling (AGPS), a novel mechanism for agent decision order optimization. We model the order determination task as a Plackett-Luce sampling process to address issues such as ranking instability and vanishing gradient during the network training process. AGPS realizes credit-based decision order determination by establishing a bridge between the significance of agents' local observations and their decision credits, thus facilitating order optimization and dependency management. Integrating AGPS with the Multi-Agent Transformer, we propose the Prioritized Multi-Agent Transformer (PMAT), a sequential decision-making MARL algorithm with decision order optimization. Experiments on benchmarks including StarCraft II Multi-Agent Challenge, Google Research Football, and Multi-Agent MuJoCo show that PMAT outperforms state-of-the-art algorithms, greatly enhancing coordination efficiency.
<div id='section'>Paperid: <span id='pid'>326, <a href='https://arxiv.org/pdf/2411.01166.pdf' target='_blank'>https://arxiv.org/pdf/2411.01166.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Weifan Long, Wen Wen, Peng Zhai, Lihua Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.01166">Role Play: Learning Adaptive Role-Specific Strategies in Multi-Agent Interactions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Zero-shot coordination problem in multi-agent reinforcement learning (MARL), which requires agents to adapt to unseen agents, has attracted increasing attention. Traditional approaches often rely on the Self-Play (SP) framework to generate a diverse set of policies in a policy pool, which serves to improve the generalization capability of the final agent. However, these frameworks may struggle to capture the full spectrum of potential strategies, especially in real-world scenarios that demand agents balance cooperation with competition. In such settings, agents need strategies that can adapt to varying and often conflicting goals. Drawing inspiration from Social Value Orientation (SVO)-where individuals maintain stable value orientations during interactions with others-we propose a novel framework called \emph{Role Play} (RP). RP employs role embeddings to transform the challenge of policy diversity into a more manageable diversity of roles. It trains a common policy with role embedding observations and employs a role predictor to estimate the joint role embeddings of other agents, helping the learning agent adapt to its assigned role. We theoretically prove that an approximate optimal policy can be achieved by optimizing the expected cumulative reward relative to an approximate role-based policy. Experimental results in both cooperative (Overcooked) and mixed-motive games (Harvest, CleanUp) reveal that RP consistently outperforms strong baselines when interacting with unseen agents, highlighting its robustness and adaptability in complex environments.
<div id='section'>Paperid: <span id='pid'>327, <a href='https://arxiv.org/pdf/2403.08219.pdf' target='_blank'>https://arxiv.org/pdf/2403.08219.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenbo Zhao, Shengjie Wang, Yixuan Fan, Yang Gao, Tao Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.08219">SpaceOctopus: An Octopus-inspired Motion Planning Framework for Multi-arm Space Robot</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Space robots have played a critical role in autonomous maintenance and space junk removal. Multi-arm space robots can efficiently complete the target capture and base reorientation tasks due to their flexibility and the collaborative capabilities between the arms. However, the complex coupling properties arising from both the multiple arms and the free-floating base present challenges to the motion planning problems of multi-arm space robots. We observe that the octopus elegantly achieves similar goals when grabbing prey and escaping from danger. Inspired by the distributed control of octopuses' limbs, we develop a multi-level decentralized motion planning framework to manage the movement of different arms of space robots. This motion planning framework integrates naturally with the multi-agent reinforcement learning (MARL) paradigm. The results indicate that our method outperforms the previous method (centralized training). Leveraging the flexibility of the decentralized framework, we reassemble policies trained for different tasks, enabling the space robot to complete trajectory planning tasks while adjusting the base attitude without further learning. Furthermore, our experiments confirm the superior robustness of our method in the face of external disturbances, changing base masses, and even the failure of one arm.
<div id='section'>Paperid: <span id='pid'>328, <a href='https://arxiv.org/pdf/2401.09455.pdf' target='_blank'>https://arxiv.org/pdf/2401.09455.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yifeng Lyu, Han Hu, Rongfei Fan, Zhi Liu, Jianping An, Shiwen Mao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.09455">Dynamic Routing for Integrated Satellite-Terrestrial Networks: A Constrained Multi-Agent Reinforcement Learning Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The integrated satellite-terrestrial network (ISTN) system has experienced significant growth, offering seamless communication services in remote areas with limited terrestrial infrastructure. However, designing a routing scheme for ISTN is exceedingly difficult, primarily due to the heightened complexity resulting from the inclusion of additional ground stations, along with the requirement to satisfy various constraints related to satellite service quality. To address these challenges, we study packet routing with ground stations and satellites working jointly to transmit packets, while prioritizing fast communication and meeting energy efficiency and packet loss requirements. Specifically, we formulate the problem of packet routing with constraints as a max-min problem using the Lagrange method. Then we propose a novel constrained Multi-Agent reinforcement learning (MARL) dynamic routing algorithm named CMADR, which efficiently balances objective improvement and constraint satisfaction during the updating of policy and Lagrange multipliers. Finally, we conduct extensive experiments and an ablation study using the OneWeb and Telesat mega-constellations. Results demonstrate that CMADR reduces the packet delay by a minimum of 21% and 15%, while meeting stringent energy consumption and packet loss rate constraints, outperforming several baseline algorithms.
<div id='section'>Paperid: <span id='pid'>329, <a href='https://arxiv.org/pdf/2309.15234.pdf' target='_blank'>https://arxiv.org/pdf/2309.15234.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Weizheng Wang, Le Mao, Ruiqi Wang, Byung-Cheol Min
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.15234">Multi-Robot Cooperative Socially-Aware Navigation Using Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In public spaces shared with humans, ensuring multi-robot systems navigate without collisions while respecting social norms is challenging, particularly with limited communication. Although current robot social navigation techniques leverage advances in reinforcement learning and deep learning, they frequently overlook robot dynamics in simulations, leading to a simulation-to-reality gap. In this paper, we bridge this gap by presenting a new multi-robot social navigation environment crafted using Dec-POSMDP and multi-agent reinforcement learning. Furthermore, we introduce SAMARL: a novel benchmark for cooperative multi-robot social navigation. SAMARL employs a unique spatial-temporal transformer combined with multi-agent reinforcement learning. This approach effectively captures the complex interactions between robots and humans, thus promoting cooperative tendencies in multi-robot systems. Our extensive experiments reveal that SAMARL outperforms existing baseline and ablation models in our designed environment. Demo videos for this work can be found at: https://sites.google.com/view/samarl
<div id='section'>Paperid: <span id='pid'>330, <a href='https://arxiv.org/pdf/2302.07363.pdf' target='_blank'>https://arxiv.org/pdf/2302.07363.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoran Wang, Yingtong Dou, Canyu Chen, Lichao Sun, Philip S. Yu, Kai Shu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.07363">Attacking Fake News Detectors via Manipulating News Social Engagement</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Social media is one of the main sources for news consumption, especially among the younger generation. With the increasing popularity of news consumption on various social media platforms, there has been a surge of misinformation which includes false information or unfounded claims. As various text- and social context-based fake news detectors are proposed to detect misinformation on social media, recent works start to focus on the vulnerabilities of fake news detectors. In this paper, we present the first adversarial attack framework against Graph Neural Network (GNN)-based fake news detectors to probe their robustness. Specifically, we leverage a multi-agent reinforcement learning (MARL) framework to simulate the adversarial behavior of fraudsters on social media. Research has shown that in real-world settings, fraudsters coordinate with each other to share different news in order to evade the detection of fake news detectors. Therefore, we modeled our MARL framework as a Markov Game with bot, cyborg, and crowd worker agents, which have their own distinctive cost, budget, and influence. We then use deep Q-learning to search for the optimal policy that maximizes the rewards. Extensive experimental results on two real-world fake news propagation datasets demonstrate that our proposed framework can effectively sabotage the GNN-based fake news detector performance. We hope this paper can provide insights for future research on fake news detection.
<div id='section'>Paperid: <span id='pid'>331, <a href='https://arxiv.org/pdf/2510.08240.pdf' target='_blank'>https://arxiv.org/pdf/2510.08240.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jingyu Zhang, Haozhu Wang, Eric Michael Smith, Sid Wang, Amr Sharaf, Mahesh Pasupuleti, Benjamin Van Durme, Daniel Khashabi, Jason Weston, Hongyuan Zhan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.08240">The Alignment Waltz: Jointly Training Agents to Collaborate for Safety</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Harnessing the power of LLMs requires a delicate dance between being helpful and harmless. This creates a fundamental tension between two competing challenges: vulnerability to adversarial attacks that elicit unsafe content, and a tendency for overrefusal on benign but sensitive prompts. Current approaches often navigate this dance with safeguard models that completely reject any content that contains unsafe portions. This approach cuts the music entirely-it may exacerbate overrefusals and fails to provide nuanced guidance for queries it refuses. To teach models a more coordinated choreography, we propose WaltzRL, a novel multi-agent reinforcement learning framework that formulates safety alignment as a collaborative, positive-sum game. WaltzRL jointly trains a conversation agent and a feedback agent, where the latter is incentivized to provide useful suggestions that improve the safety and helpfulness of the conversation agent's responses. At the core of WaltzRL is a Dynamic Improvement Reward (DIR) that evolves over time based on how well the conversation agent incorporates the feedback. At inference time, unsafe or overrefusing responses from the conversation agent are improved rather than discarded. The feedback agent is deployed together with the conversation agent and only engages adaptively when needed, preserving helpfulness and low latency on safe queries. Our experiments, conducted across five diverse datasets, demonstrate that WaltzRL significantly reduces both unsafe responses (e.g., from 39.0% to 4.6% on WildJailbreak) and overrefusals (from 45.3% to 9.9% on OR-Bench) compared to various baselines. By enabling the conversation and feedback agents to co-evolve and adaptively apply feedback, WaltzRL enhances LLM safety without degrading general capabilities, thereby advancing the Pareto front between helpfulness and harmlessness.
<div id='section'>Paperid: <span id='pid'>332, <a href='https://arxiv.org/pdf/2510.08240.pdf' target='_blank'>https://arxiv.org/pdf/2510.08240.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jingyu Zhang, Haozhu Wang, Eric Michael Smith, Sid Wang, Amr Sharaf, Mahesh Pasupuleti, Benjamin Van Durme, Daniel Khashabi, Jason Weston, Hongyuan Zhan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.08240">The Alignment Waltz: Jointly Training Agents to Collaborate for Safety</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Harnessing the power of LLMs requires a delicate dance between being helpful and harmless. This creates a fundamental tension between two competing challenges: vulnerability to adversarial attacks that elicit unsafe content, and a tendency for overrefusal on benign but sensitive prompts. Current approaches often navigate this dance with safeguard models that completely reject any content that contains unsafe portions. This approach cuts the music entirely-it may exacerbate overrefusals and fails to provide nuanced guidance for queries it refuses. To teach models a more coordinated choreography, we propose WaltzRL, a novel multi-agent reinforcement learning framework that formulates safety alignment as a collaborative, positive-sum game. WaltzRL jointly trains a conversation agent and a feedback agent, where the latter is incentivized to provide useful suggestions that improve the safety and helpfulness of the conversation agent's responses. At the core of WaltzRL is a Dynamic Improvement Reward (DIR) that evolves over time based on how well the conversation agent incorporates the feedback. At inference time, unsafe or overrefusing responses from the conversation agent are improved rather than discarded. The feedback agent is deployed together with the conversation agent and only engages adaptively when needed, preserving helpfulness and low latency on safe queries. Our experiments, conducted across five diverse datasets, demonstrate that WaltzRL significantly reduces both unsafe responses (e.g., from 39.0% to 4.6% on WildJailbreak) and overrefusals (from 45.3% to 9.9% on OR-Bench) compared to various baselines. By enabling the conversation and feedback agents to co-evolve and adaptively apply feedback, WaltzRL enhances LLM safety without degrading general capabilities, thereby advancing the Pareto front between helpfulness and harmlessness.
<div id='section'>Paperid: <span id='pid'>333, <a href='https://arxiv.org/pdf/2509.12117.pdf' target='_blank'>https://arxiv.org/pdf/2509.12117.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aryaman Reddi, Gabriele Tiboni, Jan Peters, Carlo D'Eramo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.12117">$K$-Level Policy Gradients for Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Actor-critic algorithms for deep multi-agent reinforcement learning (MARL) typically employ a policy update that responds to the current strategies of other agents. While being straightforward, this approach does not account for the updates of other agents at the same update step, resulting in miscoordination. In this paper, we introduce the $K$-Level Policy Gradient (KPG), a method that recursively updates each agent against the updated policies of other agents, speeding up the discovery of effective coordinated policies. We theoretically prove that KPG with finite iterates achieves monotonic convergence to a local Nash equilibrium under certain conditions. We provide principled implementations of KPG by applying it to the deep MARL algorithms MAPPO, MADDPG, and FACMAC. Empirically, we demonstrate superior performance over existing deep MARL algorithms in StarCraft II and multi-agent MuJoCo.
<div id='section'>Paperid: <span id='pid'>334, <a href='https://arxiv.org/pdf/2509.12117.pdf' target='_blank'>https://arxiv.org/pdf/2509.12117.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aryaman Reddi, Gabriele Tiboni, Jan Peters, Carlo D'Eramo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.12117">$K$-Level Policy Gradients for Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Actor-critic algorithms for deep multi-agent reinforcement learning (MARL) typically employ a policy update that responds to the current strategies of other agents. While being straightforward, this approach does not account for the updates of other agents at the same update step, resulting in miscoordination. In this paper, we introduce the $K$-Level Policy Gradient (KPG), a method that recursively updates each agent against the updated policies of other agents, speeding up the discovery of effective coordinated policies. We theoretically prove that KPG with finite iterates achieves monotonic convergence to a local Nash equilibrium under certain conditions. We provide principled implementations of KPG by applying it to the deep MARL algorithms MAPPO, MADDPG, and FACMAC. Empirically, we demonstrate superior performance over existing deep MARL algorithms in StarCraft II and multi-agent MuJoCo.
<div id='section'>Paperid: <span id='pid'>335, <a href='https://arxiv.org/pdf/2506.24119.pdf' target='_blank'>https://arxiv.org/pdf/2506.24119.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bo Liu, Leon Guertler, Simon Yu, Zichen Liu, Penghui Qi, Daniel Balcells, Mickel Liu, Cheston Tan, Weiyan Shi, Min Lin, Wee Sun Lee, Natasha Jaques
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.24119">SPIRAL: Self-Play on Zero-Sum Games Incentivizes Reasoning via Multi-Agent Multi-Turn Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in reinforcement learning have shown that language models can develop sophisticated reasoning through training on tasks with verifiable rewards, but these approaches depend on human-curated problem-answer pairs and domain-specific reward engineering. We introduce SPIRAL, a self-play framework where models learn by playing multi-turn, zero-sum games against continuously improving versions of themselves, eliminating the need for human supervision. Through self-play, SPIRAL generates an infinite curriculum of progressively challenging problems as models must constantly adapt to stronger opponents. To enable this self-play training at scale, We implement a fully online, multi-turn, multi-agent reinforcement learning system for LLMs and propose role-conditioned advantage estimation (RAE) to stabilize multi-agent training. Using SPIRAL, self-play on zero-sum games produces reasoning capabilities that transfer broadly. Training Qwen3-4B-Base on Kuhn Poker alone achieves 8.6% improvement on math and 8.4% on general reasoning, outperforming SFT on 25,000 expert game trajectories. Analysis reveals that this transfer occurs through three cognitive patterns: systematic decomposition, expected value calculation, and case-by-case analysis. Multi-game training (TicTacToe, Kuhn Poker, Simple Negotiation) further enhances performance as each game develops distinct reasoning strengths. Applying SPIRAL to a strong reasoning model (DeepSeek-R1-Distill-Qwen-7B) can still lead to 2.0% average improvement. These results demonstrate that zero-sum games naturally develop transferable reasoning capabilities, highlighting a promising direction for autonomous reasoning development.
<div id='section'>Paperid: <span id='pid'>336, <a href='https://arxiv.org/pdf/2506.13755.pdf' target='_blank'>https://arxiv.org/pdf/2506.13755.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Arya Fayyazi, Mehdi Kamal, Massoud Pedram
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.13755">MARCO: Hardware-Aware Neural Architecture Search for Edge Devices with Multi-Agent Reinforcement Learning and Conformal Prediction Filtering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces MARCO (Multi-Agent Reinforcement learning with Conformal Optimization), a novel hardware-aware framework for efficient neural architecture search (NAS) targeting resource-constrained edge devices. By significantly reducing search time and maintaining accuracy under strict hardware constraints, MARCO bridges the gap between automated DNN design and CAD for edge AI deployment. MARCO's core technical contribution lies in its unique combination of multi-agent reinforcement learning (MARL) with Conformal Prediction (CP) to accelerate the hardware/software co-design process for deploying deep neural networks. Unlike conventional once-for-all (OFA) supernet approaches that require extensive pretraining, MARCO decomposes the NAS task into a hardware configuration agent (HCA) and a Quantization Agent (QA). The HCA optimizes high-level design parameters, while the QA determines per-layer bit-widths under strict memory and latency budgets using a shared reward signal within a centralized-critic, decentralized-execution (CTDE) paradigm. A key innovation is the integration of a calibrated CP surrogate model that provides statistical guarantees (with a user-defined miscoverage rate) to prune unpromising candidate architectures before incurring the high costs of partial training or hardware simulation. This early filtering drastically reduces the search space while ensuring that high-quality designs are retained with a high probability. Extensive experiments on MNIST, CIFAR-10, and CIFAR-100 demonstrate that MARCO achieves a 3-4x reduction in total search time compared to an OFA baseline while maintaining near-baseline accuracy (within 0.3%). Furthermore, MARCO also reduces inference latency. Validation on a MAX78000 evaluation board confirms that simulator trends hold in practice, with simulator estimates deviating from measured values by less than 5%.
<div id='section'>Paperid: <span id='pid'>337, <a href='https://arxiv.org/pdf/2502.04399.pdf' target='_blank'>https://arxiv.org/pdf/2502.04399.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bokeng Zheng, Bo Rao, Tianxiang Zhu, Chee Wei Tan, Jingpu Duan, Zhi Zhou, Xu Chen, Xiaoxi Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.04399">Online Location Planning for AI-Defined Vehicles: Optimizing Joint Tasks of Order Serving and Spatio-Temporal Heterogeneous Model Fine-Tuning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Advances in artificial intelligence (AI) including foundation models (FMs), are increasingly transforming human society, with smart city driving the evolution of urban living.Meanwhile, vehicle crowdsensing (VCS) has emerged as a key enabler, leveraging vehicles' mobility and sensor-equipped capabilities. In particular, ride-hailing vehicles can effectively facilitate flexible data collection and contribute towards urban intelligence, despite resource limitations. Therefore, this work explores a promising scenario, where edge-assisted vehicles perform joint tasks of order serving and the emerging foundation model fine-tuning using various urban data. However, integrating the VCS AI task with the conventional order serving task is challenging, due to their inconsistent spatio-temporal characteristics: (i) The distributions of ride orders and data point-of-interests (PoIs) may not coincide in geography, both following a priori unknown patterns; (ii) they have distinct forms of temporal effects, i.e., prolonged waiting makes orders become instantly invalid while data with increased staleness gradually reduces its utility for model fine-tuning.To overcome these obstacles, we propose an online framework based on multi-agent reinforcement learning (MARL) with careful augmentation. A new quality-of-service (QoS) metric is designed to characterize and balance the utility of the two joint tasks, under the effects of varying data volumes and staleness. We also integrate graph neural networks (GNNs) with MARL to enhance state representations, capturing graph-structured, time-varying dependencies among vehicles and across locations. Extensive experiments on our testbed simulator, utilizing various real-world foundation model fine-tuning tasks and the New York City Taxi ride order dataset, demonstrate the advantage of our proposed method.
<div id='section'>Paperid: <span id='pid'>338, <a href='https://arxiv.org/pdf/2411.03603.pdf' target='_blank'>https://arxiv.org/pdf/2411.03603.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuqian Fu, Yuanheng Zhu, Haoran Li, Zijie Zhao, Jiajun Chai, Dongbin Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.03603">CPIG: Leveraging Consistency Policy with Intention Guidance for Multi-agent Exploration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Efficient exploration is crucial in cooperative multi-agent reinforcement learning (MARL), especially in sparse-reward settings. However, due to the reliance on the unimodal policy, existing methods are prone to falling into the local optima, hindering the effective exploration of better policies. Furthermore, in sparse-reward settings, each agent tends to receive a scarce reward, which poses significant challenges to inter-agent cooperation. This not only increases the difficulty of policy learning but also degrades the overall performance of multi-agent tasks. To address these issues, we propose a Consistency Policy with Intention Guidance (CPIG), with two primary components: (a) introducing a multimodal policy to enhance the agent's exploration capability, and (b) sharing the intention among agents to foster agent cooperation. For component (a), CPIG incorporates a Consistency model as the policy, leveraging its multimodal nature and stochastic characteristics to facilitate exploration. Regarding component (b), we introduce an Intention Learner to deduce the intention on the global state from each agent's local observation. This intention then serves as a guidance for the Consistency Policy, promoting cooperation among agents. The proposed method is evaluated in multi-agent particle environments (MPE) and multi-agent MuJoCo (MAMuJoCo). Empirical results demonstrate that our method not only achieves comparable performance to various baselines in dense-reward environments but also significantly enhances performance in sparse-reward settings, outperforming state-of-the-art (SOTA) algorithms by 20%.
<div id='section'>Paperid: <span id='pid'>339, <a href='https://arxiv.org/pdf/2407.08192.pdf' target='_blank'>https://arxiv.org/pdf/2407.08192.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Arya Fayyazi, Mehdi Kamal, Massoud Pedram
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.08192">Dynamic Co-Optimization Compiler: Leveraging Multi-Agent Reinforcement Learning for Enhanced DNN Accelerator Performance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces a novel Dynamic Co-Optimization Compiler (DCOC), which employs an adaptive Multi-Agent Reinforcement Learning (MARL) framework to enhance the efficiency of mapping machine learning (ML) models, particularly Deep Neural Networks (DNNs), onto diverse hardware platforms. DCOC incorporates three specialized actor-critic agents within MARL, each dedicated to different optimization facets: one for hardware and two for software. This cooperative strategy results in an integrated hardware/software co-optimization approach, improving the precision and speed of DNN deployments. By focusing on high-confidence configurations, DCOC effectively reduces the search space, achieving remarkable performance over existing methods. Our results demonstrate that DCOC enhances throughput by up to 37.95% while reducing optimization time by up to 42.2% across various DNN models, outperforming current state-of-the-art frameworks.
<div id='section'>Paperid: <span id='pid'>340, <a href='https://arxiv.org/pdf/2405.01839.pdf' target='_blank'>https://arxiv.org/pdf/2405.01839.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qian Long, Fangwei Zhong, Mingdong Wu, Yizhou Wang, Song-Chun Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.01839">SocialGFs: Learning Social Gradient Fields for Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent systems (MAS) need to adaptively cope with dynamic environments, changing agent populations, and diverse tasks. However, most of the multi-agent systems cannot easily handle them, due to the complexity of the state and task space. The social impact theory regards the complex influencing factors as forces acting on an agent, emanating from the environment, other agents, and the agent's intrinsic motivation, referring to the social force. Inspired by this concept, we propose a novel gradient-based state representation for multi-agent reinforcement learning. To non-trivially model the social forces, we further introduce a data-driven method, where we employ denoising score matching to learn the social gradient fields (SocialGFs) from offline samples, e.g., the attractive or repulsive outcomes of each force. During interactions, the agents take actions based on the multi-dimensional gradients to maximize their own rewards. In practice, we integrate SocialGFs into the widely used multi-agent reinforcement learning algorithms, e.g., MAPPO. The empirical results reveal that SocialGFs offer four advantages for multi-agent systems: 1) they can be learned without requiring online interaction, 2) they demonstrate transferability across diverse tasks, 3) they facilitate credit assignment in challenging reward settings, and 4) they are scalable with the increasing number of agents.
<div id='section'>Paperid: <span id='pid'>341, <a href='https://arxiv.org/pdf/2404.00886.pdf' target='_blank'>https://arxiv.org/pdf/2404.00886.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Liwen Zhu, Peixi Peng, Zongqing Lu, Yonghong Tian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.00886">MTLight: Efficient Multi-Task Reinforcement Learning for Traffic Signal Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Traffic signal control has a great impact on alleviating traffic congestion in modern cities. Deep reinforcement learning (RL) has been widely used for this task in recent years, demonstrating promising performance but also facing many challenges such as limited performances and sample inefficiency. To handle these challenges, MTLight is proposed to enhance the agent observation with a latent state, which is learned from numerous traffic indicators. Meanwhile, multiple auxiliary and supervisory tasks are constructed to learn the latent state, and two types of embedding latent features, the task-specific feature and task-shared feature, are used to make the latent state more abundant. Extensive experiments conducted on CityFlow demonstrate that MTLight has leading convergence speed and asymptotic performance. We further simulate under peak-hour pattern in all scenarios with increasing control difficulty and the results indicate that MTLight is highly adaptable.
<div id='section'>Paperid: <span id='pid'>342, <a href='https://arxiv.org/pdf/2402.11800.pdf' target='_blank'>https://arxiv.org/pdf/2402.11800.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Arman Adibi, Nicolo Dal Fabbro, Luca Schenato, Sanjeev Kulkarni, H. Vincent Poor, George J. Pappas, Hamed Hassani, Aritra Mitra
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.11800">Stochastic Approximation with Delayed Updates: Finite-Time Rates under Markovian Sampling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Motivated by applications in large-scale and multi-agent reinforcement learning, we study the non-asymptotic performance of stochastic approximation (SA) schemes with delayed updates under Markovian sampling. While the effect of delays has been extensively studied for optimization, the manner in which they interact with the underlying Markov process to shape the finite-time performance of SA remains poorly understood. In this context, our first main contribution is to show that under time-varying bounded delays, the delayed SA update rule guarantees exponentially fast convergence of the \emph{last iterate} to a ball around the SA operator's fixed point. Notably, our bound is \emph{tight} in its dependence on both the maximum delay $Ï_{max}$, and the mixing time $Ï_{mix}$. To achieve this tight bound, we develop a novel inductive proof technique that, unlike various existing delayed-optimization analyses, relies on establishing uniform boundedness of the iterates. As such, our proof may be of independent interest. Next, to mitigate the impact of the maximum delay on the convergence rate, we provide the first finite-time analysis of a delay-adaptive SA scheme under Markovian sampling. In particular, we show that the exponent of convergence of this scheme gets scaled down by $Ï_{avg}$, as opposed to $Ï_{max}$ for the vanilla delayed SA rule; here, $Ï_{avg}$ denotes the average delay across all iterations. Moreover, the adaptive scheme requires no prior knowledge of the delay sequence for step-size tuning. Our theoretical findings shed light on the finite-time effects of delays for a broad class of algorithms, including TD learning, Q-learning, and stochastic gradient descent under Markovian sampling.
<div id='section'>Paperid: <span id='pid'>343, <a href='https://arxiv.org/pdf/2402.00738.pdf' target='_blank'>https://arxiv.org/pdf/2402.00738.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Guangzheng Hu, Yuanheng Zhu, Haoran Li, Dongbin Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.00738">FM3Q: Factorized Multi-Agent MiniMax Q-Learning for Two-Team Zero-Sum Markov Game</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Many real-world applications involve some agents that fall into two teams, with payoffs that are equal within the same team but of opposite sign across the opponent team. The so-called two-team zero-sum Markov games (2t0sMGs) can be resolved with reinforcement learning in recent years. However, existing methods are thus inefficient in light of insufficient consideration of intra-team credit assignment, data utilization and computational intractability. In this paper, we propose the individual-global-minimax (IGMM) principle to ensure the coherence between two-team minimax behaviors and the individual greedy behaviors through Q functions in 2t0sMGs. Based on it, we present a novel multi-agent reinforcement learning framework, Factorized Multi-Agent MiniMax Q-Learning (FM3Q), which can factorize the joint minimax Q function into individual ones and iteratively solve for the IGMM-satisfied minimax Q functions for 2t0sMGs. Moreover, an online learning algorithm with neural networks is proposed to implement FM3Q and obtain the deterministic and decentralized minimax policies for two-team players. A theoretical analysis is provided to prove the convergence of FM3Q. Empirically, we use three environments to evaluate the learning efficiency and final performance of FM3Q and show its superiority on 2t0sMGs.
<div id='section'>Paperid: <span id='pid'>344, <a href='https://arxiv.org/pdf/2401.10156.pdf' target='_blank'>https://arxiv.org/pdf/2401.10156.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kaige Qu, Weihua Zhuang, Qiang Ye, Wen Wu, Xuemin Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.10156">Model-Assisted Learning for Adaptive Cooperative Perception of Connected Autonomous Vehicles</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cooperative perception (CP) is a key technology to facilitate consistent and accurate situational awareness for connected and autonomous vehicles (CAVs). To tackle the network resource inefficiency issue in traditional broadcast-based CP, unicast-based CP has been proposed to associate CAV pairs for cooperative perception via vehicle-to-vehicle transmission. In this paper, we investigate unicast-based CP among CAV pairs. With the consideration of dynamic perception workloads and channel conditions due to vehicle mobility and dynamic radio resource availability, we propose an adaptive cooperative perception scheme for CAV pairs in a mixed-traffic autonomous driving scenario with both CAVs and human-driven vehicles. We aim to determine when to switch between cooperative perception and stand-alone perception for each CAV pair, and allocate communication and computing resources to cooperative CAV pairs for maximizing the computing efficiency gain under perception task delay requirements. A model-assisted multi-agent reinforcement learning (MARL) solution is developed, which integrates MARL for an adaptive CAV cooperation decision and an optimization model for communication and computing resource allocation. Simulation results demonstrate the effectiveness of the proposed scheme in achieving high computing efficiency gain, as compared with benchmark schemes.
<div id='section'>Paperid: <span id='pid'>345, <a href='https://arxiv.org/pdf/2312.01058.pdf' target='_blank'>https://arxiv.org/pdf/2312.01058.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lei Yuan, Ziqian Zhang, Lihe Li, Cong Guan, Yang Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.01058">A Survey of Progress on Cooperative Multi-agent Reinforcement Learning in Open Environment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent Reinforcement Learning (MARL) has gained wide attention in recent years and has made progress in various fields. Specifically, cooperative MARL focuses on training a team of agents to cooperatively achieve tasks that are difficult for a single agent to handle. It has shown great potential in applications such as path planning, autonomous driving, active voltage control, and dynamic algorithm configuration. One of the research focuses in the field of cooperative MARL is how to improve the coordination efficiency of the system, while research work has mainly been conducted in simple, static, and closed environment settings. To promote the application of artificial intelligence in real-world, some research has begun to explore multi-agent coordination in open environments. These works have made progress in exploring and researching the environments where important factors might change. However, the mainstream work still lacks a comprehensive review of the research direction. In this paper, starting from the concept of reinforcement learning, we subsequently introduce multi-agent systems (MAS), cooperative MARL, typical methods, and test environments. Then, we summarize the research work of cooperative MARL from closed to open environments, extract multiple research directions, and introduce typical works. Finally, we summarize the strengths and weaknesses of the current research, and look forward to the future development direction and research problems in cooperative MARL in open environments.
<div id='section'>Paperid: <span id='pid'>346, <a href='https://arxiv.org/pdf/2311.11385.pdf' target='_blank'>https://arxiv.org/pdf/2311.11385.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ahmed Hendawy, Jan Peters, Carlo D'Eramo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.11385">Multi-Task Reinforcement Learning with Mixture of Orthogonal Experts</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-Task Reinforcement Learning (MTRL) tackles the long-standing problem of endowing agents with skills that generalize across a variety of problems. To this end, sharing representations plays a fundamental role in capturing both unique and common characteristics of the tasks. Tasks may exhibit similarities in terms of skills, objects, or physical properties while leveraging their representations eases the achievement of a universal policy. Nevertheless, the pursuit of learning a shared set of diverse representations is still an open challenge. In this paper, we introduce a novel approach for representation learning in MTRL that encapsulates common structures among the tasks using orthogonal representations to promote diversity. Our method, named Mixture Of Orthogonal Experts (MOORE), leverages a Gram-Schmidt process to shape a shared subspace of representations generated by a mixture of experts. When task-specific information is provided, MOORE generates relevant representations from this shared subspace. We assess the effectiveness of our approach on two MTRL benchmarks, namely MiniGrid and MetaWorld, showing that MOORE surpasses related baselines and establishes a new state-of-the-art result on MetaWorld.
<div id='section'>Paperid: <span id='pid'>347, <a href='https://arxiv.org/pdf/2305.13937.pdf' target='_blank'>https://arxiv.org/pdf/2305.13937.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lei Yuan, Lihe Li, Ziqian Zhang, Fuxiang Zhang, Cong Guan, Yang Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.13937">Multi-agent Continual Coordination via Progressive Task Contextualization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cooperative Multi-agent Reinforcement Learning (MARL) has attracted significant attention and played the potential for many real-world applications. Previous arts mainly focus on facilitating the coordination ability from different aspects (e.g., non-stationarity, credit assignment) in single-task or multi-task scenarios, ignoring the stream of tasks that appear in a continual manner. This ignorance makes the continual coordination an unexplored territory, neither in problem formulation nor efficient algorithms designed. Towards tackling the mentioned issue, this paper proposes an approach Multi-Agent Continual Coordination via Progressive Task Contextualization, dubbed MACPro. The key point lies in obtaining a factorized policy, using shared feature extraction layers but separated independent task heads, each specializing in a specific class of tasks. The task heads can be progressively expanded based on the learned task contextualization. Moreover, to cater to the popular CTDE paradigm in MARL, each agent learns to predict and adopt the most relevant policy head based on local information in a decentralized manner. We show in multiple multi-agent benchmarks that existing continual learning methods fail, while MACPro is able to achieve close-to-optimal performance. More results also disclose the effectiveness of MACPro from multiple aspects like high generalization ability.
<div id='section'>Paperid: <span id='pid'>348, <a href='https://arxiv.org/pdf/2305.05116.pdf' target='_blank'>https://arxiv.org/pdf/2305.05116.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lei Yuan, Feng Chen, Zhongzhang Zhang, Yang Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.05116">Communication-Robust Multi-Agent Learning by Adaptable Auxiliary Multi-Agent Adversary Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Communication can promote coordination in cooperative Multi-Agent Reinforcement Learning (MARL). Nowadays, existing works mainly focus on improving the communication efficiency of agents, neglecting that real-world communication is much more challenging as there may exist noise or potential attackers. Thus the robustness of the communication-based policies becomes an emergent and severe issue that needs more exploration. In this paper, we posit that the ego system trained with auxiliary adversaries may handle this limitation and propose an adaptable method of Multi-Agent Auxiliary Adversaries Generation for robust Communication, dubbed MA3C, to obtain a robust communication-based policy. In specific, we introduce a novel message-attacking approach that models the learning of the auxiliary attacker as a cooperative problem under a shared goal to minimize the coordination ability of the ego system, with which every information channel may suffer from distinct message attacks. Furthermore, as naive adversarial training may impede the generalization ability of the ego system, we design an attacker population generation approach based on evolutionary learning. Finally, the ego system is paired with an attacker population and then alternatively trained against the continuously evolving attackers to improve its robustness, meaning that both the ego system and the attackers are adaptable. Extensive experiments on multiple benchmarks indicate that our proposed MA3C provides comparable or better robustness and generalization ability than other baselines.
<div id='section'>Paperid: <span id='pid'>349, <a href='https://arxiv.org/pdf/2303.12319.pdf' target='_blank'>https://arxiv.org/pdf/2303.12319.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Guangzheng Hu, Haoran Li, Shasha Liu, Mingjun Ma, Yuanheng Zhu, Dongbin Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.12319">NeuronsMAE: A Novel Multi-Agent Reinforcement Learning Environment for Cooperative and Competitive Multi-Robot Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent reinforcement learning (MARL) has achieved remarkable success in various challenging problems. Meanwhile, more and more benchmarks have emerged and provided some standards to evaluate the algorithms in different fields. On the one hand, the virtual MARL environments lack knowledge of real-world tasks and actuator abilities, and on the other hand, the current task-specified multi-robot platform has poor support for the generality of multi-agent reinforcement learning algorithms and lacks support for transferring from simulation to the real environment. Bridging the gap between the virtual MARL environments and the real multi-robot platform becomes the key to promoting the practicability of MARL algorithms. This paper proposes a novel MARL environment for real multi-robot tasks named NeuronsMAE (Neurons Multi-Agent Environment). This environment supports cooperative and competitive multi-robot tasks and is configured with rich parameter interfaces to study the multi-agent policy transfer from simulation to reality. With this platform, we evaluate various popular MARL algorithms and build a new MARL benchmark for multi-robot tasks. We hope that this platform will facilitate the research and application of MARL algorithms for real robot tasks. Information about the benchmark and the open-source code will be released.
<div id='section'>Paperid: <span id='pid'>350, <a href='https://arxiv.org/pdf/2303.03767.pdf' target='_blank'>https://arxiv.org/pdf/2303.03767.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hai Ci, Mickel Liu, Xuehai Pan, Fangwei Zhong, Yizhou Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.03767">Proactive Multi-Camera Collaboration For 3D Human Pose Estimation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a multi-agent reinforcement learning (MARL) scheme for proactive Multi-Camera Collaboration in 3D Human Pose Estimation in dynamic human crowds. Traditional fixed-viewpoint multi-camera solutions for human motion capture (MoCap) are limited in capture space and susceptible to dynamic occlusions. Active camera approaches proactively control camera poses to find optimal viewpoints for 3D reconstruction. However, current methods still face challenges with credit assignment and environment dynamics. To address these issues, our proposed method introduces a novel Collaborative Triangulation Contribution Reward (CTCR) that improves convergence and alleviates multi-agent credit assignment issues resulting from using 3D reconstruction accuracy as the shared reward. Additionally, we jointly train our model with multiple world dynamics learning tasks to better capture environment dynamics and encourage anticipatory behaviors for occlusion avoidance. We evaluate our proposed method in four photo-realistic UE4 environments to ensure validity and generalizability. Empirical results show that our method outperforms fixed and active baselines in various scenarios with different numbers of cameras and humans.
<div id='section'>Paperid: <span id='pid'>351, <a href='https://arxiv.org/pdf/2302.09605.pdf' target='_blank'>https://arxiv.org/pdf/2302.09605.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Cong Guan, Feng Chen, Lei Yuan, Zongzhang Zhang, Yang Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.09605">Efficient Communication via Self-supervised Information Aggregation for Online and Offline Multi-agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Utilizing messages from teammates can improve coordination in cooperative Multi-agent Reinforcement Learning (MARL). Previous works typically combine raw messages of teammates with local information as inputs for policy. However, neglecting message aggregation poses significant inefficiency for policy learning. Motivated by recent advances in representation learning, we argue that efficient message aggregation is essential for good coordination in cooperative MARL. In this paper, we propose Multi-Agent communication via Self-supervised Information Aggregation (MASIA), where agents can aggregate the received messages into compact representations with high relevance to augment the local policy. Specifically, we design a permutation invariant message encoder to generate common information-aggregated representation from messages and optimize it via reconstructing and shooting future information in a self-supervised manner. Hence, each agent would utilize the most relevant parts of the aggregated representation for decision-making by a novel message extraction mechanism. Furthermore, considering the potential of offline learning for real-world applications, we build offline benchmarks for multi-agent communication, which is the first as we know. Empirical results demonstrate the superiority of our method in both online and offline settings. We also release the built offline benchmarks in this paper as a testbed for communication ability validation to facilitate further future research.
<div id='section'>Paperid: <span id='pid'>352, <a href='https://arxiv.org/pdf/2302.03429.pdf' target='_blank'>https://arxiv.org/pdf/2302.03429.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rundong Wang, Longtao Zheng, Wei Qiu, Bowei He, Bo An, Zinovi Rabinovich, Yujing Hu, Yingfeng Chen, Tangjie Lv, Changjie Fan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.03429">Towards Skilled Population Curriculum for Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in multi-agent reinforcement learning (MARL) allow agents to coordinate their behaviors in complex environments. However, common MARL algorithms still suffer from scalability and sparse reward issues. One promising approach to resolving them is automatic curriculum learning (ACL). ACL involves a student (curriculum learner) training on tasks of increasing difficulty controlled by a teacher (curriculum generator). Despite its success, ACL's applicability is limited by (1) the lack of a general student framework for dealing with the varying number of agents across tasks and the sparse reward problem, and (2) the non-stationarity of the teacher's task due to ever-changing student strategies. As a remedy for ACL, we introduce a novel automatic curriculum learning framework, Skilled Population Curriculum (SPC), which adapts curriculum learning to multi-agent coordination. Specifically, we endow the student with population-invariant communication and a hierarchical skill set, allowing it to learn cooperation and behavior skills from distinct tasks with varying numbers of agents. In addition, we model the teacher as a contextual bandit conditioned by student policies, enabling a team of agents to change its size while still retaining previously acquired skills. We also analyze the inherent non-stationarity of this multi-agent automatic curriculum teaching problem and provide a corresponding regret bound. Empirical results show that our method improves the performance, scalability and sample efficiency in several MARL environments.
<div id='section'>Paperid: <span id='pid'>353, <a href='https://arxiv.org/pdf/2302.00571.pdf' target='_blank'>https://arxiv.org/pdf/2302.00571.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yihong Li, Xiaoxi Zhang, Tianyu Zeng, Jingpu Duan, Chuan Wu, Di Wu, Xu Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.00571">Task Placement and Resource Allocation for Edge Machine Learning: A GNN-based Multi-Agent Reinforcement Learning Paradigm</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Machine learning (ML) tasks are one of the major workloads in today's edge computing networks. Existing edge-cloud schedulers allocate the requested amounts of resources to each task, falling short of best utilizing the limited edge resources for ML tasks. This paper proposes TapFinger, a distributed scheduler for edge clusters that minimizes the total completion time of ML tasks through co-optimizing task placement and fine-grained multi-resource allocation. To learn the tasks' uncertain resource sensitivity and enable distributed scheduling, we adopt multi-agent reinforcement learning (MARL) and propose several techniques to make it efficient, including a heterogeneous graph attention network as the MARL backbone, a tailored task selection phase in the actor network, and the integration of Bayes' theorem and masking schemes. We first implement a single-task scheduling version, which schedules at most one task each time. Then we generalize to the multi-task scheduling case, in which a sequence of tasks is scheduled simultaneously. Our design can mitigate the expanded decision space and yield fast convergence to optimal scheduling solutions. Extensive experiments using synthetic and test-bed ML task traces show that TapFinger can achieve up to 54.9% reduction in the average task completion time and improve resource efficiency as compared to state-of-the-art schedulers.
<div id='section'>Paperid: <span id='pid'>354, <a href='https://arxiv.org/pdf/2302.00571.pdf' target='_blank'>https://arxiv.org/pdf/2302.00571.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yihong Li, Xiaoxi Zhang, Tianyu Zeng, Jingpu Duan, Chuan Wu, Di Wu, Xu Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.00571">Task Placement and Resource Allocation for Edge Machine Learning: A GNN-based Multi-Agent Reinforcement Learning Paradigm</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Machine learning (ML) tasks are one of the major workloads in today's edge computing networks. Existing edge-cloud schedulers allocate the requested amounts of resources to each task, falling short of best utilizing the limited edge resources for ML tasks. This paper proposes TapFinger, a distributed scheduler for edge clusters that minimizes the total completion time of ML tasks through co-optimizing task placement and fine-grained multi-resource allocation. To learn the tasks' uncertain resource sensitivity and enable distributed scheduling, we adopt multi-agent reinforcement learning (MARL) and propose several techniques to make it efficient, including a heterogeneous graph attention network as the MARL backbone, a tailored task selection phase in the actor network, and the integration of Bayes' theorem and masking schemes. We first implement a single-task scheduling version, which schedules at most one task each time. Then we generalize to the multi-task scheduling case, in which a sequence of tasks is scheduled simultaneously. Our design can mitigate the expanded decision space and yield fast convergence to optimal scheduling solutions. Extensive experiments using synthetic and test-bed ML task traces show that TapFinger can achieve up to 54.9% reduction in the average task completion time and improve resource efficiency as compared to state-of-the-art schedulers.
<div id='section'>Paperid: <span id='pid'>355, <a href='https://arxiv.org/pdf/2301.06889.pdf' target='_blank'>https://arxiv.org/pdf/2301.06889.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Washim Uddin Mondal, Vaneet Aggarwal, Satish V. Ukkusuri
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.06889">Mean-Field Control based Approximation of Multi-Agent Reinforcement Learning in Presence of a Non-decomposable Shared Global State</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Mean Field Control (MFC) is a powerful approximation tool to solve large-scale Multi-Agent Reinforcement Learning (MARL) problems. However, the success of MFC relies on the presumption that given the local states and actions of all the agents, the next (local) states of the agents evolve conditionally independent of each other. Here we demonstrate that even in a MARL setting where agents share a common global state in addition to their local states evolving conditionally independently (thus introducing a correlation between the state transition processes of individual agents), the MFC can still be applied as a good approximation tool. The global state is assumed to be non-decomposable i.e., it cannot be expressed as a collection of local states of the agents. We compute the approximation error as $\mathcal{O}(e)$ where $e=\frac{1}{\sqrt{N}}\left[\sqrt{|\mathcal{X}|} +\sqrt{|\mathcal{U}|}\right]$. The size of the agent population is denoted by the term $N$, and $|\mathcal{X}|, |\mathcal{U}|$ respectively indicate the sizes of (local) state and action spaces of individual agents. The approximation error is found to be independent of the size of the shared global state space. We further demonstrate that in a special case if the reward and state transition functions are independent of the action distribution of the population, then the error can be improved to $e=\frac{\sqrt{|\mathcal{X}|}}{\sqrt{N}}$. Finally, we devise a Natural Policy Gradient based algorithm that solves the MFC problem with $\mathcal{O}(Îµ^{-3})$ sample complexity and obtains a policy that is within $\mathcal{O}(\max\{e,Îµ\})$ error of the optimal MARL policy for any $Îµ>0$.
<div id='section'>Paperid: <span id='pid'>356, <a href='https://arxiv.org/pdf/2301.02083.pdf' target='_blank'>https://arxiv.org/pdf/2301.02083.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shaowei Zhang, Jiahan Cao, Lei Yuan, Yang Yu, De-Chuan Zhan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.02083">Self-Motivated Multi-Agent Exploration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In cooperative multi-agent reinforcement learning (CMARL), it is critical for agents to achieve a balance between self-exploration and team collaboration. However, agents can hardly accomplish the team task without coordination and they would be trapped in a local optimum where easy cooperation is accessed without enough individual exploration. Recent works mainly concentrate on agents' coordinated exploration, which brings about the exponentially grown exploration of the state space. To address this issue, we propose Self-Motivated Multi-Agent Exploration (SMMAE), which aims to achieve success in team tasks by adaptively finding a trade-off between self-exploration and team cooperation. In SMMAE, we train an independent exploration policy for each agent to maximize their own visited state space. Each agent learns an adjustable exploration probability based on the stability of the joint team policy. The experiments on highly cooperative tasks in StarCraft II micromanagement benchmark (SMAC) demonstrate that SMMAE can explore task-related states more efficiently, accomplish coordinated behaviours and boost the learning performance.
<div id='section'>Paperid: <span id='pid'>357, <a href='https://arxiv.org/pdf/2301.00944.pdf' target='_blank'>https://arxiv.org/pdf/2301.00944.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aritra Mitra, George J. Pappas, Hamed Hassani
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.00944">Temporal Difference Learning with Compressed Updates: Error-Feedback meets Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In large-scale distributed machine learning, recent works have studied the effects of compressing gradients in stochastic optimization to alleviate the communication bottleneck. These works have collectively revealed that stochastic gradient descent (SGD) is robust to structured perturbations such as quantization, sparsification, and delays. Perhaps surprisingly, despite the surge of interest in multi-agent reinforcement learning, almost nothing is known about the analogous question: Are common reinforcement learning (RL) algorithms also robust to similar perturbations? We investigate this question by studying a variant of the classical temporal difference (TD) learning algorithm with a perturbed update direction, where a general compression operator is used to model the perturbation. Our work makes three important technical contributions. First, we prove that compressed TD algorithms, coupled with an error-feedback mechanism used widely in optimization, exhibit the same non-asymptotic theoretical guarantees as their SGD counterparts. Second, we show that our analysis framework extends seamlessly to nonlinear stochastic approximation schemes that subsume Q-learning. Third, we prove that for multi-agent TD learning, one can achieve linear convergence speedups with respect to the number of agents while communicating just $\tilde{O}(1)$ bits per iteration. Notably, these are the first finite-time results in RL that account for general compression operators and error-feedback in tandem with linear function approximation and Markovian sampling. Our proofs hinge on the construction of novel Lyapunov functions that capture the dynamics of a memory variable introduced by error-feedback.
<div id='section'>Paperid: <span id='pid'>358, <a href='https://arxiv.org/pdf/2210.03269.pdf' target='_blank'>https://arxiv.org/pdf/2210.03269.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiayu Chen, Marina Haliem, Tian Lan, Vaneet Aggarwal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2210.03269">Multi-agent Deep Covering Skill Discovery</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The use of skills (a.k.a., options) can greatly accelerate exploration in reinforcement learning, especially when only sparse reward signals are available. While option discovery methods have been proposed for individual agents, in multi-agent reinforcement learning settings, discovering collaborative options that can coordinate the behavior of multiple agents and encourage them to visit the under-explored regions of their joint state space has not been considered. In this case, we propose Multi-agent Deep Covering Option Discovery, which constructs the multi-agent options through minimizing the expected cover time of the multiple agents' joint state space. Also, we propose a novel framework to adopt the multi-agent options in the MARL process. In practice, a multi-agent task can usually be divided into some sub-tasks, each of which can be completed by a sub-group of the agents. Therefore, our algorithm framework first leverages an attention mechanism to find collaborative agent sub-groups that would benefit most from coordinated actions. Then, a hierarchical algorithm, namely HA-MSAC, is developed to learn the multi-agent options for each sub-group to complete their sub-tasks first, and then to integrate them through a high-level policy as the solution of the whole task. This hierarchical option construction allows our framework to strike a balance between scalability and effective collaboration among the agents. The evaluation based on multi-agent collaborative tasks shows that the proposed algorithm can effectively capture the agent interactions with the attention mechanism, successfully identify multi-agent options, and significantly outperforms prior works using single-agent options or no options, in terms of both faster exploration and higher task rewards.
<div id='section'>Paperid: <span id='pid'>359, <a href='https://arxiv.org/pdf/2209.07437.pdf' target='_blank'>https://arxiv.org/pdf/2209.07437.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Washim Uddin Mondal, Vaneet Aggarwal, Satish V. Ukkusuri
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2209.07437">Mean-Field Approximation of Cooperative Constrained Multi-Agent Reinforcement Learning (CMARL)</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Mean-Field Control (MFC) has recently been proven to be a scalable tool to approximately solve large-scale multi-agent reinforcement learning (MARL) problems. However, these studies are typically limited to unconstrained cumulative reward maximization framework. In this paper, we show that one can use the MFC approach to approximate the MARL problem even in the presence of constraints. Specifically, we prove that, an $N$-agent constrained MARL problem, with state, and action spaces of each individual agents being of sizes $|\mathcal{X}|$, and $|\mathcal{U}|$ respectively, can be approximated by an associated constrained MFC problem with an error, $e\triangleq \mathcal{O}\left([\sqrt{|\mathcal{X}|}+\sqrt{|\mathcal{U}|}]/\sqrt{N}\right)$. In a special case where the reward, cost, and state transition functions are independent of the action distribution of the population, we prove that the error can be improved to $e=\mathcal{O}(\sqrt{|\mathcal{X}|}/\sqrt{N})$. Also, we provide a Natural Policy Gradient based algorithm and prove that it can solve the constrained MARL problem within an error of $\mathcal{O}(e)$ with a sample complexity of $\mathcal{O}(e^{-6})$.
<div id='section'>Paperid: <span id='pid'>360, <a href='https://arxiv.org/pdf/2206.11420.pdf' target='_blank'>https://arxiv.org/pdf/2206.11420.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hanhan Zhou, Tian Lan, Vaneet Aggarwal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2206.11420">PAC: Assisted Value Factorisation with Counterfactual Predictions in Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent reinforcement learning (MARL) has witnessed significant progress with the development of value function factorization methods. It allows optimizing a joint action-value function through the maximization of factorized per-agent utilities due to monotonicity. In this paper, we show that in partially observable MARL problems, an agent's ordering over its own actions could impose concurrent constraints (across different states) on the representable function class, causing significant estimation error during training. We tackle this limitation and propose PAC, a new framework leveraging Assistive information generated from Counterfactual Predictions of optimal joint action selection, which enable explicit assistance to value function factorization through a novel counterfactual loss. A variational inference-based information encoding method is developed to collect and encode the counterfactual predictions from an estimated baseline. To enable decentralized execution, we also derive factorized per-agent policies inspired by a maximum-entropy MARL framework. We evaluate the proposed PAC on multi-agent predator-prey and a set of StarCraft II micromanagement tasks. Empirical results demonstrate improved results of PAC over state-of-the-art value-based and policy-based multi-agent reinforcement learning algorithms on all benchmarks.
<div id='section'>Paperid: <span id='pid'>361, <a href='https://arxiv.org/pdf/2201.01247.pdf' target='_blank'>https://arxiv.org/pdf/2201.01247.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hanhan Zhou, Tian Lan, Vaneet Aggarwal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2201.01247">Value Functions Factorization with Latent State Information Sharing in Decentralized Multi-Agent Policy Gradients</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Value function factorization via centralized training and decentralized execution is promising for solving cooperative multi-agent reinforcement tasks. One of the approaches in this area, QMIX, has become state-of-the-art and achieved the best performance on the StarCraft II micromanagement benchmark. However, the monotonic-mixing of per agent estimates in QMIX is known to restrict the joint action Q-values it can represent, as well as the insufficient global state information for single agent value function estimation, often resulting in suboptimality. To this end, we present LSF-SAC, a novel framework that features a variational inference-based information-sharing mechanism as extra state information to assist individual agents in the value function factorization. We demonstrate that such latent individual state information sharing can significantly expand the power of value function factorization, while fully decentralized execution can still be maintained in LSF-SAC through a soft-actor-critic design. We evaluate LSF-SAC on the StarCraft II micromanagement challenge and demonstrate that it outperforms several state-of-the-art methods in challenging collaborative tasks. We further set extensive ablation studies for locating the key factors accounting for its performance improvements. We believe that this new insight can lead to new local value estimation methods and variational deep learning algorithms. A demo video and code of implementation can be found at https://sites.google.com/view/sacmm.
<div id='section'>Paperid: <span id='pid'>362, <a href='https://arxiv.org/pdf/2502.03845.pdf' target='_blank'>https://arxiv.org/pdf/2502.03845.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhuohui Zhang, Bin Cheng, Zhipeng Wang, Yanmin Zhou, Gang Li, Ping Lu, Bin He, Jie Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.03845">PAGNet: Pluggable Adaptive Generative Networks for Information Completion in Multi-Agent Communication</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>For partially observable cooperative tasks, multi-agent systems must develop effective communication and understand the interplay among agents in order to achieve cooperative goals. However, existing multi-agent reinforcement learning (MARL) with communication methods lack evaluation metrics for information weights and information-level communication modeling. This causes agents to neglect the aggregation of multiple messages, thereby significantly reducing policy learning efficiency. In this paper, we propose pluggable adaptive generative networks (PAGNet), a novel framework that integrates generative models into MARL to enhance communication and decision-making. PAGNet enables agents to synthesize global states representations from weighted local observations and use these representations alongside learned communication weights for coordinated decision-making. This pluggable approach reduces the computational demands typically associated with the joint training of communication and policy networks. Extensive experimental evaluations across diverse benchmarks and communication scenarios demonstrate the significant performance improvements achieved by PAGNet. Furthermore, we analyze the emergent communication patterns and the quality of generated global states, providing insights into operational mechanisms.
<div id='section'>Paperid: <span id='pid'>363, <a href='https://arxiv.org/pdf/2501.14488.pdf' target='_blank'>https://arxiv.org/pdf/2501.14488.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuhan Hu, Yirong Sun, Yanjun Chen, Xinghao Chen, Xiaoyu Shen, Wei Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.14488">Breaking the Pre-Planning Barrier: Adaptive Real-Time Coordination of Heterogeneous UAVs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Unmanned Aerial Vehicles (UAVs) offer significant potential in dynamic, perception-intensive tasks such as search and rescue and environmental monitoring; however, their effectiveness is severely restricted by conventional pre-planned routing methods, which lack the flexibility to respond in real-time to evolving task demands, unexpected disturbances, and localized view limitations in real-world scenarios. To address this fundamental limitation, we introduce a novel multi-agent reinforcement learning framework named \textbf{H}eterogeneous \textbf{G}raph \textbf{A}ttention \textbf{M}ulti-agent Deep Deterministic Policy Gradient (HGAM), uniquely designed to enable adaptive real-time coordination between mission UAVs (MUAVs) and charging UAVs (CUAVs). HGAM specifically addresses the previously unsolved challenge of enabling precise, decentralized continuous-action coordination solely based on local, heterogeneous graph-based observations. Extensive simulations demonstrate that HGAM substantially surpasses existing methods, achieving, for example, a 30\% improvement in data collection coverage and a 20\% increase in charging efficiency, providing crucial insights and foundations for the future deployment of intelligent, flexible UAV networks in complex, dynamic environments.
<div id='section'>Paperid: <span id='pid'>364, <a href='https://arxiv.org/pdf/2412.08845.pdf' target='_blank'>https://arxiv.org/pdf/2412.08845.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kuan-Cheng Chen, Samuel Yen-Chi Chen, Chen-Yu Liu, Kin K. Leung
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.08845">Quantum-Train-Based Distributed Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we introduce Quantum-Train-Based Distributed Multi-Agent Reinforcement Learning (Dist-QTRL), a novel approach to addressing the scalability challenges of traditional Reinforcement Learning (RL) by integrating quantum computing principles. Quantum-Train Reinforcement Learning (QTRL) leverages parameterized quantum circuits to efficiently generate neural network parameters, achieving a \(poly(\log(N))\) reduction in the dimensionality of trainable parameters while harnessing quantum entanglement for superior data representation. The framework is designed for distributed multi-agent environments, where multiple agents, modeled as Quantum Processing Units (QPUs), operate in parallel, enabling faster convergence and enhanced scalability. Additionally, the Dist-QTRL framework can be extended to high-performance computing (HPC) environments by utilizing distributed quantum training for parameter reduction in classical neural networks, followed by inference using classical CPUs or GPUs. This hybrid quantum-HPC approach allows for further optimization in real-world applications. In this paper, we provide a mathematical formulation of the Dist-QTRL framework and explore its convergence properties, supported by empirical results demonstrating performance improvements over centric QTRL models. The results highlight the potential of quantum-enhanced RL in tackling complex, high-dimensional tasks, particularly in distributed computing settings, where our framework achieves significant speedups through parallelization without compromising model accuracy. This work paves the way for scalable, quantum-enhanced RL systems in practical applications, leveraging both quantum and classical computational resources.
<div id='section'>Paperid: <span id='pid'>365, <a href='https://arxiv.org/pdf/2406.16994.pdf' target='_blank'>https://arxiv.org/pdf/2406.16994.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gyu Seon Kim, Yeryeong Cho, Jaehyun Chung, Soohyun Park, Soyi Jung, Zhu Han, Joongheon Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.16994">Quantum Multi-Agent Reinforcement Learning for Cooperative Mobile Access in Space-Air-Ground Integrated Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Achieving global space-air-ground integrated network (SAGIN) access only with CubeSats presents significant challenges such as the access sustainability limitations in specific regions (e.g., polar regions) and the energy efficiency limitations in CubeSats. To tackle these problems, high-altitude long-endurance unmanned aerial vehicles (HALE-UAVs) can complement these CubeSat shortcomings for providing cooperatively global access sustainability and energy efficiency. However, as the number of CubeSats and HALE-UAVs, increases, the scheduling dimension of each ground station (GS) increases. As a result, each GS can fall into the curse of dimensionality, and this challenge becomes one major hurdle for efficient global access. Therefore, this paper provides a quantum multi-agent reinforcement Learning (QMARL)-based method for scheduling between GSs and CubeSats/HALE-UAVs in order to improve global access availability and energy efficiency. The main reason why the QMARL-based scheduler can be beneficial is that the algorithm facilitates a logarithmic-scale reduction in scheduling action dimensions, which is one critical feature as the number of CubeSats and HALE-UAVs expands. Additionally, individual GSs have different traffic demands depending on their locations and characteristics, thus it is essential to provide differentiated access services. The superiority of the proposed scheduler is validated through data-intensive experiments in realistic CubeSat/HALE-UAV settings.
<div id='section'>Paperid: <span id='pid'>366, <a href='https://arxiv.org/pdf/2402.03204.pdf' target='_blank'>https://arxiv.org/pdf/2402.03204.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianzhang Cai, Qichen Wang, Shuai Zhang, Ãzlem TuÄfe Demir, Cicek Cavdar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.03204">Multi-agent Reinforcement Learning for Energy Saving in Multi-Cell Massive MIMO Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We develop a multi-agent reinforcement learning (MARL) algorithm to minimize the total energy consumption of multiple massive MIMO (multiple-input multiple-output) base stations (BSs) in a multi-cell network while preserving the overall quality-of-service (QoS) by making decisions on the multi-level advanced sleep modes (ASMs) and antenna switching of these BSs. The problem is modeled as a decentralized partially observable Markov decision process (DEC-POMDP) to enable collaboration between individual BSs, which is necessary to tackle inter-cell interference. A multi-agent proximal policy optimization (MAPPO) algorithm is designed to learn a collaborative BS control policy. To enhance its scalability, a modified version called MAPPO-neighbor policy is further proposed. Simulation results demonstrate that the trained MAPPO agent achieves better performance compared to baseline policies. Specifically, compared to the auto sleep mode 1 (symbol-level sleeping) algorithm, the MAPPO-neighbor policy reduces power consumption by approximately 8.7% during low-traffic hours and improves energy efficiency by approximately 19% during high-traffic hours, respectively.
<div id='section'>Paperid: <span id='pid'>367, <a href='https://arxiv.org/pdf/2308.01519.pdf' target='_blank'>https://arxiv.org/pdf/2308.01519.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Soohyun Park, Jae Pyoung Kim, Chanyoung Park, Soyi Jung, Joongheon Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.01519">Quantum Multi-Agent Reinforcement Learning for Autonomous Mobility Cooperation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>For Industry 4.0 Revolution, cooperative autonomous mobility systems are widely used based on multi-agent reinforcement learning (MARL). However, the MARL-based algorithms suffer from huge parameter utilization and convergence difficulties with many agents. To tackle these problems, a quantum MARL (QMARL) algorithm based on the concept of actor-critic network is proposed, which is beneficial in terms of scalability, to deal with the limitations in the noisy intermediate-scale quantum (NISQ) era. Additionally, our QMARL is also beneficial in terms of efficient parameter utilization and fast convergence due to quantum supremacy. Note that the reward in our QMARL is defined as task precision over computation time in multiple agents, thus, multi-agent cooperation can be realized. For further improvement, an additional technique for scalability is proposed, which is called projection value measure (PVM). Based on PVM, our proposed QMARL can achieve the highest reward, by reducing the action dimension into a logarithmic-scale. Finally, we can conclude that our proposed QMARL with PVM outperforms the other algorithms in terms of efficient parameter utilization, fast convergence, and scalability.
<div id='section'>Paperid: <span id='pid'>368, <a href='https://arxiv.org/pdf/2307.09711.pdf' target='_blank'>https://arxiv.org/pdf/2307.09711.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Soohyun Park, Haemin Lee, Chanyoung Park, Soyi Jung, Minseok Choi, Joongheon Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.09711">Two Tales of Platoon Intelligence for Autonomous Mobility Control: Enabling Deep Learning Recipes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents the deep learning-based recent achievements to resolve the problem of autonomous mobility control and efficient resource management of autonomous vehicles and UAVs, i.e., (i) multi-agent reinforcement learning (MARL), and (ii) neural Myerson auction. Representatively, communication network (CommNet), which is one of the most popular MARL algorithms, is introduced to enable multiple agents to take actions in a distributed manner for their shared goals by training all agents' states and actions in a single neural network. Moreover, the neural Myerson auction guarantees trustfulness among multiple agents as well as achieves the optimal revenue of highly dynamic systems. Therefore, we survey the recent studies on autonomous mobility control based on MARL and neural Myerson auction. Furthermore, we emphasize that integration of MARL and neural Myerson auction is expected to be critical for efficient and trustful autonomous mobility services.
<div id='section'>Paperid: <span id='pid'>369, <a href='https://arxiv.org/pdf/2307.09652.pdf' target='_blank'>https://arxiv.org/pdf/2307.09652.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jeremy McMahan, Young Wu, Yudong Chen, Xiaojin Zhu, Qiaomin Xie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.09652">VISER: A Tractable Solution Concept for Games with Information Asymmetry</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Many real-world games suffer from information asymmetry: one player is only aware of their own payoffs while the other player has the full game information. Examples include the critical domain of security games and adversarial multi-agent reinforcement learning. Information asymmetry renders traditional solution concepts such as Strong Stackelberg Equilibrium (SSE) and Robust-Optimization Equilibrium (ROE) inoperative. We propose a novel solution concept called VISER (Victim Is Secure, Exploiter best-Responds). VISER enables an external observer to predict the outcome of such games. In particular, for security applications, VISER allows the victim to better defend itself while characterizing the most damaging attacks available to the attacker. We show that each player's VISER strategy can be computed independently in polynomial time using linear programming (LP). We also extend VISER to its Markov-perfect counterpart for Markov games, which can be solved efficiently using a series of LPs.
<div id='section'>Paperid: <span id='pid'>370, <a href='https://arxiv.org/pdf/2306.04137.pdf' target='_blank'>https://arxiv.org/pdf/2306.04137.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chanyoung Park, Gyu Seon Kim, Soohyun Park, Soyi Jung, Joongheon Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.04137">Multi-Agent Reinforcement Learning for Cooperative Air Transportation Services in City-Wide Autonomous Urban Air Mobility</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The development of urban-air-mobility (UAM) is rapidly progressing with spurs, and the demand for efficient transportation management systems is a rising need due to the multifaceted environmental uncertainties. Thus, this paper proposes a novel air transportation service management algorithm based on multi-agent deep reinforcement learning (MADRL) to address the challenges of multi-UAM cooperation. Specifically, the proposed algorithm in this paper is based on communication network (CommNet) method utilizing centralized training and distributed execution (CTDE) in multiple UAMs for providing efficient air transportation services to passengers collaboratively. Furthermore, this paper adopts actual vertiport maps and UAM specifications for constructing realistic air transportation networks. By evaluating the performance of the proposed algorithm in data-intensive simulations, the results show that the proposed algorithm outperforms existing approaches in terms of air transportation service quality. Furthermore, there are no inferior UAMs by utilizing parameter sharing in CommNet and a centralized critic network in CTDE. Therefore, it can be confirmed that the research results in this paper can provide a promising solution for autonomous air transportation management systems in city-wide urban areas.
<div id='section'>Paperid: <span id='pid'>371, <a href='https://arxiv.org/pdf/2306.01839.pdf' target='_blank'>https://arxiv.org/pdf/2306.01839.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lingfeng Sun, Haichao Zhang, Wei Xu, Masayoshi Tomizuka
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.01839">Efficient Multi-Task and Transfer Reinforcement Learning with Parameter-Compositional Framework</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this work, we investigate the potential of improving multi-task training and also leveraging it for transferring in the reinforcement learning setting. We identify several challenges towards this goal and propose a transferring approach with a parameter-compositional formulation. We investigate ways to improve the training of multi-task reinforcement learning which serves as the foundation for transferring. Then we conduct a number of transferring experiments on various manipulation tasks. Experimental results demonstrate that the proposed approach can have improved performance in the multi-task training stage, and further show effective transferring in terms of both sample efficiency and performance.
<div id='section'>Paperid: <span id='pid'>372, <a href='https://arxiv.org/pdf/2304.05632.pdf' target='_blank'>https://arxiv.org/pdf/2304.05632.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haozhi Wang, Yinchuan Li, Qing Wang, Yunfeng Shao, Jianye Hao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.05632">Multi-agent Policy Reciprocity with Theoretical Guarantee</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Modern multi-agent reinforcement learning (RL) algorithms hold great potential for solving a variety of real-world problems. However, they do not fully exploit cross-agent knowledge to reduce sample complexity and improve performance. Although transfer RL supports knowledge sharing, it is hyperparameter sensitive and complex. To solve this problem, we propose a novel multi-agent policy reciprocity (PR) framework, where each agent can fully exploit cross-agent policies even in mismatched states. We then define an adjacency space for mismatched states and design a plug-and-play module for value iteration, which enables agents to infer more precise returns. To improve the scalability of PR, deep PR is proposed for continuous control tasks. Moreover, theoretical analysis shows that agents can asymptotically reach consensus through individual perceived rewards and converge to an optimal value function, which implies the stability and effectiveness of PR, respectively. Experimental results on discrete and continuous environments demonstrate that PR outperforms various existing RL and transfer RL methods.
<div id='section'>Paperid: <span id='pid'>373, <a href='https://arxiv.org/pdf/2303.11959.pdf' target='_blank'>https://arxiv.org/pdf/2303.11959.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hengxi Zhang, Zhendong Shi, Yuanquan Hu, Wenbo Ding, Ercan E. Kuruoglu, Xiao-Ping Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.11959">Optimizing Trading Strategies in Quantitative Markets using Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Quantitative markets are characterized by swift dynamics and abundant uncertainties, making the pursuit of profit-driven stock trading actions inherently challenging. Within this context, reinforcement learning (RL), which operates on a reward-centric mechanism for optimal control, has surfaced as a potentially effective solution to the intricate financial decision-making conundrums presented. This paper delves into the fusion of two established financial trading strategies, namely the constant proportion portfolio insurance (CPPI) and the time-invariant portfolio protection (TIPP), with the multi-agent deep deterministic policy gradient (MADDPG) framework. As a result, we introduce two novel multi-agent RL (MARL) methods, CPPI-MADDPG and TIPP-MADDPG, tailored for probing strategic trading within quantitative markets. To validate these innovations, we implemented them on a diverse selection of 100 real-market shares. Our empirical findings reveal that the CPPI-MADDPG and TIPP-MADDPG strategies consistently outpace their traditional counterparts, affirming their efficacy in the realm of quantitative trading.
<div id='section'>Paperid: <span id='pid'>374, <a href='https://arxiv.org/pdf/2302.04445.pdf' target='_blank'>https://arxiv.org/pdf/2302.04445.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chanyoung Park, Won Joon Yun, Jae Pyoung Kim, Tiago Koketsu Rodrigues, Soohyun Park, Soyi Jung, Joongheon Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.04445">Quantum Multi-Agent Actor-Critic Networks for Cooperative Mobile Access in Multi-UAV Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper proposes a novel algorithm, named quantum multi-agent actor-critic networks (QMACN) for autonomously constructing a robust mobile access system employing multiple unmanned aerial vehicles (UAVs). In the context of facilitating collaboration among multiple unmanned aerial vehicles (UAVs), the application of multi-agent reinforcement learning (MARL) techniques is regarded as a promising approach. These methods enable UAVs to learn collectively, optimizing their actions within a shared environment, ultimately leading to more efficient cooperative behavior. Furthermore, the principles of a quantum computing (QC) are employed in our study to enhance the training process and inference capabilities of the UAVs involved. By leveraging the unique computational advantages of quantum computing, our approach aims to boost the overall effectiveness of the UAV system. However, employing a QC introduces scalability challenges due to the near intermediate-scale quantum (NISQ) limitation associated with qubit usage. The proposed algorithm addresses this issue by implementing a quantum centralized critic, effectively mitigating the constraints imposed by NISQ limitations. Additionally, the advantages of the QMACN with performance improvements in terms of training speed and wireless service quality are verified via various data-intensive evaluations. Furthermore, this paper validates that a noise injection scheme can be used for handling environmental uncertainties in order to realize robust mobile access.
<div id='section'>Paperid: <span id='pid'>375, <a href='https://arxiv.org/pdf/2212.14749.pdf' target='_blank'>https://arxiv.org/pdf/2212.14749.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenhan Yu, Terence Jie Chua, Jun Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2212.14749">Asynchronous Hybrid Reinforcement Learning for Latency and Reliability Optimization in the Metaverse over Wireless Communications</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Technology advancements in wireless communications and high-performance Extended Reality (XR) have empowered the developments of the Metaverse. The demand for the Metaverse applications and hence, real-time digital twinning of real-world scenes is increasing. Nevertheless, the replication of 2D physical world images into 3D virtual objects is computationally intensive and requires computation offloading. The disparity in transmitted object dimension (2D as opposed to 3D) leads to asymmetric data sizes in uplink (UL) and downlink (DL). To ensure the reliability and low latency of the system, we consider an asynchronous joint UL-DL scenario where in the UL stage, the smaller data size of the physical world images captured by multiple extended reality users (XUs) will be uploaded to the Metaverse Console (MC) to be construed and rendered. In the DL stage, the larger-size 3D virtual objects need to be transmitted back to the XUs. We design a novel multi-agent reinforcement learning algorithm structure, namely Asynchronous Actors Hybrid Critic (AAHC), to optimize the decisions pertaining to computation offloading and channel assignment in the UL stage and optimize the DL transmission power in the DL stage. Extensive experiments demonstrate that compared to proposed baselines, AAHC obtains better solutions with satisfactory training time.
<div id='section'>Paperid: <span id='pid'>376, <a href='https://arxiv.org/pdf/2211.15375.pdf' target='_blank'>https://arxiv.org/pdf/2211.15375.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chanyoung Park, Jae Pyoung Kim, Won Joon Yun, Soohyun Park, Soyi Jung, Joongheon Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2211.15375">Software Simulation and Visualization of Quantum Multi-Drone Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Quantum machine learning (QML) has received a lot of attention according to its light training parameter numbers and speeds; and the advances of QML lead to active research on quantum multi-agent reinforcement learning (QMARL). Existing classical multi-agent reinforcement learning (MARL) features non-stationarity and uncertain properties. Therefore, this paper presents a simulation software framework for novel QMARL to control autonomous multi-drones, i.e., quantum multi-drone reinforcement learning. Our proposed framework accomplishes reasonable reward convergence and service quality performance with fewer trainable parameters. Furthermore, it shows more stable training results. Lastly, our proposed software allows us to analyze the training process and results.
<div id='section'>Paperid: <span id='pid'>377, <a href='https://arxiv.org/pdf/2210.13064.pdf' target='_blank'>https://arxiv.org/pdf/2210.13064.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alessandro Zanardi, Pier Giuseppe Sessa, Nando KÃ¤slin, Saverio Bolognani, Andrea Censi, Emilio Frazzoli
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2210.13064">How Bad is Selfish Driving? Bounding the Inefficiency of Equilibria in Urban Driving Games</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We consider the interaction among agents engaging in a driving task and we model it as general-sum game. This class of games exhibits a plurality of different equilibria posing the issue of equilibrium selection. While selecting the most efficient equilibrium (in term of social cost) is often impractical from a computational standpoint, in this work we study the (in)efficiency of any equilibrium players might agree to play. More specifically, we bound the equilibrium inefficiency by modeling driving games as particular type of congestion games over spatio-temporal resources. We obtain novel guarantees that refine existing bounds on the Price of Anarchy (PoA) as a function of problem-dependent game parameters. For instance, the relative trade-off between proximity costs and personal objectives such as comfort and progress. Although the obtained guarantees concern open-loop trajectories, we observe efficient equilibria even when agents employ closed-loop policies trained via decentralized multi-agent reinforcement learning.
<div id='section'>Paperid: <span id='pid'>378, <a href='https://arxiv.org/pdf/2109.06668.pdf' target='_blank'>https://arxiv.org/pdf/2109.06668.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianye Hao, Tianpei Yang, Hongyao Tang, Chenjia Bai, Jinyi Liu, Zhaopeng Meng, Peng Liu, Zhen Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2109.06668">Exploration in Deep Reinforcement Learning: From Single-Agent to Multiagent Domain</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep Reinforcement Learning (DRL) and Deep Multi-agent Reinforcement Learning (MARL) have achieved significant successes across a wide range of domains, including game AI, autonomous vehicles, robotics, and so on. However, DRL and deep MARL agents are widely known to be sample inefficient that millions of interactions are usually needed even for relatively simple problem settings, thus preventing the wide application and deployment in real-industry scenarios. One bottleneck challenge behind is the well-known exploration problem, i.e., how efficiently exploring the environment and collecting informative experiences that could benefit policy learning towards the optimal ones. This problem becomes more challenging in complex environments with sparse rewards, noisy distractions, long horizons, and non-stationary co-learners. In this paper, we conduct a comprehensive survey on existing exploration methods for both single-agent and multi-agent RL. We start the survey by identifying several key challenges to efficient exploration. Beyond the above two main branches, we also include other notable exploration methods with different ideas and techniques. In addition to algorithmic analysis, we provide a comprehensive and unified empirical comparison of different exploration methods for DRL on a set of commonly used benchmarks. According to our algorithmic and empirical investigation, we finally summarize the open problems of exploration in DRL and deep MARL and point out a few future directions.
<div id='section'>Paperid: <span id='pid'>379, <a href='https://arxiv.org/pdf/2510.05571.pdf' target='_blank'>https://arxiv.org/pdf/2510.05571.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chengzhi Liu, Yuzhe Yang, Kaiwen Zhou, Zhen Zhang, Yue Fan, Yannan Xie, Peng Qi, Xin Eric Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.05571">Presenting a Paper is an Art: Self-Improvement Aesthetic Agents for Academic Presentations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The promotion of academic papers has become an important means of enhancing research visibility. However, existing automated methods struggle limited storytelling, insufficient aesthetic quality, and constrained self-adjustment, making it difficult to achieve efficient and engaging dissemination. At the heart of those challenges is a simple principle: \emph{there is no way to improve it when you cannot evaluate it right}. To address this, we introduce \textbf{EvoPresent}, a self-improvement agent framework that unifies coherent narratives, aesthetic-aware designs, and realistic presentation delivery via virtual characters. Central to EvoPresent is \textbf{PresAesth}, a multi-task reinforcement learning (RL) aesthetic model that provides reliable aesthetic scoring, defect adjustment, and comparative feedback, enabling iterative self-improvement even under limited aesthetic training data. To systematically evaluate the methods, we introduce \textbf{EvoPresent Benchmark}, a comprehensive benchmark comprising: \textit{Presentation Generation Quality}, built on 650 top-tier AI conference papers with multimodal resources (slides, videos and scripts) to assess both content and design; and \textit{Aesthetic Awareness}, consisting of 2,000 slide pairs with varying aesthetic levels, supporting joint training and evaluation on scoring, defect adjustment, and comparison. Our findings highlight that (i) High-quality feedback is essential for agent self-improvement, while initial capability alone does not guarantee effective self-correction. (ii) Automated generation pipelines exhibit a trade-off between visual design and content construction. (iii) Multi-task RL training shows stronger generalization in aesthetic awareness tasks.
<div id='section'>Paperid: <span id='pid'>380, <a href='https://arxiv.org/pdf/2510.05571.pdf' target='_blank'>https://arxiv.org/pdf/2510.05571.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chengzhi Liu, Yuzhe Yang, Kaiwen Zhou, Zhen Zhang, Yue Fan, Yannan Xie, Peng Qi, Xin Eric Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.05571">Presenting a Paper is an Art: Self-Improvement Aesthetic Agents for Academic Presentations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The promotion of academic papers has become an important means of enhancing research visibility. However, existing automated methods struggle limited storytelling, insufficient aesthetic quality, and constrained self-adjustment, making it difficult to achieve efficient and engaging dissemination. At the heart of those challenges is a simple principle: \emph{there is no way to improve it when you cannot evaluate it right}. To address this, we introduce \textbf{EvoPresent}, a self-improvement agent framework that unifies coherent narratives, aesthetic-aware designs, and realistic presentation delivery via virtual characters. Central to EvoPresent is \textbf{PresAesth}, a multi-task reinforcement learning (RL) aesthetic model that provides reliable aesthetic scoring, defect adjustment, and comparative feedback, enabling iterative self-improvement even under limited aesthetic training data. To systematically evaluate the methods, we introduce \textbf{EvoPresent Benchmark}, a comprehensive benchmark comprising: \textit{Presentation Generation Quality}, built on 650 top-tier AI conference papers with multimodal resources (slides, videos and scripts) to assess both content and design; and \textit{Aesthetic Awareness}, consisting of 2,000 slide pairs with varying aesthetic levels, supporting joint training and evaluation on scoring, defect adjustment, and comparison. Our findings highlight that (i) High-quality feedback is essential for agent self-improvement, while initial capability alone does not guarantee effective self-correction. (ii) Automated generation pipelines exhibit a trade-off between visual design and content construction. (iii) Multi-task RL training shows stronger generalization in aesthetic awareness tasks.
<div id='section'>Paperid: <span id='pid'>381, <a href='https://arxiv.org/pdf/2507.06628.pdf' target='_blank'>https://arxiv.org/pdf/2507.06628.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinmin He, Kai Li, Yifan Zang, Haobo Fu, Qiang Fu, Junliang Xing, Jian Cheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.06628">Goal-Oriented Skill Abstraction for Offline Multi-Task Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Offline multi-task reinforcement learning aims to learn a unified policy capable of solving multiple tasks using only pre-collected task-mixed datasets, without requiring any online interaction with the environment. However, it faces significant challenges in effectively sharing knowledge across tasks. Inspired by the efficient knowledge abstraction observed in human learning, we propose Goal-Oriented Skill Abstraction (GO-Skill), a novel approach designed to extract and utilize reusable skills to enhance knowledge transfer and task performance. Our approach uncovers reusable skills through a goal-oriented skill extraction process and leverages vector quantization to construct a discrete skill library. To mitigate class imbalances between broadly applicable and task-specific skills, we introduce a skill enhancement phase to refine the extracted skills. Furthermore, we integrate these skills using hierarchical policy learning, enabling the construction of a high-level policy that dynamically orchestrates discrete skills to accomplish specific tasks. Extensive experiments on diverse robotic manipulation tasks within the MetaWorld benchmark demonstrate the effectiveness and versatility of GO-Skill.
<div id='section'>Paperid: <span id='pid'>382, <a href='https://arxiv.org/pdf/2507.06615.pdf' target='_blank'>https://arxiv.org/pdf/2507.06615.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinmin He, Kai Li, Yifan Zang, Haobo Fu, Qiang Fu, Junliang Xing, Jian Cheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.06615">Efficient Multi-Task Reinforcement Learning with Cross-Task Policy Guidance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-task reinforcement learning endeavors to efficiently leverage shared information across various tasks, facilitating the simultaneous learning of multiple tasks. Existing approaches primarily focus on parameter sharing with carefully designed network structures or tailored optimization procedures. However, they overlook a direct and complementary way to exploit cross-task similarities: the control policies of tasks already proficient in some skills can provide explicit guidance for unmastered tasks to accelerate skills acquisition. To this end, we present a novel framework called Cross-Task Policy Guidance (CTPG), which trains a guide policy for each task to select the behavior policy interacting with the environment from all tasks' control policies, generating better training trajectories. In addition, we propose two gating mechanisms to improve the learning efficiency of CTPG: one gate filters out control policies that are not beneficial for guidance, while the other gate blocks tasks that do not necessitate guidance. CTPG is a general framework adaptable to existing parameter sharing approaches. Empirical evaluations demonstrate that incorporating CTPG with these approaches significantly enhances performance in manipulation and locomotion benchmarks.
<div id='section'>Paperid: <span id='pid'>383, <a href='https://arxiv.org/pdf/2506.05431.pdf' target='_blank'>https://arxiv.org/pdf/2506.05431.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ashwin Ramesh Babu, Sajad Mousavi, Vineet Gundecha, Sahand Ghorbanpour, Avisek Naug, Antonio Guillen, Ricardo Luna Gutierrez, Soumyendu Sarkar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.05431">Robustness Evaluation for Video Models with Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Evaluating the robustness of Video classification models is very challenging, specifically when compared to image-based models. With their increased temporal dimension, there is a significant increase in complexity and computational cost. One of the key challenges is to keep the perturbations to a minimum to induce misclassification. In this work, we propose a multi-agent reinforcement learning approach (spatial and temporal) that cooperatively learns to identify the given video's sensitive spatial and temporal regions. The agents consider temporal coherence in generating fine perturbations, leading to a more effective and visually imperceptible attack. Our method outperforms the state-of-the-art solutions on the Lp metric and the average queries. Our method enables custom distortion types, making the robustness evaluation more relevant to the use case. We extensively evaluate 4 popular models for video action recognition on two popular datasets, HMDB-51 and UCF-101.
<div id='section'>Paperid: <span id='pid'>384, <a href='https://arxiv.org/pdf/2504.12961.pdf' target='_blank'>https://arxiv.org/pdf/2504.12961.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhouyang Jiang, Bin Zhang, Airong Wei, Zhiwei Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.12961">QLLM: Do We Really Need a Mixing Network for Credit Assignment in Multi-Agent Reinforcement Learning?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Credit assignment has remained a fundamental challenge in multi-agent reinforcement learning (MARL). Previous studies have primarily addressed this issue through value decomposition methods under the centralized training with decentralized execution paradigm, where neural networks are utilized to approximate the nonlinear relationship between individual Q-values and the global Q-value. Although these approaches have achieved considerable success in various benchmark tasks, they still suffer from several limitations, including imprecise attribution of contributions, limited interpretability, and poor scalability in high-dimensional state spaces. To address these challenges, we propose a novel algorithm, \textbf{QLLM}, which facilitates the automatic construction of credit assignment functions using large language models (LLMs). Specifically, the concept of \textbf{TFCAF} is introduced, wherein the credit allocation process is represented as a direct and expressive nonlinear functional formulation. A custom-designed \textit{coder-evaluator} framework is further employed to guide the generation, verification, and refinement of executable code by LLMs, significantly mitigating issues such as hallucination and shallow reasoning during inference. Extensive experiments conducted on several standard MARL benchmarks demonstrate that the proposed method consistently outperforms existing state-of-the-art baselines. Moreover, QLLM exhibits strong generalization capability and maintains compatibility with a wide range of MARL algorithms that utilize mixing networks, positioning it as a promising and versatile solution for complex multi-agent scenarios.
<div id='section'>Paperid: <span id='pid'>385, <a href='https://arxiv.org/pdf/2504.12961.pdf' target='_blank'>https://arxiv.org/pdf/2504.12961.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhouyang Jiang, Bin Zhang, Airong Wei, Zhiwei Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.12961">QLLM: Do We Really Need a Mixing Network for Credit Assignment in Multi-Agent Reinforcement Learning?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Credit assignment has remained a fundamental challenge in multi-agent reinforcement learning (MARL). Previous studies have primarily addressed this issue through value decomposition methods under the centralized training with decentralized execution paradigm, where neural networks are utilized to approximate the nonlinear relationship between individual Q-values and the global Q-value. Although these approaches have achieved considerable success in various benchmark tasks, they still suffer from several limitations, including imprecise attribution of contributions, limited interpretability, and poor scalability in high-dimensional state spaces. To address these challenges, we propose a novel algorithm, \textbf{QLLM}, which facilitates the automatic construction of credit assignment functions using large language models (LLMs). Specifically, the concept of \textbf{TFCAF} is introduced, wherein the credit allocation process is represented as a direct and expressive nonlinear functional formulation. A custom-designed \textit{coder-evaluator} framework is further employed to guide the generation, verification, and refinement of executable code by LLMs, significantly mitigating issues such as hallucination and shallow reasoning during inference. Extensive experiments conducted on several standard MARL benchmarks demonstrate that the proposed method consistently outperforms existing state-of-the-art baselines. Moreover, QLLM exhibits strong generalization capability and maintains compatibility with a wide range of MARL algorithms that utilize mixing networks, positioning it as a promising and versatile solution for complex multi-agent scenarios.
<div id='section'>Paperid: <span id='pid'>386, <a href='https://arxiv.org/pdf/2504.12961.pdf' target='_blank'>https://arxiv.org/pdf/2504.12961.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhouyang Jiang, Bin Zhang, Airong Wei, Zhiwei Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.12961">QLLM: Do We Really Need a Mixing Network for Credit Assignment in Multi-Agent Reinforcement Learning?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Credit assignment has remained a fundamental challenge in multi-agent reinforcement learning (MARL). Previous studies have primarily addressed this issue through value decomposition methods under the centralized training with decentralized execution paradigm, where neural networks are utilized to approximate the nonlinear relationship between individual Q-values and the global Q-value. Although these approaches have achieved considerable success in various benchmark tasks, they still suffer from several limitations, including imprecise attribution of contributions, limited interpretability, and poor scalability in high-dimensional state spaces. To address these challenges, we propose a novel algorithm, \textbf{QLLM}, which facilitates the automatic construction of credit assignment functions using large language models (LLMs). Specifically, the concept of \textbf{TFCAF} is introduced, wherein the credit allocation process is represented as a direct and expressive nonlinear functional formulation. A custom-designed \textit{coder-evaluator} framework is further employed to guide the generation, verification, and refinement of executable code by LLMs, significantly mitigating issues such as hallucination and shallow reasoning during inference. Extensive experiments conducted on several standard MARL benchmarks demonstrate that the proposed method consistently outperforms existing state-of-the-art baselines. Moreover, QLLM exhibits strong generalization capability and maintains compatibility with a wide range of MARL algorithms that utilize mixing networks, positioning it as a promising and versatile solution for complex multi-agent scenarios.
<div id='section'>Paperid: <span id='pid'>387, <a href='https://arxiv.org/pdf/2503.07817.pdf' target='_blank'>https://arxiv.org/pdf/2503.07817.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kefan Song, Runnan Jiang, Rohan Chandra, Shangtong Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.07817">Group Fairness in Multi-Task Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper addresses a critical societal consideration in the application of Reinforcement Learning (RL): ensuring equitable outcomes across different demographic groups in multi-task settings. While previous work has explored fairness in single-task RL, many real-world applications are multi-task in nature and require policies to maintain fairness across all tasks. We introduce a novel formulation of multi-task group fairness in RL and propose a constrained optimization algorithm that explicitly enforces fairness constraints across multiple tasks simultaneously. We have shown that our proposed algorithm does not violate fairness constraints with high probability and with sublinear regret in the finite-horizon episodic setting. Through experiments in RiverSwim and MuJoCo environments, we demonstrate that our approach better ensures group fairness across multiple tasks compared to previous methods that lack explicit multi-task fairness constraints in both the finite-horizon setting and the infinite-horizon setting. Our results show that the proposed algorithm achieves smaller fairness gaps while maintaining comparable returns across different demographic groups and tasks, suggesting its potential for addressing fairness concerns in real-world multi-task RL applications.
<div id='section'>Paperid: <span id='pid'>388, <a href='https://arxiv.org/pdf/2501.16098.pdf' target='_blank'>https://arxiv.org/pdf/2501.16098.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Eslam Eldeeb, Hirley Alves
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.16098">Multi-Agent Meta-Offline Reinforcement Learning for Timely UAV Path Planning and Data Collection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent reinforcement learning (MARL) has been widely adopted in high-performance computing and complex data-driven decision-making in the wireless domain. However, conventional MARL schemes face many obstacles in real-world scenarios. First, most MARL algorithms are online, which might be unsafe and impractical. Second, MARL algorithms are environment-specific, meaning network configuration changes require model retraining. This letter proposes a novel meta-offline MARL algorithm that combines conservative Q-learning (CQL) and model agnostic meta-learning (MAML). CQL enables offline training by leveraging pre-collected datasets, while MAML ensures scalability and adaptability to dynamic network configurations and objectives. We propose two algorithm variants: independent training (M-I-MARL) and centralized training decentralized execution (M-CTDE-MARL). Simulation results show that the proposed algorithm outperforms conventional schemes, especially the CTDE approach that achieves 50 % faster convergence in dynamic scenarios than the benchmarks. The proposed framework enhances scalability, robustness, and adaptability in wireless communication systems by optimizing UAV trajectories and scheduling policies.
<div id='section'>Paperid: <span id='pid'>389, <a href='https://arxiv.org/pdf/2501.12991.pdf' target='_blank'>https://arxiv.org/pdf/2501.12991.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Eslam Eldeeb, Hirley Alves
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.12991">An Offline Multi-Agent Reinforcement Learning Framework for Radio Resource Management</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Offline multi-agent reinforcement learning (MARL) addresses key limitations of online MARL, such as safety concerns, expensive data collection, extended training intervals, and high signaling overhead caused by online interactions with the environment. In this work, we propose an offline MARL algorithm for radio resource management (RRM), focusing on optimizing scheduling policies for multiple access points (APs) to jointly maximize the sum and tail rates of user equipment (UEs). We evaluate three training paradigms: centralized, independent, and centralized training with decentralized execution (CTDE). Our simulation results demonstrate that the proposed offline MARL framework outperforms conventional baseline approaches, achieving over a 15\% improvement in a weighted combination of sum and tail rates. Additionally, the CTDE framework strikes an effective balance, reducing the computational complexity of centralized methods while addressing the inefficiencies of independent training. These results underscore the potential of offline MARL to deliver scalable, robust, and efficient solutions for resource management in dynamic wireless networks.
<div id='section'>Paperid: <span id='pid'>390, <a href='https://arxiv.org/pdf/2412.12442.pdf' target='_blank'>https://arxiv.org/pdf/2412.12442.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiaxu Xing, Ismail Geles, Yunlong Song, Elie Aljalbout, Davide Scaramuzza
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.12442">Multi-Task Reinforcement Learning for Quadrotors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reinforcement learning (RL) has shown great effectiveness in quadrotor control, enabling specialized policies to develop even human-champion-level performance in single-task scenarios. However, these specialized policies often struggle with novel tasks, requiring a complete retraining of the policy from scratch. To address this limitation, this paper presents a novel multi-task reinforcement learning (MTRL) framework tailored for quadrotor control, leveraging the shared physical dynamics of the platform to enhance sample efficiency and task performance. By employing a multi-critic architecture and shared task encoders, our framework facilitates knowledge transfer across tasks, enabling a single policy to execute diverse maneuvers, including high-speed stabilization, velocity tracking, and autonomous racing. Our experimental results, validated both in simulation and real-world scenarios, demonstrate that our framework outperforms baseline approaches in terms of sample efficiency and overall task performance.
<div id='section'>Paperid: <span id='pid'>391, <a href='https://arxiv.org/pdf/2408.07841.pdf' target='_blank'>https://arxiv.org/pdf/2408.07841.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Avisek Naug, Antonio Guillen, Ricardo Luna, Vineet Gundecha, Desik Rengarajan, Sahand Ghorbanpour, Sajad Mousavi, Ashwin Ramesh Babu, Dejan Markovikj, Lekhapriya D Kashyap, Soumyendu Sarkar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.07841">SustainDC: Benchmarking for Sustainable Data Center Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Machine learning has driven an exponential increase in computational demand, leading to massive data centers that consume significant amounts of energy and contribute to climate change. This makes sustainable data center control a priority. In this paper, we introduce SustainDC, a set of Python environments for benchmarking multi-agent reinforcement learning (MARL) algorithms for data centers (DC). SustainDC supports custom DC configurations and tasks such as workload scheduling, cooling optimization, and auxiliary battery management, with multiple agents managing these operations while accounting for the effects of each other. We evaluate various MARL algorithms on SustainDC, showing their performance across diverse DC designs, locations, weather conditions, grid carbon intensity, and workload requirements. Our results highlight significant opportunities for improvement of data center operations using MARL algorithms. Given the increasing use of DC due to AI, SustainDC provides a crucial platform for the development and benchmarking of advanced algorithms essential for achieving sustainable computing and addressing other heterogeneous real-world challenges.
<div id='section'>Paperid: <span id='pid'>392, <a href='https://arxiv.org/pdf/2404.10991.pdf' target='_blank'>https://arxiv.org/pdf/2404.10991.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Soumyendu Sarkar, Vineet Gundecha, Sahand Ghorbanpour, Alexander Shmakov, Ashwin Ramesh Babu, Avisek Naug, Alexandre Pichard, Mathieu Cocho
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.10991">Function Approximation for Reinforcement Learning Controller for Energy from Spread Waves</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The industrial multi-generator Wave Energy Converters (WEC) must handle multiple simultaneous waves coming from different directions called spread waves. These complex devices in challenging circumstances need controllers with multiple objectives of energy capture efficiency, reduction of structural stress to limit maintenance, and proactive protection against high waves. The Multi-Agent Reinforcement Learning (MARL) controller trained with the Proximal Policy Optimization (PPO) algorithm can handle these complexities. In this paper, we explore different function approximations for the policy and critic networks in modeling the sequential nature of the system dynamics and find that they are key to better performance. We investigated the performance of a fully connected neural network (FCN), LSTM, and Transformer model variants with varying depths and gated residual connections. Our results show that the transformer model of moderate depth with gated residual connections around the multi-head attention, multi-layer perceptron, and the transformer block (STrXL) proposed in this paper is optimal and boosts energy efficiency by an average of 22.1% for these complex spread waves over the existing spring damper (SD) controller. Furthermore, unlike the default SD controller, the transformer controller almost eliminated the mechanical stress from the rotational yaw motion for angled waves. Demo: https://tinyurl.com/yueda3jh
<div id='section'>Paperid: <span id='pid'>393, <a href='https://arxiv.org/pdf/2403.14092.pdf' target='_blank'>https://arxiv.org/pdf/2403.14092.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Soumyendu Sarkar, Avisek Naug, Ricardo Luna, Antonio Guillen, Vineet Gundecha, Sahand Ghorbanpour, Sajad Mousavi, Dejan Markovikj, Ashwin Ramesh Babu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.14092">Carbon Footprint Reduction for Sustainable Data Centers in Real-Time</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As machine learning workloads significantly increase energy consumption, sustainable data centers with low carbon emissions are becoming a top priority for governments and corporations worldwide. This requires a paradigm shift in optimizing power consumption in cooling and IT loads, shifting flexible loads based on the availability of renewable energy in the power grid, and leveraging battery storage from the uninterrupted power supply in data centers, using collaborative agents. The complex association between these optimization strategies and their dependencies on variable external factors like weather and the power grid carbon intensity makes this a hard problem. Currently, a real-time controller to optimize all these goals simultaneously in a dynamic real-world setting is lacking. We propose a Data Center Carbon Footprint Reduction (DC-CFR) multi-agent Reinforcement Learning (MARL) framework that optimizes data centers for the multiple objectives of carbon footprint reduction, energy consumption, and energy cost. The results show that the DC-CFR MARL agents effectively resolved the complex interdependencies in optimizing cooling, load shifting, and energy storage in real-time for various locations under real-world dynamic weather and grid carbon intensity conditions. DC-CFR significantly outperformed the industry standard ASHRAE controller with a considerable reduction in carbon emissions (14.5%), energy usage (14.4%), and energy cost (13.7%) when evaluated over one year across multiple geographical regions.
<div id='section'>Paperid: <span id='pid'>394, <a href='https://arxiv.org/pdf/2402.08421.pdf' target='_blank'>https://arxiv.org/pdf/2402.08421.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Eslam Eldeeb, Houssem Sifaou, Osvaldo Simeone, Mohammad Shehab, Hirley Alves
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.08421">Conservative and Risk-Aware Offline Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reinforcement learning (RL) has been widely adopted for controlling and optimizing complex engineering systems such as next-generation wireless networks. An important challenge in adopting RL is the need for direct access to the physical environment. This limitation is particularly severe in multi-agent systems, for which conventional multi-agent reinforcement learning (MARL) requires a large number of coordinated online interactions with the environment during training. When only offline data is available, a direct application of online MARL schemes would generally fail due to the epistemic uncertainty entailed by the lack of exploration during training. In this work, we propose an offline MARL scheme that integrates distributional RL and conservative Q-learning to address the environment's inherent aleatoric uncertainty and the epistemic uncertainty arising from the use of offline data. We explore both independent and joint learning strategies. The proposed MARL scheme, referred to as multi-agent conservative quantile regression, addresses general risk-sensitive design criteria and is applied to the trajectory planning problem in drone networks, showcasing its advantages.
<div id='section'>Paperid: <span id='pid'>395, <a href='https://arxiv.org/pdf/2312.14472.pdf' target='_blank'>https://arxiv.org/pdf/2312.14472.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinmin He, Kai Li, Yifan Zang, Haobo Fu, Qiang Fu, Junliang Xing, Jian Cheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.14472">Not All Tasks Are Equally Difficult: Multi-Task Deep Reinforcement Learning with Dynamic Depth Routing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-task reinforcement learning endeavors to accomplish a set of different tasks with a single policy. To enhance data efficiency by sharing parameters across multiple tasks, a common practice segments the network into distinct modules and trains a routing network to recombine these modules into task-specific policies. However, existing routing approaches employ a fixed number of modules for all tasks, neglecting that tasks with varying difficulties commonly require varying amounts of knowledge. This work presents a Dynamic Depth Routing (D2R) framework, which learns strategic skipping of certain intermediate modules, thereby flexibly choosing different numbers of modules for each task. Under this framework, we further introduce a ResRouting method to address the issue of disparate routing paths between behavior and target policies during off-policy training. In addition, we design an automatic route-balancing mechanism to encourage continued routing exploration for unmastered tasks without disturbing the routing of mastered ones. We conduct extensive experiments on various robotics manipulation tasks in the Meta-World benchmark, where D2R achieves state-of-the-art performance with significantly improved learning efficiency.
<div id='section'>Paperid: <span id='pid'>396, <a href='https://arxiv.org/pdf/2312.09009.pdf' target='_blank'>https://arxiv.org/pdf/2312.09009.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dapeng Li, Na Lou, Bin Zhang, Zhiwei Xu, Guoliang Fan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.09009">Adaptive parameter sharing for multi-agent reinforcement learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Parameter sharing, as an important technique in multi-agent systems, can effectively solve the scalability issue in large-scale agent problems. However, the effectiveness of parameter sharing largely depends on the environment setting. When agents have different identities or tasks, naive parameter sharing makes it difficult to generate sufficiently differentiated strategies for agents. Inspired by research pertaining to the brain in biology, we propose a novel parameter sharing method. It maps each type of agent to different regions within a shared network based on their identity, resulting in distinct subnetworks. Therefore, our method can increase the diversity of strategies among different agents without introducing additional training parameters. Through experiments conducted in multiple environments, our method has shown better performance than other parameter sharing methods.
<div id='section'>Paperid: <span id='pid'>397, <a href='https://arxiv.org/pdf/2309.14757.pdf' target='_blank'>https://arxiv.org/pdf/2309.14757.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Eslam Eldeeb, Mohammad Shehab, Hirley Alves
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.14757">Age Minimization in Massive IoT via UAV Swarm: A Multi-agent Reinforcement Learning Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In many massive IoT communication scenarios, the IoT devices require coverage from dynamic units that can move close to the IoT devices and reduce the uplink energy consumption. A robust solution is to deploy a large number of UAVs (UAV swarm) to provide coverage and a better line of sight (LoS) for the IoT network. However, the study of these massive IoT scenarios with a massive number of serving units leads to high dimensional problems with high complexity. In this paper, we apply multi-agent deep reinforcement learning to address the high-dimensional problem that results from deploying a swarm of UAVs to collect fresh information from IoT devices. The target is to minimize the overall age of information in the IoT network. The results reveal that both cooperative and partially cooperative multi-agent deep reinforcement learning approaches are able to outperform the high-complexity centralized deep reinforcement learning approach, which stands helpless in large-scale networks.
<div id='section'>Paperid: <span id='pid'>398, <a href='https://arxiv.org/pdf/2308.04905.pdf' target='_blank'>https://arxiv.org/pdf/2308.04905.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Guillermo BernÃ¡rdez, JosÃ© SuÃ¡rez-Varela, Xiang Shi, Shihan Xiao, Xiangle Cheng, Pere Barlet-Ros, Albert Cabellos-Aparicio
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.04905">GraphCC: A Practical Graph Learning-based Approach to Congestion Control in Datacenters</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Congestion Control (CC) plays a fundamental role in optimizing traffic in Data Center Networks (DCN). Currently, DCNs mainly implement two main CC protocols: DCTCP and DCQCN. Both protocols -- and their main variants -- are based on Explicit Congestion Notification (ECN), where intermediate switches mark packets when they detect congestion. The ECN configuration is thus a crucial aspect on the performance of CC protocols. Nowadays, network experts set static ECN parameters carefully selected to optimize the average network performance. However, today's high-speed DCNs experience quick and abrupt changes that severely change the network state (e.g., dynamic traffic workloads, incast events, failures). This leads to under-utilization and sub-optimal performance. This paper presents GraphCC, a novel Machine Learning-based framework for in-network CC optimization. Our distributed solution relies on a novel combination of Multi-agent Reinforcement Learning (MARL) and Graph Neural Networks (GNN), and it is compatible with widely deployed ECN-based CC protocols. GraphCC deploys distributed agents on switches that communicate with their neighbors to cooperate and optimize the global ECN configuration. In our evaluation, we test the performance of GraphCC under a wide variety of scenarios, focusing on the capability of this solution to adapt to new scenarios unseen during training (e.g., new traffic workloads, failures, upgrades). We compare GraphCC with a state-of-the-art MARL-based solution for ECN tuning -- ACC -- and observe that our proposed solution outperforms the state-of-the-art baseline in all of the evaluation scenarios, showing improvements up to $20\%$ in Flow Completion Time as well as significant reductions in buffer occupancy ($38.0-85.7\%$).
<div id='section'>Paperid: <span id='pid'>399, <a href='https://arxiv.org/pdf/2305.07856.pdf' target='_blank'>https://arxiv.org/pdf/2305.07856.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bin Zhang, Hangyu Mao, Lijuan Li, Zhiwei Xu, Dapeng Li, Rui Zhao, Guoliang Fan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.07856">Stackelberg Decision Transformer for Asynchronous Action Coordination in Multi-Agent Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Asynchronous action coordination presents a pervasive challenge in Multi-Agent Systems (MAS), which can be represented as a Stackelberg game (SG). However, the scalability of existing Multi-Agent Reinforcement Learning (MARL) methods based on SG is severely constrained by network structures or environmental limitations. To address this issue, we propose the Stackelberg Decision Transformer (STEER), a heuristic approach that resolves the difficulties of hierarchical coordination among agents. STEER efficiently manages decision-making processes in both spatial and temporal contexts by incorporating the hierarchical decision structure of SG, the modeling capability of autoregressive sequence models, and the exploratory learning methodology of MARL. Our research contributes to the development of an effective and adaptable asynchronous action coordination method that can be widely applied to various task types and environmental configurations in MAS. Experimental results demonstrate that our method can converge to Stackelberg equilibrium solutions and outperforms other existing methods in complex scenarios.
<div id='section'>Paperid: <span id='pid'>400, <a href='https://arxiv.org/pdf/2304.12532.pdf' target='_blank'>https://arxiv.org/pdf/2304.12532.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dapeng Li, Zhiwei Xu, Bin Zhang, Guoliang Fan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.12532">SEA: A Spatially Explicit Architecture for Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Spatial information is essential in various fields. How to explicitly model according to the spatial location of agents is also very important for the multi-agent problem, especially when the number of agents is changing and the scale is enormous. Inspired by the point cloud task in computer vision, we propose a spatial information extraction structure for multi-agent reinforcement learning in this paper. Agents can effectively share the neighborhood and global information through a spatially encoder-decoder structure. Our method follows the centralized training with decentralized execution (CTDE) paradigm. In addition, our structure can be applied to various existing mainstream reinforcement learning algorithms with minor modifications and can deal with the problem with a variable number of agents. The experiments in several multi-agent scenarios show that the existing methods can get convincing results by adding our spatially explicit architecture.
<div id='section'>Paperid: <span id='pid'>401, <a href='https://arxiv.org/pdf/2304.10351.pdf' target='_blank'>https://arxiv.org/pdf/2304.10351.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bin Zhang, Lijuan Li, Zhiwei Xu, Dapeng Li, Guoliang Fan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.10351">Inducing Stackelberg Equilibrium through Spatio-Temporal Sequential Decision-Making in Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In multi-agent reinforcement learning (MARL), self-interested agents attempt to establish equilibrium and achieve coordination depending on game structure. However, existing MARL approaches are mostly bound by the simultaneous actions of all agents in the Markov game (MG) framework, and few works consider the formation of equilibrium strategies via asynchronous action coordination. In view of the advantages of Stackelberg equilibrium (SE) over Nash equilibrium, we construct a spatio-temporal sequential decision-making structure derived from the MG and propose an N-level policy model based on a conditional hypernetwork shared by all agents. This approach allows for asymmetric training with symmetric execution, with each agent responding optimally conditioned on the decisions made by superior agents. Agents can learn heterogeneous SE policies while still maintaining parameter sharing, which leads to reduced cost for learning and storage and enhanced scalability as the number of agents increases. Experiments demonstrate that our method effectively converges to the SE policies in repeated matrix game scenarios, and performs admirably in immensely complex settings including cooperative tasks and mixed tasks.
<div id='section'>Paperid: <span id='pid'>402, <a href='https://arxiv.org/pdf/2303.18157.pdf' target='_blank'>https://arxiv.org/pdf/2303.18157.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Guillermo BernÃ¡rdez, JosÃ© SuÃ¡rez-Varela, Albert LÃ³pez, Xiang Shi, Shihan Xiao, Xiangle Cheng, Pere Barlet-Ros, Albert Cabellos-Aparicio
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.18157">MAGNNETO: A Graph Neural Network-based Multi-Agent system for Traffic Engineering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current trends in networking propose the use of Machine Learning (ML) for a wide variety of network optimization tasks. As such, many efforts have been made to produce ML-based solutions for Traffic Engineering (TE), which is a fundamental problem in ISP networks. Nowadays, state-of-the-art TE optimizers rely on traditional optimization techniques, such as Local search, Constraint Programming, or Linear programming. In this paper, we present MAGNNETO, a distributed ML-based framework that leverages Multi-Agent Reinforcement Learning and Graph Neural Networks for distributed TE optimization. MAGNNETO deploys a set of agents across the network that learn and communicate in a distributed fashion via message exchanges between neighboring agents. Particularly, we apply this framework to optimize link weights in OSPF, with the goal of minimizing network congestion. In our evaluation, we compare MAGNNETO against several state-of-the-art TE optimizers in more than 75 topologies (up to 153 nodes and 354 links), including realistic traffic loads. Our experimental results show that, thanks to its distributed nature, MAGNNETO achieves comparable performance to state-of-the-art TE optimizers with significantly lower execution times. Moreover, our ML-based solution demonstrates a strong generalization capability to successfully operate in new networks unseen during training.
<div id='section'>Paperid: <span id='pid'>403, <a href='https://arxiv.org/pdf/2302.10058.pdf' target='_blank'>https://arxiv.org/pdf/2302.10058.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jing Wang, Meichen Song, Feng Gao, Boyi Liu, Zhaoran Wang, Yi Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.10058">Differentiable Arbitrating in Zero-sum Markov Games</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We initiate the study of how to perturb the reward in a zero-sum Markov game with two players to induce a desirable Nash equilibrium, namely arbitrating. Such a problem admits a bi-level optimization formulation. The lower level requires solving the Nash equilibrium under a given reward function, which makes the overall problem challenging to optimize in an end-to-end way. We propose a backpropagation scheme that differentiates through the Nash equilibrium, which provides the gradient feedback for the upper level. In particular, our method only requires a black-box solver for the (regularized) Nash equilibrium (NE). We develop the convergence analysis for the proposed framework with proper black-box NE solvers and demonstrate the empirical successes in two multi-agent reinforcement learning (MARL) environments.
<div id='section'>Paperid: <span id='pid'>404, <a href='https://arxiv.org/pdf/2302.05910.pdf' target='_blank'>https://arxiv.org/pdf/2302.05910.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>David Mguni, Haojun Chen, Taher Jafferjee, Jianhong Wang, Long Fei, Xidong Feng, Stephen McAleer, Feifei Tong, Jun Wang, Yaodong Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.05910">MANSA: Learning Fast and Slow in Multi-Agent Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In multi-agent reinforcement learning (MARL), independent learning (IL) often shows remarkable performance and easily scales with the number of agents. Yet, using IL can be inefficient and runs the risk of failing to successfully train, particularly in scenarios that require agents to coordinate their actions. Using centralised learning (CL) enables MARL agents to quickly learn how to coordinate their behaviour but employing CL everywhere is often prohibitively expensive in real-world applications. Besides, using CL in value-based methods often needs strong representational constraints (e.g. individual-global-max condition) that can lead to poor performance if violated. In this paper, we introduce a novel plug & play IL framework named Multi-Agent Network Selection Algorithm (MANSA) which selectively employs CL only at states that require coordination. At its core, MANSA has an additional agent that uses switching controls to quickly learn the best states to activate CL during training, using CL only where necessary and vastly reducing the computational burden of CL. Our theory proves MANSA preserves cooperative MARL convergence properties, boosts IL performance and can optimally make use of a fixed budget on the number CL calls. We show empirically in Level-based Foraging (LBF) and StarCraft Multi-agent Challenge (SMAC) that MANSA achieves fast, superior and more reliable performance while making 40% fewer CL calls in SMAC and using CL at only 1% CL calls in LBF.
<div id='section'>Paperid: <span id='pid'>405, <a href='https://arxiv.org/pdf/2302.02180.pdf' target='_blank'>https://arxiv.org/pdf/2302.02180.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiwei Xu, Bin Zhang, Dapeng Li, Guangchong Zhou, Zeren Zhang, Guoliang Fan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.02180">Dual Self-Awareness Value Decomposition Framework without Individual Global Max for Cooperative Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Value decomposition methods have gained popularity in the field of cooperative multi-agent reinforcement learning. However, almost all existing methods follow the principle of Individual Global Max (IGM) or its variants, which limits their problem-solving capabilities. To address this, we propose a dual self-awareness value decomposition framework, inspired by the notion of dual self-awareness in psychology, that entirely rejects the IGM premise. Each agent consists of an ego policy for action selection and an alter ego value function to solve the credit assignment problem. The value function factorization can ignore the IGM assumption by utilizing an explicit search procedure. On the basis of the above, we also suggest a novel anti-ego exploration mechanism to avoid the algorithm becoming stuck in a local optimum. As the first fully IGM-free value decomposition method, our proposed framework achieves desirable performance in various cooperative tasks.
<div id='section'>Paperid: <span id='pid'>406, <a href='https://arxiv.org/pdf/2209.05656.pdf' target='_blank'>https://arxiv.org/pdf/2209.05656.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Soumyendu Sarkar, Vineet Gundecha, Sahand Ghorbanpour, Alexander Shmakov, Ashwin Ramesh Babu, Alexandre Pichard, Mathieu Cocho
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2209.05656">Skip Training for Multi-Agent Reinforcement Learning Controller for Industrial Wave Energy Converters</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent Wave Energy Converters (WEC) are equipped with multiple legs and generators to maximize energy generation. Traditional controllers have shown limitations to capture complex wave patterns and the controllers must efficiently maximize the energy capture. This paper introduces a Multi-Agent Reinforcement Learning controller (MARL), which outperforms the traditionally used spring damper controller. Our initial studies show that the complex nature of problems makes it hard for training to converge. Hence, we propose a novel skip training approach which enables the MARL training to overcome performance saturation and converge to more optimum controllers compared to default MARL training, boosting power generation. We also present another novel hybrid training initialization (STHTI) approach, where the individual agents of the MARL controllers can be initially trained against the baseline Spring Damper (SD) controller individually and then be trained one agent at a time or all together in future iterations to accelerate convergence. We achieved double-digit gains in energy efficiency over the baseline Spring Damper controller with the proposed MARL controllers using the Asynchronous Advantage Actor-Critic (A3C) algorithm.
<div id='section'>Paperid: <span id='pid'>407, <a href='https://arxiv.org/pdf/2205.15434.pdf' target='_blank'>https://arxiv.org/pdf/2205.15434.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Oliver Slumbers, David Henry Mguni, Stephen Marcus McAleer, Stefano B. Blumberg, Jun Wang, Yaodong Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2205.15434">A Game-Theoretic Framework for Managing Risk in Multi-Agent Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In order for agents in multi-agent systems (MAS) to be safe, they need to take into account the risks posed by the actions of other agents. However, the dominant paradigm in game theory (GT) assumes that agents are not affected by risk from other agents and only strive to maximise their expected utility. For example, in hybrid human-AI driving systems, it is necessary to limit large deviations in reward resulting from car crashes. Although there are equilibrium concepts in game theory that take into account risk aversion, they either assume that agents are risk-neutral with respect to the uncertainty caused by the actions of other agents, or they are not guaranteed to exist. We introduce a new GT-based Risk-Averse Equilibrium (RAE) that always produces a solution that minimises the potential variance in reward accounting for the strategy of other agents. Theoretically and empirically, we show RAE shares many properties with a Nash Equilibrium (NE), establishing convergence properties and generalising to risk-dominant NE in certain cases. To tackle large-scale problems, we extend RAE to the PSRO multi-agent reinforcement learning (MARL) framework. We empirically demonstrate the minimum reward variance benefits of RAE in matrix games with high-risk outcomes. Results on MARL experiments show RAE generalises to risk-dominant NE in a trust dilemma game and that it reduces instances of crashing by 7x in an autonomous driving setting versus the best performing baseline.
<div id='section'>Paperid: <span id='pid'>408, <a href='https://arxiv.org/pdf/2201.11783.pdf' target='_blank'>https://arxiv.org/pdf/2201.11783.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ramnath Kumar, Tristan Deleu, Yoshua Bengio
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2201.11783">Boosting Exploration in Multi-Task Reinforcement Learning using Adversarial Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Advancements in reinforcement learning (RL) have been remarkable in recent years. However, the limitations of traditional training methods have become increasingly evident, particularly in meta-RL settings where agents face new, unseen tasks. Conventional training approaches are susceptible to failure in such situations as they need more robustness to adversity. Our proposed adversarial training regime for Multi-Task Reinforcement Learning (MT-RL) addresses the limitations of conventional training methods in RL, especially in meta-RL environments where the agent faces new tasks. The adversarial component challenges the agent, forcing it to improve its decision-making abilities in dynamic and unpredictable situations. This component operates without relying on manual intervention or domain-specific knowledge, making it a highly versatile solution. Experiments conducted in multiple MT-RL environments demonstrate that adversarial training leads to better exploration and a deeper understanding of the environment. The adversarial training regime for MT-RL presents a new perspective on training and development for RL agents and is a valuable contribution to the field.
<div id='section'>Paperid: <span id='pid'>409, <a href='https://arxiv.org/pdf/2109.01795.pdf' target='_blank'>https://arxiv.org/pdf/2109.01795.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaotie Deng, Ningyuan Li, David Mguni, Jun Wang, Yaodong Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2109.01795">On the Complexity of Computing Markov Perfect Equilibrium in General-Sum Stochastic Games</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Similar to the role of Markov decision processes in reinforcement learning, Stochastic Games (SGs) lay the foundation for the study of multi-agent reinforcement learning (MARL) and sequential agent interactions. In this paper, we derive that computing an approximate Markov Perfect Equilibrium (MPE) in a finite-state discounted Stochastic Game within the exponential precision is \textbf{PPAD}-complete. We adopt a function with a polynomially bounded description in the strategy space to convert the MPE computation to a fixed-point problem, even though the stochastic game may demand an exponential number of pure strategies, in the number of states, for each agent. The completeness result follows the reduction of the fixed-point problem to {\sc End of the Line}. Our results indicate that finding an MPE in SGs is highly unlikely to be \textbf{NP}-hard unless \textbf{NP}=\textbf{co-NP}. Our work offers confidence for MARL research to study MPE computation on general-sum SGs and to develop fruitful algorithms as currently on zero-sum SGs.
<div id='section'>Paperid: <span id='pid'>410, <a href='https://arxiv.org/pdf/2509.15099.pdf' target='_blank'>https://arxiv.org/pdf/2509.15099.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Taoyuan Yu, Kui Wang, Zongdian Li, Tao Yu, Kei Sakaguchi, Walid Saad
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.15099">Digital Twin-based Cooperative Autonomous Driving in Smart Intersections: A Multi-Agent Reinforcement Learning Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Unsignalized intersections pose safety and efficiency challenges due to complex traffic flows and blind spots. In this paper, a digital twin (DT)-based cooperative driving system with roadside unit (RSU)-centric architecture is proposed for enhancing safety and efficiency at unsignalized intersections. The system leverages comprehensive bird-eye-view (BEV) perception to eliminate blind spots and employs a hybrid reinforcement learning (RL) framework combining offline pre-training with online fine-tuning. Specifically, driving policies are initially trained using conservative Q-learning (CQL) with behavior cloning (BC) on real datasets, then fine-tuned using multi-agent proximal policy optimization (MAPPO) with self-attention mechanisms to handle dynamic multi-agent coordination. The RSU implements real-time commands via vehicle-to-infrastructure (V2I) communications. Experimental results show that the proposed method yields failure rates below 0.03\% coordinating up to three connected autonomous vehicles (CAVs), significantly outperforming traditional methods. In addition, the system exhibits sub-linear computational scaling with inference times under 40 ms. Furthermore, it demonstrates robust generalization across diverse unsignalized intersection scenarios, indicating its practicality and readiness for real-world deployment.
<div id='section'>Paperid: <span id='pid'>411, <a href='https://arxiv.org/pdf/2509.15099.pdf' target='_blank'>https://arxiv.org/pdf/2509.15099.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Taoyuan Yu, Kui Wang, Zongdian Li, Tao Yu, Kei Sakaguchi, Walid Saad
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.15099">Digital Twin-based Cooperative Autonomous Driving in Smart Intersections: A Multi-Agent Reinforcement Learning Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Unsignalized intersections pose safety and efficiency challenges due to complex traffic flows and blind spots. In this paper, a digital twin (DT)-based cooperative driving system with roadside unit (RSU)-centric architecture is proposed for enhancing safety and efficiency at unsignalized intersections. The system leverages comprehensive bird-eye-view (BEV) perception to eliminate blind spots and employs a hybrid reinforcement learning (RL) framework combining offline pre-training with online fine-tuning. Specifically, driving policies are initially trained using conservative Q-learning (CQL) with behavior cloning (BC) on real datasets, then fine-tuned using multi-agent proximal policy optimization (MAPPO) with self-attention mechanisms to handle dynamic multi-agent coordination. The RSU implements real-time commands via vehicle-to-infrastructure (V2I) communications. Experimental results show that the proposed method yields failure rates below 0.03\% coordinating up to three connected autonomous vehicles (CAVs), significantly outperforming traditional methods. In addition, the system exhibits sub-linear computational scaling with inference times under 40 ms. Furthermore, it demonstrates robust generalization across diverse unsignalized intersection scenarios, indicating its practicality and readiness for real-world deployment.
<div id='section'>Paperid: <span id='pid'>412, <a href='https://arxiv.org/pdf/2508.06269.pdf' target='_blank'>https://arxiv.org/pdf/2508.06269.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhuoran Li, Xun Wang, Hai Zhong, Longbo Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.06269">OM2P: Offline Multi-Agent Mean-Flow Policy</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generative models, especially diffusion and flow-based models, have been promising in offline multi-agent reinforcement learning. However, integrating powerful generative models into this framework poses unique challenges. In particular, diffusion and flow-based policies suffer from low sampling efficiency due to their iterative generation processes, making them impractical in time-sensitive or resource-constrained settings. To tackle these difficulties, we propose OM2P (Offline Multi-Agent Mean-Flow Policy), a novel offline MARL algorithm to achieve efficient one-step action sampling. To address the misalignment between generative objectives and reward maximization, we introduce a reward-aware optimization scheme that integrates a carefully-designed mean-flow matching loss with Q-function supervision. Additionally, we design a generalized timestep distribution and a derivative-free estimation strategy to reduce memory overhead and improve training stability. Empirical evaluations on Multi-Agent Particle and MuJoCo benchmarks demonstrate that OM2P achieves superior performance, with up to a 3.8x reduction in GPU memory usage and up to a 10.8x speed-up in training time. Our approach represents the first to successfully integrate mean-flow model into offline MARL, paving the way for practical and scalable generative policies in cooperative multi-agent settings.
<div id='section'>Paperid: <span id='pid'>413, <a href='https://arxiv.org/pdf/2505.04231.pdf' target='_blank'>https://arxiv.org/pdf/2505.04231.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Taoyuan Yu, Kui Wang, Zongdian Li, Tao Yu, Kei Sakaguchi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.04231">Multi-Agent Reinforcement Learning-based Cooperative Autonomous Driving in Smart Intersections</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Unsignalized intersections pose significant safety and efficiency challenges due to complex traffic flows. This paper proposes a novel roadside unit (RSU)-centric cooperative driving system leveraging global perception and vehicle-to-infrastructure (V2I) communication. The core of the system is an RSU-based decision-making module using a two-stage hybrid reinforcement learning (RL) framework. At first, policies are pre-trained offline using conservative Q-learning (CQL) combined with behavior cloning (BC) on collected dataset. Subsequently, these policies are fine-tuned in the simulation using multi-agent proximal policy optimization (MAPPO), aligned with a self-attention mechanism to effectively solve inter-agent dependencies. RSUs perform real-time inference based on the trained models to realize vehicle control via V2I communications. Extensive experiments in CARLA environment demonstrate high effectiveness of the proposed system, by: \textit{(i)} achieving failure rates below 0.03\% in coordinating three connected and autonomous vehicles (CAVs) through complex intersection scenarios, significantly outperforming the traditional Autoware control method, and \textit{(ii)} exhibiting strong robustness across varying numbers of controlled agents and shows promising generalization capabilities on other maps.
<div id='section'>Paperid: <span id='pid'>414, <a href='https://arxiv.org/pdf/2502.08985.pdf' target='_blank'>https://arxiv.org/pdf/2502.08985.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xun Wang, Zhuoran Li, Hai Zhong, Longbo Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.08985">Few is More: Task-Efficient Skill-Discovery for Multi-Task Offline Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As a data-driven approach, offline MARL learns superior policies solely from offline datasets, ideal for domains rich in historical data but with high interaction costs and risks. However, most existing methods are task-specific, requiring retraining for new tasks, leading to redundancy and inefficiency. To address this issue, in this paper, we propose a task-efficient multi-task offline MARL algorithm, Skill-Discovery Conservative Q-Learning (SD-CQL). Unlike existing offline skill-discovery methods, SD-CQL discovers skills by reconstructing the next observation. It then evaluates fixed and variable actions separately and employs behavior-regularized conservative Q-learning to execute the optimal action for each skill. This approach eliminates the need for local-global alignment and enables strong multi-task generalization from limited small-scale source tasks. Substantial experiments on StarCraftII demonstrates the superior generalization performance and task-efficiency of SD-CQL. It achieves the best performance on $\textbf{10}$ out of $14$ task sets, with up to $\textbf{65%}$ improvement on individual task sets, and is within $4\%$ of the best baseline on the remaining four.
<div id='section'>Paperid: <span id='pid'>415, <a href='https://arxiv.org/pdf/2501.17079.pdf' target='_blank'>https://arxiv.org/pdf/2501.17079.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Christian Fabian, Kai Cui, Heinz Koeppl
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.17079">Learning Mean Field Control on Sparse Graphs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large agent networks are abundant in applications and nature and pose difficult challenges in the field of multi-agent reinforcement learning (MARL) due to their computational and theoretical complexity. While graphon mean field games and their extensions provide efficient learning algorithms for dense and moderately sparse agent networks, the case of realistic sparser graphs remains largely unsolved. Thus, we propose a novel mean field control model inspired by local weak convergence to include sparse graphs such as power law networks with coefficients above two. Besides a theoretical analysis, we design scalable learning algorithms which apply to the challenging class of graph sequences with finite first moment. We compare our model and algorithms for various examples on synthetic and real world networks with mean field algorithms based on Lp graphons and graphexes. As it turns out, our approach outperforms existing methods in many examples and on various networks due to the special design aiming at an important, but so far hard to solve class of MARL problems.
<div id='section'>Paperid: <span id='pid'>416, <a href='https://arxiv.org/pdf/2411.01184.pdf' target='_blank'>https://arxiv.org/pdf/2411.01184.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chanjuan Liu, Jinmiao Cong, Bingcai Chen, Yaochu Jin, Enqiang Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.01184">Guiding Multi-agent Multi-task Reinforcement Learning by a Hierarchical Framework with Logical Reward Shaping</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent hierarchical reinforcement learning (MAHRL) has been studied as an effective means to solve intelligent decision problems in complex and large-scale environments. However, most current MAHRL algorithms follow the traditional way of using reward functions in reinforcement learning, which limits their use to a single task. This study aims to design a multi-agent cooperative algorithm with logic reward shaping (LRS), which uses a more flexible way of setting the rewards, allowing for the effective completion of multi-tasks. LRS uses Linear Temporal Logic (LTL) to express the internal logic relation of subtasks within a complex task. Then, it evaluates whether the subformulae of the LTL expressions are satisfied based on a designed reward structure. This helps agents to learn to effectively complete tasks by adhering to the LTL expressions, thus enhancing the interpretability and credibility of their decisions. To enhance coordination and cooperation among multiple agents, a value iteration technique is designed to evaluate the actions taken by each agent. Based on this evaluation, a reward function is shaped for coordination, which enables each agent to evaluate its status and complete the remaining subtasks through experiential learning. Experiments have been conducted on various types of tasks in the Minecraft-like environment. The results demonstrate that the proposed algorithm can improve the performance of multi-agents when learning to complete multi-tasks.
<div id='section'>Paperid: <span id='pid'>417, <a href='https://arxiv.org/pdf/2410.19450.pdf' target='_blank'>https://arxiv.org/pdf/2410.19450.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hai Zhong, Xun Wang, Zhuoran Li, Longbo Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.19450">Offline-to-Online Multi-Agent Reinforcement Learning with Offline Value Function Memory and Sequential Exploration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Offline-to-Online Reinforcement Learning has emerged as a powerful paradigm, leveraging offline data for initialization and online fine-tuning to enhance both sample efficiency and performance. However, most existing research has focused on single-agent settings, with limited exploration of the multi-agent extension, i.e., Offline-to-Online Multi-Agent Reinforcement Learning (O2O MARL). In O2O MARL, two critical challenges become more prominent as the number of agents increases: (i) the risk of unlearning pre-trained Q-values due to distributional shifts during the transition from offline-to-online phases, and (ii) the difficulty of efficient exploration in the large joint state-action space. To tackle these challenges, we propose a novel O2O MARL framework called Offline Value Function Memory with Sequential Exploration (OVMSE). First, we introduce the Offline Value Function Memory (OVM) mechanism to compute target Q-values, preserving knowledge gained during offline training, ensuring smoother transitions, and enabling efficient fine-tuning. Second, we propose a decentralized Sequential Exploration (SE) strategy tailored for O2O MARL, which effectively utilizes the pre-trained offline policy for exploration, thereby significantly reducing the joint state-action space to be explored. Extensive experiments on the StarCraft Multi-Agent Challenge (SMAC) demonstrate that OVMSE significantly outperforms existing baselines, achieving superior sample efficiency and overall performance.
<div id='section'>Paperid: <span id='pid'>418, <a href='https://arxiv.org/pdf/2409.19391.pdf' target='_blank'>https://arxiv.org/pdf/2409.19391.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pihe Hu, Shaolong Li, Zhuoran Li, Ling Pan, Longbo Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.19391">Value-Based Deep Multi-Agent Reinforcement Learning with Dynamic Sparse Training</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep Multi-agent Reinforcement Learning (MARL) relies on neural networks with numerous parameters in multi-agent scenarios, often incurring substantial computational overhead. Consequently, there is an urgent need to expedite training and enable model compression in MARL. This paper proposes the utilization of dynamic sparse training (DST), a technique proven effective in deep supervised learning tasks, to alleviate the computational burdens in MARL training. However, a direct adoption of DST fails to yield satisfactory MARL agents, leading to breakdowns in value learning within deep sparse value-based MARL models. Motivated by this challenge, we introduce an innovative Multi-Agent Sparse Training (MAST) framework aimed at simultaneously enhancing the reliability of learning targets and the rationality of sample distribution to improve value learning in sparse models. Specifically, MAST incorporates the Soft Mellowmax Operator with a hybrid TD-($Î»$) schema to establish dependable learning targets. Additionally, it employs a dual replay buffer mechanism to enhance the distribution of training samples. Building upon these aspects, MAST utilizes gradient-based topology evolution to exclusively train multiple MARL agents using sparse networks. Our comprehensive experimental investigation across various value-based MARL algorithms on multiple benchmarks demonstrates, for the first time, significant reductions in redundancy of up to $20\times$ in Floating Point Operations (FLOPs) for both training and inference, with less than $3\%$ performance degradation.
<div id='section'>Paperid: <span id='pid'>419, <a href='https://arxiv.org/pdf/2408.10556.pdf' target='_blank'>https://arxiv.org/pdf/2408.10556.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yun Qu, Boyuan Wang, Jianzhun Shao, Yuhang Jiang, Chen Chen, Zhenbin Ye, Lin Liu, Junfeng Yang, Lin Lai, Hongyang Qin, Minwen Deng, Juchao Zhuo, Deheng Ye, Qiang Fu, Wei Yang, Guang Yang, Lanxiao Huang, Xiangyang Ji
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.10556">Hokoff: Real Game Dataset from Honor of Kings and its Offline Reinforcement Learning Benchmarks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The advancement of Offline Reinforcement Learning (RL) and Offline Multi-Agent Reinforcement Learning (MARL) critically depends on the availability of high-quality, pre-collected offline datasets that represent real-world complexities and practical applications. However, existing datasets often fall short in their simplicity and lack of realism. To address this gap, we propose Hokoff, a comprehensive set of pre-collected datasets that covers both offline RL and offline MARL, accompanied by a robust framework, to facilitate further research. This data is derived from Honor of Kings, a recognized Multiplayer Online Battle Arena (MOBA) game known for its intricate nature, closely resembling real-life situations. Utilizing this framework, we benchmark a variety of offline RL and offline MARL algorithms. We also introduce a novel baseline algorithm tailored for the inherent hierarchical action space of the game. We reveal the incompetency of current offline RL approaches in handling task complexity, generalization and multi-task learning.
<div id='section'>Paperid: <span id='pid'>420, <a href='https://arxiv.org/pdf/2405.20018.pdf' target='_blank'>https://arxiv.org/pdf/2405.20018.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziyan Wang, Meng Fang, Tristan Tomilin, Fei Fang, Yali Du
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.20018">Safe Multi-agent Reinforcement Learning with Natural Language Constraints</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The role of natural language constraints in Safe Multi-agent Reinforcement Learning (MARL) is crucial, yet often overlooked. While Safe MARL has vast potential, especially in fields like robotics and autonomous vehicles, its full potential is limited by the need to define constraints in pre-designed mathematical terms, which requires extensive domain expertise and reinforcement learning knowledge, hindering its broader adoption. To address this limitation and make Safe MARL more accessible and adaptable, we propose a novel approach named Safe Multi-agent Reinforcement Learning with Natural Language constraints (SMALL). Our method leverages fine-tuned language models to interpret and process free-form textual constraints, converting them into semantic embeddings that capture the essence of prohibited states and behaviours. These embeddings are then integrated into the multi-agent policy learning process, enabling agents to learn policies that minimize constraint violations while optimizing rewards. To evaluate the effectiveness of SMALL, we introduce the LaMaSafe, a multi-task benchmark designed to assess the performance of multiple agents in adhering to natural language constraints. Empirical evaluations across various environments demonstrate that SMALL achieves comparable rewards and significantly fewer constraint violations, highlighting its effectiveness in understanding and enforcing natural language constraints.
<div id='section'>Paperid: <span id='pid'>421, <a href='https://arxiv.org/pdf/2405.02972.pdf' target='_blank'>https://arxiv.org/pdf/2405.02972.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Siyuan Li, Xi Lin, Hansong Xu, Kun Hua, Xiaomin Jin, Gaolei Li, Jianhua Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.02972">Multi-Agent RL-Based Industrial AIGC Service Offloading over Wireless Edge Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Currently, the generative model has garnered considerable attention due to its application in addressing the challenge of scarcity of abnormal samples in the industrial Internet of Things (IoT). However, challenges persist regarding the edge deployment of generative models and the optimization of joint edge AI-generated content (AIGC) tasks. In this paper, we focus on the edge optimization of AIGC task execution and propose GMEL, a generative model-driven industrial AIGC collaborative edge learning framework. This framework aims to facilitate efficient few-shot learning by leveraging realistic sample synthesis and edge-based optimization capabilities. First, a multi-task AIGC computational offloading model is presented to ensure the efficient execution of heterogeneous AIGC tasks on edge servers. Then, we propose an attention-enhanced multi-agent reinforcement learning (AMARL) algorithm aimed at refining offloading policies within the IoT system, thereby supporting generative model-driven edge learning. Finally, our experimental results demonstrate the effectiveness of the proposed algorithm in optimizing the total system latency of the edge-based AIGC task completion.
<div id='section'>Paperid: <span id='pid'>422, <a href='https://arxiv.org/pdf/2403.03172.pdf' target='_blank'>https://arxiv.org/pdf/2403.03172.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Liangzhou Wang, Kaiwen Zhu, Fengming Zhu, Xinghu Yao, Shujie Zhang, Deheng Ye, Haobo Fu, Qiang Fu, Wei Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.03172">Reaching Consensus in Cooperative Multi-Agent Reinforcement Learning with Goal Imagination</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reaching consensus is key to multi-agent coordination. To accomplish a cooperative task, agents need to coherently select optimal joint actions to maximize the team reward. However, current cooperative multi-agent reinforcement learning (MARL) methods usually do not explicitly take consensus into consideration, which may cause miscoordination problem. In this paper, we propose a model-based consensus mechanism to explicitly coordinate multiple agents. The proposed Multi-agent Goal Imagination (MAGI) framework guides agents to reach consensus with an Imagined common goal. The common goal is an achievable state with high value, which is obtained by sampling from the distribution of future states. We directly model this distribution with a self-supervised generative model, thus alleviating the "curse of dimensinality" problem induced by multi-agent multi-step policy rollout commonly used in model-based methods. We show that such efficient consensus mechanism can guide all agents cooperatively reaching valuable future states. Results on Multi-agent Particle-Environments and Google Research Football environment demonstrate the superiority of MAGI in both sample efficiency and performance.
<div id='section'>Paperid: <span id='pid'>423, <a href='https://arxiv.org/pdf/2401.13460.pdf' target='_blank'>https://arxiv.org/pdf/2401.13460.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mikayel Samvelyan, Davide Paglieri, Minqi Jiang, Jack Parker-Holder, Tim RocktÃ¤schel
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.13460">Multi-Agent Diagnostics for Robustness via Illuminated Diversity</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the rapidly advancing field of multi-agent systems, ensuring robustness in unfamiliar and adversarial settings is crucial. Notwithstanding their outstanding performance in familiar environments, these systems often falter in new situations due to overfitting during the training phase. This is especially pronounced in settings where both cooperative and competitive behaviours are present, encapsulating a dual nature of overfitting and generalisation challenges. To address this issue, we present Multi-Agent Diagnostics for Robustness via Illuminated Diversity (MADRID), a novel approach for generating diverse adversarial scenarios that expose strategic vulnerabilities in pre-trained multi-agent policies. Leveraging the concepts from open-ended learning, MADRID navigates the vast space of adversarial settings, employing a target policy's regret to gauge the vulnerabilities of these settings. We evaluate the effectiveness of MADRID on the 11vs11 version of Google Research Football, one of the most complex environments for multi-agent reinforcement learning. Specifically, we employ MADRID for generating a diverse array of adversarial settings for TiZero, the state-of-the-art approach which "masters" the game through 45 days of training on a large-scale distributed infrastructure. We expose key shortcomings in TiZero's tactical decision-making, underlining the crucial importance of rigorous evaluation in multi-agent systems.
<div id='section'>Paperid: <span id='pid'>424, <a href='https://arxiv.org/pdf/2401.12686.pdf' target='_blank'>https://arxiv.org/pdf/2401.12686.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Christian Fabian, Kai Cui, Heinz Koeppl
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.12686">Learning Mean Field Games on Sparse Graphs: A Hybrid Graphex Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Learning the behavior of large agent populations is an important task for numerous research areas. Although the field of multi-agent reinforcement learning (MARL) has made significant progress towards solving these systems, solutions for many agents often remain computationally infeasible and lack theoretical guarantees. Mean Field Games (MFGs) address both of these issues and can be extended to Graphon MFGs (GMFGs) to include network structures between agents. Despite their merits, the real world applicability of GMFGs is limited by the fact that graphons only capture dense graphs. Since most empirically observed networks show some degree of sparsity, such as power law graphs, the GMFG framework is insufficient for capturing these network topologies. Thus, we introduce the novel concept of Graphex MFGs (GXMFGs) which builds on the graph theoretical concept of graphexes. Graphexes are the limiting objects to sparse graph sequences that also have other desirable features such as the small world property. Learning equilibria in these games is challenging due to the rich and sparse structure of the underlying graphs. To tackle these challenges, we design a new learning algorithm tailored to the GXMFG setup. This hybrid graphex learning approach leverages that the system mainly consists of a highly connected core and a sparse periphery. After defining the system and providing a theoretical analysis, we state our learning approach and demonstrate its learning capabilities on both synthetic graphs and real-world networks. This comparison shows that our GXMFG learning algorithm successfully extends MFGs to a highly relevant class of hard, realistic learning problems that are not accurately addressed by current MARL and MFG methods.
<div id='section'>Paperid: <span id='pid'>425, <a href='https://arxiv.org/pdf/2312.12973.pdf' target='_blank'>https://arxiv.org/pdf/2312.12973.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Anam Tahir, Kai Cui, Heinz Koeppl
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.12973">Sparse Mean Field Load Balancing in Large Localized Queueing Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Scalable load balancing algorithms are of great interest in cloud networks and data centers, necessitating the use of tractable techniques to compute optimal load balancing policies for good performance. However, most existing scalable techniques, especially asymptotically scaling methods based on mean field theory, have not been able to model large queueing networks with strong locality. Meanwhile, general multi-agent reinforcement learning techniques can be hard to scale and usually lack a theoretical foundation. In this work, we address this challenge by leveraging recent advances in sparse mean field theory to learn a near-optimal load balancing policy in sparsely connected queueing networks in a tractable manner, which may be preferable to global approaches in terms of wireless communication overhead. Importantly, we obtain a general load balancing framework for a large class of sparse bounded-degree wireless topologies. By formulating a novel mean field control problem in the context of graphs with bounded degree, we reduce the otherwise difficult multi-agent problem to a single-agent problem. Theoretically, the approach is justified by approximation guarantees. Empirically, the proposed methodology performs well on several realistic and scalable wireless network topologies as compared to a number of well-known load balancing heuristics and existing scalable multi-agent reinforcement learning methods.
<div id='section'>Paperid: <span id='pid'>426, <a href='https://arxiv.org/pdf/2312.09539.pdf' target='_blank'>https://arxiv.org/pdf/2312.09539.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiao Du, Yutong Ye, Pengyu Zhang, Yaning Yang, Mingsong Chen, Ting Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.09539">Situation-Dependent Causal Influence-Based Cooperative Multi-agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Learning to collaborate has witnessed significant progress in multi-agent reinforcement learning (MARL). However, promoting coordination among agents and enhancing exploration capabilities remain challenges. In multi-agent environments, interactions between agents are limited in specific situations. Effective collaboration between agents thus requires a nuanced understanding of when and how agents' actions influence others. To this end, in this paper, we propose a novel MARL algorithm named Situation-Dependent Causal Influence-Based Cooperative Multi-agent Reinforcement Learning (SCIC), which incorporates a novel Intrinsic reward mechanism based on a new cooperation criterion measured by situation-dependent causal influence among agents. Our approach aims to detect inter-agent causal influences in specific situations based on the criterion using causal intervention and conditional mutual information. This effectively assists agents in exploring states that can positively impact other agents, thus promoting cooperation between agents. The resulting update links coordinated exploration and intrinsic reward distribution, which enhance overall collaboration and performance. Experimental results on various MARL benchmarks demonstrate the superiority of our method compared to state-of-the-art approaches.
<div id='section'>Paperid: <span id='pid'>427, <a href='https://arxiv.org/pdf/2312.05783.pdf' target='_blank'>https://arxiv.org/pdf/2312.05783.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kunyang Lin, Yufeng Wang, Peihao Chen, Runhao Zeng, Siyuan Zhou, Mingkui Tan, Chuang Gan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.05783">DCIR: Dynamic Consistency Intrinsic Reward for Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Learning optimal behavior policy for each agent in multi-agent systems is an essential yet difficult problem. Despite fruitful progress in multi-agent reinforcement learning, the challenge of addressing the dynamics of whether two agents should exhibit consistent behaviors is still under-explored. In this paper, we propose a new approach that enables agents to learn whether their behaviors should be consistent with that of other agents by utilizing intrinsic rewards to learn the optimal policy for each agent. We begin by defining behavior consistency as the divergence in output actions between two agents when provided with the same observation. Subsequently, we introduce dynamic consistency intrinsic reward (DCIR) to stimulate agents to be aware of others' behaviors and determine whether to be consistent with them. Lastly, we devise a dynamic scale network (DSN) that provides learnable scale factors for the agent at every time step to dynamically ascertain whether to award consistent behavior and the magnitude of rewards. We evaluate DCIR in multiple environments including Multi-agent Particle, Google Research Football and StarCraft II Micromanagement, demonstrating its efficacy.
<div id='section'>Paperid: <span id='pid'>428, <a href='https://arxiv.org/pdf/2312.03644.pdf' target='_blank'>https://arxiv.org/pdf/2312.03644.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziyan Wang, Yali Du, Yudi Zhang, Meng Fang, Biwei Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.03644">MACCA: Offline Multi-agent Reinforcement Learning with Causal Credit Assignment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Offline Multi-agent Reinforcement Learning (MARL) is valuable in scenarios where online interaction is impractical or risky. While independent learning in MARL offers flexibility and scalability, accurately assigning credit to individual agents in offline settings poses challenges because interactions with an environment are prohibited. In this paper, we propose a new framework, namely Multi-Agent Causal Credit Assignment (MACCA), to address credit assignment in the offline MARL setting. Our approach, MACCA, characterizing the generative process as a Dynamic Bayesian Network, captures relationships between environmental variables, states, actions, and rewards. Estimating this model on offline data, MACCA can learn each agent's contribution by analyzing the causal relationship of their individual rewards, ensuring accurate and interpretable credit assignment. Additionally, the modularity of our approach allows it to seamlessly integrate with various offline MARL methods. Theoretically, we proved that under the setting of the offline dataset, the underlying causal structure and the function for generating the individual rewards of agents are identifiable, which laid the foundation for the correctness of our modeling. In our experiments, we demonstrate that MACCA not only outperforms state-of-the-art methods but also enhances performance when integrated with other backbones.
<div id='section'>Paperid: <span id='pid'>429, <a href='https://arxiv.org/pdf/2311.00201.pdf' target='_blank'>https://arxiv.org/pdf/2311.00201.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tong Yang, Shicong Cen, Yuting Wei, Yuxin Chen, Yuejie Chi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.00201">Federated Natural Policy Gradient and Actor Critic Methods for Multi-task Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Federated reinforcement learning (RL) enables collaborative decision making of multiple distributed agents without sharing local data trajectories. In this work, we consider a multi-task setting, in which each agent has its own private reward function corresponding to different tasks, while sharing the same transition kernel of the environment. Focusing on infinite-horizon Markov decision processes, the goal is to learn a globally optimal policy that maximizes the sum of the discounted total rewards of all the agents in a decentralized manner, where each agent only communicates with its neighbors over some prescribed graph topology.
  We develop federated vanilla and entropy-regularized natural policy gradient (NPG) methods in the tabular setting under softmax parameterization, where gradient tracking is applied to estimate the global Q-function to mitigate the impact of imperfect information sharing. We establish non-asymptotic global convergence guarantees under exact policy evaluation, where the rates are nearly independent of the size of the state-action space and illuminate the impacts of network size and connectivity. To the best of our knowledge, this is the first time that near dimension-free global convergence is established for federated multi-task RL using policy optimization. We further go beyond the tabular setting by proposing a federated natural actor critic (NAC) method for multi-task RL with function approximation, and establish its finite-time sample complexity taking the errors of function approximation into account.
<div id='section'>Paperid: <span id='pid'>430, <a href='https://arxiv.org/pdf/2310.00871.pdf' target='_blank'>https://arxiv.org/pdf/2310.00871.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuyou Zhang, Yaru Niu, Xingyu Liu, Ding Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.00871">COMPOSER: Scalable and Robust Modular Policies for Snake Robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Snake robots have showcased remarkable compliance and adaptability in their interaction with environments, mirroring the traits of their natural counterparts. While their hyper-redundant and high-dimensional characteristics add to this adaptability, they also pose great challenges to robot control. Instead of perceiving the hyper-redundancy and flexibility of snake robots as mere challenges, there lies an unexplored potential in leveraging these traits to enhance robustness and generalizability at the control policy level. We seek to develop a control policy that effectively breaks down the high dimensionality of snake robots while harnessing their redundancy. In this work, we consider the snake robot as a modular robot and formulate the control of the snake robot as a cooperative Multi-Agent Reinforcement Learning (MARL) problem. Each segment of the snake robot functions as an individual agent. Specifically, we incorporate a self-attention mechanism to enhance the cooperative behavior between agents. A high-level imagination policy is proposed to provide additional rewards to guide the low-level control policy. We validate the proposed method COMPOSER with five snake robot tasks, including goal reaching, wall climbing, shape formation, tube crossing, and block pushing. COMPOSER achieves the highest success rate across all tasks when compared to a centralized baseline and four modular policy baselines. Additionally, we show enhanced robustness against module corruption and significantly superior zero-shot generalizability in our proposed method. The videos of this work are available on our project page: https://sites.google.com/view/composer-snake/.
<div id='section'>Paperid: <span id='pid'>431, <a href='https://arxiv.org/pdf/2309.12696.pdf' target='_blank'>https://arxiv.org/pdf/2309.12696.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianzhun Shao, Yun Qu, Chen Chen, Hongchang Zhang, Xiangyang Ji
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.12696">Counterfactual Conservative Q Learning for Offline Multi-agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Offline multi-agent reinforcement learning is challenging due to the coupling effect of both distribution shift issue common in offline setting and the high dimension issue common in multi-agent setting, making the action out-of-distribution (OOD) and value overestimation phenomenon excessively severe. Tomitigate this problem, we propose a novel multi-agent offline RL algorithm, named CounterFactual Conservative Q-Learning (CFCQL) to conduct conservative value estimation. Rather than regarding all the agents as a high dimensional single one and directly applying single agent methods to it, CFCQL calculates conservative regularization for each agent separately in a counterfactual way and then linearly combines them to realize an overall conservative value estimation. We prove that it still enjoys the underestimation property and the performance guarantee as those single agent conservative methods do, but the induced regularization and safe policy improvement bound are independent of the agent number, which is therefore theoretically superior to the direct treatment referred to above, especially when the agent number is large. We further conduct experiments on four environments including both discrete and continuous action settings on both existing and our man-made datasets, demonstrating that CFCQL outperforms existing methods on most datasets and even with a remarkable margin on some of them.
<div id='section'>Paperid: <span id='pid'>432, <a href='https://arxiv.org/pdf/2307.11620.pdf' target='_blank'>https://arxiv.org/pdf/2307.11620.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiangsen Wang, Haoran Xu, Yinan Zheng, Xianyuan Zhan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.11620">Offline Multi-Agent Reinforcement Learning with Implicit Global-to-Local Value Regularization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Offline reinforcement learning (RL) has received considerable attention in recent years due to its attractive capability of learning policies from offline datasets without environmental interactions. Despite some success in the single-agent setting, offline multi-agent RL (MARL) remains to be a challenge. The large joint state-action space and the coupled multi-agent behaviors pose extra complexities for offline policy optimization. Most existing offline MARL studies simply apply offline data-related regularizations on individual agents, without fully considering the multi-agent system at the global level. In this work, we present OMIGA, a new offline m ulti-agent RL algorithm with implicit global-to-local v alue regularization. OMIGA provides a principled framework to convert global-level value regularization into equivalent implicit local value regularizations and simultaneously enables in-sample learning, thus elegantly bridging multi-agent value decomposition and policy learning with offline regularizations. Based on comprehensive experiments on the offline multi-agent MuJoCo and StarCraft II micro-management tasks, we show that OMIGA achieves superior performance over the state-of-the-art offline MARL methods in almost all tasks.
<div id='section'>Paperid: <span id='pid'>433, <a href='https://arxiv.org/pdf/2307.01472.pdf' target='_blank'>https://arxiv.org/pdf/2307.01472.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhuoran Li, Ling Pan, Longbo Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.01472">Beyond Conservatism: Diffusion Policies in Offline Multi-agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a novel Diffusion Offline Multi-agent Model (DOM2) for offline Multi-Agent Reinforcement Learning (MARL). Different from existing algorithms that rely mainly on conservatism in policy design, DOM2 enhances policy expressiveness and diversity based on diffusion. Specifically, we incorporate a diffusion model into the policy network and propose a trajectory-based data-augmentation scheme in training. These key ingredients make our algorithm more robust to environment changes and achieve significant improvements in performance, generalization and data-efficiency. Our extensive experimental results demonstrate that DOM2 outperforms existing state-of-the-art methods in multi-agent particle and multi-agent MuJoCo environments, and generalizes significantly better in shifted environments thanks to its high expressiveness and diversity. Furthermore, DOM2 shows superior data efficiency and can achieve state-of-the-art performance with $20+$ times less data compared to existing algorithms.
<div id='section'>Paperid: <span id='pid'>434, <a href='https://arxiv.org/pdf/2304.08493.pdf' target='_blank'>https://arxiv.org/pdf/2304.08493.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chanyoung Park, Haemin Lee, Won Joon Yun, Soyi Jung, Joongheon Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.08493">Coordinated Multi-Agent Reinforcement Learning for Unmanned Aerial Vehicle Swarms in Autonomous Mobile Access Applications</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper proposes a novel centralized training and distributed execution (CTDE)-based multi-agent deep reinforcement learning (MADRL) method for multiple unmanned aerial vehicles (UAVs) control in autonomous mobile access applications. For the purpose, a single neural network is utilized in centralized training for cooperation among multiple agents while maximizing the total quality of service (QoS) in mobile access applications.
<div id='section'>Paperid: <span id='pid'>435, <a href='https://arxiv.org/pdf/2304.00755.pdf' target='_blank'>https://arxiv.org/pdf/2304.00755.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xianghua Zeng, Hao Peng, Angsheng Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.00755">Effective and Stable Role-Based Multi-Agent Collaboration by Structural Information Principles</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Role-based learning is a promising approach to improving the performance of Multi-Agent Reinforcement Learning (MARL). Nevertheless, without manual assistance, current role-based methods cannot guarantee stably discovering a set of roles to effectively decompose a complex task, as they assume either a predefined role structure or practical experience for selecting hyperparameters. In this article, we propose a mathematical Structural Information principles-based Role Discovery method, namely SIRD, and then present a SIRD optimizing MARL framework, namely SR-MARL, for multi-agent collaboration. The SIRD transforms role discovery into a hierarchical action space clustering. Specifically, the SIRD consists of structuralization, sparsification, and optimization modules, where an optimal encoding tree is generated to perform abstracting to discover roles. The SIRD is agnostic to specific MARL algorithms and flexibly integrated with various value function factorization approaches. Empirical evaluations on the StarCraft II micromanagement benchmark demonstrate that, compared with state-of-the-art MARL algorithms, the SR-MARL framework improves the average test win rate by 0.17%, 6.08%, and 3.24%, and reduces the deviation by 16.67%, 30.80%, and 66.30%, under easy, hard, and super hard scenarios.
<div id='section'>Paperid: <span id='pid'>436, <a href='https://arxiv.org/pdf/2303.10665.pdf' target='_blank'>https://arxiv.org/pdf/2303.10665.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kai Cui, Christian Fabian, Anam Tahir, Heinz Koeppl
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.10665">Major-Minor Mean Field Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent reinforcement learning (MARL) remains difficult to scale to many agents. Recent MARL using Mean Field Control (MFC) provides a tractable and rigorous approach to otherwise difficult cooperative MARL. However, the strict MFC assumption of many independent, weakly-interacting agents is too inflexible in practice. We generalize MFC to instead simultaneously model many similar and few complex agents -- as Major-Minor Mean Field Control (M3FC). Theoretically, we give approximation results for finite agent control, and verify the sufficiency of stationary policies for optimality together with a dynamic programming principle. Algorithmically, we propose Major-Minor Mean Field MARL (M3FMARL) for finite agent systems instead of the limiting system. The algorithm is shown to approximate the policy gradient of the underlying M3FC MDP. Finally, we demonstrate its capabilities experimentally in various scenarios. We observe a strong performance in comparison to state-of-the-art policy gradient MARL methods.
<div id='section'>Paperid: <span id='pid'>437, <a href='https://arxiv.org/pdf/2303.03376.pdf' target='_blank'>https://arxiv.org/pdf/2303.03376.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mikayel Samvelyan, Akbir Khan, Michael Dennis, Minqi Jiang, Jack Parker-Holder, Jakob Foerster, Roberta Raileanu, Tim RocktÃ¤schel
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.03376">MAESTRO: Open-Ended Environment Design for Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Open-ended learning methods that automatically generate a curriculum of increasingly challenging tasks serve as a promising avenue toward generally capable reinforcement learning agents. Existing methods adapt curricula independently over either environment parameters (in single-agent settings) or co-player policies (in multi-agent settings). However, the strengths and weaknesses of co-players can manifest themselves differently depending on environmental features. It is thus crucial to consider the dependency between the environment and co-player when shaping a curriculum in multi-agent domains. In this work, we use this insight and extend Unsupervised Environment Design (UED) to multi-agent environments. We then introduce Multi-Agent Environment Design Strategist for Open-Ended Learning (MAESTRO), the first multi-agent UED approach for two-player zero-sum settings. MAESTRO efficiently produces adversarial, joint curricula over both environments and co-players and attains minimax-regret guarantees at Nash equilibrium. Our experiments show that MAESTRO outperforms a number of strong baselines on competitive two-player games, spanning discrete and continuous control settings.
<div id='section'>Paperid: <span id='pid'>438, <a href='https://arxiv.org/pdf/2302.06872.pdf' target='_blank'>https://arxiv.org/pdf/2302.06872.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shanqi Liu, Yujing Hu, Runze Wu, Dong Xing, Yu Xiong, Changjie Fan, Kun Kuang, Yong Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.06872">Adaptive Value Decomposition with Greedy Marginal Contribution Computation for Cooperative Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Real-world cooperation often requires intensive coordination among agents simultaneously. This task has been extensively studied within the framework of cooperative multi-agent reinforcement learning (MARL), and value decomposition methods are among those cutting-edge solutions. However, traditional methods that learn the value function as a monotonic mixing of per-agent utilities cannot solve the tasks with non-monotonic returns. This hinders their application in generic scenarios. Recent methods tackle this problem from the perspective of implicit credit assignment by learning value functions with complete expressiveness or using additional structures to improve cooperation. However, they are either difficult to learn due to large joint action spaces or insufficient to capture the complicated interactions among agents which are essential to solving tasks with non-monotonic returns. To address these problems, we propose a novel explicit credit assignment method to address the non-monotonic problem. Our method, Adaptive Value decomposition with Greedy Marginal contribution (AVGM), is based on an adaptive value decomposition that learns the cooperative value of a group of dynamically changing agents. We first illustrate that the proposed value decomposition can consider the complicated interactions among agents and is feasible to learn in large-scale scenarios. Then, our method uses a greedy marginal contribution computed from the value decomposition as an individual credit to incentivize agents to learn the optimal cooperative policy. We further extend the module with an action encoder to guarantee the linear time complexity for computing the greedy marginal contribution. Experimental results demonstrate that our method achieves significant performance improvements in several non-monotonic domains.
<div id='section'>Paperid: <span id='pid'>439, <a href='https://arxiv.org/pdf/2301.04012.pdf' target='_blank'>https://arxiv.org/pdf/2301.04012.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Won Joon Yun, Jae Pyoung Kim, Soyi Jung, Jae-Hyun Kim, Joongheon Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.04012">Quantum Multi-Agent Actor-Critic Neural Networks for Internet-Connected Multi-Robot Coordination in Smart Factory Management</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As one of the latest fields of interest in both academia and industry, quantum computing has garnered significant attention. Among various topics in quantum computing, variational quantum circuits (VQC) have been noticed for their ability to carry out quantum deep reinforcement learning (QRL). This paper verifies the potential of QRL, which will be further realized by implementing quantum multi-agent reinforcement learning (QMARL) from QRL, especially for Internet-connected autonomous multi-robot control and coordination in smart factory applications. However, the extension is not straightforward due to the non-stationarity of classical MARL. To cope with this, the centralized training and decentralized execution (CTDE) QMARL framework is proposed under the Internet connection. A smart factory environment with the Internet of Things (IoT)-based multiple agents is used to show the efficacy of the proposed algorithm. The simulation corroborates that the proposed QMARL-based autonomous multi-robot control and coordination performs better than the other frameworks.
<div id='section'>Paperid: <span id='pid'>440, <a href='https://arxiv.org/pdf/2210.12566.pdf' target='_blank'>https://arxiv.org/pdf/2210.12566.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tim Seyde, Peter Werner, Wilko Schwarting, Igor Gilitschenski, Martin Riedmiller, Daniela Rus, Markus Wulfmeier
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2210.12566">Solving Continuous Control via Q-learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While there has been substantial success for solving continuous control with actor-critic methods, simpler critic-only methods such as Q-learning find limited application in the associated high-dimensional action spaces. However, most actor-critic methods come at the cost of added complexity: heuristics for stabilisation, compute requirements and wider hyperparameter search spaces. We show that a simple modification of deep Q-learning largely alleviates these issues. By combining bang-bang action discretization with value decomposition, framing single-agent control as cooperative multi-agent reinforcement learning (MARL), this simple critic-only approach matches performance of state-of-the-art continuous actor-critic methods when learning from features or pixels. We extend classical bandit examples from cooperative MARL to provide intuition for how decoupled critics leverage state information to coordinate joint optimization, and demonstrate surprisingly strong performance across a variety of continuous control tasks.
<div id='section'>Paperid: <span id='pid'>441, <a href='https://arxiv.org/pdf/2210.05367.pdf' target='_blank'>https://arxiv.org/pdf/2210.05367.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wubing Chen, Wenbin Li, Xiao Liu, Shangdong Yang, Yang Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2210.05367">Learning Explicit Credit Assignment for Cooperative Multi-Agent Reinforcement Learning via Polarization Policy Gradient</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cooperative multi-agent policy gradient (MAPG) algorithms have recently attracted wide attention and are regarded as a general scheme for the multi-agent system. Credit assignment plays an important role in MAPG and can induce cooperation among multiple agents. However, most MAPG algorithms cannot achieve good credit assignment because of the game-theoretic pathology known as \textit{centralized-decentralized mismatch}. To address this issue, this paper presents a novel method, \textit{\underline{M}ulti-\underline{A}gent \underline{P}olarization \underline{P}olicy \underline{G}radient} (MAPPG). MAPPG takes a simple but efficient polarization function to transform the optimal consistency of joint and individual actions into easily realized constraints, thus enabling efficient credit assignment in MAPG. Theoretically, we prove that individual policies of MAPPG can converge to the global optimum. Empirically, we evaluate MAPPG on the well-known matrix game and differential game, and verify that MAPPG can converge to the global optimum for both discrete and continuous action spaces. We also evaluate MAPPG on a set of StarCraft II micromanagement tasks and demonstrate that MAPPG outperforms the state-of-the-art MAPG algorithms.
<div id='section'>Paperid: <span id='pid'>442, <a href='https://arxiv.org/pdf/2209.07420.pdf' target='_blank'>https://arxiv.org/pdf/2209.07420.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kai Cui, Mengguang Li, Christian Fabian, Heinz Koeppl
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2209.07420">Scalable Task-Driven Robotic Swarm Control via Collision Avoidance and Learning Mean-Field Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, reinforcement learning and its multi-agent analogue have achieved great success in solving various complex control problems. However, multi-agent reinforcement learning remains challenging both in its theoretical analysis and empirical design of algorithms, especially for large swarms of embodied robotic agents where a definitive toolchain remains part of active research. We use emerging state-of-the-art mean-field control techniques in order to convert many-agent swarm control into more classical single-agent control of distributions. This allows profiting from advances in single-agent reinforcement learning at the cost of assuming weak interaction between agents. However, the mean-field model is violated by the nature of real systems with embodied, physically colliding agents. Thus, we combine collision avoidance and learning of mean-field control into a unified framework for tractably designing intelligent robotic swarm behavior. On the theoretical side, we provide novel approximation guarantees for general mean-field control both in continuous spaces and with collision avoidance. On the practical side, we show that our approach outperforms multi-agent reinforcement learning and allows for decentralized open-loop application while avoiding collisions, both in simulation and real UAV swarms. Overall, we propose a framework for the design of swarm behavior that is both mathematically well-founded and practically useful, enabling the solution of otherwise intractable swarm problems.
<div id='section'>Paperid: <span id='pid'>443, <a href='https://arxiv.org/pdf/2509.15103.pdf' target='_blank'>https://arxiv.org/pdf/2509.15103.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Simin Li, Zheng Yuwei, Zihao Mao, Linhao Wang, Ruixiao Xu, Chengdong Ma, Xin Yu, Yuqing Ma, Qi Dou, Xin Wang, Jie Luo, Bo An, Yaodong Yang, Weifeng Lv, Xianglong Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.15103">Vulnerable Agent Identification in Large-Scale Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Partial agent failure becomes inevitable when systems scale up, making it crucial to identify the subset of agents whose compromise would most severely degrade overall performance. In this paper, we study this Vulnerable Agent Identification (VAI) problem in large-scale multi-agent reinforcement learning (MARL). We frame VAI as a Hierarchical Adversarial Decentralized Mean Field Control (HAD-MFC), where the upper level involves an NP-hard combinatorial task of selecting the most vulnerable agents, and the lower level learns worst-case adversarial policies for these agents using mean-field MARL. The two problems are coupled together, making HAD-MFC difficult to solve. To solve this, we first decouple the hierarchical process by Fenchel-Rockafellar transform, resulting a regularized mean-field Bellman operator for upper level that enables independent learning at each level, thus reducing computational complexity. We then reformulate the upper-level combinatorial problem as a MDP with dense rewards from our regularized mean-field Bellman operator, enabling us to sequentially identify the most vulnerable agents by greedy and RL algorithms. This decomposition provably preserves the optimal solution of the original HAD-MFC. Experiments show our method effectively identifies more vulnerable agents in large-scale MARL and the rule-based system, fooling system into worse failures, and learns a value function that reveals the vulnerability of each agent.
<div id='section'>Paperid: <span id='pid'>444, <a href='https://arxiv.org/pdf/2509.15103.pdf' target='_blank'>https://arxiv.org/pdf/2509.15103.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Simin Li, Zheng Yuwei, Zihao Mao, Linhao Wang, Ruixiao Xu, Chengdong Ma, Xin Yu, Yuqing Ma, Qi Dou, Xin Wang, Jie Luo, Bo An, Yaodong Yang, Weifeng Lv, Xianglong Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.15103">Vulnerable Agent Identification in Large-Scale Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Partial agent failure becomes inevitable when systems scale up, making it crucial to identify the subset of agents whose compromise would most severely degrade overall performance. In this paper, we study this Vulnerable Agent Identification (VAI) problem in large-scale multi-agent reinforcement learning (MARL). We frame VAI as a Hierarchical Adversarial Decentralized Mean Field Control (HAD-MFC), where the upper level involves an NP-hard combinatorial task of selecting the most vulnerable agents, and the lower level learns worst-case adversarial policies for these agents using mean-field MARL. The two problems are coupled together, making HAD-MFC difficult to solve. To solve this, we first decouple the hierarchical process by Fenchel-Rockafellar transform, resulting a regularized mean-field Bellman operator for upper level that enables independent learning at each level, thus reducing computational complexity. We then reformulate the upper-level combinatorial problem as a MDP with dense rewards from our regularized mean-field Bellman operator, enabling us to sequentially identify the most vulnerable agents by greedy and RL algorithms. This decomposition provably preserves the optimal solution of the original HAD-MFC. Experiments show our method effectively identifies more vulnerable agents in large-scale MARL and the rule-based system, fooling system into worse failures, and learns a value function that reveals the vulnerability of each agent.
<div id='section'>Paperid: <span id='pid'>445, <a href='https://arxiv.org/pdf/2508.12480.pdf' target='_blank'>https://arxiv.org/pdf/2508.12480.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Constantin Ruhdorfer, Matteo Bortoletto, Andreas Bulling
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.12480">The Yokai Learning Environment: Tracking Beliefs Over Space and Time</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Developing collaborative AI hinges on Theory of Mind (ToM) - the ability to reason about the beliefs of others to build and maintain common ground. Existing ToM benchmarks, however, are restricted to passive observer settings or lack an assessment of how agents establish and maintain common ground over time. To address these gaps, we introduce the Yokai Learning Environment (YLE) - a multi-agent reinforcement learning (RL) environment based on the cooperative card game Yokai. In the YLE, agents take turns peeking at hidden cards and moving them to form clusters based on colour. Success requires tracking evolving beliefs, remembering past observations, using hints as grounded communication, and maintaining common ground with teammates. Our evaluation yields two key findings: First, current RL agents struggle to solve the YLE, even when given access to perfect memory. Second, while belief modelling improves performance, agents are still unable to effectively generalise to unseen partners or form accurate beliefs over longer games, exposing a reliance on brittle conventions rather than robust belief tracking. We use the YLE to investigate research questions in belief modelling, memory, partner generalisation, and scaling to higher-order ToM.
<div id='section'>Paperid: <span id='pid'>446, <a href='https://arxiv.org/pdf/2508.10340.pdf' target='_blank'>https://arxiv.org/pdf/2508.10340.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chak Lam Shek, Guangyao Shi, Pratap Tokekar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.10340">Multi-Agent Trust Region Policy Optimisation: A Joint Constraint Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent reinforcement learning (MARL) requires coordinated and stable policy updates among interacting agents. Heterogeneous-Agent Trust Region Policy Optimization (HATRPO) enforces per-agent trust region constraints using Kullback-Leibler (KL) divergence to stabilize training. However, assigning each agent the same KL threshold can lead to slow and locally optimal updates, especially in heterogeneous settings. To address this limitation, we propose two approaches for allocating the KL divergence threshold across agents: HATRPO-W, a Karush-Kuhn-Tucker-based (KKT-based) method that optimizes threshold assignment under global KL constraints, and HATRPO-G, a greedy algorithm that prioritizes agents based on improvement-to-divergence ratio. By connecting sequential policy optimization with constrained threshold scheduling, our approach enables more flexible and effective learning in heterogeneous-agent settings. Experimental results demonstrate that our methods significantly boost the performance of HATRPO, achieving faster convergence and higher final rewards across diverse MARL benchmarks. Specifically, HATRPO-W and HATRPO-G achieve comparable improvements in final performance, each exceeding 22.5%. Notably, HATRPO-W also demonstrates more stable learning dynamics, as reflected by its lower variance.
<div id='section'>Paperid: <span id='pid'>447, <a href='https://arxiv.org/pdf/2508.06336.pdf' target='_blank'>https://arxiv.org/pdf/2508.06336.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Constantin Ruhdorfer, Matteo Bortoletto, Victor Oei, Anna Penzkofer, Andreas Bulling
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.06336">Unsupervised Partner Design Enables Robust Ad-hoc Teamwork</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce Unsupervised Partner Design (UPD) - a population-free, multi-agent reinforcement learning framework for robust ad-hoc teamwork that adaptively generates training partners without requiring pretrained partners or manual parameter tuning. UPD constructs diverse partners by stochastically mixing an ego agent's policy with biased random behaviours and scores them using a variance-based learnability metric that prioritises partners near the ego agent's current learning frontier. We show that UPD can be integrated with unsupervised environment design, resulting in the first method enabling fully unsupervised curricula over both level and partner distributions in a cooperative setting. Through extensive evaluations on Overcooked-AI and the Overcooked Generalisation Challenge, we demonstrate that this dynamic partner curriculum is highly effective: UPD consistently outperforms both population-based and population-free baselines as well as ablations. In a user study, we further show that UPD achieves higher returns than all baselines and was perceived as significantly more adaptive, more human-like, a better collaborator, and less frustrating.
<div id='section'>Paperid: <span id='pid'>448, <a href='https://arxiv.org/pdf/2507.23604.pdf' target='_blank'>https://arxiv.org/pdf/2507.23604.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tommaso Marzi, Cesare Alippi, Andrea Cini
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.23604">Hierarchical Message-Passing Policies for Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Decentralized Multi-Agent Reinforcement Learning (MARL) methods allow for learning scalable multi-agent policies, but suffer from partial observability and induced non-stationarity. These challenges can be addressed by introducing mechanisms that facilitate coordination and high-level planning. Specifically, coordination and temporal abstraction can be achieved through communication (e.g., message passing) and Hierarchical Reinforcement Learning (HRL) approaches to decision-making. However, optimization issues limit the applicability of hierarchical policies to multi-agent systems. As such, the combination of these approaches has not been fully explored. To fill this void, we propose a novel and effective methodology for learning multi-agent hierarchies of message-passing policies. We adopt the feudal HRL framework and rely on a hierarchical graph structure for planning and coordination among agents. Agents at lower levels in the hierarchy receive goals from the upper levels and exchange messages with neighboring agents at the same level. To learn hierarchical multi-agent policies, we design a novel reward-assignment method based on training the lower-level policies to maximize the advantage function associated with the upper levels. Results on relevant benchmarks show that our method performs favorably compared to the state of the art.
<div id='section'>Paperid: <span id='pid'>449, <a href='https://arxiv.org/pdf/2506.07232.pdf' target='_blank'>https://arxiv.org/pdf/2506.07232.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinran Li, Chenjia Bai, Zijian Li, Jiakun Zheng, Ting Xiao, Jun Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.07232">Learn as Individuals, Evolve as a Team: Multi-agent LLMs Adaptation in Embodied Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large language models (LLMs) possess extensive knowledge bases and strong reasoning capabilities, making them promising tools for complex, multi-agent planning in embodied environments. However, despite LLMs' advanced abilities and the sophisticated modular design of agentic methods, existing LLM-based planning algorithms remain limited by weak adaptation capabilities to multi-agent embodied scenarios. We address this limitation by introducing a framework that enables LLM agents to learn and evolve both before and during test time, equipping them with environment-relevant knowledge for better planning and enhanced communication for improved cooperation. Inspired by centralized training with decentralized execution in multi-agent reinforcement learning, we propose a \textit{Learn as Individuals, Evolve as a Team (LIET)} paradigm for multi-agent LLMs adaptation. At the individual level, LLM agents learn a local utility function from exploratory datasets to better comprehend the embodied environment, which is then queried during test time to support informed decision-making. At the team level, LLM agents collaboratively and iteratively maintain and update a shared cooperation knowledge list based on new experiences, using it to guide more effective communication. By combining individual learning with team evolution, LIET enables comprehensive and flexible adaptation for LLM agents. Our experiments on Communicative Watch-And-Help and ThreeD-World Multi-Agent Transport benchmarks demonstrate that LIET, instantiated with both LLaMA and GPT-4o, outperforms existing baselines and exhibits strong cooperative planning abilities.
<div id='section'>Paperid: <span id='pid'>450, <a href='https://arxiv.org/pdf/2505.24618.pdf' target='_blank'>https://arxiv.org/pdf/2505.24618.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Victor Casamayor Pujol, Boris Sedlak, Tommaso Salvatori, Karl Friston, Schahram Dustdar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.24618">Distributed Intelligence in the Computing Continuum with Active Inference</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The Computing Continuum (CC) is an emerging Internet-based computing paradigm that spans from local Internet of Things sensors and constrained edge devices to large-scale cloud data centers. Its goal is to orchestrate a vast array of diverse and distributed computing resources to support the next generation of Internet-based applications. However, the distributed, heterogeneous, and dynamic nature of CC platforms demands distributed intelligence for adaptive and resilient service management. This article introduces a distributed stream processing pipeline as a CC use case, where each service is managed by an Active Inference (AIF) agent. These agents collaborate to fulfill service needs specified by SLOiDs, a term we introduce to denote Service Level Objectives that are aware of its deployed devices, meaning that non-functional requirements must consider the characteristics of the hosting device. We demonstrate how AIF agents can be modeled and deployed alongside distributed services to manage them autonomously. Our experiments show that AIF agents achieve over 90% SLOiD fulfillment when using tested transition models, and around 80% when learning the models during deployment. We compare their performance to a multi-agent reinforcement learning algorithm, finding that while both approaches yield similar results, MARL requires extensive training, whereas AIF agents can operate effectively from the start. Additionally, we evaluate the behavior of AIF agents in offloading scenarios, observing a strong capacity for adaptation. Finally, we outline key research directions to advance AIF integration in CC platforms.
<div id='section'>Paperid: <span id='pid'>451, <a href='https://arxiv.org/pdf/2505.06997.pdf' target='_blank'>https://arxiv.org/pdf/2505.06997.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenhao Lu, Zhengqiu Zhu, Yong Zhao, Yonglin Tian, Junjie Zeng, Jun Zhang, Zhong Liu, Fei-Yue Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.06997">A Multi-Agent Reinforcement Learning Approach for Cooperative Air-Ground-Human Crowdsensing in Emergency Rescue</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Mobile crowdsensing is evolving beyond traditional human-centric models by integrating heterogeneous entities like unmanned aerial vehicles (UAVs) and unmanned ground vehicles (UGVs). Optimizing task allocation among these diverse agents is critical, particularly in challenging emergency rescue scenarios characterized by complex environments, limited communication, and partial observability. This paper tackles the Heterogeneous-Entity Collaborative-Sensing Task Allocation (HECTA) problem specifically for emergency rescue, considering humans, UAVs, and UGVs. We introduce a novel ``Hard-Cooperative'' policy where UGVs prioritize recharging low-battery UAVs, alongside performing their sensing tasks. The primary objective is maximizing the task completion rate (TCR) under strict time constraints. We rigorously formulate this NP-hard problem as a decentralized partially observable Markov decision process (Dec-POMDP) to effectively handle sequential decision-making under uncertainty. To solve this, we propose HECTA4ER, a novel multi-agent reinforcement learning algorithm built upon a Centralized Training with Decentralized Execution architecture. HECTA4ER incorporates tailored designs, including specialized modules for complex feature extraction, utilization of action-observation history via hidden states, and a mixing network integrating global and local information, specifically addressing the challenges of partial observability. Furthermore, theoretical analysis confirms the algorithm's convergence properties. Extensive simulations demonstrate that HECTA4ER significantly outperforms baseline algorithms, achieving an average 18.42% increase in TCR. Crucially, a real-world case study validates the algorithm's effectiveness and robustness in dynamic sensing scenarios, highlighting its strong potential for practical application in emergency response.
<div id='section'>Paperid: <span id='pid'>452, <a href='https://arxiv.org/pdf/2505.05968.pdf' target='_blank'>https://arxiv.org/pdf/2505.05968.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dan Qiao, Wenhao Li, Shanchao Yang, Hongyuan Zha, Baoxiang Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.05968">Offline Multi-agent Reinforcement Learning via Score Decomposition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Offline cooperative multi-agent reinforcement learning (MARL) faces unique challenges due to distributional shifts, particularly stemming from the high dimensionality of joint action spaces and the presence of out-of-distribution joint action selections. In this work, we highlight that a fundamental challenge in offline MARL arises from the multi-equilibrium nature of cooperative tasks, which induces a highly multimodal joint behavior policy space coupled with heterogeneous-quality behavior data. This makes it difficult for individual policy regularization to align with a consistent coordination pattern, leading to the policy distribution shift problems. To tackle this challenge, we design a sequential score function decomposition method that distills per-agent regularization signals from the joint behavior policy, which induces coordinated modality selection under decentralized execution constraints. Then we leverage a flexible diffusion-based generative model to learn these score functions from multimodal offline data, and integrate them into joint-action critics to guide policy updates toward high-reward, in-distribution regions under a shared team reward. Our approach achieves state-of-the-art performance across multiple particle environments and Multi-agent MuJoCo benchmarks consistently. To the best of our knowledge, this is the first work to explicitly address the distributional gap between offline and online MARL, paving the way for more generalizable offline policy-based MARL methods.
<div id='section'>Paperid: <span id='pid'>453, <a href='https://arxiv.org/pdf/2501.10529.pdf' target='_blank'>https://arxiv.org/pdf/2501.10529.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sergio Rozada, Santiago Paternain, Juan Andres Bazerque, Antonio G. Marques
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.10529">A Tensor Low-Rank Approximation for Value Functions in Multi-Task Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In pursuit of reinforcement learning systems that could train in physical environments, we investigate multi-task approaches as a means to alleviate the need for massive data acquisition. In a tabular scenario where the Q-functions are collected across tasks, we model our learning problem as optimizing a higher order tensor structure. Recognizing that close-related tasks may require similar actions, our proposed method imposes a low-rank condition on this aggregated Q-tensor. The rationale behind this approach to multi-task learning is that the low-rank structure enforces the notion of similarity, without the need to explicitly prescribe which tasks are similar, but inferring this information from a reduced amount of data simultaneously with the stochastic optimization of the Q-tensor. The efficiency of our low-rank tensor approach to multi-task learning is demonstrated in two numerical experiments, first in a benchmark environment formed by a collection of inverted pendulums, and then into a practical scenario involving multiple wireless communication devices.
<div id='section'>Paperid: <span id='pid'>454, <a href='https://arxiv.org/pdf/2501.09429.pdf' target='_blank'>https://arxiv.org/pdf/2501.09429.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Benjamin Patrick Evans, Sihan Zeng, Sumitra Ganesh, Leo Ardon
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.09429">ADAGE: A generic two-layer framework for adaptive agent based modelling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Agent-based models (ABMs) are valuable for modelling complex, potentially out-of-equilibria scenarios. However, ABMs have long suffered from the Lucas critique, stating that agent behaviour should adapt to environmental changes. Furthermore, the environment itself often adapts to these behavioural changes, creating a complex bi-level adaptation problem. Recent progress integrating multi-agent reinforcement learning into ABMs introduces adaptive agent behaviour, beginning to address the first part of this critique, however, the approaches are still relatively ad hoc, lacking a general formulation, and furthermore, do not tackle the second aspect of simultaneously adapting environmental level characteristics in addition to the agent behaviours. In this work, we develop a generic two-layer framework for ADaptive AGEnt based modelling (ADAGE) for addressing these problems. This framework formalises the bi-level problem as a Stackelberg game with conditional behavioural policies, providing a consolidated framework for adaptive agent-based modelling based on solving a coupled set of non-linear equations. We demonstrate how this generic approach encapsulates several common (previously viewed as distinct) ABM tasks, such as policy design, calibration, scenario generation, and robust behavioural learning under one unified framework. We provide example simulations on multiple complex economic and financial environments, showing the strength of the novel framework under these canonical settings, addressing long-standing critiques of traditional ABMs.
<div id='section'>Paperid: <span id='pid'>455, <a href='https://arxiv.org/pdf/2411.01766.pdf' target='_blank'>https://arxiv.org/pdf/2411.01766.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Cheng Zhang, Lan Wei, Ji Fan, Zening Liu, Yongming Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.01766">Lyapunov-guided Multi-Agent Reinforcement Learning for Delay-Sensitive Wireless Scheduling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, a two-stage intelligent scheduler is proposed to minimize the packet-level delay jitter while guaranteeing delay bound. Firstly, Lyapunov technology is employed to transform the delay-violation constraint into a sequential slot-level queue stability problem. Secondly, a hierarchical scheme is proposed to solve the resource allocation between multiple base stations and users, where the multi-agent reinforcement learning (MARL) gives the user priority and the number of scheduled packets, while the underlying scheduler allocates the resource. Our proposed scheme achieves lower delay jitter and delay violation rate than the Round-Robin Earliest Deadline First algorithm and MARL with delay violation penalty.
<div id='section'>Paperid: <span id='pid'>456, <a href='https://arxiv.org/pdf/2410.16600.pdf' target='_blank'>https://arxiv.org/pdf/2410.16600.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ian Gemp, Andreas Haupt, Luke Marris, Siqi Liu, Georgios Piliouras
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.16600">Convex Markov Games: A New Frontier for Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Behavioral diversity, expert imitation, fairness, safety goals and others give rise to preferences in sequential decision making domains that do not decompose additively across time. We introduce the class of convex Markov games that allow general convex preferences over occupancy measures. Despite infinite time horizon and strictly higher generality than Markov games, pure strategy Nash equilibria exist. Furthermore, equilibria can be approximated empirically by performing gradient descent on an upper bound of exploitability. Our experiments reveal novel solutions to classic repeated normal-form games, find fair solutions in a repeated asymmetric coordination game, and prioritize safe long-term behavior in a robot warehouse environment. In the prisoner's dilemma, our algorithm leverages transient imitation to find a policy profile that deviates from observed human play only slightly, yet achieves higher per-player utility while also being three orders of magnitude less exploitable.
<div id='section'>Paperid: <span id='pid'>457, <a href='https://arxiv.org/pdf/2410.15841.pdf' target='_blank'>https://arxiv.org/pdf/2410.15841.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenzhe Fan, Zishun Yu, Chengdong Ma, Changye Li, Yaodong Yang, Xinhua Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.15841">Towards Efficient Collaboration via Graph Modeling in Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In multi-agent reinforcement learning, a commonly considered paradigm is centralized training with decentralized execution. However, in this framework, decentralized execution restricts the development of coordinated policies due to the local observation limitation. In this paper, we consider the cooperation among neighboring agents during execution and formulate their interactions as a graph. Thus, we introduce a novel encoder-decoder architecture named Factor-based Multi-Agent Transformer ($f$-MAT) that utilizes a transformer to enable communication between neighboring agents during both training and execution. By dividing agents into different overlapping groups and representing each group with a factor, $f$-MAT achieves efficient message passing and parallel action generation through factor-based attention layers. Empirical results in networked systems such as traffic scheduling and power control demonstrate that $f$-MAT achieves superior performance compared to strong baselines, thereby paving the way for handling complex collaborative problems.
<div id='section'>Paperid: <span id='pid'>458, <a href='https://arxiv.org/pdf/2408.13092.pdf' target='_blank'>https://arxiv.org/pdf/2408.13092.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jihwan Oh, Sungnyun Kim, Gahee Kim, Sunghwan Kim, Se-Young Yun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.13092">Diffusion-based Episodes Augmentation for Offline Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Offline multi-agent reinforcement learning (MARL) is increasingly recognized as crucial for effectively deploying RL algorithms in environments where real-time interaction is impractical, risky, or costly. In the offline setting, learning from a static dataset of past interactions allows for the development of robust and safe policies without the need for live data collection, which can be fraught with challenges. Building on this foundational importance, we present EAQ, Episodes Augmentation guided by Q-total loss, a novel approach for offline MARL framework utilizing diffusion models. EAQ integrates the Q-total function directly into the diffusion model as a guidance to maximize the global returns in an episode, eliminating the need for separate training. Our focus primarily lies on cooperative scenarios, where agents are required to act collectively towards achieving a shared goal-essentially, maximizing global returns. Consequently, we demonstrate that our episodes augmentation in a collaborative manner significantly boosts offline MARL algorithm compared to the original dataset, improving the normalized return by +17.3% and +12.9% for medium and poor behavioral policies in SMAC simulator, respectively.
<div id='section'>Paperid: <span id='pid'>459, <a href='https://arxiv.org/pdf/2408.12038.pdf' target='_blank'>https://arxiv.org/pdf/2408.12038.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kshama Dwarakanath, Svitlana Vyetrenko, Tucker Balch
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.12038">Empirical Equilibria in Agent-based Economic systems with Learning agents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present an agent-based simulator for economic systems with heterogeneous households, firms, central bank, and government agents. These agents interact to define production, consumption, and monetary flow. Each agent type has distinct objectives, such as households seeking utility from consumption and the central bank targeting inflation and production. We define this multi-agent economic system using an OpenAI Gym-style environment, enabling agents to optimize their objectives through reinforcement learning. Standard multi-agent reinforcement learning (MARL) schemes, like independent learning, enable agents to learn concurrently but do not address whether the resulting strategies are at equilibrium. This study integrates the Policy Space Response Oracle (PSRO) algorithm, which has shown superior performance over independent MARL in games with homogeneous agents, with economic agent-based modeling. We use PSRO to develop agent policies approximating Nash equilibria of the empirical economic game, thereby linking to economic equilibria. Our results demonstrate that PSRO strategies achieve lower regret values than independent MARL strategies in our economic system with four agent types. This work aims to bridge artificial intelligence, economics, and empirical game theory towards future research.
<div id='section'>Paperid: <span id='pid'>460, <a href='https://arxiv.org/pdf/2405.13205.pdf' target='_blank'>https://arxiv.org/pdf/2405.13205.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Amutheezan Sivagnanam, Ava Pettet, Hunter Lee, Ayan Mukhopadhyay, Abhishek Dubey, Aron Laszka
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.13205">Multi-Agent Reinforcement Learning with Hierarchical Coordination for Emergency Responder Stationing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>An emergency responder management (ERM) system dispatches responders, such as ambulances, when it receives requests for medical aid. ERM systems can also proactively reposition responders between predesignated waiting locations to cover any gaps that arise due to the prior dispatch of responders or significant changes in the distribution of anticipated requests. Optimal repositioning is computationally challenging due to the exponential number of ways to allocate responders between locations and the uncertainty in future requests. The state-of-the-art approach in proactive repositioning is a hierarchical approach based on spatial decomposition and online Monte Carlo tree search, which may require minutes of computation for each decision in a domain where seconds can save lives. We address the issue of long decision times by introducing a novel reinforcement learning (RL) approach, based on the same hierarchical decomposition, but replacing online search with learning. To address the computational challenges posed by large, variable-dimensional, and discrete state and action spaces, we propose: (1) actor-critic based agents that incorporate transformers to handle variable-dimensional states and actions, (2) projections to fixed-dimensional observations to handle complex states, and (3) combinatorial techniques to map continuous actions to discrete allocations. We evaluate our approach using real-world data from two U.S. cities, Nashville, TN and Seattle, WA. Our experiments show that compared to the state of the art, our approach reduces computation time per decision by three orders of magnitude, while also slightly reducing average ambulance response time by 5 seconds.
<div id='section'>Paperid: <span id='pid'>461, <a href='https://arxiv.org/pdf/2405.08036.pdf' target='_blank'>https://arxiv.org/pdf/2405.08036.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chang Huang, Shatong Zhu, Junqiao Zhao, Hongtu Zhou, Chen Ye, Tiantian Feng, Changjun Jiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.08036">POWQMIX: Weighted Value Factorization with Potentially Optimal Joint Actions Recognition for Cooperative Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Value function factorization methods are commonly used in cooperative multi-agent reinforcement learning, with QMIX receiving significant attention. Many QMIX-based methods introduce monotonicity constraints between the joint action value and individual action values to achieve decentralized execution. However, such constraints limit the representation capacity of value factorization, restricting the joint action values it can represent and hindering the learning of the optimal policy. To address this challenge, we propose the Potentially Optimal Joint Actions Weighted QMIX (POWQMIX) algorithm, which recognizes the potentially optimal joint actions and assigns higher weights to the corresponding losses of these joint actions during training. We theoretically prove that with such a weighted training approach the optimal policy is guaranteed to be recovered. Experiments in matrix games, difficulty-enhanced predator-prey, and StarCraft II Multi-Agent Challenge environments demonstrate that our algorithm outperforms the state-of-the-art value-based multi-agent reinforcement learning methods.
<div id='section'>Paperid: <span id='pid'>462, <a href='https://arxiv.org/pdf/2401.05563.pdf' target='_blank'>https://arxiv.org/pdf/2401.05563.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kshama Dwarakanath, Svitlana Vyetrenko, Toks Oyebode, Tucker Balch
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.05563">Transparency as Delayed Observability in Multi-Agent Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Is transparency always beneficial in complex systems such as traffic networks and stock markets? How is transparency defined in multi-agent systems, and what is its optimal degree at which social welfare is highest? We take an agent-based view to define transparency (or its lacking) as delay in agent observability of environment states, and utilize simulations to analyze the impact of delay on social welfare. To model the adaptation of agent strategies with varying delays, we model agents as learners maximizing the same objectives under different delays in a simulated environment. Focusing on two agent types - constrained and unconstrained, we use multi-agent reinforcement learning to evaluate the impact of delay on agent outcomes and social welfare. Empirical demonstration of our framework in simulated financial markets shows opposing trends in outcomes of the constrained and unconstrained agents with delay, with an optimal partial transparency regime at which social welfare is maximal.
<div id='section'>Paperid: <span id='pid'>463, <a href='https://arxiv.org/pdf/2310.13550.pdf' target='_blank'>https://arxiv.org/pdf/2310.13550.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruiquan Huang, Yuan Cheng, Jing Yang, Vincent Tan, Yingbin Liang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.13550">Provable Benefits of Multi-task RL under Non-Markovian Decision Making Processes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In multi-task reinforcement learning (RL) under Markov decision processes (MDPs), the presence of shared latent structures among multiple MDPs has been shown to yield significant benefits to the sample efficiency compared to single-task RL. In this paper, we investigate whether such a benefit can extend to more general sequential decision making problems, such as partially observable MDPs (POMDPs) and more general predictive state representations (PSRs). The main challenge here is that the large and complex model space makes it hard to identify what types of common latent structure of multi-task PSRs can reduce the model complexity and improve sample efficiency. To this end, we posit a joint model class for tasks and use the notion of $Î·$-bracketing number to quantify its complexity; this number also serves as a general metric to capture the similarity of tasks and thus determines the benefit of multi-task over single-task RL. We first study upstream multi-task learning over PSRs, in which all tasks share the same observation and action spaces. We propose a provably efficient algorithm UMT-PSR for finding near-optimal policies for all PSRs, and demonstrate that the advantage of multi-task learning manifests if the joint model class of PSRs has a smaller $Î·$-bracketing number compared to that of individual single-task learning. We also provide several example multi-task PSRs with small $Î·$-bracketing numbers, which reap the benefits of multi-task learning. We further investigate downstream learning, in which the agent needs to learn a new target task that shares some commonalities with the upstream tasks via a similarity constraint. By exploiting the learned PSRs from the upstream, we develop a sample-efficient algorithm that provably finds a near-optimal policy.
<div id='section'>Paperid: <span id='pid'>464, <a href='https://arxiv.org/pdf/2310.08746.pdf' target='_blank'>https://arxiv.org/pdf/2310.08746.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aakriti Agrawal, Rohith Aralikatti, Yanchao Sun, Furong Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.08746">Robustness to Multi-Modal Environment Uncertainty in MARL using Curriculum Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent reinforcement learning (MARL) plays a pivotal role in tackling real-world challenges. However, the seamless transition of trained policies from simulations to real-world requires it to be robust to various environmental uncertainties. Existing works focus on finding Nash Equilibrium or the optimal policy under uncertainty in one environment variable (i.e. action, state or reward). This is because a multi-agent system itself is highly complex and unstationary. However, in real-world situation uncertainty can occur in multiple environment variables simultaneously. This work is the first to formulate the generalised problem of robustness to multi-modal environment uncertainty in MARL. To this end, we propose a general robust training approach for multi-modal uncertainty based on curriculum learning techniques. We handle two distinct environmental uncertainty simultaneously and present extensive results across both cooperative and competitive MARL environments, demonstrating that our approach achieves state-of-the-art levels of robustness.
<div id='section'>Paperid: <span id='pid'>465, <a href='https://arxiv.org/pdf/2309.11057.pdf' target='_blank'>https://arxiv.org/pdf/2309.11057.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhili Zhang, H M Sabbir Ahmad, Ehsan Sabouni, Yanchao Sun, Furong Huang, Wenchao Li, Fei Miao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.11057">Safety Guaranteed Robust Multi-Agent Reinforcement Learning with Hierarchical Control for Connected and Automated Vehicles</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We address the problem of coordination and control of Connected and Automated Vehicles (CAVs) in the presence of imperfect observations in mixed traffic environment. A commonly used approach is learning-based decision-making, such as reinforcement learning (RL). However, most existing safe RL methods suffer from two limitations: (i) they assume accurate state information, and (ii) safety is generally defined over the expectation of the trajectories. It remains challenging to design optimal coordination between multi-agents while ensuring hard safety constraints under system state uncertainties (e.g., those that arise from noisy sensor measurements, communication, or state estimation methods) at every time step. We propose a safety guaranteed hierarchical coordination and control scheme called Safe-RMM to address the challenge. Specifically, the high-level coordination policy of CAVs in mixed traffic environment is trained by the Robust Multi-Agent Proximal Policy Optimization (RMAPPO) method. Though trained without uncertainty, our method leverages a worst-case Q network to ensure the model's robust performances when state uncertainties are present during testing. The low-level controller is implemented using model predictive control (MPC) with robust Control Barrier Functions (CBFs) to guarantee safety through their forward invariance property. We compare our method with baselines in different road networks in the CARLA simulator. Results show that our method provides best evaluated safety and efficiency in challenging mixed traffic environments with uncertainties.
<div id='section'>Paperid: <span id='pid'>466, <a href='https://arxiv.org/pdf/2307.14854.pdf' target='_blank'>https://arxiv.org/pdf/2307.14854.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lijun Sun, Yu-Cheng Chang, Chao Lyu, Chin-Teng Lin, Yuhui Shi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.14854">MatrixWorld: A pursuit-evasion platform for safe multi-agent coordination and autocurricula</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent reinforcement learning (MARL) achieves encouraging performance in solving complex tasks. However, the safety of MARL policies is one critical concern that impedes their real-world applications. Popular multi-agent benchmarks focus on diverse tasks yet provide limited safety support. Therefore, this work proposes a safety-constrained multi-agent environment: MatrixWorld, based on the general pursuit-evasion game. Particularly, a safety-constrained multi-agent action execution model is proposed for the software implementation of safe multi-agent environments based on diverse safety definitions. It (1) extends the vertex conflict among homogeneous / cooperative agents to heterogeneous / adversarial settings, and (2) proposes three types of resolutions for each type of conflict, aiming at providing rational and unbiased feedback for safe MARL. Besides, MatrixWorld is also a lightweight co-evolution framework for the learning of pursuit tasks, evasion tasks, or both, where more pursuit-evasion variants can be designed based on different practical meanings of safety. As a brief survey, we review and analyze the co-evolution mechanism in the multi-agent setting, which clearly reveals its relationships with autocurricula, self-play, arms races, and adversarial learning. Thus, MatrixWorld can also serve as the first environment for autocurricula research, where ideas can be quickly verified and well understood.
<div id='section'>Paperid: <span id='pid'>467, <a href='https://arxiv.org/pdf/2304.07520.pdf' target='_blank'>https://arxiv.org/pdf/2304.07520.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sirui Chen, Zhaowei Zhang, Yaodong Yang, Yali Du
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.07520">STAS: Spatial-Temporal Return Decomposition for Multi-agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Centralized Training with Decentralized Execution (CTDE) has been proven to be an effective paradigm in cooperative multi-agent reinforcement learning (MARL). One of the major challenges is credit assignment, which aims to credit agents by their contributions. While prior studies have shown great success, their methods typically fail to work in episodic reinforcement learning scenarios where global rewards are revealed only at the end of the episode. They lack the functionality to model complicated relations of the delayed global reward in the temporal dimension and suffer from inefficiencies. To tackle this, we introduce Spatial-Temporal Attention with Shapley (STAS), a novel method that learns credit assignment in both temporal and spatial dimensions. It first decomposes the global return back to each time step, then utilizes the Shapley Value to redistribute the individual payoff from the decomposed global reward. To mitigate the computational complexity of the Shapley Value, we introduce an approximation of marginal contribution and utilize Monte Carlo sampling to estimate it. We evaluate our method on an Alice & Bob example and MPE environments across different scenarios. Our results demonstrate that our method effectively assigns spatial-temporal credit, outperforming all state-of-the-art baselines.
<div id='section'>Paperid: <span id='pid'>468, <a href='https://arxiv.org/pdf/2302.00797.pdf' target='_blank'>https://arxiv.org/pdf/2302.00797.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zun Li, Marc Lanctot, Kevin R. McKee, Luke Marris, Ian Gemp, Daniel Hennes, Paul Muller, Kate Larson, Yoram Bachrach, Michael P. Wellman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.00797">Combining Deep Reinforcement Learning and Search with Generative Models for Game-Theoretic Opponent Modeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Opponent modeling methods typically involve two crucial steps: building a belief distribution over opponents' strategies, and exploiting this opponent model by playing a best response. However, existing approaches typically require domain-specific heurstics to come up with such a model, and algorithms for approximating best responses are hard to scale in large, imperfect information domains.
  In this work, we introduce a scalable and generic multiagent training regime for opponent modeling using deep game-theoretic reinforcement learning. We first propose Generative Best Respoonse (GenBR), a best response algorithm based on Monte-Carlo Tree Search (MCTS) with a learned deep generative model that samples world states during planning. This new method scales to large imperfect information domains and can be plug and play in a variety of multiagent algorithms. We use this new method under the framework of Policy Space Response Oracles (PSRO), to automate the generation of an \emph{offline opponent model} via iterative game-theoretic reasoning and population-based training. We propose using solution concepts based on bargaining theory to build up an opponent mixture, which we find identifying profiles that are near the Pareto frontier. Then GenBR keeps updating an \emph{online opponent model} and reacts against it during gameplay. We conduct behavioral studies where human participants negotiate with our agents in Deal-or-No-Deal, a class of bilateral bargaining games. Search with generative modeling finds stronger policies during both training time and test time, enables online Bayesian co-player prediction, and can produce agents that achieve comparable social welfare and Nash bargaining score negotiating with humans as humans trading among themselves.
<div id='section'>Paperid: <span id='pid'>469, <a href='https://arxiv.org/pdf/2210.06012.pdf' target='_blank'>https://arxiv.org/pdf/2210.06012.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Leo Ardon, Jared Vann, Deepeka Garg, Tom Spooner, Sumitra Ganesh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2210.06012">Phantom -- A RL-driven multi-agent framework to model complex systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Agent based modelling (ABM) is a computational approach to modelling complex systems by specifying the behaviour of autonomous decision-making components or agents in the system and allowing the system dynamics to emerge from their interactions. Recent advances in the field of Multi-agent reinforcement learning (MARL) have made it feasible to study the equilibrium of complex environments where multiple agents learn simultaneously. However, most ABM frameworks are not RL-native, in that they do not offer concepts and interfaces that are compatible with the use of MARL to learn agent behaviours. In this paper, we introduce a new open-source framework, Phantom, to bridge the gap between ABM and MARL. Phantom is an RL-driven framework for agent-based modelling of complex multi-agent systems including, but not limited to economic systems and markets. The framework aims to provide the tools to simplify the ABM specification in a MARL-compatible way - including features to encode dynamic partial observability, agent utility functions, heterogeneity in agent preferences or types, and constraints on the order in which agents can act (e.g. Stackelberg games, or more complex turn-taking environments). In this paper, we present these features, their design rationale and present two new environments leveraging the framework.
<div id='section'>Paperid: <span id='pid'>470, <a href='https://arxiv.org/pdf/2111.06318.pdf' target='_blank'>https://arxiv.org/pdf/2111.06318.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wei Zhou, Dong Chen, Jun Yan, Zhaojian Li, Huilin Yin, Wanchen Ge
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2111.06318">Multi-agent Reinforcement Learning for Cooperative Lane Changing of Connected and Autonomous Vehicles in Mixed Traffic</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autonomous driving has attracted significant research interests in the past two decades as it offers many potential benefits, including releasing drivers from exhausting driving and mitigating traffic congestion, among others. Despite promising progress, lane-changing remains a great challenge for autonomous vehicles (AV), especially in mixed and dynamic traffic scenarios. Recently, reinforcement learning (RL), a powerful data-driven control method, has been widely explored for lane-changing decision makings in AVs with encouraging results demonstrated. However, the majority of those studies are focused on a single-vehicle setting, and lane-changing in the context of multiple AVs coexisting with human-driven vehicles (HDVs) have received scarce attention. In this paper, we formulate the lane-changing decision making of multiple AVs in a mixed-traffic highway environment as a multi-agent reinforcement learning (MARL) problem, where each AV makes lane-changing decisions based on the motions of both neighboring AVs and HDVs. Specifically, a multi-agent advantage actor-critic network (MA2C) is developed with a novel local reward design and a parameter sharing scheme. In particular, a multi-objective reward function is proposed to incorporate fuel efficiency, driving comfort, and safety of autonomous driving. Comprehensive experimental results, conducted under three different traffic densities and various levels of human driver aggressiveness, show that our proposed MARL framework consistently outperforms several state-of-the-art benchmarks in terms of efficiency, safety and driver comfort.
<div id='section'>Paperid: <span id='pid'>471, <a href='https://arxiv.org/pdf/2509.24226.pdf' target='_blank'>https://arxiv.org/pdf/2509.24226.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jingqi Li, Gechen Qu, Jason J. Choi, Somayeh Sojoudi, Claire Tomlin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.24226">Multi-Agent Guided Policy Search for Non-Cooperative Dynamic Games</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent reinforcement learning (MARL) optimizes strategic interactions in non-cooperative dynamic games, where agents have misaligned objectives. However, data-driven methods such as multi-agent policy gradients (MA-PG) often suffer from instability and limit-cycle behaviors. Prior stabilization techniques typically rely on entropy-based exploration, which slows learning and increases variance. We propose a model-based approach that incorporates approximate priors into the reward function as regularization. In linear quadratic (LQ) games, we prove that such priors stabilize policy gradients and guarantee local exponential convergence to an approximate Nash equilibrium. We then extend this idea to infinite-horizon nonlinear games by introducing Multi-agent Guided Policy Search (MA-GPS), which constructs short-horizon local LQ approximations from trajectories of current policies to guide training. Experiments on nonlinear vehicle platooning and a six-player strategic basketball formation show that MA-GPS achieves faster convergence and more stable learning than existing MARL methods.
<div id='section'>Paperid: <span id='pid'>472, <a href='https://arxiv.org/pdf/2509.24226.pdf' target='_blank'>https://arxiv.org/pdf/2509.24226.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jingqi Li, Gechen Qu, Jason J. Choi, Somayeh Sojoudi, Claire Tomlin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.24226">Multi-Agent Guided Policy Search for Non-Cooperative Dynamic Games</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent reinforcement learning (MARL) optimizes strategic interactions in non-cooperative dynamic games, where agents have misaligned objectives. However, data-driven methods such as multi-agent policy gradients (MA-PG) often suffer from instability and limit-cycle behaviors. Prior stabilization techniques typically rely on entropy-based exploration, which slows learning and increases variance. We propose a model-based approach that incorporates approximate priors into the reward function as regularization. In linear quadratic (LQ) games, we prove that such priors stabilize policy gradients and guarantee local exponential convergence to an approximate Nash equilibrium. We then extend this idea to infinite-horizon nonlinear games by introducing Multi-agent Guided Policy Search (MA-GPS), which constructs short-horizon local LQ approximations from trajectories of current policies to guide training. Experiments on nonlinear vehicle platooning and a six-player strategic basketball formation show that MA-GPS achieves faster convergence and more stable learning than existing MARL methods.
<div id='section'>Paperid: <span id='pid'>473, <a href='https://arxiv.org/pdf/2509.24047.pdf' target='_blank'>https://arxiv.org/pdf/2509.24047.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Runyu Zhang, Na Li, Asuman Ozdaglar, Jeff Shamma, Gioele Zardini
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.24047">Optimism as Risk-Seeking in Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Risk sensitivity has become a central theme in reinforcement learning (RL), where convex risk measures and robust formulations provide principled ways to model preferences beyond expected return. Recent extensions to multi-agent RL (MARL) have largely emphasized the risk-averse setting, prioritizing robustness to uncertainty. In cooperative MARL, however, such conservatism often leads to suboptimal equilibria, and a parallel line of work has shown that optimism can promote cooperation. Existing optimistic methods, though effective in practice, are typically heuristic and lack theoretical grounding. Building on the dual representation for convex risk measures, we propose a principled framework that interprets risk-seeking objectives as optimism. We introduce optimistic value functions, which formalize optimism as divergence-penalized risk-seeking evaluations. Building on this foundation, we derive a policy-gradient theorem for optimistic value functions, including explicit formulas for the entropic risk/KL-penalty setting, and develop decentralized optimistic actor-critic algorithms that implement these updates. Empirical results on cooperative benchmarks demonstrate that risk-seeking optimism consistently improves coordination over both risk-neutral baselines and heuristic optimistic methods. Our framework thus unifies risk-sensitive learning and optimism, offering a theoretically grounded and practically effective approach to cooperation in MARL.
<div id='section'>Paperid: <span id='pid'>474, <a href='https://arxiv.org/pdf/2509.24047.pdf' target='_blank'>https://arxiv.org/pdf/2509.24047.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Runyu Zhang, Na Li, Asuman Ozdaglar, Jeff Shamma, Gioele Zardini
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.24047">Optimism as Risk-Seeking in Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Risk sensitivity has become a central theme in reinforcement learning (RL), where convex risk measures and robust formulations provide principled ways to model preferences beyond expected return. Recent extensions to multi-agent RL (MARL) have largely emphasized the risk-averse setting, prioritizing robustness to uncertainty. In cooperative MARL, however, such conservatism often leads to suboptimal equilibria, and a parallel line of work has shown that optimism can promote cooperation. Existing optimistic methods, though effective in practice, are typically heuristic and lack theoretical grounding. Building on the dual representation for convex risk measures, we propose a principled framework that interprets risk-seeking objectives as optimism. We introduce optimistic value functions, which formalize optimism as divergence-penalized risk-seeking evaluations. Building on this foundation, we derive a policy-gradient theorem for optimistic value functions, including explicit formulas for the entropic risk/KL-penalty setting, and develop decentralized optimistic actor-critic algorithms that implement these updates. Empirical results on cooperative benchmarks demonstrate that risk-seeking optimism consistently improves coordination over both risk-neutral baselines and heuristic optimistic methods. Our framework thus unifies risk-sensitive learning and optimism, offering a theoretically grounded and practically effective approach to cooperation in MARL.
<div id='section'>Paperid: <span id='pid'>475, <a href='https://arxiv.org/pdf/2509.18545.pdf' target='_blank'>https://arxiv.org/pdf/2509.18545.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ioannis Panitsas, Tolga O. Atalay, Dragoslav Stojadinovic, Angelos Stavrou, Leandros Tassiulas
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.18545">Accelerating Network Slice Placement with Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cellular networks are increasingly realized through software-based entities, with core functions deployed as Virtual Network Functions (VNFs) on Commercial-off-the-Shelf (COTS) hardware. Network slicing has emerged as a key enabler of 5G by providing logically isolated Quality of Service (QoS) guarantees for diverse applications. With the adoption of cloud-native infrastructures, the placement of network slices across heterogeneous multi-cloud environments poses new challenges due to variable resource capabilities and slice-specific requirements. This paper introduces a modular framework for autonomous and near-optimal VNF placement based on a disaggregated Multi-Agent Reinforcement Learning (MARL) approach. The framework incorporates real traffic profiles to estimate slice resource demands and employs a MARL-based scheduler to minimize deployment cost while meeting QoS constraints. Experimental evaluation on a multi-cloud testbed shows a 19x speed-up compared to combinatorial optimization, with deployment costs within 7.8% of the optimal. While the method incurs up to 2.42x more QoS violations under high load, the trade-off provides significantly faster decision-making and reduced computational complexity. These results suggest that MARL-based approaches offer a scalable and cost-efficient solution for real-time network slice placement in heterogeneous infrastructures.
<div id='section'>Paperid: <span id='pid'>476, <a href='https://arxiv.org/pdf/2509.18545.pdf' target='_blank'>https://arxiv.org/pdf/2509.18545.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ioannis Panitsas, Tolga O. Atalay, Dragoslav Stojadinovic, Angelos Stavrou, Leandros Tassiulas
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.18545">Accelerating Network Slice Placement with Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cellular networks are increasingly realized through software-based entities, with core functions deployed as Virtual Network Functions (VNFs) on Commercial-off-the-Shelf (COTS) hardware. Network slicing has emerged as a key enabler of 5G by providing logically isolated Quality of Service (QoS) guarantees for diverse applications. With the adoption of cloud-native infrastructures, the placement of network slices across heterogeneous multi-cloud environments poses new challenges due to variable resource capabilities and slice-specific requirements. This paper introduces a modular framework for autonomous and near-optimal VNF placement based on a disaggregated Multi-Agent Reinforcement Learning (MARL) approach. The framework incorporates real traffic profiles to estimate slice resource demands and employs a MARL-based scheduler to minimize deployment cost while meeting QoS constraints. Experimental evaluation on a multi-cloud testbed shows a 19x speed-up compared to combinatorial optimization, with deployment costs within 7.8% of the optimal. While the method incurs up to 2.42x more QoS violations under high load, the trade-off provides significantly faster decision-making and reduced computational complexity. These results suggest that MARL-based approaches offer a scalable and cost-efficient solution for real-time network slice placement in heterogeneous infrastructures.
<div id='section'>Paperid: <span id='pid'>477, <a href='https://arxiv.org/pdf/2508.21066.pdf' target='_blank'>https://arxiv.org/pdf/2508.21066.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuan Gong, Xionghui Wang, Jie Wu, Shiyin Wang, Yitong Wang, Xinglong Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.21066">OneReward: Unified Mask-Guided Image Generation via Multi-Task Human Preference Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we introduce OneReward, a unified reinforcement learning framework that enhances the model's generative capabilities across multiple tasks under different evaluation criteria using only \textit{One Reward} model. By employing a single vision-language model (VLM) as the generative reward model, which can distinguish the winner and loser for a given task and a given evaluation criterion, it can be effectively applied to multi-task generation models, particularly in contexts with varied data and diverse task objectives. We utilize OneReward for mask-guided image generation, which can be further divided into several sub-tasks such as image fill, image extend, object removal, and text rendering, involving a binary mask as the edit area. Although these domain-specific tasks share same conditioning paradigm, they differ significantly in underlying data distributions and evaluation metrics. Existing methods often rely on task-specific supervised fine-tuning (SFT), which limits generalization and training efficiency. Building on OneReward, we develop Seedream 3.0 Fill, a mask-guided generation model trained via multi-task reinforcement learning directly on a pre-trained base model, eliminating the need for task-specific SFT. Experimental results demonstrate that our unified edit model consistently outperforms both commercial and open-source competitors, such as Ideogram, Adobe Photoshop, and FLUX Fill [Pro], across multiple evaluation dimensions. Code and model are available at: https://one-reward.github.io
<div id='section'>Paperid: <span id='pid'>478, <a href='https://arxiv.org/pdf/2507.09495.pdf' target='_blank'>https://arxiv.org/pdf/2507.09495.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hang Wang, Junshan Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.09495">GenAI-based Multi-Agent Reinforcement Learning towards Distributed Agent Intelligence: A Generative-RL Agent Perspective</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent reinforcement learning faces fundamental challenges that conventional approaches have failed to overcome: exponentially growing joint action spaces, non-stationary environments where simultaneous learning creates moving targets, and partial observability that constrains coordination. Current methods remain reactive, employing stimulus-response mechanisms that fail when facing novel scenarios. We argue for a transformative paradigm shift from reactive to proactive multi-agent intelligence through generative AI-based reinforcement learning. This position advocates reconceptualizing agents not as isolated policy optimizers, but as sophisticated generative models capable of synthesizing complex multi-agent dynamics and making anticipatory decisions based on predictive understanding of future interactions. Rather than responding to immediate observations, generative-RL agents can model environment evolution, predict other agents' behaviors, generate coordinated action sequences, and engage in strategic reasoning accounting for long-term dynamics. This approach leverages pattern recognition and generation capabilities of generative AI to enable proactive decision-making, seamless coordination through enhanced communication, and dynamic adaptation to evolving scenarios. We envision this paradigm shift will unlock unprecedented possibilities for distributed intelligence, moving beyond individual optimization toward emergent collective behaviors representing genuine collaborative intelligence. The implications extend across autonomous systems, robotics, and human-AI collaboration, promising solutions to coordination challenges intractable under traditional reactive frameworks.
<div id='section'>Paperid: <span id='pid'>479, <a href='https://arxiv.org/pdf/2506.08149.pdf' target='_blank'>https://arxiv.org/pdf/2506.08149.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hang Wang, Dechen Gao, Junshan Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.08149">Ego-centric Learning of Communicative World Models for Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We study multi-agent reinforcement learning (MARL) for tasks in complex high-dimensional environments, such as autonomous driving. MARL is known to suffer from the \textit{partial observability} and \textit{non-stationarity} issues. To tackle these challenges, information sharing is often employed, which however faces major hurdles in practice, including overwhelming communication overhead and scalability concerns. By making use of generative AI embodied in world model together with its latent representation, we develop {\it CALL}, \underline{C}ommunic\underline{a}tive Wor\underline{l}d Mode\underline{l}, for MARL, where 1) each agent first learns its world model that encodes its state and intention into low-dimensional latent representation with smaller memory footprint, which can be shared with other agents of interest via lightweight communication; and 2) each agent carries out ego-centric learning while exploiting lightweight information sharing to enrich her world model, and then exploits its generalization capacity to improve prediction for better planning. We characterize the gain on the prediction accuracy from the information sharing and its impact on performance gap. Extensive experiments are carried out on the challenging local trajectory planning tasks in the CARLA platform to demonstrate the performance gains of using \textit{CALL}.
<div id='section'>Paperid: <span id='pid'>480, <a href='https://arxiv.org/pdf/2505.20922.pdf' target='_blank'>https://arxiv.org/pdf/2505.20922.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yang Zhang, Xinran Li, Jianing Ye, Delin Qu, Shuang Qiu, Chongjie Zhang, Xiu Li, Chenjia Bai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.20922">Revisiting Multi-Agent World Modeling from a Diffusion-Inspired Perspective</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>World models have recently attracted growing interest in Multi-Agent Reinforcement Learning (MARL) due to their ability to improve sample efficiency for policy learning. However, accurately modeling environments in MARL is challenging due to the exponentially large joint action space and highly uncertain dynamics inherent in multi-agent systems. To address this, we reduce modeling complexity by shifting from jointly modeling the entire state-action transition dynamics to focusing on the state space alone at each timestep through sequential agent modeling. Specifically, our approach enables the model to progressively resolve uncertainty while capturing the structured dependencies among agents, providing a more accurate representation of how agents influence the state. Interestingly, this sequential revelation of agents' actions in a multi-agent system aligns with the reverse process in diffusion models--a class of powerful generative models known for their expressiveness and training stability compared to autoregressive or latent variable models. Leveraging this insight, we develop a flexible and robust world model for MARL using diffusion models. Our method, Diffusion-Inspired Multi-Agent world model (DIMA), achieves state-of-the-art performance across multiple multi-agent control benchmarks, significantly outperforming prior world models in terms of final return and sample efficiency, including MAMuJoCo and Bi-DexHands. DIMA establishes a new paradigm for constructing multi-agent world models, advancing the frontier of MARL research.
<div id='section'>Paperid: <span id='pid'>481, <a href='https://arxiv.org/pdf/2504.15425.pdf' target='_blank'>https://arxiv.org/pdf/2504.15425.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Songyuan Zhang, Oswin So, Mitchell Black, Zachary Serlin, Chuchu Fan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.15425">Solving Multi-Agent Safe Optimal Control with Distributed Epigraph Form MARL</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Tasks for multi-robot systems often require the robots to collaborate and complete a team goal while maintaining safety. This problem is usually formalized as a constrained Markov decision process (CMDP), which targets minimizing a global cost and bringing the mean of constraint violation below a user-defined threshold. Inspired by real-world robotic applications, we define safety as zero constraint violation. While many safe multi-agent reinforcement learning (MARL) algorithms have been proposed to solve CMDPs, these algorithms suffer from unstable training in this setting. To tackle this, we use the epigraph form for constrained optimization to improve training stability and prove that the centralized epigraph form problem can be solved in a distributed fashion by each agent. This results in a novel centralized training distributed execution MARL algorithm named Def-MARL. Simulation experiments on 8 different tasks across 2 different simulators show that Def-MARL achieves the best overall performance, satisfies safety constraints, and maintains stable training. Real-world hardware experiments on Crazyflie quadcopters demonstrate the ability of Def-MARL to safely coordinate agents to complete complex collaborative tasks compared to other methods.
<div id='section'>Paperid: <span id='pid'>482, <a href='https://arxiv.org/pdf/2501.10116.pdf' target='_blank'>https://arxiv.org/pdf/2501.10116.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zifeng Shi, Meiqin Liu, Senlin Zhang, Ronghao Zheng, Shanling Dong, Ping Wei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.10116">GAWM: Global-Aware World Model for Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, Model-based Multi-Agent Reinforcement Learning (MARL) has demonstrated significant advantages over model-free methods in terms of sample efficiency by using independent environment dynamics world models for data sample augmentation. However, without considering the limited sample size, these methods still lag behind model-free methods in terms of final convergence performance and stability. This is primarily due to the world model's insufficient and unstable representation of global states in partially observable environments. This limitation hampers the ability to ensure global consistency in the data samples and results in a time-varying and unstable distribution mismatch between the pseudo data samples generated by the world model and the real samples. This issue becomes particularly pronounced in more complex multi-agent environments. To address this challenge, we propose a model-based MARL method called GAWM, which enhances the centralized world model's ability to achieve globally unified and accurate representation of state information while adhering to the CTDE paradigm. GAWM uniquely leverages an additional Transformer architecture to fuse local observation information from different agents, thereby improving its ability to extract and represent global state information. This enhancement not only improves sample efficiency but also enhances training stability, leading to superior convergence performance, particularly in complex and challenging multi-agent environments. This advancement enables model-based methods to be effectively applied to more complex multi-agent environments. Experimental results demonstrate that GAWM outperforms various model-free and model-based approaches, achieving exceptional performance in the challenging domains of SMAC.
<div id='section'>Paperid: <span id='pid'>483, <a href='https://arxiv.org/pdf/2501.00211.pdf' target='_blank'>https://arxiv.org/pdf/2501.00211.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Noor Aboueleneen, Yahuza Bello, Abdullatif Albaseer, Ahmed Refaey Hussein, Mohamed Abdallah, Ekram Hossain
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.00211">Distributed Traffic Control in Complex Dynamic Roadblocks: A Multi-Agent Deep RL Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autonomous Vehicles (AVs) represent a transformative advancement in the transportation industry. These vehicles have sophisticated sensors, advanced algorithms, and powerful computing systems that allow them to navigate and operate without direct human intervention. However, AVs' systems still get overwhelmed when they encounter a complex dynamic change in the environment resulting from an accident or a roadblock for maintenance. The advanced features of Sixth Generation (6G) technology are set to offer strong support to AVs, enabling real-time data exchange and management of complex driving maneuvers. This paper proposes a Multi-Agent Reinforcement Learning (MARL) framework to improve AVs' decision-making in dynamic and complex Intelligent Transportation Systems (ITS) utilizing 6G-V2X communication. The primary objective is to enable AVs to avoid roadblocks efficiently by changing lanes while maintaining optimal traffic flow and maximizing the mean harmonic speed. To ensure realistic operations, key constraints such as minimum vehicle speed, roadblock count, and lane change frequency are integrated. We train and test the proposed MARL model with two traffic simulation scenarios using the SUMO and TraCI interface. Through extensive simulations, we demonstrate that the proposed model adapts to various traffic conditions and achieves efficient and robust traffic flow management. The trained model effectively navigates dynamic roadblocks, promoting improved traffic efficiency in AV operations with more than 70% efficiency over other benchmark solutions.
<div id='section'>Paperid: <span id='pid'>484, <a href='https://arxiv.org/pdf/2412.08138.pdf' target='_blank'>https://arxiv.org/pdf/2412.08138.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuchang Sun, Xinran Li, Tao Lin, Jun Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.08138">Learn How to Query from Unlabeled Data Streams in Federated Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Federated learning (FL) enables collaborative learning among decentralized clients while safeguarding the privacy of their local data. Existing studies on FL typically assume offline labeled data available at each client when the training starts. Nevertheless, the training data in practice often arrive at clients in a streaming fashion without ground-truth labels. Given the expensive annotation cost, it is critical to identify a subset of informative samples for labeling on clients. However, selecting samples locally while accommodating the global training objective presents a challenge unique to FL. In this work, we tackle this conundrum by framing the data querying process in FL as a collaborative decentralized decision-making problem and proposing an effective solution named LeaDQ, which leverages multi-agent reinforcement learning algorithms. In particular, under the implicit guidance from global information, LeaDQ effectively learns the local policies for distributed clients and steers them towards selecting samples that can enhance the global model's accuracy. Extensive simulations on image and text tasks show that LeaDQ advances the model performance in various FL scenarios, outperforming the benchmarking algorithms.
<div id='section'>Paperid: <span id='pid'>485, <a href='https://arxiv.org/pdf/2411.19639.pdf' target='_blank'>https://arxiv.org/pdf/2411.19639.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zifeng Shi, Meiqin Liu, Senlin Zhang, Ronghao Zheng, Shanling Dong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.19639">RMIO: A Model-Based MARL Framework for Scenarios with Observation Loss in Some Agents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, model-based reinforcement learning (MBRL) has emerged as a solution to address sample complexity in multi-agent reinforcement learning (MARL) by modeling agent-environment dynamics to improve sample efficiency. However, most MBRL methods assume complete and continuous observations from each agent during the inference stage, which can be overly idealistic in practical applications. A novel model-based MARL approach called RMIO is introduced to address this limitation, specifically designed for scenarios where observation is lost in some agent. RMIO leverages the world model to reconstruct missing observations, and further reduces reconstruction errors through inter-agent information integration to ensure stable multi-agent decision-making. Secondly, unlike CTCE methods such as MAMBA, RMIO adopts the CTDE paradigm in standard environment, and enabling limited communication only when agents lack observation data, thereby reducing reliance on communication. Additionally, RMIO improves asymptotic performance through strategies such as reward smoothing, a dual-layer experience replay buffer, and an RNN-augmented policy model, surpassing previous work. Our experiments conducted in both the SMAC and MaMuJoCo environments demonstrate that RMIO outperforms current state-of-the-art approaches in terms of asymptotic convergence performance and policy robustness, both in standard mission settings and in scenarios involving observation loss.
<div id='section'>Paperid: <span id='pid'>486, <a href='https://arxiv.org/pdf/2410.17221.pdf' target='_blank'>https://arxiv.org/pdf/2410.17221.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhaolin Ren, Runyu Zhang, Bo Dai, Na Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.17221">Scalable spectral representations for multi-agent reinforcement learning in network MDPs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Network Markov Decision Processes (MDPs), a popular model for multi-agent control, pose a significant challenge to efficient learning due to the exponential growth of the global state-action space with the number of agents. In this work, utilizing the exponential decay property of network dynamics, we first derive scalable spectral local representations for network MDPs, which induces a network linear subspace for the local $Q$-function of each agent. Building on these local spectral representations, we design a scalable algorithmic framework for continuous state-action network MDPs, and provide end-to-end guarantees for the convergence of our algorithm. Empirically, we validate the effectiveness of our scalable representation-based approach on two benchmark problems, and demonstrate the advantages of our approach over generic function approximation approaches to representing the local $Q$-functions.
<div id='section'>Paperid: <span id='pid'>487, <a href='https://arxiv.org/pdf/2410.15876.pdf' target='_blank'>https://arxiv.org/pdf/2410.15876.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Woosung Koh, Wonbeen Oh, Siyeol Kim, Suhin Shin, Hyeongjin Kim, Jaein Jang, Junghyun Lee, Se-Young Yun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.15876">FlickerFusion: Intra-trajectory Domain Generalizing Multi-Agent RL</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent reinforcement learning has demonstrated significant potential in addressing complex cooperative tasks across various real-world applications. However, existing MARL approaches often rely on the restrictive assumption that the number of entities (e.g., agents, obstacles) remains constant between training and inference. This overlooks scenarios where entities are dynamically removed or added during the inference trajectory -- a common occurrence in real-world environments like search and rescue missions and dynamic combat situations. In this paper, we tackle the challenge of intra-trajectory dynamic entity composition under zero-shot out-of-domain (OOD) generalization, where such dynamic changes cannot be anticipated beforehand. Our empirical studies reveal that existing MARL methods suffer significant performance degradation and increased uncertainty in these scenarios. In response, we propose FlickerFusion, a novel OOD generalization method that acts as a universally applicable augmentation technique for MARL backbone methods. FlickerFusion stochastically drops out parts of the observation space, emulating being in-domain when inferenced OOD. The results show that FlickerFusion not only achieves superior inference rewards but also uniquely reduces uncertainty vis-Ã -vis the backbone, compared to existing methods. Benchmarks, implementations, and model weights are organized and open-sourced at flickerfusion305.github.io, accompanied by ample demo video renderings.
<div id='section'>Paperid: <span id='pid'>488, <a href='https://arxiv.org/pdf/2410.01101.pdf' target='_blank'>https://arxiv.org/pdf/2410.01101.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenhao Zhan, Scott Fujimoto, Zheqing Zhu, Jason D. Lee, Daniel R. Jiang, Yonathan Efroni
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.01101">Exploiting Structure in Offline Multi-Agent RL: The Benefits of Low Interaction Rank</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We study the problem of learning an approximate equilibrium in the offline multi-agent reinforcement learning (MARL) setting. We introduce a structural assumption -- the interaction rank -- and establish that functions with low interaction rank are significantly more robust to distribution shift compared to general ones. Leveraging this observation, we demonstrate that utilizing function classes with low interaction rank, when combined with regularization and no-regret learning, admits decentralized, computationally and statistically efficient learning in offline MARL. Our theoretical results are complemented by experiments that showcase the potential of critic architectures with low interaction rank in offline MARL, contrasting with commonly used single-agent value decomposition architectures.
<div id='section'>Paperid: <span id='pid'>489, <a href='https://arxiv.org/pdf/2409.02645.pdf' target='_blank'>https://arxiv.org/pdf/2409.02645.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jannik Peters, Constantin Waubert de Puiseau, Hasan Tercan, Arya Gopikrishnan, Gustavo Adolpho Lucas De Carvalho, Christian Bitter, Tobias Meisen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.02645">Emergent Language: A Survey and Taxonomy</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The field of emergent language represents a novel area of research within the domain of artificial intelligence, particularly within the context of multi-agent reinforcement learning. Although the concept of studying language emergence is not new, early approaches were primarily concerned with explaining human language formation, with little consideration given to its potential utility for artificial agents. In contrast, studies based on reinforcement learning aim to develop communicative capabilities in agents that are comparable to or even superior to human language. Thus, they extend beyond the learned statistical representations that are common in natural language processing research. This gives rise to a number of fundamental questions, from the prerequisites for language emergence to the criteria for measuring its success. This paper addresses these questions by providing a comprehensive review of 181 scientific publications on emergent language in artificial intelligence. Its objective is to serve as a reference for researchers interested in or proficient in the field. Consequently, the main contributions are the definition and overview of the prevailing terminology, the analysis of existing evaluation methods and metrics, and the description of the identified research gaps.
<div id='section'>Paperid: <span id='pid'>490, <a href='https://arxiv.org/pdf/2407.10403.pdf' target='_blank'>https://arxiv.org/pdf/2407.10403.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhenyu Song, Ronghao Zheng, Senlin Zhang, Meiqin Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.10403">Cooperative Reward Shaping for Multi-Agent Pathfinding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The primary objective of Multi-Agent Pathfinding (MAPF) is to plan efficient and conflict-free paths for all agents. Traditional multi-agent path planning algorithms struggle to achieve efficient distributed path planning for multiple agents. In contrast, Multi-Agent Reinforcement Learning (MARL) has been demonstrated as an effective approach to achieve this objective. By modeling the MAPF problem as a MARL problem, agents can achieve efficient path planning and collision avoidance through distributed strategies under partial observation. However, MARL strategies often lack cooperation among agents due to the absence of global information, which subsequently leads to reduced MAPF efficiency. To address this challenge, this letter introduces a unique reward shaping technique based on Independent Q-Learning (IQL). The aim of this method is to evaluate the influence of one agent on its neighbors and integrate such an interaction into the reward function, leading to active cooperation among agents. This reward shaping method facilitates cooperation among agents while operating in a distributed manner. The proposed approach has been evaluated through experiments across various scenarios with different scales and agent counts. The results are compared with those from other state-of-the-art (SOTA) planners. The evidence suggests that the approach proposed in this letter parallels other planners in numerous aspects, and outperforms them in scenarios featuring a large number of agents.
<div id='section'>Paperid: <span id='pid'>491, <a href='https://arxiv.org/pdf/2406.13399.pdf' target='_blank'>https://arxiv.org/pdf/2406.13399.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhi Yao, Zhiqing Tang, Jiong Lou, Ping Shen, Weijia Jia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.13399">VELO: A Vector Database-Assisted Cloud-Edge Collaborative LLM QoS Optimization Framework</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The Large Language Model (LLM) has gained significant popularity and is extensively utilized across various domains. Most LLM deployments occur within cloud data centers, where they encounter substantial response delays and incur high costs, thereby impacting the Quality of Services (QoS) at the network edge. Leveraging vector database caching to store LLM request results at the edge can substantially mitigate response delays and cost associated with similar requests, which has been overlooked by previous research. Addressing these gaps, this paper introduces a novel Vector database-assisted cloud-Edge collaborative LLM QoS Optimization (VELO) framework. Firstly, we propose the VELO framework, which ingeniously employs vector database to cache the results of some LLM requests at the edge to reduce the response time of subsequent similar requests. Diverging from direct optimization of the LLM, our VELO framework does not necessitate altering the internal structure of LLM and is broadly applicable to diverse LLMs. Subsequently, building upon the VELO framework, we formulate the QoS optimization problem as a Markov Decision Process (MDP) and devise an algorithm grounded in Multi-Agent Reinforcement Learning (MARL) to decide whether to request the LLM in the cloud or directly return the results from the vector database at the edge. Moreover, to enhance request feature extraction and expedite training, we refine the policy network of MARL and integrate expert demonstrations. Finally, we implement the proposed algorithm within a real edge system. Experimental findings confirm that our VELO framework substantially enhances user satisfaction by concurrently diminishing delay and resource consumption for edge users utilizing LLMs.
<div id='section'>Paperid: <span id='pid'>492, <a href='https://arxiv.org/pdf/2406.08844.pdf' target='_blank'>https://arxiv.org/pdf/2406.08844.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Runyu Zhang, Jeff Shamma, Na Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.08844">Equilibrium Selection for Multi-agent Reinforcement Learning: A Unified Framework</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While there are numerous works in multi-agent reinforcement learning (MARL), most of them focus on designing algorithms and proving convergence to a Nash equilibrium (NE) or other equilibrium such as coarse correlated equilibrium. However, NEs can be non-unique and their performance varies drastically. Thus, it is important to design algorithms that converge to Nash equilibrium with better rewards or social welfare. In contrast, classical game theory literature has extensively studied equilibrium selection for multi-agent learning in normal-form games, demonstrating that decentralized learning algorithms can asymptotically converge to potential-maximizing or Pareto-optimal NEs. These insights motivate this paper to investigate equilibrium selection in the MARL setting. We focus on the stochastic game model, leveraging classical equilibrium selection results from normal-form games to propose a unified framework for equilibrium selection in stochastic games. The proposed framework is highly modular and can extend various learning rules and their corresponding equilibrium selection results from normal-form games to the stochastic game setting.
<div id='section'>Paperid: <span id='pid'>493, <a href='https://arxiv.org/pdf/2406.00761.pdf' target='_blank'>https://arxiv.org/pdf/2406.00761.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Po-Shao Lin, Jia-Fong Yeh, Yi-Ting Chen, Winston H. Hsu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.00761">Shared-unique Features and Task-aware Prioritized Sampling on Multi-task Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We observe that current state-of-the-art (SOTA) methods suffer from the performance imbalance issue when performing multi-task reinforcement learning (MTRL) tasks. While these methods may achieve impressive performance on average, they perform extremely poorly on a few tasks. To address this, we propose a new and effective method called STARS, which consists of two novel strategies: a shared-unique feature extractor and task-aware prioritized sampling. First, the shared-unique feature extractor learns both shared and task-specific features to enable better synergy of knowledge between different tasks. Second, the task-aware sampling strategy is combined with the prioritized experience replay for efficient learning on tasks with poor performance. The effectiveness and stability of our STARS are verified through experiments on the mainstream Meta-World benchmark. From the results, our STARS statistically outperforms current SOTA methods and alleviates the performance imbalance issue. Besides, we visualize the learned features to support our claims and enhance the interpretability of STARS.
<div id='section'>Paperid: <span id='pid'>494, <a href='https://arxiv.org/pdf/2405.16077.pdf' target='_blank'>https://arxiv.org/pdf/2405.16077.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yudan Wang, Peiyao Xiao, Hao Ban, Kaiyi Ji, Shaofeng Zou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.16077">Theoretical Study of Conflict-Avoidant Multi-Objective Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-task reinforcement learning (MTRL) has shown great promise in many real-world applications. Existing MTRL algorithms often aim to learn a policy that optimizes individual objective functions simultaneously with a given prior preference (or weights) on different tasks. However, these methods often suffer from the issue of \textit{gradient conflict} such that the tasks with larger gradients dominate the update direction, resulting in a performance degeneration on other tasks. In this paper, we develop a novel dynamic weighting multi-task actor-critic algorithm (MTAC) under two options of sub-procedures named as CA and FC in task weight updates. MTAC-CA aims to find a conflict-avoidant (CA) update direction that maximizes the minimum value improvement among tasks, and MTAC-FC targets at a much faster convergence rate. We provide a comprehensive finite-time convergence analysis for both algorithms. We show that MTAC-CA can find a $Îµ+Îµ_{\text{app}}$-accurate Pareto stationary policy using $\mathcal{O}({Îµ^{-5}})$ samples, while ensuring a small $Îµ+\sqrt{Îµ_{\text{app}}}$-level CA distance (defined as the distance to the CA direction), where $Îµ_{\text{app}}$ is the function approximation error. The analysis also shows that MTAC-FC improves the sample complexity to $\mathcal{O}(Îµ^{-3})$, but with a constant-level CA distance. Our experiments on MT10 demonstrate the improved performance of our algorithms over existing MTRL methods with fixed preference.
<div id='section'>Paperid: <span id='pid'>495, <a href='https://arxiv.org/pdf/2312.15555.pdf' target='_blank'>https://arxiv.org/pdf/2312.15555.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Huiqun Li, Hanhan Zhou, Yifei Zou, Dongxiao Yu, Tian Lan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.15555">ConcaveQ: Non-Monotonic Value Function Factorization via Concave Representations in Deep Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Value function factorization has achieved great success in multi-agent reinforcement learning by optimizing joint action-value functions through the maximization of factorized per-agent utilities. To ensure Individual-Global-Maximum property, existing works often focus on value factorization using monotonic functions, which are known to result in restricted representation expressiveness. In this paper, we analyze the limitations of monotonic factorization and present ConcaveQ, a novel non-monotonic value function factorization approach that goes beyond monotonic mixing functions and employs neural network representations of concave mixing functions. Leveraging the concave property in factorization, an iterative action selection scheme is developed to obtain optimal joint actions during training. It is used to update agents' local policy networks, enabling fully decentralized execution. The effectiveness of the proposed ConcaveQ is validated across scenarios involving multi-agent predator-prey environment and StarCraft II micromanagement tasks. Empirical results exhibit significant improvement of ConcaveQ over state-of-the-art multi-agent reinforcement learning approaches.
<div id='section'>Paperid: <span id='pid'>496, <a href='https://arxiv.org/pdf/2311.13714.pdf' target='_blank'>https://arxiv.org/pdf/2311.13714.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kunal Garg, Songyuan Zhang, Oswin So, Charles Dawson, Chuchu Fan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.13714">Learning Safe Control for Multi-Robot Systems: Methods, Verification, and Open Challenges</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this survey, we review the recent advances in control design methods for robotic multi-agent systems (MAS), focussing on learning-based methods with safety considerations. We start by reviewing various notions of safety and liveness properties, and modeling frameworks used for problem formulation of MAS. Then we provide a comprehensive review of learning-based methods for safe control design for multi-robot systems. We start with various types of shielding-based methods, such as safety certificates, predictive filters, and reachability tools. Then, we review the current state of control barrier certificate learning in both a centralized and distributed manner, followed by a comprehensive review of multi-agent reinforcement learning with a particular focus on safety. Next, we discuss the state-of-the-art verification tools for the correctness of learning-based methods. Based on the capabilities and the limitations of the state of the art methods in learning and verification for MAS, we identify various broad themes for open challenges: how to design methods that can achieve good performance along with safety guarantees; how to decompose single-agent based centralized methods for MAS; how to account for communication-related practical issues; and how to assess transfer of theoretical guarantees to practice.
<div id='section'>Paperid: <span id='pid'>497, <a href='https://arxiv.org/pdf/2306.10715.pdf' target='_blank'>https://arxiv.org/pdf/2306.10715.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiarong Liu, Yifan Zhong, Siyi Hu, Haobo Fu, Qiang Fu, Xiaojun Chang, Yaodong Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.10715">Maximum Entropy Heterogeneous-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent reinforcement learning (MARL) has been shown effective for cooperative games in recent years. However, existing state-of-the-art methods face challenges related to sample complexity, training instability, and the risk of converging to a suboptimal Nash Equilibrium. In this paper, we propose a unified framework for learning stochastic policies to resolve these issues. We embed cooperative MARL problems into probabilistic graphical models, from which we derive the maximum entropy (MaxEnt) objective for MARL. Based on the MaxEnt framework, we propose Heterogeneous-Agent Soft Actor-Critic (HASAC) algorithm. Theoretically, we prove the monotonic improvement and convergence to quantal response equilibrium (QRE) properties of HASAC. Furthermore, we generalize a unified template for MaxEnt algorithmic design named Maximum Entropy Heterogeneous-Agent Mirror Learning (MEHAML), which provides any induced method with the same guarantees as HASAC. We evaluate HASAC on six benchmarks: Bi-DexHands, Multi-Agent MuJoCo, StarCraft Multi-Agent Challenge, Google Research Football, Multi-Agent Particle Environment, and Light Aircraft Game. Results show that HASAC consistently outperforms strong baselines, exhibiting better sample efficiency, robustness, and sufficient exploration. See our page at https://sites.google.com/view/meharl.
<div id='section'>Paperid: <span id='pid'>498, <a href='https://arxiv.org/pdf/2306.06808.pdf' target='_blank'>https://arxiv.org/pdf/2306.06808.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiangwei Wang, Shuo Yang, Ziyan An, Songyang Han, Zhili Zhang, Rahul Mangharam, Meiyi Ma, Fei Miao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.06808">Multi-Agent Reinforcement Learning Guided by Signal Temporal Logic Specifications</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reward design is a key component of deep reinforcement learning, yet some tasks and designer's objectives may be unnatural to define as a scalar cost function. Among the various techniques, formal methods integrated with DRL have garnered considerable attention due to their expressiveness and flexibility to define the reward and requirements for different states and actions of the agent. However, how to leverage Signal Temporal Logic (STL) to guide multi-agent reinforcement learning reward design remains unexplored. Complex interactions, heterogeneous goals and critical safety requirements in multi-agent systems make this problem even more challenging. In this paper, we propose a novel STL-guided multi-agent reinforcement learning framework. The STL requirements are designed to include both task specifications according to the objective of each agent and safety specifications, and the robustness values of the STL specifications are leveraged to generate rewards. We validate the advantages of our method through empirical studies. The experimental results demonstrate significant reward performance improvements compared to MARL without STL guidance, along with a remarkable increase in the overall safety rate of the multi-agent systems.
<div id='section'>Paperid: <span id='pid'>499, <a href='https://arxiv.org/pdf/2212.01441.pdf' target='_blank'>https://arxiv.org/pdf/2212.01441.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuyang Zhang, Runyu Zhang, Yuantao Gu, Na Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2212.01441">Multi-Agent Reinforcement Learning with Reward Delays</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper considers multi-agent reinforcement learning (MARL) where the rewards are received after delays and the delay time varies across agents and across time steps. Based on the V-learning framework, this paper proposes MARL algorithms that efficiently deal with reward delays. When the delays are finite, our algorithm reaches a coarse correlated equilibrium (CCE) with rate $\tilde{\mathcal{O}}(\frac{H^3\sqrt{S\mathcal{T}_K}}{K}+\frac{H^3\sqrt{SA}}{\sqrt{K}})$ where $K$ is the number of episodes, $H$ is the planning horizon, $S$ is the size of the state space, $A$ is the size of the largest action space, and $\mathcal{T}_K$ is the measure of total delay formally defined in the paper. Moreover, our algorithm is extended to cases with infinite delays through a reward skipping scheme. It achieves convergence rate similar to the finite delay case.
<div id='section'>Paperid: <span id='pid'>500, <a href='https://arxiv.org/pdf/2210.03022.pdf' target='_blank'>https://arxiv.org/pdf/2210.03022.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dianbo Liu, Vedant Shah, Oussama Boussif, Cristian Meo, Anirudh Goyal, Tianmin Shu, Michael Mozer, Nicolas Heess, Yoshua Bengio
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2210.03022">Stateful active facilitator: Coordination and Environmental Heterogeneity in Cooperative Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In cooperative multi-agent reinforcement learning, a team of agents works together to achieve a common goal. Different environments or tasks may require varying degrees of coordination among agents in order to achieve the goal in an optimal way. The nature of coordination will depend on the properties of the environment -- its spatial layout, distribution of obstacles, dynamics, etc. We term this variation of properties within an environment as heterogeneity. Existing literature has not sufficiently addressed the fact that different environments may have different levels of heterogeneity. We formalize the notions of coordination level and heterogeneity level of an environment and present HECOGrid, a suite of multi-agent RL environments that facilitates empirical evaluation of different MARL approaches across different levels of coordination and environmental heterogeneity by providing a quantitative control over coordination and heterogeneity levels of the environment. Further, we propose a Centralized Training Decentralized Execution learning approach called Stateful Active Facilitator (SAF) that enables agents to work efficiently in high-coordination and high-heterogeneity environments through a differentiable and shared knowledge source used during training and dynamic selection from a shared pool of policies. We evaluate SAF and compare its performance against baselines IPPO and MAPPO on HECOGrid. Our results show that SAF consistently outperforms the baselines across different tasks and different heterogeneity and coordination levels. We release the code for HECOGrid as well as all our experiments.
<div id='section'>Paperid: <span id='pid'>501, <a href='https://arxiv.org/pdf/2205.14590.pdf' target='_blank'>https://arxiv.org/pdf/2205.14590.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chinmay Maheshwari, Manxi Wu, Druv Pai, Shankar Sastry
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2205.14590">Independent and Decentralized Learning in Markov Potential Games</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We study a multi-agent reinforcement learning dynamics, and analyze its asymptotic behavior in infinite-horizon discounted Markov potential games. We focus on the independent and decentralized setting, where players do not know the game parameters, and cannot communicate or coordinate. In each stage, players update their estimate of Q-function that evaluates their total contingent payoff based on the realized one-stage reward in an asynchronous manner. Then, players independently update their policies by incorporating an optimal one-stage deviation strategy based on the estimated Q-function. Inspired by the actor-critic algorithm in single-agent reinforcement learning, a key feature of our learning dynamics is that agents update their Q-function estimates at a faster timescale than the policies. Leveraging tools from two-timescale asynchronous stochastic approximation theory, we characterize the convergent set of learning dynamics.
<div id='section'>Paperid: <span id='pid'>502, <a href='https://arxiv.org/pdf/2510.09156.pdf' target='_blank'>https://arxiv.org/pdf/2510.09156.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jing Li, Zhijie Sun, Zhicheng Zhou, Suming Qiu, Junjie Huang, Haijia Sun, Linyuan Qiu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.09156">Agentic-KGR: Co-evolutionary Knowledge Graph Construction through Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current knowledge-enhanced large language models (LLMs) rely on static, pre-constructed knowledge bases that suffer from coverage gaps and temporal obsolescence, limiting their effectiveness in dynamic information environments. We present Agentic-KGR, a novel framework enabling co-evolution between LLMs and knowledge graphs (KGs) through multi-round reinforcement learning (RL). Our approach introduces three key innovations: (1) a dynamic schema expansion mechanism that systematically extends graph ontologies beyond pre-defined boundaries during training; (2) a retrieval-augmented memory system enabling synergistic co-evolution between model parameters and knowledge structures through continuous optimization; (3) a learnable multi-scale prompt compression approach that preserves critical information while reducing computational complexity through adaptive sequence optimization. Experimental results demonstrate substantial improvements over supervised baselines and single-round RL approaches in knowledge extraction tasks. When integrated with GraphRAG, our method achieves superior performance in downstream QA tasks, with significant gains in both accuracy and knowledge coverage compared to existing methods.
<div id='section'>Paperid: <span id='pid'>503, <a href='https://arxiv.org/pdf/2509.23462.pdf' target='_blank'>https://arxiv.org/pdf/2509.23462.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alakh Sharma, Gaurish Trivedi, Kartikey Bhandari, Yash Sinha, Dhruv Kumar, Pratik Narang, Jagat Sesh Challa
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.23462">Generative Evolutionary Meta-Solver (GEMS): Scalable Surrogate-Free Multi-Agent Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Scalable multi-agent reinforcement learning (MARL) remains a central challenge for AI. Existing population-based methods, like Policy-Space Response Oracles, PSRO, require storing explicit policy populations and constructing full payoff matrices, incurring quadratic computation and linear memory costs. We present Generative Evolutionary Meta-Solver (GEMS), a surrogate-free framework that replaces explicit populations with a compact set of latent anchors and a single amortized generator. Instead of exhaustively constructing the payoff matrix, GEMS relies on unbiased Monte Carlo rollouts, multiplicative-weights meta-dynamics, and a model-free empirical-Bernstein UCB oracle to adaptively expand the policy set. Best responses are trained within the generator using an advantage-based trust-region objective, eliminating the need to store and train separate actors. We evaluated GEMS in a variety of Two-player and Multi-Player games such as the Deceptive Messages Game, Kuhn Poker and Multi-Particle environment. We find that GEMS is up to ~6x faster, has 1.3x less memory usage than PSRO, while also reaps higher rewards simultaneously. These results demonstrate that GEMS retains the game theoretic guarantees of PSRO, while overcoming its fundamental inefficiencies, hence enabling scalable multi-agent learning in multiple domains.
<div id='section'>Paperid: <span id='pid'>504, <a href='https://arxiv.org/pdf/2509.23462.pdf' target='_blank'>https://arxiv.org/pdf/2509.23462.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alakh Sharma, Gaurish Trivedi, Kartikey Bhandari, Yash Sinha, Dhruv Kumar, Pratik Narang, Jagat Sesh Challa
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.23462">Generative Evolutionary Meta-Solver (GEMS): Scalable Surrogate-Free Multi-Agent Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Scalable multi-agent reinforcement learning (MARL) remains a central challenge for AI. Existing population-based methods, like Policy-Space Response Oracles, PSRO, require storing explicit policy populations and constructing full payoff matrices, incurring quadratic computation and linear memory costs. We present Generative Evolutionary Meta-Solver (GEMS), a surrogate-free framework that replaces explicit populations with a compact set of latent anchors and a single amortized generator. Instead of exhaustively constructing the payoff matrix, GEMS relies on unbiased Monte Carlo rollouts, multiplicative-weights meta-dynamics, and a model-free empirical-Bernstein UCB oracle to adaptively expand the policy set. Best responses are trained within the generator using an advantage-based trust-region objective, eliminating the need to store and train separate actors. We evaluated GEMS in a variety of Two-player and Multi-Player games such as the Deceptive Messages Game, Kuhn Poker and Multi-Particle environment. We find that GEMS is up to ~6x faster, has 1.3x less memory usage than PSRO, while also reaps higher rewards simultaneously. These results demonstrate that GEMS retains the game theoretic guarantees of PSRO, while overcoming its fundamental inefficiencies, hence enabling scalable multi-agent learning in multiple domains.
<div id='section'>Paperid: <span id='pid'>505, <a href='https://arxiv.org/pdf/2508.18610.pdf' target='_blank'>https://arxiv.org/pdf/2508.18610.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shrenik Jadhav, Birva Sevak, Srijita Das, Akhtar Hussain, Wencong Su, Van-Hai Bui
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.18610">Scalable Fairness Shaping with LLM-Guided Multi-Agent Reinforcement Learning for Peer-to-Peer Electricity Markets</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Peer-to-peer (P2P) energy trading is becoming central to modern distribution systems as rooftop PV and home energy management systems become pervasive, yet most existing market and reinforcement learning designs emphasize efficiency or private profit and offer little real-time guidance to ensure equitable outcomes under uncertainty. To address this gap, a fairness-aware multiagent reinforcement learning framework, FairMarket-RL, is proposed in which a large language model (LLM) critic shapes bidding policies within a continuous double auction under partial observability and discrete price-quantity actions. After each trading slot, the LLM returns normalized fairness scores Fairness-to-Grid (FTG), Fairness-Between-Sellers (FBS), and Fairness-of-Pricing (FPP) that are integrated into the reward via ramped coefficients and tunable scaling, so that fairness guidance complements, rather than overwhelms, economic incentives. The environment models realistic residential load and PV profiles and enforce hard constraints on prices, physical feasibility, and policy-update stability. Across a progression of experiments from a small pilot to a larger simulated community and a mixed-asset real-world dataset, the framework shifts exchanges toward local P2P trades, lowers consumer costs relative to grid-only procurement, sustains strong fairness across participants, and preserves utility viability. Sensitivity analyses over solar availability and aggregate demand further indicate robust performance, suggesting a scalable, LLM-guided pathway to decentralized electricity markets that are economically efficient, socially equitable, and technically sound.
<div id='section'>Paperid: <span id='pid'>506, <a href='https://arxiv.org/pdf/2507.22278.pdf' target='_blank'>https://arxiv.org/pdf/2507.22278.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sunny Amatya, Yi Ren, Zhe Xu, Wenlong Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.22278">Successor Features for Transfer in Alternating Markov Games</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper explores successor features for knowledge transfer in zero-sum, complete-information, and turn-based games. Prior research in single-agent systems has shown that successor features can provide a ``jump start" for agents when facing new tasks with varying reward structures. However, knowledge transfer in games typically relies on value and equilibrium transfers, which heavily depends on the similarity between tasks. This reliance can lead to failures when the tasks differ significantly. To address this issue, this paper presents an application of successor features to games and presents a novel algorithm called Game Generalized Policy Improvement (GGPI), designed to address Markov games in multi-agent reinforcement learning. The proposed algorithm enables the transfer of learning values and policies across games. An upper bound of the errors for transfer is derived as a function the similarity of the task. Through experiments with a turn-based pursuer-evader game, we demonstrate that the GGPI algorithm can generate high-reward interactions and one-shot policy transfer. When further tested in a wider set of initial conditions, the GGPI algorithm achieves higher success rates with improved path efficiency compared to those of the baseline algorithms.
<div id='section'>Paperid: <span id='pid'>507, <a href='https://arxiv.org/pdf/2507.16941.pdf' target='_blank'>https://arxiv.org/pdf/2507.16941.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Daniel Correa, Tero Kaarlela, Jose Fuentes, Paulo Padrao, Alain Duran, Leonardo Bobadilla
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.16941">Multi-agent Reinforcement Learning for Robotized Coral Reef Sample Collection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a reinforcement learning (RL) environment for developing an autonomous underwater robotic coral sampling agent, a crucial coral reef conservation and research task. Using software-in-the-loop (SIL) and hardware-in-the-loop (HIL), an RL-trained artificial intelligence (AI) controller is developed using a digital twin (DT) in simulation and subsequently verified in physical experiments. An underwater motion capture (MOCAP) system provides real-time 3D position and orientation feedback during verification testing for precise synchronization between the digital and physical domains. A key novelty of this approach is the combined use of a general-purpose game engine for simulation, deep RL, and real-time underwater motion capture for an effective zero-shot sim-to-real strategy.
<div id='section'>Paperid: <span id='pid'>508, <a href='https://arxiv.org/pdf/2506.22708.pdf' target='_blank'>https://arxiv.org/pdf/2506.22708.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shrenik Jadhav, Birva Sevak, Srijita Das, Akhtar Hussain, Wencong Su, Van-Hai Bui
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.22708">FairMarket-RL: LLM-Guided Fairness Shaping for Multi-Agent Reinforcement Learning in Peer-to-Peer Markets</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Peer-to-peer (P2P) trading is increasingly recognized as a key mechanism for decentralized market regulation, yet existing approaches often lack robust frameworks to ensure fairness. This paper presents FairMarket-RL, a novel hybrid framework that combines Large Language Models (LLMs) with Reinforcement Learning (RL) to enable fairness-aware trading agents. In a simulated P2P microgrid with multiple sellers and buyers, the LLM acts as a real-time fairness critic, evaluating each trading episode using two metrics: Fairness-To-Buyer (FTB) and Fairness-Between-Sellers (FBS). These fairness scores are integrated into agent rewards through scheduled Î»-coefficients, forming an adaptive LLM-guided reward shaping loop that replaces brittle, rule-based fairness constraints. Agents are trained using Independent Proximal Policy Optimization (IPPO) and achieve equitable outcomes, fulfilling over 90% of buyer demand, maintaining fair seller margins, and consistently reaching FTB and FBS scores above 0.80. The training process demonstrates that fairness feedback improves convergence, reduces buyer shortfalls, and narrows profit disparities between sellers. With its language-based critic, the framework scales naturally, and its extension to a large power distribution system with household prosumers illustrates its practical applicability. FairMarket-RL thus offers a scalable, equity-driven solution for autonomous trading in decentralized energy systems.
<div id='section'>Paperid: <span id='pid'>509, <a href='https://arxiv.org/pdf/2505.03771.pdf' target='_blank'>https://arxiv.org/pdf/2505.03771.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ritik Raj, Akshat Ramachandran, Jeff Nye, Shashank Nemawarkar, Tushar Krishna
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.03771">OneDSE: A Unified Microprocessor Metric Prediction and Design Space Exploration Framework</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the slowing of Moores Law and increasing impact of power constraints, processor designs rely on architectural innovation to achieve differentiating performance. However, the innovation complexity has simultaneously increased the design space of modern high performance processors. Specifically, we identify two key challenges in prior Design Space Exploration (DSE) approaches for modern CPU design - (a) cost model (prediction method) is either slow or microarchitecture-specific or workload-specific and single model is inefficient to learn the whole design space (b) optimization (exploration method) is slow and inaccurate in the large CPU parameter space. This work presents a novel solution called OneDSE to address these emerging challenges in modern CPU design. OneDSE is a unified cost model (metric predictor) and optimizer (CPU parameter explorer) with three key techniques - 1. Transformer-based workload-Aware CPU Estimation (TrACE) framework to predict metrics in the parameter space (TrACE-p) and parameters in the in the metric space (TrACE-m). TrACE-p outperforms State of The Art (SOTA) IPC prediction methods by 5.71x and 28x for single and multiple workloads respectively while being two orders of magnitude faster. 2. We also propose a novel Metric spAce Search opTimizer (MAST) that leverages TrACE-m and outperforms SoTA metaheuristics by 1.19x while being an order of magnitude faster. 3. We propose Subsystem-based Multi-Agent Reinforcement-learning based fine-Tuning (SMART)-TrACE that achieves a 10.6% reduction in prediction error compared to TrACE, enabling more accurate and efficient exploration of the CPU design space.
<div id='section'>Paperid: <span id='pid'>510, <a href='https://arxiv.org/pdf/2505.03771.pdf' target='_blank'>https://arxiv.org/pdf/2505.03771.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ritik Raj, Akshat Ramachandran, Jeff Nye, Shashank Nemawarkar, Tushar Krishna
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.03771">OneDSE: A Unified Microprocessor Metric Prediction and Design Space Exploration Framework</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the slowing of Moores Law and increasing impact of power constraints, processor designs rely on architectural innovation to achieve differentiating performance. However, the innovation complexity has simultaneously increased the design space of modern high performance processors. Specifically, we identify two key challenges in prior Design Space Exploration (DSE) approaches for modern CPU design - (a) cost model (prediction method) is either slow or microarchitecture-specific or workload-specific and single model is inefficient to learn the whole design space (b) optimization (exploration method) is slow and inaccurate in the large CPU parameter space. This work presents a novel solution called OneDSE to address these emerging challenges in modern CPU design. OneDSE is a unified cost model (metric predictor) and optimizer (CPU parameter explorer) with three key techniques - 1. Transformer-based workload-Aware CPU Estimation (TrACE) framework to predict metrics in the parameter space (TrACE-p) and parameters in the in the metric space (TrACE-m). TrACE-p outperforms State of The Art (SOTA) IPC prediction methods by 5.71x and 28x for single and multiple workloads respectively while being two orders of magnitude faster. 2. We also propose a novel Metric spAce Search opTimizer (MAST) that leverages TrACE-m and outperforms SoTA metaheuristics by 1.19x while being an order of magnitude faster. 3. We propose Subsystem-based Multi-Agent Reinforcement-learning based fine-Tuning (SMART)-TrACE that achieves a 10.6% reduction in prediction error compared to TrACE, enabling more accurate and efficient exploration of the CPU design space.
<div id='section'>Paperid: <span id='pid'>511, <a href='https://arxiv.org/pdf/2504.11569.pdf' target='_blank'>https://arxiv.org/pdf/2504.11569.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Heming Fu, Guojun Xiong, Jian Li, Shan Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.11569">Multi-Agent Reinforcement Learning for Decentralized Reservoir Management via Murmuration Intelligence</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Conventional centralized water management systems face critical limitations from computational complexity and uncertainty propagation. We present MurmuRL, a novel decentralized framework inspired by starling murmurations intelligence, integrating bio-inspired alignment, separation, and cohesion rules with multi-agent reinforcement learning. MurmuRL enables individual reservoirs to make autonomous local decisions while achieving emergent global coordination. Experiments on grid networks demonstrate that MurmuRL achieves 8.8% higher final performance while using 27% less computing overhead compared to centralized approaches. Notably, strategic diversity scales super-linearly with system size, exhibiting sophisticated coordination patterns and enhanced resilience during extreme events. MurmuRL offers a scalable solution for managing complex water systems by leveraging principles of natural collective behavior.
<div id='section'>Paperid: <span id='pid'>512, <a href='https://arxiv.org/pdf/2501.01140.pdf' target='_blank'>https://arxiv.org/pdf/2501.01140.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Min Whoo Lee, Kibeom Kim, Soo Wung Shin, Minsu Lee, Byoung-Tak Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.01140">Communicating Unexpectedness for Out-of-Distribution Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Applying multi-agent reinforcement learning methods to realistic settings is challenging as it may require the agents to quickly adapt to unexpected situations that are rarely or never encountered in training. Recent methods for generalization to such out-of-distribution settings are limited to more specific, restricted instances of distribution shifts. To tackle adaptation to distribution shifts, we propose Unexpected Encoding Scheme, a novel decentralized multi-agent reinforcement learning algorithm where agents communicate "unexpectedness," the aspects of the environment that are surprising. In addition to a message yielded by the original reward-driven communication, each agent predicts the next observation based on previous experience, measures the discrepancy between the prediction and the actually encountered observation, and encodes this discrepancy as a message. Experiments on multi-robot warehouse environment support that our proposed method adapts robustly to dynamically changing training environments as well as out-of-distribution environment.
<div id='section'>Paperid: <span id='pid'>513, <a href='https://arxiv.org/pdf/2412.03925.pdf' target='_blank'>https://arxiv.org/pdf/2412.03925.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Talha Azfar, Kaicong Huang, Andrew Tracy, Sandra Misiewicz, Chenxi Liu, Ruimin Ke
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.03925">Traffic Co-Simulation Framework Empowered by Infrastructure Camera Sensing and Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Traffic simulations are commonly used to optimize urban traffic flow, with reinforcement learning (RL) showing promising potential for automated traffic signal control, particularly in intelligent transportation systems involving connected automated vehicles. Multi-agent reinforcement learning (MARL) is particularly effective for learning control strategies for traffic lights in a network using iterative simulations. However, existing methods often assume perfect vehicle detection, which overlooks real-world limitations related to infrastructure availability and sensor reliability. This study proposes a co-simulation framework integrating CARLA and SUMO, which combines high-fidelity 3D modeling with large-scale traffic flow simulation. Cameras mounted on traffic light poles within the CARLA environment use a YOLO-based computer vision system to detect and count vehicles, providing real-time traffic data as input for adaptive signal control in SUMO. MARL agents trained with four different reward structures leverage this visual feedback to optimize signal timings and improve network-wide traffic flow. Experiments in a multi-intersection test-bed demonstrate the effectiveness of the proposed MARL approach in enhancing traffic conditions using real-time camera based detection. The framework also evaluates the robustness of MARL under faulty or sparse sensing and compares the performance of YOLOv5 and YOLOv8 for vehicle detection. Results show that while better accuracy improves performance, MARL agents can still achieve significant improvements with imperfect detection, demonstrating scalability and adaptability for real-world scenarios.
<div id='section'>Paperid: <span id='pid'>514, <a href='https://arxiv.org/pdf/2412.03925.pdf' target='_blank'>https://arxiv.org/pdf/2412.03925.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Talha Azfar, Kaicong Huang, Andrew Tracy, Sandra Misiewicz, Chenxi Liu, Ruimin Ke
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.03925">Traffic Co-Simulation Framework Empowered by Infrastructure Camera Sensing and Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Traffic simulations are commonly used to optimize urban traffic flow, with reinforcement learning (RL) showing promising potential for automated traffic signal control, particularly in intelligent transportation systems involving connected automated vehicles. Multi-agent reinforcement learning (MARL) is particularly effective for learning control strategies for traffic lights in a network using iterative simulations. However, existing methods often assume perfect vehicle detection, which overlooks real-world limitations related to infrastructure availability and sensor reliability. This study proposes a co-simulation framework integrating CARLA and SUMO, which combines high-fidelity 3D modeling with large-scale traffic flow simulation. Cameras mounted on traffic light poles within the CARLA environment use a YOLO-based computer vision system to detect and count vehicles, providing real-time traffic data as input for adaptive signal control in SUMO. MARL agents trained with four different reward structures leverage this visual feedback to optimize signal timings and improve network-wide traffic flow. Experiments in a multi-intersection test-bed demonstrate the effectiveness of the proposed MARL approach in enhancing traffic conditions using real-time camera based detection. The framework also evaluates the robustness of MARL under faulty or sparse sensing and compares the performance of YOLOv5 and YOLOv8 for vehicle detection. Results show that while better accuracy improves performance, MARL agents can still achieve significant improvements with imperfect detection, demonstrating scalability and adaptability for real-world scenarios.
<div id='section'>Paperid: <span id='pid'>515, <a href='https://arxiv.org/pdf/2410.15221.pdf' target='_blank'>https://arxiv.org/pdf/2410.15221.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Vindula Jayawardana, Baptiste Freydt, Ao Qu, Cameron Hickert, Zhongxia Yan, Cathy Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.15221">IntersectionZoo: Eco-driving for Benchmarking Multi-Agent Contextual Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite the popularity of multi-agent reinforcement learning (RL) in simulated and two-player applications, its success in messy real-world applications has been limited. A key challenge lies in its generalizability across problem variations, a common necessity for many real-world problems. Contextual reinforcement learning (CRL) formalizes learning policies that generalize across problem variations. However, the lack of standardized benchmarks for multi-agent CRL has hindered progress in the field. Such benchmarks are desired to be based on real-world applications to naturally capture the many open challenges of real-world problems that affect generalization. To bridge this gap, we propose IntersectionZoo, a comprehensive benchmark suite for multi-agent CRL through the real-world application of cooperative eco-driving in urban road networks. The task of cooperative eco-driving is to control a fleet of vehicles to reduce fleet-level vehicular emissions. By grounding IntersectionZoo in a real-world application, we naturally capture real-world problem characteristics, such as partial observability and multiple competing objectives. IntersectionZoo is built on data-informed simulations of 16,334 signalized intersections derived from 10 major US cities, modeled in an open-source industry-grade microscopic traffic simulator. By modeling factors affecting vehicular exhaust emissions (e.g., temperature, road conditions, travel demand), IntersectionZoo provides one million data-driven traffic scenarios. Using these traffic scenarios, we benchmark popular multi-agent RL and human-like driving algorithms and demonstrate that the popular multi-agent RL algorithms struggle to generalize in CRL settings.
<div id='section'>Paperid: <span id='pid'>516, <a href='https://arxiv.org/pdf/2410.07863.pdf' target='_blank'>https://arxiv.org/pdf/2410.07863.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fanqi Kong, Yizhe Huang, Song-Chun Zhu, Siyuan Qi, Xue Feng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.07863">Learning to Balance Altruism and Self-interest Based on Empathy in Mixed-Motive Games</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Real-world multi-agent scenarios often involve mixed motives, demanding altruistic agents capable of self-protection against potential exploitation. However, existing approaches often struggle to achieve both objectives. In this paper, based on that empathic responses are modulated by inferred social relationships between agents, we propose LASE Learning to balance Altruism and Self-interest based on Empathy), a distributed multi-agent reinforcement learning algorithm that fosters altruistic cooperation through gifting while avoiding exploitation by other agents in mixed-motive games. LASE allocates a portion of its rewards to co-players as gifts, with this allocation adapting dynamically based on the social relationship -- a metric evaluating the friendliness of co-players estimated by counterfactual reasoning. In particular, social relationship measures each co-player by comparing the estimated $Q$-function of current joint action to a counterfactual baseline which marginalizes the co-player's action, with its action distribution inferred by a perspective-taking module. Comprehensive experiments are performed in spatially and temporally extended mixed-motive games, demonstrating LASE's ability to promote group collaboration without compromising fairness and its capacity to adapt policies to various types of interactive co-players.
<div id='section'>Paperid: <span id='pid'>517, <a href='https://arxiv.org/pdf/2409.20067.pdf' target='_blank'>https://arxiv.org/pdf/2409.20067.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Laixi Shi, Jingchu Gai, Eric Mazumdar, Yuejie Chi, Adam Wierman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.20067">Breaking the Curse of Multiagency in Robust Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Standard multi-agent reinforcement learning (MARL) algorithms are vulnerable to sim-to-real gaps. To address this, distributionally robust Markov games (RMGs) have been proposed to enhance robustness in MARL by optimizing the worst-case performance when game dynamics shift within a prescribed uncertainty set. RMGs remains under-explored, from reasonable problem formulation to the development of sample-efficient algorithms. Two notorious and open challenges are the formulation of the uncertainty set and whether the corresponding RMGs can overcome the curse of multiagency, where the sample complexity scales exponentially with the number of agents. In this work, we propose a natural class of RMGs inspired by behavioral economics, where each agent's uncertainty set is shaped by both the environment and the integrated behavior of other agents. We first establish the well-posedness of this class of RMGs by proving the existence of game-theoretic solutions such as robust Nash equilibria and coarse correlated equilibria (CCE). Assuming access to a generative model, we then introduce a sample-efficient algorithm for learning the CCE whose sample complexity scales polynomially with all relevant parameters. To the best of our knowledge, this is the first algorithm to break the curse of multiagency for RMGs, regardless of the uncertainty set formulation.
<div id='section'>Paperid: <span id='pid'>518, <a href='https://arxiv.org/pdf/2409.17348.pdf' target='_blank'>https://arxiv.org/pdf/2409.17348.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Huao Li, Hossein Nourkhiz Mahjoub, Behdad Chalaki, Vaishnav Tadiparthi, Kwonjoon Lee, Ehsan Moradi-Pari, Charles Michael Lewis, Katia P Sycara
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.17348">Language Grounded Multi-agent Reinforcement Learning with Human-interpretable Communication</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-Agent Reinforcement Learning (MARL) methods have shown promise in enabling agents to learn a shared communication protocol from scratch and accomplish challenging team tasks. However, the learned language is usually not interpretable to humans or other agents not co-trained together, limiting its applicability in ad-hoc teamwork scenarios. In this work, we propose a novel computational pipeline that aligns the communication space between MARL agents with an embedding space of human natural language by grounding agent communications on synthetic data generated by embodied Large Language Models (LLMs) in interactive teamwork scenarios. Our results demonstrate that introducing language grounding not only maintains task performance but also accelerates the emergence of communication. Furthermore, the learned communication protocols exhibit zero-shot generalization capabilities in ad-hoc teamwork scenarios with unseen teammates and novel task states. This work presents a significant step toward enabling effective communication and collaboration between artificial agents and humans in real-world teamwork settings.
<div id='section'>Paperid: <span id='pid'>519, <a href='https://arxiv.org/pdf/2409.00717.pdf' target='_blank'>https://arxiv.org/pdf/2409.00717.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Natalia Zhang, Xinqi Wang, Qiwen Cui, Runlong Zhou, Sham M. Kakade, Simon S. Du
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.00717">Preference-Based Multi-Agent Reinforcement Learning: Data Coverage and Algorithmic Techniques</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We initiate the study of Preference-Based Multi-Agent Reinforcement Learning (PbMARL), exploring both theoretical foundations and empirical validations. We define the task as identifying the Nash equilibrium from a preference-only offline dataset in general-sum games, a problem marked by the challenge of sparse feedback signals. Our theory establishes the upper complexity bounds for Nash Equilibrium in effective PbMARL, demonstrating that single-policy coverage is inadequate and highlighting the importance of unilateral dataset coverage. These theoretical insights are verified through comprehensive experiments. To enhance the practical performance, we further introduce two algorithmic techniques. (1) We propose a Mean Squared Error (MSE) regularization along the time axis to achieve a more uniform reward distribution and improve reward learning outcomes. (2) We propose an additional penalty based on the distribution of the dataset to incorporate pessimism, improving stability and effectiveness during training. Our findings underscore the multifaceted approach required for PbMARL, paving the way for effective preference-based multi-agent systems.
<div id='section'>Paperid: <span id='pid'>520, <a href='https://arxiv.org/pdf/2408.12214.pdf' target='_blank'>https://arxiv.org/pdf/2408.12214.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xia Jiang, Yaoxin Wu, Yuan Wang, Yingqian Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.12214">Bridging Large Language Models and Optimization: A Unified Framework for Text-attributed Combinatorial Optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To advance capabilities of large language models (LLMs) in solving combinatorial optimization problems (COPs), this paper presents the Language-based Neural COP Solver (LNCS), a novel framework that is unified for the end-to-end resolution of diverse text-attributed COPs. LNCS leverages LLMs to encode problem instances into a unified semantic space, and integrates their embeddings with a Transformer-based solution generator to produce high-quality solutions. By training the solution generator with conflict-free multi-task reinforcement learning, LNCS effectively enhances LLM performance in tackling COPs of varying types and sizes, achieving state-of-the-art results across diverse problems. Extensive experiments validate the effectiveness and generalizability of the LNCS, highlighting its potential as a unified and practical framework for real-world COP applications.
<div id='section'>Paperid: <span id='pid'>521, <a href='https://arxiv.org/pdf/2408.09675.pdf' target='_blank'>https://arxiv.org/pdf/2408.09675.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruiqi Zhang, Jing Hou, Florian Walter, Shangding Gu, Jiayi Guan, Florian RÃ¶hrbein, Yali Du, Panpan Cai, Guang Chen, Alois Knoll
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.09675">Multi-Agent Reinforcement Learning for Autonomous Driving: A Survey</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reinforcement Learning (RL) is a potent tool for sequential decision-making and has achieved performance surpassing human capabilities across many challenging real-world tasks. As the extension of RL in the multi-agent system domain, multi-agent RL (MARL) not only need to learn the control policy but also requires consideration regarding interactions with all other agents in the environment, mutual influences among different system components, and the distribution of computational resources. This augments the complexity of algorithmic design and poses higher requirements on computational resources. Simultaneously, simulators are crucial to obtain realistic data, which is the fundamentals of RL. In this paper, we first propose a series of metrics of simulators and summarize the features of existing benchmarks. Second, to ease comprehension, we recall the foundational knowledge and then synthesize the recently advanced studies of MARL-related autonomous driving and intelligent transportation systems. Specifically, we examine their environmental modeling, state representation, perception units, and algorithm design. Conclusively, we discuss open challenges as well as prospects and opportunities. We hope this paper can help the researchers integrate MARL technologies and trigger more insightful ideas toward the intelligent and autonomous driving.
<div id='section'>Paperid: <span id='pid'>522, <a href='https://arxiv.org/pdf/2408.05609.pdf' target='_blank'>https://arxiv.org/pdf/2408.05609.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Vindula Jayawardana, Baptiste Freydt, Ao Qu, Cameron Hickert, Edgar Sanchez, Catherine Tang, Mark Taylor, Blaine Leonard, Cathy Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.05609">Mitigating Metropolitan Carbon Emissions with Dynamic Eco-driving at Scale</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The sheer scale and diversity of transportation make it a formidable sector to decarbonize. Here, we consider an emerging opportunity to reduce carbon emissions: the growing adoption of semi-autonomous vehicles, which can be programmed to mitigate stop-and-go traffic through intelligent speed commands and, thus, reduce emissions. But would such dynamic eco-driving move the needle on climate change? A comprehensive impact analysis has been out of reach due to the vast array of traffic scenarios and the complexity of vehicle emissions. We address this challenge with large-scale scenario modeling efforts and by using multi-task deep reinforcement learning with a carefully designed network decomposition strategy. We perform an in-depth prospective impact assessment of dynamic eco-driving at 6,011 signalized intersections across three major US metropolitan cities, simulating a million traffic scenarios. Overall, we find that vehicle trajectories optimized for emissions can cut city-wide intersection carbon emissions by 11-22%, without harming throughput or safety, and with reasonable assumptions, equivalent to the national emissions of Israel and Nigeria, respectively. We find that 10% eco-driving adoption yields 25%-50% of the total reduction, and nearly 70% of the benefits come from 20% of intersections, suggesting near-term implementation pathways. However, the composition of this high-impact subset of intersections varies considerably across different adoption levels, with minimal overlap, calling for careful strategic planning for eco-driving deployments. Moreover, the impact of eco-driving, when considered jointly with projections of vehicle electrification and hybrid vehicle adoption remains significant. More broadly, this work paves the way for large-scale analysis of traffic externalities, such as time, safety, and air quality, and the potential impact of solution strategies.
<div id='section'>Paperid: <span id='pid'>523, <a href='https://arxiv.org/pdf/2407.16404.pdf' target='_blank'>https://arxiv.org/pdf/2407.16404.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuyang Zhu, Ziqing Zhu, Linghua Zhu, Yujian Ye, Siqi Bu, Sasa Z. Djokic
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.16404">Evaluating Uncertainties in Electricity Markets via Machine Learning and Quantum Computing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The analysis of decision-making process in electricity markets is crucial for understanding and resolving issues related to market manipulation and reduced social welfare. Traditional Multi-Agent Reinforcement Learning (MARL) method can model decision-making of generation companies (GENCOs), but faces challenges due to uncertainties in policy functions, reward functions, and inter-agent interactions. Quantum computing offers a promising solution to resolve these uncertainties, and this paper introduces the Quantum Multi-Agent Deep Q-Network (Q-MADQN) method, which integrates variational quantum circuits into the traditional MARL framework. The main contributions of the paper are: identifying the correspondence between market uncertainties and quantum properties, proposing the Q-MADQN algorithm for simulating electricity market bidding, and demonstrating that Q-MADQN allows for a more thorough exploration and simulates more potential bidding strategies of profit-oriented GENCOs, compared to conventional methods, without compromising computational efficiency. The proposed method is illustrated on IEEE 30-bus test network, confirming that it offers a more accurate model for simulating complex market dynamics.
<div id='section'>Paperid: <span id='pid'>524, <a href='https://arxiv.org/pdf/2406.08002.pdf' target='_blank'>https://arxiv.org/pdf/2406.08002.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yizhe Huang, Anji Liu, Fanqi Kong, Yaodong Yang, Song-Chun Zhu, Xue Feng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.08002">Efficient Adaptation in Mixed-Motive Environments via Hierarchical Opponent Modeling and Planning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite the recent successes of multi-agent reinforcement learning (MARL) algorithms, efficiently adapting to co-players in mixed-motive environments remains a significant challenge. One feasible approach is to hierarchically model co-players' behavior based on inferring their characteristics. However, these methods often encounter difficulties in efficient reasoning and utilization of inferred information. To address these issues, we propose Hierarchical Opponent modeling and Planning (HOP), a novel multi-agent decision-making algorithm that enables few-shot adaptation to unseen policies in mixed-motive environments. HOP is hierarchically composed of two modules: an opponent modeling module that infers others' goals and learns corresponding goal-conditioned policies, and a planning module that employs Monte Carlo Tree Search (MCTS) to identify the best response. Our approach improves efficiency by updating beliefs about others' goals both across and within episodes and by using information from the opponent modeling module to guide planning. Experimental results demonstrate that in mixed-motive environments, HOP exhibits superior few-shot adaptation capabilities when interacting with various unseen agents, and excels in self-play scenarios. Furthermore, the emergence of social intelligence during our experiments underscores the potential of our approach in complex multi-agent environments.
<div id='section'>Paperid: <span id='pid'>525, <a href='https://arxiv.org/pdf/2404.18909.pdf' target='_blank'>https://arxiv.org/pdf/2404.18909.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Laixi Shi, Eric Mazumdar, Yuejie Chi, Adam Wierman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.18909">Sample-Efficient Robust Multi-Agent Reinforcement Learning in the Face of Environmental Uncertainty</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To overcome the sim-to-real gap in reinforcement learning (RL), learned policies must maintain robustness against environmental uncertainties. While robust RL has been widely studied in single-agent regimes, in multi-agent environments, the problem remains understudied -- despite the fact that the problems posed by environmental uncertainties are often exacerbated by strategic interactions. This work focuses on learning in distributionally robust Markov games (RMGs), a robust variant of standard Markov games, wherein each agent aims to learn a policy that maximizes its own worst-case performance when the deployed environment deviates within its own prescribed uncertainty set. This results in a set of robust equilibrium strategies for all agents that align with classic notions of game-theoretic equilibria. Assuming a non-adaptive sampling mechanism from a generative model, we propose a sample-efficient model-based algorithm (DRNVI) with finite-sample complexity guarantees for learning robust variants of various notions of game-theoretic equilibria. We also establish an information-theoretic lower bound for solving RMGs, which confirms the near-optimal sample complexity of DRNVI with respect to problem-dependent factors such as the size of the state space, the target accuracy, and the horizon length.
<div id='section'>Paperid: <span id='pid'>526, <a href='https://arxiv.org/pdf/2404.18798.pdf' target='_blank'>https://arxiv.org/pdf/2404.18798.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rolando Fernandez, Garrett Warnell, Derrik E. Asher, Peter Stone
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.18798">Multi-Agent Synchronization Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In multi-agent reinforcement learning (MARL), coordination plays a crucial role in enhancing agents' performance beyond what they could achieve through cooperation alone. The interdependence of agents' actions, coupled with the need for communication, leads to a domain where effective coordination is crucial. In this paper, we introduce and define $\textit{Multi-Agent Synchronization Tasks}$ (MSTs), a novel subset of multi-agent tasks. We describe one MST, that we call $\textit{Synchronized Predator-Prey}$, offering a detailed description that will serve as the basis for evaluating a selection of recent state-of-the-art (SOTA) MARL algorithms explicitly designed to address coordination challenges through the use of communication strategies. Furthermore, we present empirical evidence that reveals the limitations of the algorithms assessed to solve MSTs, demonstrating their inability to scale effectively beyond 2-agent coordination tasks in scenarios where communication is a requisite component. Finally, the results raise questions about the applicability of recent SOTA approaches for complex coordination tasks (i.e. MSTs) and prompt further exploration into the underlying causes of their limitations in this context.
<div id='section'>Paperid: <span id='pid'>527, <a href='https://arxiv.org/pdf/2403.04001.pdf' target='_blank'>https://arxiv.org/pdf/2403.04001.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Suzan Ece Ada, Hanne Say, Emre Ugur, Erhan Oztop
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.04001">Bidirectional Progressive Neural Networks with Episodic Return Progress for Emergent Task Sequencing and Robotic Skill Transfer</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human brain and behavior provide a rich venue that can inspire novel control and learning methods for robotics. In an attempt to exemplify such a development by inspiring how humans acquire knowledge and transfer skills among tasks, we introduce a novel multi-task reinforcement learning framework named Episodic Return Progress with Bidirectional Progressive Neural Networks (ERP-BPNN). The proposed ERP-BPNN model (1) learns in a human-like interleaved manner by (2) autonomous task switching based on a novel intrinsic motivation signal and, in contrast to existing methods, (3) allows bidirectional skill transfer among tasks. ERP-BPNN is a general architecture applicable to several multi-task learning settings; in this paper, we present the details of its neural architecture and show its ability to enable effective learning and skill transfer among morphologically different robots in a reaching task. The developed Bidirectional Progressive Neural Network (BPNN) architecture enables bidirectional skill transfer without requiring incremental training and seamlessly integrates with online task arbitration. The task arbitration mechanism developed is based on soft Episodic Return progress (ERP), a novel intrinsic motivation (IM) signal. To evaluate our method, we use quantifiable robotics metrics such as 'expected distance to goal' and 'path straightness' in addition to the usual reward-based measure of episodic return common in reinforcement learning. With simulation experiments, we show that ERP-BPNN achieves faster cumulative convergence and improves performance in all metrics considered among morphologically different robots compared to the baselines.
<div id='section'>Paperid: <span id='pid'>528, <a href='https://arxiv.org/pdf/2401.00212.pdf' target='_blank'>https://arxiv.org/pdf/2401.00212.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Eduardo Sebastian, Thai Duong, Nikolay Atanasov, Eduardo Montijano, Carlos Sagues
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.00212">Physics-Informed Multi-Agent Reinforcement Learning for Distributed Multi-Robot Problems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The networked nature of multi-robot systems presents challenges in the context of multi-agent reinforcement learning. Centralized control policies do not scale with increasing numbers of robots, whereas independent control policies do not exploit the information provided by other robots, exhibiting poor performance in cooperative-competitive tasks. In this work we propose a physics-informed reinforcement learning approach able to learn distributed multi-robot control policies that are both scalable and make use of all the available information to each robot. Our approach has three key characteristics. First, it imposes a port-Hamiltonian structure on the policy representation, respecting energy conservation properties of physical robot systems and the networked nature of robot team interactions. Second, it uses self-attention to ensure a sparse policy representation able to handle time-varying information at each robot from the interaction graph. Third, we present a soft actor-critic reinforcement learning algorithm parameterized by our self-attention port-Hamiltonian control policy, which accounts for the correlation among robots during training while overcoming the need of value function factorization. Extensive simulations in different multi-robot scenarios demonstrate the success of the proposed approach, surpassing previous multi-robot reinforcement learning solutions in scalability, while achieving similar or superior performance (with averaged cumulative reward up to x2 greater than the state-of-the-art with robot teams x6 larger than the number of robots at training time). We also validate our approach on multiple real robots in the Georgia Tech Robotarium under imperfect communication, demonstrating zero-shot sim-to-real transfer and scalability across number of robots.
<div id='section'>Paperid: <span id='pid'>529, <a href='https://arxiv.org/pdf/2307.15922.pdf' target='_blank'>https://arxiv.org/pdf/2307.15922.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yingya Guo, Qi Tang, Yulong Ma, Han Tian, Kai Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.15922">Distributed Traffic Engineering in Hybrid Software Defined Networks: A Multi-agent Reinforcement Learning Framework</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Traffic Engineering (TE) is an efficient technique to balance network flows and thus improves the performance of a hybrid Software Defined Network (SDN). Previous TE solutions mainly leverage heuristic algorithms to centrally optimize link weight setting or traffic splitting ratios under the static traffic demand. Note that as the network scale becomes larger and network management gains more complexity, it is notably that the centralized TE methods suffer from a high computation overhead and a long reaction time to optimize routing of flows when the network traffic demand dynamically fluctuates or network failures happen. To enable adaptive and efficient routing in TE, we propose a Multi-agent Reinforcement Learning method CMRL that divides the routing optimization of a large network into multiple small-scale routing decisionmaking problems. To coordinate the multiple agents for achieving a global optimization goal, we construct an interactive environment for training the routing agents that own partial link utilization observations. To optimize credit assignment of multi-agent, we introduce the difference reward assignment mechanism for encouraging agents to take better action. Extensive simulations conducted on the real traffic traces demonstrate the superiority of CMRL in improving TE performance, especially when traffic demands change or network failures happen.
<div id='section'>Paperid: <span id='pid'>530, <a href='https://arxiv.org/pdf/2307.05419.pdf' target='_blank'>https://arxiv.org/pdf/2307.05419.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pedro Enrique Iturria-Rivera, Marcel Chenier, Bernard Herscovici, Burak Kantarci, Melike Erol-Kantarci
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.05419">Channel Selection for Wi-Fi 7 Multi-Link Operation via Optimistic-Weighted VDN and Parallel Transfer Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Dense and unplanned IEEE 802.11 Wireless Fidelity(Wi-Fi) deployments and the continuous increase of throughput and latency stringent services for users have led to machine learning algorithms to be considered as promising techniques in the industry and the academia. Specifically, the ongoing IEEE 802.11be EHT -- Extremely High Throughput, known as Wi-Fi 7 -- amendment propose, for the first time, Multi-Link Operation (MLO). Among others, this new feature will increase the complexity of channel selection due the novel multiple interfaces proposal. In this paper, we present a Parallel Transfer Reinforcement Learning (PTRL)-based cooperative Multi-Agent Reinforcement Learning (MARL) algorithm named Parallel Transfer Reinforcement Learning Optimistic-Weighted Value Decomposition Networks (oVDN) to improve intelligent channel selection in IEEE 802.11be MLO-capable networks. Additionally, we compare the impact of different parallel transfer learning alternatives and a centralized non-transfer MARL baseline. Two PTRL methods are presented: Multi-Agent System (MAS) Joint Q-function Transfer, where the joint Q-function is transferred and MAS Best/Worst Experience Transfer where the best and worst experiences are transferred among MASs. Simulation results show that oVDNg -- only the best experiences are utilized -- is the best algorithm variant. Moreover, oVDNg offers a gain up to 3%, 7.2% and 11% when compared with VDN, VDN-nonQ and non-PTRL baselines. Furthermore, oVDNg experienced a reward convergence gain in the 5 GHz interface of 33.3% over oVDNb and oVDN where only worst and both types of experiences are considered, respectively. Finally, our best PTRL alternative showed an improvement over the non-PTRL baseline in terms of speed of convergence up to 40 episodes and reward up to 135%.
<div id='section'>Paperid: <span id='pid'>531, <a href='https://arxiv.org/pdf/2306.11336.pdf' target='_blank'>https://arxiv.org/pdf/2306.11336.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mohamed K. Abdelaziz, Mohammed S. Elbamby, Sumudu Samarakoon, Mehdi Bennis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.11336">Cooperative Multi-Agent Learning for Navigation via Structured State Abstraction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cooperative multi-agent reinforcement learning (MARL) for navigation enables agents to cooperate to achieve their navigation goals. Using emergent communication, agents learn a communication protocol to coordinate and share information that is needed to achieve their navigation tasks. In emergent communication, symbols with no pre-specified usage rules are exchanged, in which the meaning and syntax emerge through training. Learning a navigation policy along with a communication protocol in a MARL environment is highly complex due to the huge state space to be explored. To cope with this complexity, this work proposes a novel neural network architecture, for jointly learning an adaptive state space abstraction and a communication protocol among agents participating in navigation tasks. The goal is to come up with an adaptive abstractor that significantly reduces the size of the state space to be explored, without degradation in the policy performance. Simulation results show that the proposed method reaches a better policy, in terms of achievable rewards, resulting in fewer training iterations compared to the case where raw states or fixed state abstraction are used. Moreover, it is shown that a communication protocol emerges during training which enables the agents to learn better policies within fewer training iterations.
<div id='section'>Paperid: <span id='pid'>532, <a href='https://arxiv.org/pdf/2306.07465.pdf' target='_blank'>https://arxiv.org/pdf/2306.07465.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haozhe Jiang, Qiwen Cui, Zhihan Xiong, Maryam Fazel, Simon S. Du
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.07465">A Black-box Approach for Non-stationary Multi-agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We investigate learning the equilibria in non-stationary multi-agent systems and address the challenges that differentiate multi-agent learning from single-agent learning. Specifically, we focus on games with bandit feedback, where testing an equilibrium can result in substantial regret even when the gap to be tested is small, and the existence of multiple optimal solutions (equilibria) in stationary games poses extra challenges. To overcome these obstacles, we propose a versatile black-box approach applicable to a broad spectrum of problems, such as general-sum games, potential games, and Markov games, when equipped with appropriate learning and testing oracles for stationary environments. Our algorithms can achieve $\widetilde{O}\left(Î^{1/4}T^{3/4}\right)$ regret when the degree of nonstationarity, as measured by total variation $Î$, is known, and $\widetilde{O}\left(Î^{1/5}T^{4/5}\right)$ regret when $Î$ is unknown, where $T$ is the number of rounds. Meanwhile, our algorithm inherits the favorable dependence on number of agents from the oracles. As a side contribution that may be independent of interest, we show how to test for various types of equilibria by a black-box reduction to single-agent learning, which includes Nash equilibria, correlated equilibria, and coarse correlated equilibria.
<div id='section'>Paperid: <span id='pid'>533, <a href='https://arxiv.org/pdf/2304.01447.pdf' target='_blank'>https://arxiv.org/pdf/2304.01447.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ariyan Bighashdel, Daan de Geus, Pavol Jancura, Gijs Dubbelman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.01447">Off-Policy Action Anticipation in Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Learning anticipation in Multi-Agent Reinforcement Learning (MARL) is a reasoning paradigm where agents anticipate the learning steps of other agents to improve cooperation among themselves. As MARL uses gradient-based optimization, learning anticipation requires using Higher-Order Gradients (HOG), with so-called HOG methods. Existing HOG methods are based on policy parameter anticipation, i.e., agents anticipate the changes in policy parameters of other agents. Currently, however, these existing HOG methods have only been applied to differentiable games or games with small state spaces. In this work, we demonstrate that in the case of non-differentiable games with large state spaces, existing HOG methods do not perform well and are inefficient due to their inherent limitations related to policy parameter anticipation and multiple sampling stages. To overcome these problems, we propose Off-Policy Action Anticipation (OffPA2), a novel framework that approaches learning anticipation through action anticipation, i.e., agents anticipate the changes in actions of other agents, via off-policy sampling. We theoretically analyze our proposed OffPA2 and employ it to develop multiple HOG methods that are applicable to non-differentiable games with large state spaces. We conduct a large set of experiments and illustrate that our proposed HOG methods outperform the existing ones regarding efficiency and performance.
<div id='section'>Paperid: <span id='pid'>534, <a href='https://arxiv.org/pdf/2303.08307.pdf' target='_blank'>https://arxiv.org/pdf/2303.08307.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ariyan Bighashdel, Daan de Geus, Pavol Jancura, Gijs Dubbelman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.08307">Coordinating Fully-Cooperative Agents Using Hierarchical Learning Anticipation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Learning anticipation is a reasoning paradigm in multi-agent reinforcement learning, where agents, during learning, consider the anticipated learning of other agents. There has been substantial research into the role of learning anticipation in improving cooperation among self-interested agents in general-sum games. Two primary examples are Learning with Opponent-Learning Awareness (LOLA), which anticipates and shapes the opponent's learning process to ensure cooperation among self-interested agents in various games such as iterated prisoner's dilemma, and Look-Ahead (LA), which uses learning anticipation to guarantee convergence in games with cyclic behaviors. So far, the effectiveness of applying learning anticipation to fully-cooperative games has not been explored. In this study, we aim to research the influence of learning anticipation on coordination among common-interested agents. We first illustrate that both LOLA and LA, when applied to fully-cooperative games, degrade coordination among agents, causing worst-case outcomes. Subsequently, to overcome this miscoordination behavior, we propose Hierarchical Learning Anticipation (HLA), where agents anticipate the learning of other agents in a hierarchical fashion. Specifically, HLA assigns agents to several hierarchy levels to properly regulate their reasonings. Our theoretical and empirical findings confirm that HLA can significantly improve coordination among common-interested agents in fully-cooperative normal-form games. With HLA, to the best of our knowledge, we are the first to unlock the benefits of learning anticipation for fully-cooperative games.
<div id='section'>Paperid: <span id='pid'>535, <a href='https://arxiv.org/pdf/2302.10418.pdf' target='_blank'>https://arxiv.org/pdf/2302.10418.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yongsheng Mei, Hanhan Zhou, Tian Lan, Guru Venkataramani, Peng Wei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.10418">MAC-PO: Multi-Agent Experience Replay via Collective Priority Optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Experience replay is crucial for off-policy reinforcement learning (RL) methods. By remembering and reusing the experiences from past different policies, experience replay significantly improves the training efficiency and stability of RL algorithms. Many decision-making problems in practice naturally involve multiple agents and require multi-agent reinforcement learning (MARL) under centralized training decentralized execution paradigm. Nevertheless, existing MARL algorithms often adopt standard experience replay where the transitions are uniformly sampled regardless of their importance. Finding prioritized sampling weights that are optimized for MARL experience replay has yet to be explored. To this end, we propose MAC-PO, which formulates optimal prioritized experience replay for multi-agent problems as a regret minimization over the sampling weights of transitions. Such optimization is relaxed and solved using the Lagrangian multiplier approach to obtain the close-form optimal sampling weights. By minimizing the resulting policy regret, we can narrow the gap between the current policy and a nominal optimal policy, thus acquiring an improved prioritization scheme for multi-agent tasks. Our experimental results on Predator-Prey and StarCraft Multi-Agent Challenge environments demonstrate the effectiveness of our method, having a better ability to replay important transitions and outperforming other state-of-the-art baselines.
<div id='section'>Paperid: <span id='pid'>536, <a href='https://arxiv.org/pdf/2509.20338.pdf' target='_blank'>https://arxiv.org/pdf/2509.20338.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Umer Siddique, Abhinav Sinha, Yongcan Cao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.20338">Adaptive Event-Triggered Policy Gradient for Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Conventional multi-agent reinforcement learning (MARL) methods rely on time-triggered execution, where agents sample and communicate actions at fixed intervals. This approach is often computationally expensive and communication-intensive. To address this limitation, we propose ET-MAPG (Event-Triggered Multi-Agent Policy Gradient reinforcement learning), a framework that jointly learns an agent's control policy and its event-triggering policy. Unlike prior work that decouples these mechanisms, ET-MAPG integrates them into a unified learning process, enabling agents to learn not only what action to take but also when to execute it. For scenarios with inter-agent communication, we introduce AET-MAPG, an attention-based variant that leverages a self-attention mechanism to learn selective communication patterns. AET-MAPG empowers agents to determine not only when to trigger an action but also with whom to communicate and what information to exchange, thereby optimizing coordination. Both methods can be integrated with any policy gradient MARL algorithm. Extensive experiments across diverse MARL benchmarks demonstrate that our approaches achieve performance comparable to state-of-the-art, time-triggered baselines while significantly reducing both computational load and communication overhead.
<div id='section'>Paperid: <span id='pid'>537, <a href='https://arxiv.org/pdf/2509.20338.pdf' target='_blank'>https://arxiv.org/pdf/2509.20338.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Umer Siddique, Abhinav Sinha, Yongcan Cao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.20338">Adaptive Event-Triggered Policy Gradient for Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Conventional multi-agent reinforcement learning (MARL) methods rely on time-triggered execution, where agents sample and communicate actions at fixed intervals. This approach is often computationally expensive and communication-intensive. To address this limitation, we propose ET-MAPG (Event-Triggered Multi-Agent Policy Gradient reinforcement learning), a framework that jointly learns an agent's control policy and its event-triggering policy. Unlike prior work that decouples these mechanisms, ET-MAPG integrates them into a unified learning process, enabling agents to learn not only what action to take but also when to execute it. For scenarios with inter-agent communication, we introduce AET-MAPG, an attention-based variant that leverages a self-attention mechanism to learn selective communication patterns. AET-MAPG empowers agents to determine not only when to trigger an action but also with whom to communicate and what information to exchange, thereby optimizing coordination. Both methods can be integrated with any policy gradient MARL algorithm. Extensive experiments across diverse MARL benchmarks demonstrate that our approaches achieve performance comparable to state-of-the-art, time-triggered baselines while significantly reducing both computational load and communication overhead.
<div id='section'>Paperid: <span id='pid'>538, <a href='https://arxiv.org/pdf/2508.19488.pdf' target='_blank'>https://arxiv.org/pdf/2508.19488.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xavier Cadet, Simona Boboila, Sie Hendrata Dharmawan, Alina Oprea, Peter Chin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.19488">PoolFlip: A Multi-Agent Reinforcement Learning Security Environment for Cyber Defense</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cyber defense requires automating defensive decision-making under stealthy, deceptive, and continuously evolving adversarial strategies. The FlipIt game provides a foundational framework for modeling interactions between a defender and an advanced adversary that compromises a system without being immediately detected. In FlipIt, the attacker and defender compete to control a shared resource by performing a Flip action and paying a cost. However, the existing FlipIt frameworks rely on a small number of heuristics or specialized learning techniques, which can lead to brittleness and the inability to adapt to new attacks. To address these limitations, we introduce PoolFlip, a multi-agent gym environment that extends the FlipIt game to allow efficient learning for attackers and defenders. Furthermore, we propose Flip-PSRO, a multi-agent reinforcement learning (MARL) approach that leverages population-based training to train defender agents equipped to generalize against a range of unknown, potentially adaptive opponents. Our empirical results suggest that Flip-PSRO defenders are $2\times$ more effective than baselines to generalize to a heuristic attack not exposed in training. In addition, our newly designed ownership-based utility functions ensure that Flip-PSRO defenders maintain a high level of control while optimizing performance.
<div id='section'>Paperid: <span id='pid'>539, <a href='https://arxiv.org/pdf/2508.17696.pdf' target='_blank'>https://arxiv.org/pdf/2508.17696.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Woojun Kim, Katia Sycara
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.17696">Fair Cooperation in Mixed-Motive Games via Conflict-Aware Gradient Adjustment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent reinforcement learning in mixed-motive settings presents a fundamental challenge: agents must balance individual interests with collective goals, which are neither fully aligned nor strictly opposed. To address this, reward restructuring methods such as gifting and intrinsic motivation have been proposed. However, these approaches primarily focus on promoting cooperation by managing the trade-off between individual and collective returns, without explicitly addressing fairness with respect to the agents' task-specific rewards. In this paper, we propose an adaptive conflict-aware gradient adjustment method that promotes cooperation while ensuring fairness in individual rewards. The proposed method dynamically balances policy gradients derived from individual and collective objectives in situations where the two objectives are in conflict. By explicitly resolving such conflicts, our method improves collective performance while preserving fairness across agents. We provide theoretical results that guarantee monotonic non-decreasing improvement in both the collective and individual objectives and ensure fairness. Empirical results in sequential social dilemma environments demonstrate that our approach outperforms baselines in terms of social welfare while ensuring fairness among agents.
<div id='section'>Paperid: <span id='pid'>540, <a href='https://arxiv.org/pdf/2507.10142.pdf' target='_blank'>https://arxiv.org/pdf/2507.10142.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Siyi Hu, Mohamad A Hady, Jianglin Qiao, Jimmy Cao, Mahardhika Pratama, Ryszard Kowalczyk
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.10142">Adaptability in Multi-Agent Reinforcement Learning: A Framework and Unified Review</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-Agent Reinforcement Learning (MARL) has shown clear effectiveness in coordinating multiple agents across simulated benchmarks and constrained scenarios. However, its deployment in real-world multi-agent systems (MAS) remains limited, primarily due to the complex and dynamic nature of such environments. These challenges arise from multiple interacting sources of variability, including fluctuating agent populations, evolving task goals, and inconsistent execution conditions. Together, these factors demand that MARL algorithms remain effective under continuously changing system configurations and operational demands. To better capture and assess this capacity for adjustment, we introduce the concept of \textit{adaptability} as a unified and practically grounded lens through which to evaluate the reliability of MARL algorithms under shifting conditions, broadly referring to any changes in the environment dynamics that may occur during learning or execution. Centred on the notion of adaptability, we propose a structured framework comprising three key dimensions: learning adaptability, policy adaptability, and scenario-driven adaptability. By adopting this adaptability perspective, we aim to support more principled assessments of MARL performance beyond narrowly defined benchmarks. Ultimately, this survey contributes to the development of algorithms that are better suited for deployment in dynamic, real-world multi-agent systems.
<div id='section'>Paperid: <span id='pid'>541, <a href='https://arxiv.org/pdf/2506.15207.pdf' target='_blank'>https://arxiv.org/pdf/2506.15207.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mohamad A. Hady, Siyi Hu, Mahardhika Pratama, Jimmy Cao, Ryszard Kowalczyk
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.15207">Multi-Agent Reinforcement Learning for Autonomous Multi-Satellite Earth Observation: A Realistic Case Study</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The exponential growth of Low Earth Orbit (LEO) satellites has revolutionised Earth Observation (EO) missions, addressing challenges in climate monitoring, disaster management, and more. However, autonomous coordination in multi-satellite systems remains a fundamental challenge. Traditional optimisation approaches struggle to handle the real-time decision-making demands of dynamic EO missions, necessitating the use of Reinforcement Learning (RL) and Multi-Agent Reinforcement Learning (MARL). In this paper, we investigate RL-based autonomous EO mission planning by modelling single-satellite operations and extending to multi-satellite constellations using MARL frameworks. We address key challenges, including energy and data storage limitations, uncertainties in satellite observations, and the complexities of decentralised coordination under partial observability. By leveraging a near-realistic satellite simulation environment, we evaluate the training stability and performance of state-of-the-art MARL algorithms, including PPO, IPPO, MAPPO, and HAPPO. Our results demonstrate that MARL can effectively balance imaging and resource management while addressing non-stationarity and reward interdependency in multi-satellite coordination. The insights gained from this study provide a foundation for autonomous satellite operations, offering practical guidelines for improving policy learning in decentralised EO missions.
<div id='section'>Paperid: <span id='pid'>542, <a href='https://arxiv.org/pdf/2506.09434.pdf' target='_blank'>https://arxiv.org/pdf/2506.09434.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Michael Amir, Matteo Bettini, Amanda Prorok
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.09434">When Is Diversity Rewarded in Cooperative Multi-Agent Learning?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The success of teams in robotics, nature, and society often depends on the division of labor among diverse specialists; however, a principled explanation for when such diversity surpasses a homogeneous team is still missing. Focusing on multi-agent task allocation problems, our goal is to study this question from the perspective of reward design: what kinds of objectives are best suited for heterogeneous teams? We first consider an instantaneous, non-spatial setting where the global reward is built by two generalized aggregation operators: an inner operator that maps the $N$ agents' effort allocations on individual tasks to a task score, and an outer operator that merges the $M$ task scores into the global team reward. We prove that the curvature of these operators determines whether heterogeneity can increase reward, and that for broad reward families this collapses to a simple convexity test. Next, we ask what incentivizes heterogeneity to emerge when embodied, time-extended agents must learn an effort allocation policy. To study heterogeneity in such settings, we use multi-agent reinforcement learning (MARL) as our computational paradigm, and introduce Heterogeneous Environment Design (HED), a gradient-based algorithm that optimizes the parameter space of underspecified MARL environments to find scenarios where heterogeneity is advantageous. Experiments in matrix games and an embodied Multi-Goal-Capture environment show that, despite the difference in settings, HED rediscovers the reward regimes predicted by our theory to maximize the advantage of heterogeneity, both validating HED and connecting our theoretical insights to reward design in MARL. Together, these results help us understand when behavioral diversity delivers a measurable benefit.
<div id='section'>Paperid: <span id='pid'>543, <a href='https://arxiv.org/pdf/2506.09434.pdf' target='_blank'>https://arxiv.org/pdf/2506.09434.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Michael Amir, Matteo Bettini, Amanda Prorok
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.09434">When Is Diversity Rewarded in Cooperative Multi-Agent Learning?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The success of teams in robotics, nature, and society often depends on the division of labor among diverse specialists; however, a principled explanation for when such diversity surpasses a homogeneous team is still missing. Focusing on multi-agent task allocation problems, we study this question from the perspective of reward design: what kinds of objectives are best suited for heterogeneous teams? We first consider an instantaneous, non-spatial setting where the global reward is built by two generalized aggregation operators: an inner operator that maps the $N$ agents' effort allocations on individual tasks to a task score, and an outer operator that merges the $M$ task scores into the global team reward. We prove that the curvature of these operators determines whether heterogeneity can increase reward, and that for broad reward families this collapses to a simple convexity test. Next, we ask what incentivizes heterogeneity to emerge when embodied, time-extended agents must learn an effort allocation policy. To study heterogeneity in such settings, we use multi-agent reinforcement learning (MARL) as our computational paradigm, and introduce Heterogeneity Gain Parameter Search (HetGPS), a gradient-based algorithm that optimizes the parameter space of underspecified MARL environments to find scenarios where heterogeneity is advantageous. Across different environments, we show that HetGPS rediscovers the reward regimes predicted by our theory to maximize the advantage of heterogeneity, both validating HetGPS and connecting our theoretical insights to reward design in MARL. Together, these results help us understand when behavioral diversity delivers a measurable benefit.
<div id='section'>Paperid: <span id='pid'>544, <a href='https://arxiv.org/pdf/2506.09434.pdf' target='_blank'>https://arxiv.org/pdf/2506.09434.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Michael Amir, Matteo Bettini, Amanda Prorok
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.09434">When Is Diversity Rewarded in Cooperative Multi-Agent Learning?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The success of teams in robotics, nature, and society often depends on the division of labor among diverse specialists; however, a principled explanation for when such diversity surpasses a homogeneous team is still missing. Focusing on multi-agent task allocation problems, we study this question from the perspective of reward design: what kinds of objectives are best suited for heterogeneous teams? We first consider an instantaneous, non-spatial setting where the global reward is built by two generalized aggregation operators: an inner operator that maps the $N$ agents' effort allocations on individual tasks to a task score, and an outer operator that merges the $M$ task scores into the global team reward. We prove that the curvature of these operators determines whether heterogeneity can increase reward, and that for broad reward families this collapses to a simple convexity test. Next, we ask what incentivizes heterogeneity to emerge when embodied, time-extended agents must learn an effort allocation policy. To study heterogeneity in such settings, we use multi-agent reinforcement learning (MARL) as our computational paradigm, and introduce Heterogeneity Gain Parameter Search (HetGPS), a gradient-based algorithm that optimizes the parameter space of underspecified MARL environments to find scenarios where heterogeneity is advantageous. Across different environments, we show that HetGPS rediscovers the reward regimes predicted by our theory to maximize the advantage of heterogeneity, both validating HetGPS and connecting our theoretical insights to reward design in MARL. Together, these results help us understand when behavioral diversity delivers a measurable benefit.
<div id='section'>Paperid: <span id='pid'>545, <a href='https://arxiv.org/pdf/2505.22151.pdf' target='_blank'>https://arxiv.org/pdf/2505.22151.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Claude Formanek, Omayma Mahjoub, Louay Ben Nessir, Sasha Abramowitz, Ruan de Kock, Wiem Khlifi, Simon Du Toit, Felix Chalumeau, Daniel Rajaonarivonivelomanantsoa, Arnol Fokam, Siddarth Singh, Ulrich Mbou Sob, Arnu Pretorius
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.22151">Oryx: a Performant and Scalable Algorithm for Many-Agent Coordination in Offline MARL</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A key challenge in offline multi-agent reinforcement learning (MARL) is achieving effective many-agent multi-step coordination in complex environments. In this work, we propose Oryx, a novel algorithm for offline cooperative MARL to directly address this challenge. Oryx adapts the recently proposed retention-based architecture Sable and combines it with a sequential form of implicit constraint Q-learning (ICQ), to develop a novel offline auto-regressive policy update scheme. This allows Oryx to solve complex coordination challenges while maintaining temporal coherence over lengthy trajectories. We evaluate Oryx across a diverse set of benchmarks from prior works (SMAC, RWARE, and Multi-Agent MuJoCo) covering tasks of both discrete and continuous control, varying in scale and difficulty. Oryx achieves state-of-the-art performance on more than 80% of the 65 tested datasets, outperforming prior offline MARL methods and demonstrating robust generalisation across domains with many agents and long horizons. Finally, we introduce new datasets to push the limits of many-agent coordination in offline MARL, and demonstrate Oryx's superior ability to scale effectively in such settings. We will make all of our datasets, experimental data, and code available upon publication.
<div id='section'>Paperid: <span id='pid'>546, <a href='https://arxiv.org/pdf/2505.11100.pdf' target='_blank'>https://arxiv.org/pdf/2505.11100.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lang Feng, Jiahao Lin, Dong Xing, Li Zhang, De Ma, Gang Pan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.11100">Bidirectional Distillation: A Mixed-Play Framework for Multi-Agent Generalizable Behaviors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Population-population generalization is a challenging problem in multi-agent reinforcement learning (MARL), particularly when agents encounter unseen co-players. However, existing self-play-based methods are constrained by the limitation of inside-space generalization. In this study, we propose Bidirectional Distillation (BiDist), a novel mixed-play framework, to overcome this limitation in MARL. BiDist leverages knowledge distillation in two alternating directions: forward distillation, which emulates the historical policies' space and creates an implicit self-play, and reverse distillation, which systematically drives agents towards novel distributions outside the known policy space in a non-self-play manner. In addition, BiDist operates as a concise and efficient solution without the need for the complex and costly storage of past policies. We provide both theoretical analysis and empirical evidence to support BiDist's effectiveness. Our results highlight its remarkable generalization ability across a variety of cooperative, competitive, and social dilemma tasks, and reveal that BiDist significantly diversifies the policy distribution space. We also present comprehensive ablation studies to reinforce BiDist's effectiveness and key success factors. Source codes are available in the supplementary material.
<div id='section'>Paperid: <span id='pid'>547, <a href='https://arxiv.org/pdf/2504.21048.pdf' target='_blank'>https://arxiv.org/pdf/2504.21048.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mohamad A. Hady, Siyi Hu, Mahardhika Pratama, Jimmy Cao, Ryszard Kowalczyk
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.21048">Multi-Agent Reinforcement Learning for Resources Allocation Optimization: A Survey</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-Agent Reinforcement Learning (MARL) has become a powerful framework for numerous real-world applications, modeling distributed decision-making and learning from interactions with complex environments. Resource Allocation Optimization (RAO) benefits significantly from MARL's ability to tackle dynamic and decentralized contexts. MARL-based approaches are increasingly applied to RAO challenges across sectors playing pivotal roles to Industry 4.0 developments. This survey provides a comprehensive review of recent MARL algorithms for RAO, encompassing core concepts, classifications, and a structured taxonomy. By outlining the current research landscape and identifying primary challenges and future directions, this survey aims to support researchers and practitioners in leveraging MARL's potential to advance resource allocation solutions.
<div id='section'>Paperid: <span id='pid'>548, <a href='https://arxiv.org/pdf/2504.14422.pdf' target='_blank'>https://arxiv.org/pdf/2504.14422.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Paul Fischer, Sebastian Kaltenbach, Sergey Litvinov, Sauro Succi, Petros Koumoutsakos
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.14422">Optimal Lattice Boltzmann Closures through Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The Lattice Boltzmann method (LBM) offers a powerful and versatile approach to simulating diverse hydrodynamic phenomena, spanning microfluidics to aerodynamics. The vast range of spatiotemporal scales inherent in these systems currently renders full resolution impractical, necessitating the development of effective closure models for under-resolved simulations. Under-resolved LBMs are unstable, and while there is a number of important efforts to stabilize them, they often face limitations in generalizing across scales and physical systems. We present a novel, data-driven, multiagent reinforcement learning (MARL) approach that drastically improves stability and accuracy of coarse-grained LBM simulations. The proposed method uses a convolutional neural network to dynamically control the local relaxation parameter for the LB across the simulation grid. The LB-MARL framework is showcased in turbulent Kolmogorov flows. We find that the MARL closures stabilize the simulations and recover the energy spectra of significantly more expensive fully resolved simulations while maintaining computational efficiency. The learned closure model can be transferred to flow scenarios unseen during training and has improved robustness and spectral accuracy compared to traditional LBM models. We believe that MARL closures open new frontiers for efficient and accurate simulations of a multitude of complex problems not accessible to present-day LB methods alone.
<div id='section'>Paperid: <span id='pid'>549, <a href='https://arxiv.org/pdf/2503.13547.pdf' target='_blank'>https://arxiv.org/pdf/2503.13547.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xu Guo, Xiangwang Hou, Minrui Xu, Jianrui Chen, Jingjing Wang, Jun Du, Yong Ren
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.13547">Adaptive AUV Hunting Policy with Covert Communication via Diffusion Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Collaborative underwater target hunting, facilitated by multiple autonomous underwater vehicles (AUVs), plays a significant role in various domains, especially military missions. Existing research predominantly focuses on designing efficient and high-success-rate hunting policy, particularly addressing the target's evasion capabilities. However, in real-world scenarios, the target can not only adjust its evasion policy based on its observations and predictions but also possess eavesdropping capabilities. If communication among hunter AUVs, such as hunting policy exchanges, is intercepted by the target, it can adapt its escape policy accordingly, significantly reducing the success rate of the hunting mission. To address this challenge, we propose a covert communication-guaranteed collaborative target hunting framework, which ensures efficient hunting in complex underwater environments while defending against the target's eavesdropping. To the best of our knowledge, this is the first study to incorporate the confidentiality of inter-agent communication into the design of target hunting policy. Furthermore, given the complexity of coordinating multiple AUVs in dynamic and unpredictable environments, we propose an adaptive multi-agent diffusion policy (AMADP), which incorporates the strong generative ability of diffusion models into the multi-agent reinforcement learning (MARL) algorithm. Experimental results demonstrate that AMADP achieves faster convergence and higher hunting success rates while maintaining covertness constraints.
<div id='section'>Paperid: <span id='pid'>550, <a href='https://arxiv.org/pdf/2502.08365.pdf' target='_blank'>https://arxiv.org/pdf/2502.08365.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Riccardo Zamboni, Mirco Mutti, Marcello Restelli
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.08365">Towards Unsupervised Multi-Agent Reinforcement Learning via Task-Agnostic Exploration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In reinforcement learning, we typically refer to unsupervised pre-training when we aim to pre-train a policy without a priori access to the task specification, i.e. rewards, to be later employed for efficient learning of downstream tasks. In single-agent settings, the problem has been extensively studied and mostly understood. A popular approach, called task-agnostic exploration, casts the unsupervised objective as maximizing the entropy of the state distribution induced by the agent's policy, from which principles and methods follow.
  In contrast, little is known about it in multi-agent settings, which are ubiquitous in the real world. What are the pros and cons of alternative problem formulations in this setting? How hard is the problem in theory, how can we solve it in practice? In this paper, we address these questions by first characterizing those alternative formulations and highlighting how the problem, even when tractable in theory, is non-trivial in practice. Then, we present a scalable, decentralized, trust-region policy search algorithm to address the problem in practical settings. Finally, we provide numerical validations to both corroborate the theoretical findings and pave the way for unsupervised multi-agent reinforcement learning via task-agnostic exploration in challenging domains, showing that optimizing for a specific objective, namely mixture entropy, provides an excellent trade-off between tractability and performances.
<div id='section'>Paperid: <span id='pid'>551, <a href='https://arxiv.org/pdf/2501.18138.pdf' target='_blank'>https://arxiv.org/pdf/2501.18138.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Woojun Kim, Katia Sycara
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.18138">B3C: A Minimalist Approach to Offline Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Overestimation arising from selecting unseen actions during policy evaluation is a major challenge in offline reinforcement learning (RL). A minimalist approach in the single-agent setting -- adding behavior cloning (BC) regularization to existing online RL algorithms -- has been shown to be effective; however, this approach is understudied in multi-agent settings. In particular, overestimation becomes worse in multi-agent settings due to the presence of multiple actions, resulting in the BC regularization-based approach easily suffering from either over-regularization or critic divergence. To address this, we propose a simple yet effective method, Behavior Cloning regularization with Critic Clipping (B3C), which clips the target critic value in policy evaluation based on the maximum return in the dataset and pushes the limit of the weight on the RL objective over BC regularization, thereby improving performance. Additionally, we leverage existing value factorization techniques, particularly non-linear factorization, which is understudied in offline settings. Integrated with non-linear value factorization, B3C outperforms state-of-the-art algorithms on various offline multi-agent benchmarks.
<div id='section'>Paperid: <span id='pid'>552, <a href='https://arxiv.org/pdf/2412.16244.pdf' target='_blank'>https://arxiv.org/pdf/2412.16244.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Matteo Bettini, Ryan Kortvelesy, Amanda Prorok
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.16244">The impact of behavioral diversity in multi-agent reinforcement learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Many of the world's most pressing issues, such as climate change and global peace, require complex collective problem-solving skills. Recent studies indicate that diversity in individuals' behaviors is key to developing such skills and increasing collective performance. Yet behavioral diversity in collective artificial learning is understudied, with today's machine learning paradigms commonly favoring homogeneous agent strategies over heterogeneous ones, mainly due to computational considerations. In this work, we employ diversity measurement and control paradigms to study the impact of behavioral heterogeneity in several facets of multi-agent reinforcement learning. Through experiments in team play and other cooperative tasks, we show the emergence of unbiased behavioral roles that improve team outcomes; how behavioral diversity synergizes with morphological diversity; how diverse agents are more effective at finding cooperative solutions in sparse reward settings; and how behaviorally heterogeneous teams learn and retain latent skills to overcome repeated disruptions. Overall, our results indicate that, by controlling diversity, we can obtain non-trivial benefits over homogeneous training paradigms, demonstrating that diversity is a fundamental component of collective artificial learning, an insight thus far overlooked.
<div id='section'>Paperid: <span id='pid'>553, <a href='https://arxiv.org/pdf/2411.10031.pdf' target='_blank'>https://arxiv.org/pdf/2411.10031.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jingyuan Zhou, Longhao Yan, Jinhao Liang, Kaidi Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.10031">Enforcing Cooperative Safety for Reinforcement Learning-based Mixed-Autonomy Platoon Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>It is recognized that the control of mixed-autonomy platoons comprising connected and automated vehicles (CAVs) and human-driven vehicles (HDVs) can enhance traffic flow. Among existing methods, Multi-Agent Reinforcement Learning (MARL) appears to be a promising control strategy because it can manage complex scenarios in real time. However, current research on MARL-based mixed-autonomy platoon control suffers from several limitations. First, existing MARL approaches address safety by penalizing safety violations in the reward function, thus lacking theoretical safety guarantees due to the black-box nature of RL. Second, few studies have explored the cooperative safety of multi-CAV platoons, where CAVs can be coordinated to further enhance the system-level safety involving the safety of both CAVs and HDVs. Third, existing work tends to make an unrealistic assumption that the behavior of HDVs and CAVs is publicly known and rationale. To bridge the research gaps, we propose a safe MARL framework for mixed-autonomy platoons. Specifically, this framework (i) characterizes cooperative safety by designing a cooperative Control Barrier Function (CBF), enabling CAVs to collaboratively improve the safety of the entire platoon, (ii) provides a safety guarantee to the MARL-based controller by integrating the CBF-based safety constraints into MARL through a differentiable quadratic programming (QP) layer, and (iii) incorporates a conformal prediction module that enables each CAV to estimate the unknown behaviors of the surrounding vehicles with uncertainty qualification. Simulation results show that our proposed control strategy can effectively enhance the system-level safety through CAV cooperation of a mixed-autonomy platoon with a minimal impact on control performance.
<div id='section'>Paperid: <span id='pid'>554, <a href='https://arxiv.org/pdf/2410.19382.pdf' target='_blank'>https://arxiv.org/pdf/2410.19382.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jemma Daniel, Ruan de Kock, Louay Ben Nessir, Sasha Abramowitz, Omayma Mahjoub, Wiem Khlifi, Claude Formanek, Arnu Pretorius
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.19382">Multi-Agent Reinforcement Learning with Selective State-Space Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The Transformer model has demonstrated success across a wide range of domains, including in Multi-Agent Reinforcement Learning (MARL) where the Multi-Agent Transformer (MAT) has emerged as a leading algorithm in the field. However, a significant drawback of Transformer models is their quadratic computational complexity relative to input size, making them computationally expensive when scaling to larger inputs. This limitation restricts MAT's scalability in environments with many agents. Recently, State-Space Models (SSMs) have gained attention due to their computational efficiency, but their application in MARL remains unexplored. In this work, we investigate the use of Mamba, a recent SSM, in MARL and assess whether it can match the performance of MAT while providing significant improvements in efficiency. We introduce a modified version of MAT that incorporates standard and bi-directional Mamba blocks, as well as a novel "cross-attention" Mamba block. Extensive testing shows that our Multi-Agent Mamba (MAM) matches the performance of MAT across multiple standard multi-agent environments, while offering superior scalability to larger agent scenarios. This is significant for the MARL community, because it indicates that SSMs could replace Transformers without compromising performance, whilst also supporting more effective scaling to higher numbers of agents. Our project page is available at https://sites.google.com/view/multi-agent-mamba .
<div id='section'>Paperid: <span id='pid'>555, <a href='https://arxiv.org/pdf/2410.17351.pdf' target='_blank'>https://arxiv.org/pdf/2410.17351.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aditya Vikram Singh, Ethan Rathbun, Emma Graham, Lisa Oakley, Simona Boboila, Alina Oprea, Peter Chin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.17351">Hierarchical Multi-agent Reinforcement Learning for Cyber Network Defense</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in multi-agent reinforcement learning (MARL) have created opportunities to solve complex real-world tasks. Cybersecurity is a notable application area, where defending networks against sophisticated adversaries remains a challenging task typically performed by teams of security operators. In this work, we explore novel MARL strategies for building autonomous cyber network defenses that address challenges such as large policy spaces, partial observability, and stealthy, deceptive adversarial strategies. To facilitate efficient and generalized learning, we propose a hierarchical Proximal Policy Optimization (PPO) architecture that decomposes the cyber defense task into specific sub-tasks like network investigation and host recovery. Our approach involves training sub-policies for each sub-task using PPO enhanced with cybersecurity domain expertise. These sub-policies are then leveraged by a master defense policy that coordinates their selection to solve complex network defense tasks. Furthermore, the sub-policies can be fine-tuned and transferred with minimal cost to defend against shifts in adversarial behavior or changes in network settings. We conduct extensive experiments using CybORG Cage 4, the state-of-the-art MARL environment for cyber defense. Comparisons with multiple baselines across different adversaries show that our hierarchical learning approach achieves top performance in terms of convergence speed, episodic return, and several interpretable metrics relevant to cybersecurity, including the fraction of clean machines on the network, precision, and false positives.
<div id='section'>Paperid: <span id='pid'>556, <a href='https://arxiv.org/pdf/2410.17351.pdf' target='_blank'>https://arxiv.org/pdf/2410.17351.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aditya Vikram Singh, Ethan Rathbun, Emma Graham, Lisa Oakley, Simona Boboila, Alina Oprea, Peter Chin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.17351">Hierarchical Multi-agent Reinforcement Learning for Cyber Network Defense</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in multi-agent reinforcement learning (MARL) have created opportunities to solve complex real-world tasks. Cybersecurity is a notable application area, where defending networks against sophisticated adversaries remains a challenging task typically performed by teams of security operators. In this work, we explore novel MARL strategies for building autonomous cyber network defenses that address challenges such as large policy spaces, partial observability, and stealthy, deceptive adversarial strategies. To facilitate efficient and generalized learning, we propose a hierarchical Proximal Policy Optimization (PPO) architecture that decomposes the cyber defense task into specific sub-tasks like network investigation and host recovery. Our approach involves training sub-policies for each sub-task using PPO enhanced with cybersecurity domain expertise. These sub-policies are then leveraged by a master defense policy that coordinates their selection to solve complex network defense tasks. Furthermore, the sub-policies can be fine-tuned and transferred with minimal cost to defend against shifts in adversarial behavior or changes in network settings. We conduct extensive experiments using CybORG Cage 4, the state-of-the-art MARL environment for cyber defense. Comparisons with multiple baselines across different adversaries show that our hierarchical learning approach achieves top performance in terms of convergence speed, episodic return, and several interpretable metrics relevant to cybersecurity, including the fraction of clean machines on the network, precision, and false positives.
<div id='section'>Paperid: <span id='pid'>557, <a href='https://arxiv.org/pdf/2410.01706.pdf' target='_blank'>https://arxiv.org/pdf/2410.01706.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Omayma Mahjoub, Sasha Abramowitz, Ruan de Kock, Wiem Khlifi, Simon du Toit, Jemma Daniel, Louay Ben Nessir, Louise Beyers, Claude Formanek, Liam Clark, Arnu Pretorius
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.01706">Sable: a Performant, Efficient and Scalable Sequence Model for MARL</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As multi-agent reinforcement learning (MARL) progresses towards solving larger and more complex problems, it becomes increasingly important that algorithms exhibit the key properties of (1) strong performance, (2) memory efficiency, and (3) scalability. In this work, we introduce Sable, a performant, memory-efficient, and scalable sequence modeling approach to MARL. Sable works by adapting the retention mechanism in Retentive Networks (Sun et al., 2023) to achieve computationally efficient processing of multi-agent observations with long context memory for temporal reasoning. Through extensive evaluations across six diverse environments, we demonstrate how Sable is able to significantly outperform existing state-of-the-art methods in a large number of diverse tasks (34 out of 45 tested). Furthermore, Sable maintains performance as we scale the number of agents, handling environments with more than a thousand agents while exhibiting a linear increase in memory usage. Finally, we conduct ablation studies to isolate the source of Sable's performance gains and confirm its efficient computational memory usage.
<div id='section'>Paperid: <span id='pid'>558, <a href='https://arxiv.org/pdf/2409.12001.pdf' target='_blank'>https://arxiv.org/pdf/2409.12001.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Claude Formanek, Louise Beyers, Callum Rhys Tilbury, Jonathan P. Shock, Arnu Pretorius
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.12001">Putting Data at the Centre of Offline Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Offline multi-agent reinforcement learning (MARL) is an exciting direction of research that uses static datasets to find optimal control policies for multi-agent systems. Though the field is by definition data-driven, efforts have thus far neglected data in their drive to achieve state-of-the-art results. We first substantiate this claim by surveying the literature, showing how the majority of works generate their own datasets without consistent methodology and provide sparse information about the characteristics of these datasets. We then show why neglecting the nature of the data is problematic, through salient examples of how tightly algorithmic performance is coupled to the dataset used, necessitating a common foundation for experiments in the field. In response, we take a big step towards improving data usage and data awareness in offline MARL, with three key contributions: (1) a clear guideline for generating novel datasets; (2) a standardisation of over 80 existing datasets, hosted in a publicly available repository, using a consistent storage format and easy-to-use API; and (3) a suite of analysis tools that allow us to understand these datasets better, aiding further development.
<div id='section'>Paperid: <span id='pid'>559, <a href='https://arxiv.org/pdf/2409.11561.pdf' target='_blank'>https://arxiv.org/pdf/2409.11561.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Weizheng Wang, Aniket Bera, Byung-Cheol Min
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.11561">Hypergraph-based Coordinated Task Allocation and Socially-aware Navigation for Multi-Robot Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A team of multiple robots seamlessly and safely working in human-filled public environments requires adaptive task allocation and socially-aware navigation that account for dynamic human behavior. Current approaches struggle with highly dynamic pedestrian movement and the need for flexible task allocation. We propose Hyper-SAMARL, a hypergraph-based system for multi-robot task allocation and socially-aware navigation, leveraging multi-agent reinforcement learning (MARL). Hyper-SAMARL models the environmental dynamics between robots, humans, and points of interest (POIs) using a hypergraph, enabling adaptive task assignment and socially-compliant navigation through a hypergraph diffusion mechanism. Our framework, trained with MARL, effectively captures interactions between robots and humans, adapting tasks based on real-time changes in human activity. Experimental results demonstrate that Hyper-SAMARL outperforms baseline models in terms of social navigation, task completion efficiency, and adaptability in various simulated scenarios.
<div id='section'>Paperid: <span id='pid'>560, <a href='https://arxiv.org/pdf/2409.07127.pdf' target='_blank'>https://arxiv.org/pdf/2409.07127.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dongkun Huo, Huateng Zhang, Yixue Hao, Yuanlin Ye, Long Hu, Rui Wang, Min Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.07127">DCMAC: Demand-aware Customized Multi-Agent Communication via Upper Bound Training</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Efficient communication can enhance the overall performance of collaborative multi-agent reinforcement learning. A common approach is to share observations through full communication, leading to significant communication overhead. Existing work attempts to perceive the global state by conducting teammate model based on local information. However, they ignore that the uncertainty generated by prediction may lead to difficult training. To address this problem, we propose a Demand-aware Customized Multi-Agent Communication (DCMAC) protocol, which use an upper bound training to obtain the ideal policy. By utilizing the demand parsing module, agent can interpret the gain of sending local message on teammate, and generate customized messages via compute the correlation between demands and local observation using cross-attention mechanism. Moreover, our method can adapt to the communication resources of agents and accelerate the training progress by appropriating the ideal policy which is trained with joint observation. Experimental results reveal that DCMAC significantly outperforms the baseline algorithms in both unconstrained and communication constrained scenarios.
<div id='section'>Paperid: <span id='pid'>561, <a href='https://arxiv.org/pdf/2407.09124.pdf' target='_blank'>https://arxiv.org/pdf/2407.09124.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shun Kotoku, Takatomo Mihana, AndrÃ© RÃ¶hm, Ryoichi Horisaki
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.09124">Decentralized multi-agent reinforcement learning algorithm using a cluster-synchronized laser network</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent reinforcement learning (MARL) studies crucial principles that are applicable to a variety of fields, including wireless networking and autonomous driving. We propose a photonic-based decision-making algorithm to address one of the most fundamental problems in MARL, called the competitive multi-armed bandit (CMAB) problem. Our numerical simulations demonstrate that chaotic oscillations and cluster synchronization of optically coupled lasers, along with our proposed decentralized coupling adjustment, efficiently balance exploration and exploitation while facilitating cooperative decision-making without explicitly sharing information among agents. Our study demonstrates how decentralized reinforcement learning can be achieved by exploiting complex physical processes controlled by simple algorithms.
<div id='section'>Paperid: <span id='pid'>562, <a href='https://arxiv.org/pdf/2407.05766.pdf' target='_blank'>https://arxiv.org/pdf/2407.05766.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Amine Tellache, Amdjed Mokhtari, Abdelaziz Amara Korba, Yacine Ghamri-Doudane
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.05766">Multi-agent Reinforcement Learning-based Network Intrusion Detection System</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Intrusion Detection Systems (IDS) play a crucial role in ensuring the security of computer networks. Machine learning has emerged as a popular approach for intrusion detection due to its ability to analyze and detect patterns in large volumes of data. However, current ML-based IDS solutions often struggle to keep pace with the ever-changing nature of attack patterns and the emergence of new attack types. Additionally, these solutions face challenges related to class imbalance, where the number of instances belonging to different classes (normal and intrusions) is significantly imbalanced, which hinders their ability to effectively detect minor classes. In this paper, we propose a novel multi-agent reinforcement learning (RL) architecture, enabling automatic, efficient, and robust network intrusion detection. To enhance the capabilities of the proposed model, we have improved the DQN algorithm by implementing the weighted mean square loss function and employing cost-sensitive learning techniques. Our solution introduces a resilient architecture designed to accommodate the addition of new attacks and effectively adapt to changes in existing attack patterns. Experimental results realized using CIC-IDS-2017 dataset, demonstrate that our approach can effectively handle the class imbalance problem and provide a fine grained classification of attacks with a very low false positive rate. In comparison to the current state-of-the-art works, our solution demonstrates a significant superiority in both detection rate and false positive rate.
<div id='section'>Paperid: <span id='pid'>563, <a href='https://arxiv.org/pdf/2407.01343.pdf' target='_blank'>https://arxiv.org/pdf/2407.01343.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Callum Rhys Tilbury, Claude Formanek, Louise Beyers, Jonathan P. Shock, Arnu Pretorius
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.01343">Coordination Failure in Cooperative Offline MARL</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Offline multi-agent reinforcement learning (MARL) leverages static datasets of experience to learn optimal multi-agent control. However, learning from static data presents several unique challenges to overcome. In this paper, we focus on coordination failure and investigate the role of joint actions in multi-agent policy gradients with offline data, focusing on a common setting we refer to as the 'Best Response Under Data' (BRUD) approach. By using two-player polynomial games as an analytical tool, we demonstrate a simple yet overlooked failure mode of BRUD-based algorithms, which can lead to catastrophic coordination failure in the offline setting. Building on these insights, we propose an approach to mitigate such failure, by prioritising samples from the dataset based on joint-action similarity during policy learning and demonstrate its effectiveness in detailed experiments. More generally, however, we argue that prioritised dataset sampling is a promising area for innovation in offline MARL that can be combined with other effective approaches such as critic and policy regularisation. Importantly, our work shows how insights drawn from simplified, tractable games can lead to useful, theoretically grounded insights that transfer to more complex contexts. A core dimension of offering is an interactive notebook, from which almost all of our results can be reproduced, in a browser.
<div id='section'>Paperid: <span id='pid'>564, <a href='https://arxiv.org/pdf/2406.13992.pdf' target='_blank'>https://arxiv.org/pdf/2406.13992.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Muhammad Aneeq uz Zaman, Mathieu LauriÃ¨re, Alec Koppel, Tamer BaÅar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.13992">Robust Cooperative Multi-Agent Reinforcement Learning:A Mean-Field Type Game Perspective</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we study the problem of robust cooperative multi-agent reinforcement learning (RL) where a large number of cooperative agents with distributed information aim to learn policies in the presence of \emph{stochastic} and \emph{non-stochastic} uncertainties whose distributions are respectively known and unknown. Focusing on policy optimization that accounts for both types of uncertainties, we formulate the problem in a worst-case (minimax) framework, which is is intractable in general. Thus, we focus on the Linear Quadratic setting to derive benchmark solutions. First, since no standard theory exists for this problem due to the distributed information structure, we utilize the Mean-Field Type Game (MFTG) paradigm to establish guarantees on the solution quality in the sense of achieved Nash equilibrium of the MFTG. This in turn allows us to compare the performance against the corresponding original robust multi-agent control problem. Then, we propose a Receding-horizon Gradient Descent Ascent RL algorithm to find the MFTG Nash equilibrium and we prove a non-asymptotic rate of convergence. Finally, we provide numerical experiments to demonstrate the efficacy of our approach relative to a baseline algorithm.
<div id='section'>Paperid: <span id='pid'>565, <a href='https://arxiv.org/pdf/2406.09068.pdf' target='_blank'>https://arxiv.org/pdf/2406.09068.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Claude Formanek, Callum Rhys Tilbury, Louise Beyers, Jonathan Shock, Arnu Pretorius
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.09068">Dispelling the Mirage of Progress in Offline MARL through Standardised Baselines and Evaluation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Offline multi-agent reinforcement learning (MARL) is an emerging field with great promise for real-world applications. Unfortunately, the current state of research in offline MARL is plagued by inconsistencies in baselines and evaluation protocols, which ultimately makes it difficult to accurately assess progress, trust newly proposed innovations, and allow researchers to easily build upon prior work. In this paper, we firstly identify significant shortcomings in existing methodologies for measuring the performance of novel algorithms through a representative study of published offline MARL work. Secondly, by directly comparing to this prior work, we demonstrate that simple, well-implemented baselines can achieve state-of-the-art (SOTA) results across a wide range of tasks. Specifically, we show that on 35 out of 47 datasets used in prior work (almost 75% of cases), we match or surpass the performance of the current purported SOTA. Strikingly, our baselines often substantially outperform these more sophisticated algorithms. Finally, we correct for the shortcomings highlighted from this prior work by introducing a straightforward standardised methodology for evaluation and by providing our baseline implementations with statistically robust results across several scenarios, useful for comparisons in future work. Our proposal includes simple and sensible steps that are easy to adopt, which in combination with solid baselines and comparative results, could substantially improve the overall rigour of empirical science in offline MARL moving forward.
<div id='section'>Paperid: <span id='pid'>566, <a href='https://arxiv.org/pdf/2405.17439.pdf' target='_blank'>https://arxiv.org/pdf/2405.17439.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao Zhou, Chengming Hu, Xue Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.17439">An Overview of Machine Learning-Enabled Optimization for Reconfigurable Intelligent Surfaces-Aided 6G Networks: From Reinforcement Learning to Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reconfigurable intelligent surface (RIS) becomes a promising technique for 6G networks by reshaping signal propagation in smart radio environments. However, it also leads to significant complexity for network management due to the large number of elements and dedicated phase-shift optimization. In this work, we provide an overview of machine learning (ML)-enabled optimization for RIS-aided 6G networks. In particular, we focus on various reinforcement learning (RL) techniques, e.g., deep Q-learning, multi-agent reinforcement learning, transfer reinforcement learning, hierarchical reinforcement learning, and offline reinforcement learning. Different from existing studies, this work further discusses how large language models (LLMs) can be combined with RL to handle network optimization problems. It shows that LLM offers new opportunities to enhance the capabilities of RL algorithms in terms of generalization, reward function design, multi-modal information processing, etc. Finally, we identify the future challenges and directions of ML-enabled optimization for RIS-aided 6G networks.
<div id='section'>Paperid: <span id='pid'>567, <a href='https://arxiv.org/pdf/2405.15054.pdf' target='_blank'>https://arxiv.org/pdf/2405.15054.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Matteo Bettini, Ryan Kortvelesy, Amanda Prorok
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.15054">Controlling Behavioral Diversity in Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The study of behavioral diversity in Multi-Agent Reinforcement Learning (MARL) is a nascent yet promising field. In this context, the present work deals with the question of how to control the diversity of a multi-agent system. With no existing approaches to control diversity to a set value, current solutions focus on blindly promoting it via intrinsic rewards or additional loss functions, effectively changing the learning objective and lacking a principled measure for it. To address this, we introduce Diversity Control (DiCo), a method able to control diversity to an exact value of a given metric by representing policies as the sum of a parameter-shared component and dynamically scaled per-agent components. By applying constraints directly to the policy architecture, DiCo leaves the learning objective unchanged, enabling its applicability to any actor-critic MARL algorithm. We theoretically prove that DiCo achieves the desired diversity, and we provide several experiments, both in cooperative and competitive tasks, that show how DiCo can be employed as a novel paradigm to increase performance and sample efficiency in MARL. Multimedia results are available on the paper's website: https://sites.google.com/view/dico-marl.
<div id='section'>Paperid: <span id='pid'>568, <a href='https://arxiv.org/pdf/2405.14078.pdf' target='_blank'>https://arxiv.org/pdf/2405.14078.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Han-Dong Lim, Donghwan Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.14078">A finite time analysis of distributed Q-learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent reinforcement learning (MARL) has witnessed a remarkable surge in interest, fueled by the empirical success achieved in applications of single-agent reinforcement learning (RL). In this study, we consider a distributed Q-learning scenario, wherein a number of agents cooperatively solve a sequential decision making problem without access to the central reward function which is an average of the local rewards. In particular, we study finite-time analysis of a distributed Q-learning algorithm, and provide a new sample complexity result of $\tilde{\mathcal{O}}\left( \min\left\{\frac{1}{Îµ^2}\frac{t_{\text{mix}}}{(1-Î³)^6 d_{\min}^4 } ,\frac{1}Îµ\frac{\sqrt{|\gS||\gA|}}{(1-Ï_2(\boldsymbol{W}))(1-Î³)^4 d_{\min}^3} \right\}\right)$ under tabular lookup
<div id='section'>Paperid: <span id='pid'>569, <a href='https://arxiv.org/pdf/2404.07559.pdf' target='_blank'>https://arxiv.org/pdf/2404.07559.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dan Qiao, Yu-Xiang Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.07559">Differentially Private Reinforcement Learning with Self-Play</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We study the problem of multi-agent reinforcement learning (multi-agent RL) with differential privacy (DP) constraints. This is well-motivated by various real-world applications involving sensitive data, where it is critical to protect users' private information. We first extend the definitions of Joint DP (JDP) and Local DP (LDP) to two-player zero-sum episodic Markov Games, where both definitions ensure trajectory-wise privacy protection. Then we design a provably efficient algorithm based on optimistic Nash value iteration and privatization of Bernstein-type bonuses. The algorithm is able to satisfy JDP and LDP requirements when instantiated with appropriate privacy mechanisms. Furthermore, for both notions of DP, our regret bound generalizes the best known result under the single-agent RL case, while our regret could also reduce to the best known result for multi-agent RL without privacy constraints. To the best of our knowledge, these are the first line of results towards understanding trajectory-wise privacy protection in multi-agent RL.
<div id='section'>Paperid: <span id='pid'>570, <a href='https://arxiv.org/pdf/2404.03227.pdf' target='_blank'>https://arxiv.org/pdf/2404.03227.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xingran Chen, Navid NaderiAlizadeh, Alejandro Ribeiro, Shirin Saeedi Bidokhti
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.03227">Decentralized Learning Strategies for Estimation Error Minimization with Graph Neural Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We address the challenge of sampling and remote estimation for autoregressive Markovian processes in a multi-hop wireless network with statistically-identical agents. Agents cache the most recent samples from others and communicate over wireless collision channels governed by an underlying graph topology. Our goal is to minimize time-average estimation error and/or age of information with decentralized scalable sampling and transmission policies, considering both oblivious (where decision-making is independent of the physical processes) and non-oblivious policies (where decision-making depends on physical processes). We prove that in oblivious policies, minimizing estimation error is equivalent to minimizing the age of information. The complexity of the problem, especially the multi-dimensional action spaces and arbitrary network topologies, makes theoretical methods for finding optimal transmission policies intractable. We optimize the policies using a graphical multi-agent reinforcement learning framework, where each agent employs a permutation-equivariant graph neural network architecture. Theoretically, we prove that our proposed framework exhibits desirable transferability properties, allowing transmission policies trained on small- or moderate-size networks to be executed effectively on large-scale topologies. Numerical experiments demonstrate that (i) Our proposed framework outperforms state-of-the-art baselines; (ii) The trained policies are transferable to larger networks, and their performance gains increase with the number of agents; (iii) The training procedure withstands non-stationarity even if we utilize independent learning techniques; and, (iv) Recurrence is pivotal in both independent learning and centralized training and decentralized execution, and improves the resilience to non-stationarity in independent learning.
<div id='section'>Paperid: <span id='pid'>571, <a href='https://arxiv.org/pdf/2404.03227.pdf' target='_blank'>https://arxiv.org/pdf/2404.03227.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xingran Chen, Navid NaderiAlizadeh, Alejandro Ribeiro, Shirin Saeedi Bidokhti
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.03227">Decentralized Learning Strategies for Estimation Error Minimization with Graph Neural Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We address the challenge of sampling and remote estimation for autoregressive Markovian processes in a multi-hop wireless network with statistically-identical agents. Agents cache the most recent samples from others and communicate over wireless collision channels governed by an underlying graph topology. Our goal is to minimize time-average estimation error and/or age of information with decentralized scalable sampling and transmission policies, considering both oblivious (where decision-making is independent of the physical processes) and non-oblivious policies (where decision-making depends on physical processes). We prove that in oblivious policies, minimizing estimation error is equivalent to minimizing the age of information. The complexity of the problem, especially the multi-dimensional action spaces and arbitrary network topologies, makes theoretical methods for finding optimal transmission policies intractable. We optimize the policies using a graphical multi-agent reinforcement learning framework, where each agent employs a permutation-equivariant graph neural network architecture. Theoretically, we prove that our proposed framework exhibits desirable transferability properties, allowing transmission policies trained on small- or moderate-size networks to be executed effectively on large-scale topologies. Numerical experiments demonstrate that (i) Our proposed framework outperforms state-of-the-art baselines; (ii) The trained policies are transferable to larger networks, and their performance gains increase with the number of agents; (iii) The training procedure withstands non-stationarity even if we utilize independent learning techniques; and, (iv) Recurrence is pivotal in both independent learning and centralized training and decentralized execution, and improves the resilience to non-stationarity in independent learning.
<div id='section'>Paperid: <span id='pid'>572, <a href='https://arxiv.org/pdf/2404.01999.pdf' target='_blank'>https://arxiv.org/pdf/2404.01999.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Samuel Tovey, Christoph Lohrmann, Christian Holm
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.01999">Emergence of Chemotactic Strategies with Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reinforcement learning (RL) is a flexible and efficient method for programming micro-robots in complex environments. Here we investigate whether reinforcement learning can provide insights into biological systems when trained to perform chemotaxis. Namely, whether we can learn about how intelligent agents process given information in order to swim towards a target. We run simulations covering a range of agent shapes, sizes, and swim speeds to determine if the physical constraints on biological swimmers, namely Brownian motion, lead to regions where reinforcement learners' training fails. We find that the RL agents can perform chemotaxis as soon as it is physically possible and, in some cases, even before the active swimming overpowers the stochastic environment. We study the efficiency of the emergent policy and identify convergence in agent size and swim speeds. Finally, we study the strategy adopted by the reinforcement learning algorithm to explain how the agents perform their tasks. To this end, we identify three emerging dominant strategies and several rare approaches taken. These strategies, whilst producing almost identical trajectories in simulation, are distinct and give insight into the possible mechanisms behind which biological agents explore their environment and respond to changing conditions.
<div id='section'>Paperid: <span id='pid'>573, <a href='https://arxiv.org/pdf/2403.08936.pdf' target='_blank'>https://arxiv.org/pdf/2403.08936.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Peihong Yu, Manav Mishra, Alec Koppel, Carl Busart, Priya Narayan, Dinesh Manocha, Amrit Bedi, Pratap Tokekar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.08936">Beyond Joint Demonstrations: Personalized Expert Guidance for Efficient Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-Agent Reinforcement Learning (MARL) algorithms face the challenge of efficient exploration due to the exponential increase in the size of the joint state-action space. While demonstration-guided learning has proven beneficial in single-agent settings, its direct applicability to MARL is hindered by the practical difficulty of obtaining joint expert demonstrations. In this work, we introduce a novel concept of personalized expert demonstrations, tailored for each individual agent or, more broadly, each individual type of agent within a heterogeneous team. These demonstrations solely pertain to single-agent behaviors and how each agent can achieve personal goals without encompassing any cooperative elements, thus naively imitating them will not achieve cooperation due to potential conflicts. To this end, we propose an approach that selectively utilizes personalized expert demonstrations as guidance and allows agents to learn to cooperate, namely personalized expert-guided MARL (PegMARL). This algorithm utilizes two discriminators: the first provides incentives based on the alignment of individual agent behavior with demonstrations, and the second regulates incentives based on whether the behaviors lead to the desired outcome. We evaluate PegMARL using personalized demonstrations in both discrete and continuous environments. The experimental results demonstrate that PegMARL outperforms state-of-the-art MARL algorithms in solving coordinated tasks, achieving strong performance even when provided with suboptimal personalized demonstrations. We also showcase PegMARL's capability of leveraging joint demonstrations in the StarCraft scenario and converging effectively even with demonstrations from non-co-trained policies.
<div id='section'>Paperid: <span id='pid'>574, <a href='https://arxiv.org/pdf/2402.02097.pdf' target='_blank'>https://arxiv.org/pdf/2402.02097.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haobin Jiang, Ziluo Ding, Zongqing Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.02097">Settling Decentralized Multi-Agent Coordinated Exploration by Novelty Sharing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Exploration in decentralized cooperative multi-agent reinforcement learning faces two challenges. One is that the novelty of global states is unavailable, while the novelty of local observations is biased. The other is how agents can explore in a coordinated way. To address these challenges, we propose MACE, a simple yet effective multi-agent coordinated exploration method. By communicating only local novelty, agents can take into account other agents' local novelty to approximate the global novelty. Further, we newly introduce weighted mutual information to measure the influence of one agent's action on other agents' accumulated novelty. We convert it as an intrinsic reward in hindsight to encourage agents to exert more influence on other agents' exploration and boost coordinated exploration. Empirically, we show that MACE achieves superior performance in three multi-agent environments with sparse rewards.
<div id='section'>Paperid: <span id='pid'>575, <a href='https://arxiv.org/pdf/2402.01111.pdf' target='_blank'>https://arxiv.org/pdf/2402.01111.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dan Qiao, Yu-Xiang Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.01111">Near-Optimal Reinforcement Learning with Self-Play under Adaptivity Constraints</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We study the problem of multi-agent reinforcement learning (MARL) with adaptivity constraints -- a new problem motivated by real-world applications where deployments of new policies are costly and the number of policy updates must be minimized. For two-player zero-sum Markov Games, we design a (policy) elimination based algorithm that achieves a regret of $\widetilde{O}(\sqrt{H^3 S^2 ABK})$, while the batch complexity is only $O(H+\log\log K)$. In the above, $S$ denotes the number of states, $A,B$ are the number of actions for the two players respectively, $H$ is the horizon and $K$ is the number of episodes. Furthermore, we prove a batch complexity lower bound $Î©(\frac{H}{\log_{A}K}+\log\log K)$ for all algorithms with $\widetilde{O}(\sqrt{K})$ regret bound, which matches our upper bound up to logarithmic factors. As a byproduct, our techniques naturally extend to learning bandit games and reward-free MARL within near optimal batch complexity. To the best of our knowledge, these are the first line of results towards understanding MARL with low adaptivity.
<div id='section'>Paperid: <span id='pid'>576, <a href='https://arxiv.org/pdf/2402.00787.pdf' target='_blank'>https://arxiv.org/pdf/2402.00787.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Benjamin Patrick Evans, Sumitra Ganesh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.00787">Learning and Calibrating Heterogeneous Bounded Rational Market Behaviour with Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Agent-based models (ABMs) have shown promise for modelling various real world phenomena incompatible with traditional equilibrium analysis. However, a critical concern is the manual definition of behavioural rules in ABMs. Recent developments in multi-agent reinforcement learning (MARL) offer a way to address this issue from an optimisation perspective, where agents strive to maximise their utility, eliminating the need for manual rule specification. This learning-focused approach aligns with established economic and financial models through the use of rational utility-maximising agents. However, this representation departs from the fundamental motivation for ABMs: that realistic dynamics emerging from bounded rationality and agent heterogeneity can be modelled. To resolve this apparent disparity between the two approaches, we propose a novel technique for representing heterogeneous processing-constrained agents within a MARL framework. The proposed approach treats agents as constrained optimisers with varying degrees of strategic skills, permitting departure from strict utility maximisation. Behaviour is learnt through repeated simulations with policy gradients to adjust action likelihoods. To allow efficient computation, we use parameterised shared policy learning with distributions of agent skill levels. Shared policy learning avoids the need for agents to learn individual policies yet still enables a spectrum of bounded rational behaviours. We validate our model's effectiveness using real-world data on a range of canonical $n$-agent settings, demonstrating significantly improved predictive capability.
<div id='section'>Paperid: <span id='pid'>577, <a href='https://arxiv.org/pdf/2402.00091.pdf' target='_blank'>https://arxiv.org/pdf/2402.00091.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinxuan Chen, Mustafa Ozger, Cicek Cavdar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.00091">Nash Soft Actor-Critic LEO Satellite Handover Management Algorithm for Flying Vehicles</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Compared with the terrestrial networks (TN), which can only support limited coverage areas, low-earth orbit (LEO) satellites can provide seamless global coverage and high survivability in case of emergencies. Nevertheless, the swift movement of the LEO satellites poses a challenge: frequent handovers are inevitable, compromising the quality of service (QoS) of users and leading to discontinuous connectivity. Moreover, considering LEO satellite connectivity for different flying vehicles (FVs) when coexisting with ground terminals, an efficient satellite handover decision control and mobility management strategy is required to reduce the number of handovers and allocate resources that align with different users' requirements. In this paper, a novel distributed satellite handover strategy based on Multi-Agent Reinforcement Learning (MARL) and game theory named Nash-SAC has been proposed to solve these problems. From the simulation results, the Nash-SAC-based handover strategy can effectively reduce the handovers by over 16 percent and the blocking rate by over 18 percent, outperforming local benchmarks such as traditional Q-learning. It also greatly improves the network utility used to quantify the performance of the whole system by up to 48 percent and caters to different users requirements, providing reliable and robust connectivity for both FVs and ground terminals.
<div id='section'>Paperid: <span id='pid'>578, <a href='https://arxiv.org/pdf/2312.08468.pdf' target='_blank'>https://arxiv.org/pdf/2312.08468.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wiem Khlifi, Siddarth Singh, Omayma Mahjoub, Ruan de Kock, Abidine Vall, Rihab Gorsane, Arnu Pretorius
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.08468">On Diagnostics for Understanding Agent Training Behaviour in Cooperative MARL</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cooperative multi-agent reinforcement learning (MARL) has made substantial strides in addressing the distributed decision-making challenges. However, as multi-agent systems grow in complexity, gaining a comprehensive understanding of their behaviour becomes increasingly challenging. Conventionally, tracking team rewards over time has served as a pragmatic measure to gauge the effectiveness of agents in learning optimal policies. Nevertheless, we argue that relying solely on the empirical returns may obscure crucial insights into agent behaviour. In this paper, we explore the application of explainable AI (XAI) tools to gain profound insights into agent behaviour. We employ these diagnostics tools within the context of Level-Based Foraging and Multi-Robot Warehouse environments and apply them to a diverse array of MARL algorithms. We demonstrate how our diagnostics can enhance the interpretability and explainability of MARL systems, providing a better understanding of agent behaviour.
<div id='section'>Paperid: <span id='pid'>579, <a href='https://arxiv.org/pdf/2312.08466.pdf' target='_blank'>https://arxiv.org/pdf/2312.08466.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Omayma Mahjoub, Ruan de Kock, Siddarth Singh, Wiem Khlifi, Abidine Vall, Kale-ab Tessera, Arnu Pretorius
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.08466">Efficiently Quantifying Individual Agent Importance in Cooperative MARL</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Measuring the contribution of individual agents is challenging in cooperative multi-agent reinforcement learning (MARL). In cooperative MARL, team performance is typically inferred from a single shared global reward. Arguably, among the best current approaches to effectively measure individual agent contributions is to use Shapley values. However, calculating these values is expensive as the computational complexity grows exponentially with respect to the number of agents. In this paper, we adapt difference rewards into an efficient method for quantifying the contribution of individual agents, referred to as Agent Importance, offering a linear computational complexity relative to the number of agents. We show empirically that the computed values are strongly correlated with the true Shapley values, as well as the true underlying individual agent rewards, used as the ground truth in environments where these are available. We demonstrate how Agent Importance can be used to help study MARL systems by diagnosing algorithmic failures discovered in prior MARL benchmarking work. Our analysis illustrates Agent Importance as a valuable explainability component for future MARL benchmarks.
<div id='section'>Paperid: <span id='pid'>580, <a href='https://arxiv.org/pdf/2312.08463.pdf' target='_blank'>https://arxiv.org/pdf/2312.08463.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Siddarth Singh, Omayma Mahjoub, Ruan de Kock, Wiem Khlifi, Abidine Vall, Kale-ab Tessera, Arnu Pretorius
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.08463">How much can change in a year? Revisiting Evaluation in Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Establishing sound experimental standards and rigour is important in any growing field of research. Deep Multi-Agent Reinforcement Learning (MARL) is one such nascent field. Although exciting progress has been made, MARL has recently come under scrutiny for replicability issues and a lack of standardised evaluation methodology, specifically in the cooperative setting. Although protocols have been proposed to help alleviate the issue, it remains important to actively monitor the health of the field. In this work, we extend the database of evaluation methodology previously published by containing meta-data on MARL publications from top-rated conferences and compare the findings extracted from this updated database to the trends identified in their work. Our analysis shows that many of the worrying trends in performance reporting remain. This includes the omission of uncertainty quantification, not reporting all relevant evaluation details and a narrowing of algorithmic development classes. Promisingly, we do observe a trend towards more difficult scenarios in SMAC-v1, which if continued into SMAC-v2 will encourage novel algorithmic development. Our data indicate that replicability needs to be approached more proactively by the MARL community to ensure trust in the field as we move towards exciting new frontiers.
<div id='section'>Paperid: <span id='pid'>581, <a href='https://arxiv.org/pdf/2311.18598.pdf' target='_blank'>https://arxiv.org/pdf/2311.18598.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kale-ab Tessera, Callum Rhys Tilbury, Sasha Abramowitz, Ruan de Kock, Omayma Mahjoub, Benjamin Rosman, Sara Hooker, Arnu Pretorius
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.18598">Generalisable Agents for Neural Network Optimisation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Optimising deep neural networks is a challenging task due to complex training dynamics, high computational requirements, and long training times. To address this difficulty, we propose the framework of Generalisable Agents for Neural Network Optimisation (GANNO) -- a multi-agent reinforcement learning (MARL) approach that learns to improve neural network optimisation by dynamically and responsively scheduling hyperparameters during training. GANNO utilises an agent per layer that observes localised network dynamics and accordingly takes actions to adjust these dynamics at a layerwise level to collectively improve global performance. In this paper, we use GANNO to control the layerwise learning rate and show that the framework can yield useful and responsive schedules that are competitive with handcrafted heuristics. Furthermore, GANNO is shown to perform robustly across a wide variety of unseen initial conditions, and can successfully generalise to harder problems than it was trained on. Our work presents an overview of the opportunities that this paradigm offers for training neural networks, along with key challenges that remain to be overcome.
<div id='section'>Paperid: <span id='pid'>582, <a href='https://arxiv.org/pdf/2310.02387.pdf' target='_blank'>https://arxiv.org/pdf/2310.02387.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ioannis Panageas, Nikolas Patris, Stratis Skoulakis, Volkan Cevher
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.02387">Exponential Lower Bounds for Fictitious Play in Potential Games</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Fictitious Play (FP) is a simple and natural dynamic for repeated play with many applications in game theory and multi-agent reinforcement learning. It was introduced by Brown (1949,1951) and its convergence properties for two-player zero-sum games was established later by Robinson (1951). Potential games Monderer and Shapley (1996b) is another class of games which exhibit the FP property (Monderer and Shapley (1996a)), i.e., FP dynamics converges to a Nash equilibrium if all agents follows it. Nevertheless, except for two-player zero-sum games and for specific instances of payoff matrices (Abernethy et al. (2021)) or for adversarial tie-breaking rules (Daskalakis and Pan (2014)), the convergence rate of FP is unknown. In this work, we focus on the rate of convergence of FP when applied to potential games and more specifically identical payoff games. We prove that FP can take exponential time (in the number of strategies) to reach a Nash equilibrium, even if the game is restricted to two agents and for arbitrary tie-breaking rules. To prove this, we recursively construct a two-player coordination game with a unique Nash equilibrium. Moreover, every approximate Nash equilibrium in the constructed game must be close to the pure Nash equilibrium in $\ell_1$-distance.
<div id='section'>Paperid: <span id='pid'>583, <a href='https://arxiv.org/pdf/2308.10284.pdf' target='_blank'>https://arxiv.org/pdf/2308.10284.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hadi Nekoei, Xutong Zhao, Janarthanan Rajendran, Miao Liu, Sarath Chandar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.10284">Towards Few-shot Coordination: Revisiting Ad-hoc Teamplay Challenge In the Game of Hanabi</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cooperative Multi-agent Reinforcement Learning (MARL) algorithms with Zero-Shot Coordination (ZSC) have gained significant attention in recent years. ZSC refers to the ability of agents to coordinate zero-shot (without additional interaction experience) with independently trained agents. While ZSC is crucial for cooperative MARL agents, it might not be possible for complex tasks and changing environments. Agents also need to adapt and improve their performance with minimal interaction with other agents. In this work, we show empirically that state-of-the-art ZSC algorithms have poor performance when paired with agents trained with different learning methods, and they require millions of interaction samples to adapt to these new partners. To investigate this issue, we formally defined a framework based on a popular cooperative multi-agent game called Hanabi to evaluate the adaptability of MARL methods. In particular, we created a diverse set of pre-trained agents and defined a new metric called adaptation regret that measures the agent's ability to efficiently adapt and improve its coordination performance when paired with some held-out pool of partners on top of its ZSC performance. After evaluating several SOTA algorithms using our framework, our experiments reveal that naive Independent Q-Learning (IQL) agents in most cases adapt as quickly as the SOTA ZSC algorithm Off-Belief Learning (OBL). This finding raises an interesting research question: How to design MARL algorithms with high ZSC performance and capability of fast adaptation to unseen partners. As a first step, we studied the role of different hyper-parameters and design choices on the adaptability of current MARL algorithms. Our experiments show that two categories of hyper-parameters controlling the training data diversity and optimization process have a significant impact on the adaptability of Hanabi agents.
<div id='section'>Paperid: <span id='pid'>584, <a href='https://arxiv.org/pdf/2308.08858.pdf' target='_blank'>https://arxiv.org/pdf/2308.08858.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Songtao Feng, Ming Yin, Yu-Xiang Wang, Jing Yang, Yingbin Liang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.08858">Improving Sample Efficiency of Model-Free Algorithms for Zero-Sum Markov Games</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The problem of two-player zero-sum Markov games has recently attracted increasing interests in theoretical studies of multi-agent reinforcement learning (RL). In particular, for finite-horizon episodic Markov decision processes (MDPs), it has been shown that model-based algorithms can find an $Îµ$-optimal Nash Equilibrium (NE) with the sample complexity of $O(H^3SAB/Îµ^2)$, which is optimal in the dependence of the horizon $H$ and the number of states $S$ (where $A$ and $B$ denote the number of actions of the two players, respectively). However, none of the existing model-free algorithms can achieve such an optimality. In this work, we propose a model-free stage-based Q-learning algorithm and show that it achieves the same sample complexity as the best model-based algorithm, and hence for the first time demonstrate that model-free algorithms can enjoy the same optimality in the $H$ dependence as model-based algorithms. The main improvement of the dependency on $H$ arises by leveraging the popular variance reduction technique based on the reference-advantage decomposition previously used only for single-agent RL. However, such a technique relies on a critical monotonicity property of the value function, which does not hold in Markov games due to the update of the policy via the coarse correlated equilibrium (CCE) oracle. Thus, to extend such a technique to Markov games, our algorithm features a key novel design of updating the reference value functions as the pair of optimistic and pessimistic value functions whose value difference is the smallest in the history in order to achieve the desired improvement in the sample efficiency.
<div id='section'>Paperid: <span id='pid'>585, <a href='https://arxiv.org/pdf/2307.15530.pdf' target='_blank'>https://arxiv.org/pdf/2307.15530.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jingqing Ruan, Xiaotian Hao, Dong Li, Hangyu Mao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.15530">Learning to Collaborate by Grouping: a Consensus-oriented Strategy for Multi-agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent systems require effective coordination between groups and individuals to achieve common goals. However, current multi-agent reinforcement learning (MARL) methods primarily focus on improving individual policies and do not adequately address group-level policies, which leads to weak cooperation. To address this issue, we propose a novel Consensus-oriented Strategy (CoS) that emphasizes group and individual policies simultaneously. Specifically, CoS comprises two main components: (a) the vector quantized group consensus module, which extracts discrete latent embeddings that represent the stable and discriminative group consensus, and (b) the group consensus-oriented strategy, which integrates the group policy using a hypernet and the individual policies using the group consensus, thereby promoting coordination at both the group and individual levels. Through empirical experiments on cooperative navigation tasks with both discrete and continuous spaces, as well as Google research football, we demonstrate that CoS outperforms state-of-the-art MARL algorithms and achieves better collaboration, thus providing a promising solution for achieving effective coordination in multi-agent systems.
<div id='section'>Paperid: <span id='pid'>586, <a href='https://arxiv.org/pdf/2307.00994.pdf' target='_blank'>https://arxiv.org/pdf/2307.00994.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Samuel Tovey, David Zimmer, Christoph Lohrmann, Tobias Merkt, Simon Koppenhoefer, Veit-Lorenz Heuthe, Clemens Bechinger, Christian Holm
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.00994">Environmental effects on emergent strategy in micro-scale multi-agent reinforcement learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-Agent Reinforcement Learning (MARL) is a promising candidate for realizing efficient control of microscopic particles, of which micro-robots are a subset. However, the microscopic particles' environment presents unique challenges, such as Brownian motion at sufficiently small length-scales. In this work, we explore the role of temperature in the emergence and efficacy of strategies in MARL systems using particle-based Langevin molecular dynamics simulations as a realistic representation of micro-scale environments. To this end, we perform experiments on two different multi-agent tasks in microscopic environments at different temperatures, detecting the source of a concentration gradient and rotation of a rod. We find that at higher temperatures, the RL agents identify new strategies for achieving these tasks, highlighting the importance of understanding this regime and providing insight into optimal training strategies for bridging the generalization gap between simulation and reality. We also introduce a novel Python package for studying microscopic agents using reinforcement learning (RL) to accompany our results.
<div id='section'>Paperid: <span id='pid'>587, <a href='https://arxiv.org/pdf/2306.00187.pdf' target='_blank'>https://arxiv.org/pdf/2306.00187.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kailash Gogineni, Yongsheng Mei, Peng Wei, Tian Lan, Guru Venkataramani
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.00187">AccMER: Accelerating Multi-Agent Experience Replay with Cache Locality-aware Prioritization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-Agent Experience Replay (MER) is a key component of off-policy reinforcement learning~(RL) algorithms. By remembering and reusing experiences from the past, experience replay significantly improves the stability of RL algorithms and their learning efficiency. In many scenarios, multiple agents interact in a shared environment during online training under centralized training and decentralized execution~(CTDE) paradigm. Current multi-agent reinforcement learning~(MARL) algorithms consider experience replay with uniform sampling or based on priority weights to improve transition data sample efficiency in the sampling phase. However, moving transition data histories for each agent through the processor memory hierarchy is a performance limiter. Also, as the agents' transitions continuously renew every iteration, the finite cache capacity results in increased cache misses.
  To this end, we propose \name, that repeatedly reuses the transitions~(experiences) for a window of $n$ steps in order to improve the cache locality and minimize the transition data movement, instead of sampling new transitions at each step. Specifically, our optimization uses priority weights to select the transitions so that only high-priority transitions will be reused frequently, thereby improving the cache performance. Our experimental results on the Predator-Prey environment demonstrate the effectiveness of reusing the essential transitions based on the priority weights, where we observe an end-to-end training time reduction of $25.4\%$~(for $32$ agents) compared to existing prioritized MER algorithms without notable degradation in the mean reward.
<div id='section'>Paperid: <span id='pid'>588, <a href='https://arxiv.org/pdf/2305.05898.pdf' target='_blank'>https://arxiv.org/pdf/2305.05898.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiyun Li, Ziyi Ni, Jingqing Ruan, Linghui Meng, Jing Shi, Tielin Zhang, Bo Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.05898">Mixture of personality improved Spiking actor network for efficient multi-agent cooperation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Adaptive human-agent and agent-agent cooperation are becoming more and more critical in the research area of multi-agent reinforcement learning (MARL), where remarked progress has been made with the help of deep neural networks. However, many established algorithms can only perform well during the learning paradigm but exhibit poor generalization during cooperation with other unseen partners. The personality theory in cognitive psychology describes that humans can well handle the above cooperation challenge by predicting others' personalities first and then their complex actions. Inspired by this two-step psychology theory, we propose a biologically plausible mixture of personality (MoP) improved spiking actor network (SAN), whereby a determinantal point process is used to simulate the complex formation and integration of different types of personality in MoP, and dynamic and spiking neurons are incorporated into the SAN for the efficient reinforcement learning. The benchmark Overcooked task, containing a strong requirement for cooperative cooking, is selected to test the proposed MoP-SAN. The experimental results show that the MoP-SAN can achieve both high performances during not only the learning paradigm but also the generalization test (i.e., cooperation with other unseen agents) paradigm where most counterpart deep actor networks failed. Necessary ablation experiments and visualization analyses were conducted to explain why MoP and SAN are effective in multi-agent reinforcement learning scenarios while DNN performs poorly in the generalization test.
<div id='section'>Paperid: <span id='pid'>589, <a href='https://arxiv.org/pdf/2305.02128.pdf' target='_blank'>https://arxiv.org/pdf/2305.02128.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Matteo Bettini, Ajay Shankar, Amanda Prorok
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.02128">System Neural Diversity: Measuring Behavioral Heterogeneity in Multi-Agent Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Evolutionary science provides evidence that diversity confers resilience in natural systems. Yet, traditional multi-agent reinforcement learning techniques commonly enforce homogeneity to increase training sample efficiency. When a system of learning agents is not constrained to homogeneous policies, individuals may develop diverse behaviors, resulting in emergent complementarity that benefits the system. Despite this, there is a surprising lack of tools that quantify behavioral diversity. Such techniques would pave the way towards understanding the impact of diversity in collective artificial intelligence and enabling its control. In this paper, we introduce System Neural Diversity (SND): a measure of behavioral heterogeneity in multi-agent systems. We discuss and prove its theoretical properties, and compare it with alternate, state-of-the-art behavioral diversity metrics used in the robotics domain. Through simulations of a variety of cooperative multi-robot tasks, we show how our metric constitutes an important tool that enables measurement and control of behavioral heterogeneity. In dynamic tasks, where the problem is affected by repeated disturbances during training, we show that SND allows us to measure latent resilience skills acquired by the agents, while other proxies, such as task performance (reward), fail to. Finally, we show how the metric can be employed to control diversity, allowing us to enforce a desired heterogeneity set-point or range. We demonstrate how this paradigm can be used to bootstrap the exploration phase, finding optimal policies faster, thus enabling novel and more efficient MARL paradigms.
<div id='section'>Paperid: <span id='pid'>590, <a href='https://arxiv.org/pdf/2305.00684.pdf' target='_blank'>https://arxiv.org/pdf/2305.00684.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dylan J. Foster, Dean P. Foster, Noah Golowich, Alexander Rakhlin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.00684">On the Complexity of Multi-Agent Decision Making: From Learning in Games to Partial Monitoring</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A central problem in the theory of multi-agent reinforcement learning (MARL) is to understand what structural conditions and algorithmic principles lead to sample-efficient learning guarantees, and how these considerations change as we move from few to many agents. We study this question in a general framework for interactive decision making with multiple agents, encompassing Markov games with function approximation and normal-form games with bandit feedback. We focus on equilibrium computation, in which a centralized learning algorithm aims to compute an equilibrium by controlling multiple agents that interact with an unknown environment. Our main contributions are:
  - We provide upper and lower bounds on the optimal sample complexity for multi-agent decision making based on a multi-agent generalization of the Decision-Estimation Coefficient, a complexity measure introduced by Foster et al. (2021) in the single-agent counterpart to our setting. Compared to the best results for the single-agent setting, our bounds have additional gaps. We show that no "reasonable" complexity measure can close these gaps, highlighting a striking separation between single and multiple agents.
  - We show that characterizing the statistical complexity for multi-agent decision making is equivalent to characterizing the statistical complexity of single-agent decision making, but with hidden (unobserved) rewards, a framework that subsumes variants of the partial monitoring problem. As a consequence, we characterize the statistical complexity for hidden-reward interactive decision making to the best extent possible.
  Building on this development, we provide several new structural results, including 1) conditions under which the statistical complexity of multi-agent decision making can be reduced to that of single-agent, and 2) conditions under which the so-called curse of multiple agents can be avoided.
<div id='section'>Paperid: <span id='pid'>591, <a href='https://arxiv.org/pdf/2304.00977.pdf' target='_blank'>https://arxiv.org/pdf/2304.00977.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Claude Formanek, Callum Rhys Tilbury, Jonathan Shock, Kale-ab Tessera, Arnu Pretorius
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.00977">Selective Reincarnation: Offline-to-Online Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>'Reincarnation' in reinforcement learning has been proposed as a formalisation of reusing prior computation from past experiments when training an agent in an environment. In this paper, we present a brief foray into the paradigm of reincarnation in the multi-agent (MA) context. We consider the case where only some agents are reincarnated, whereas the others are trained from scratch -- selective reincarnation. In the fully-cooperative MA setting with heterogeneous agents, we demonstrate that selective reincarnation can lead to higher returns than training fully from scratch, and faster convergence than training with full reincarnation. However, the choice of which agents to reincarnate in a heterogeneous system is vitally important to the outcome of the training -- in fact, a poor choice can lead to considerably worse results than the alternatives. We argue that a rich field of work exists here, and we hope that our effort catalyses further energy in bringing the topic of reincarnation to the multi-agent realm.
<div id='section'>Paperid: <span id='pid'>592, <a href='https://arxiv.org/pdf/2303.09032.pdf' target='_blank'>https://arxiv.org/pdf/2303.09032.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xutong Zhao, Yangchen Pan, Chenjun Xiao, Sarath Chandar, Janarthanan Rajendran
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.09032">Conditionally Optimistic Exploration for Cooperative Deep Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Efficient exploration is critical in cooperative deep Multi-Agent Reinforcement Learning (MARL). In this work, we propose an exploration method that effectively encourages cooperative exploration based on the idea of sequential action-computation scheme. The high-level intuition is that to perform optimism-based exploration, agents would explore cooperative strategies if each agent's optimism estimate captures a structured dependency relationship with other agents. Assuming agents compute actions following a sequential order at \textit{each environment timestep}, we provide a perspective to view MARL as tree search iterations by considering agents as nodes at different depths of the search tree. Inspired by the theoretically justified tree search algorithm UCT (Upper Confidence bounds applied to Trees), we develop a method called Conditionally Optimistic Exploration (COE). COE augments each agent's state-action value estimate with an action-conditioned optimistic bonus derived from the visitation count of the global state and joint actions of preceding agents. COE is performed during training and disabled at deployment, making it compatible with any value decomposition method for centralized training with decentralized execution. Experiments across various cooperative MARL benchmarks show that COE outperforms current state-of-the-art exploration methods on hard-exploration tasks.
<div id='section'>Paperid: <span id='pid'>593, <a href='https://arxiv.org/pdf/2302.04151.pdf' target='_blank'>https://arxiv.org/pdf/2302.04151.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mert Kayaalp, Fatima Ghadieh, Ali H. Sayed
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.04151">Policy Evaluation in Decentralized POMDPs with Belief Sharing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Most works on multi-agent reinforcement learning focus on scenarios where the state of the environment is fully observable. In this work, we consider a cooperative policy evaluation task in which agents are not assumed to observe the environment state directly. Instead, agents can only have access to noisy observations and to belief vectors. It is well-known that finding global posterior distributions under multi-agent settings is generally NP-hard. As a remedy, we propose a fully decentralized belief forming strategy that relies on individual updates and on localized interactions over a communication network. In addition to the exchange of the beliefs, agents exploit the communication network by exchanging value function parameter estimates as well. We analytically show that the proposed strategy allows information to diffuse over the network, which in turn allows the agents' parameters to have a bounded difference with a centralized baseline. A multi-sensor target tracking application is considered in the simulations.
<div id='section'>Paperid: <span id='pid'>594, <a href='https://arxiv.org/pdf/2302.00521.pdf' target='_blank'>https://arxiv.org/pdf/2302.00521.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Claude Formanek, Asad Jeewa, Jonathan Shock, Arnu Pretorius
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.00521">Off-the-Grid MARL: Datasets with Baselines for Offline Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Being able to harness the power of large datasets for developing cooperative multi-agent controllers promises to unlock enormous value for real-world applications. Many important industrial systems are multi-agent in nature and are difficult to model using bespoke simulators. However, in industry, distributed processes can often be recorded during operation, and large quantities of demonstrative data stored. Offline multi-agent reinforcement learning (MARL) provides a promising paradigm for building effective decentralised controllers from such datasets. However, offline MARL is still in its infancy and therefore lacks standardised benchmark datasets and baselines typically found in more mature subfields of reinforcement learning (RL). These deficiencies make it difficult for the community to sensibly measure progress. In this work, we aim to fill this gap by releasing off-the-grid MARL (OG-MARL): a growing repository of high-quality datasets with baselines for cooperative offline MARL research. Our datasets provide settings that are characteristic of real-world systems, including complex environment dynamics, heterogeneous agents, non-stationarity, many agents, partial observability, suboptimality, sparse rewards and demonstrated coordination. For each setting, we provide a range of different dataset types (e.g. Good, Medium, Poor, and Replay) and profile the composition of experiences for each dataset. We hope that OG-MARL will serve the community as a reliable source of datasets and help drive progress, while also providing an accessible entry point for researchers new to the field.
<div id='section'>Paperid: <span id='pid'>595, <a href='https://arxiv.org/pdf/2301.07137.pdf' target='_blank'>https://arxiv.org/pdf/2301.07137.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Matteo Bettini, Ajay Shankar, Amanda Prorok
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.07137">Heterogeneous Multi-Robot Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cooperative multi-robot tasks can benefit from heterogeneity in the robots' physical and behavioral traits. In spite of this, traditional Multi-Agent Reinforcement Learning (MARL) frameworks lack the ability to explicitly accommodate policy heterogeneity, and typically constrain agents to share neural network parameters. This enforced homogeneity limits application in cases where the tasks benefit from heterogeneous behaviors. In this paper, we crystallize the role of heterogeneity in MARL policies. Towards this end, we introduce Heterogeneous Graph Neural Network Proximal Policy Optimization (HetGPPO), a paradigm for training heterogeneous MARL policies that leverages a Graph Neural Network for differentiable inter-agent communication. HetGPPO allows communicating agents to learn heterogeneous behaviors while enabling fully decentralized training in partially observable environments. We complement this with a taxonomical overview that exposes more heterogeneity classes than previously identified. To motivate the need for our model, we present a characterization of techniques that homogeneous models can leverage to emulate heterogeneous behavior, and show how this "apparent heterogeneity" is brittle in real-world conditions. Through simulations and real-world experiments, we show that: (i) when homogeneous methods fail due to strong heterogeneous requirements, HetGPPO succeeds, and, (ii) when homogeneous methods are able to learn apparently heterogeneous behaviors, HetGPPO achieves higher resilience to both training and deployment noise.
<div id='section'>Paperid: <span id='pid'>596, <a href='https://arxiv.org/pdf/2210.08872.pdf' target='_blank'>https://arxiv.org/pdf/2210.08872.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiqun Chen, Hangyu Mao, Jiaxin Mao, Shiguang Wu, Tianle Zhang, Bin Zhang, Wei Yang, Hongxing Chang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2210.08872">PTDE: Personalized Training with Distilled Execution for Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Centralized Training with Decentralized Execution (CTDE) has emerged as a widely adopted paradigm in multi-agent reinforcement learning, emphasizing the utilization of global information for learning an enhanced joint $Q$-function or centralized critic. In contrast, our investigation delves into harnessing global information to directly enhance individual $Q$-functions or individual actors. Notably, we discover that applying identical global information universally across all agents proves insufficient for optimal performance. Consequently, we advocate for the customization of global information tailored to each agent, creating agent-personalized global information to bolster overall performance. Furthermore, we introduce a novel paradigm named Personalized Training with Distilled Execution (PTDE), wherein agent-personalized global information is distilled into the agent's local information. This distilled information is then utilized during decentralized execution, resulting in minimal performance degradation. PTDE can be seamlessly integrated with state-of-the-art algorithms, leading to notable performance enhancements across diverse benchmarks, including the SMAC benchmark, Google Research Football (GRF) benchmark, and Learning to Rank (LTR) task.
<div id='section'>Paperid: <span id='pid'>597, <a href='https://arxiv.org/pdf/2205.13005.pdf' target='_blank'>https://arxiv.org/pdf/2205.13005.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ryan Kortvelesy, Amanda Prorok
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2205.13005">QGNN: Value Function Factorisation with Graph Neural Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In multi-agent reinforcement learning, the use of a global objective is a powerful tool for incentivising cooperation. Unfortunately, it is not sample-efficient to train individual agents with a global reward, because it does not necessarily correlate with an agent's individual actions. This problem can be solved by factorising the global value function into local value functions. Early work in this domain performed factorisation by conditioning local value functions purely on local information. Recently, it has been shown that providing both local information and an encoding of the global state can promote cooperative behaviour. In this paper we propose QGNN, the first value factorisation method to use a graph neural network (GNN) based model. The multi-layer message passing architecture of QGNN provides more representational complexity than models in prior work, allowing it to produce a more effective factorisation. QGNN also introduces a permutation invariant mixer which is able to match the performance of other methods, even with significantly fewer parameters. We evaluate our method against several baselines, including QMIX-Att, GraphMIX, QMIX, VDN, and hybrid architectures. Our experiments include Starcraft, the standard benchmark for credit assignment; Estimate Game, a custom environment that explicitly models inter-agent dependencies; and Coalition Structure Generation, a foundational problem with real-world applications. The results show that QGNN outperforms state-of-the-art value factorisation baselines consistently.
<div id='section'>Paperid: <span id='pid'>598, <a href='https://arxiv.org/pdf/2510.11474.pdf' target='_blank'>https://arxiv.org/pdf/2510.11474.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ardian Selmonaj, Giacomo Del Rio, Adrian Schneider, Alessandro Antonucci
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.11474">Coordinated Strategies in Realistic Air Combat by Hierarchical Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Achieving mission objectives in a realistic simulation of aerial combat is highly challenging due to imperfect situational awareness and nonlinear flight dynamics. In this work, we introduce a novel 3D multi-agent air combat environment and a Hierarchical Multi-Agent Reinforcement Learning framework to tackle these challenges. Our approach combines heterogeneous agent dynamics, curriculum learning, league-play, and a newly adapted training algorithm. To this end, the decision-making process is organized into two abstraction levels: low-level policies learn precise control maneuvers, while high-level policies issue tactical commands based on mission objectives. Empirical results show that our hierarchical approach improves both learning efficiency and combat performance in complex dogfight scenarios.
<div id='section'>Paperid: <span id='pid'>599, <a href='https://arxiv.org/pdf/2509.26002.pdf' target='_blank'>https://arxiv.org/pdf/2509.26002.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ardian Selmonaj, Giacomo Del Rio, Adrian Schneider, Alessandro Antonucci
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.26002">Towards Human Engagement with Realistic AI Combat Pilots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a system that enables real-time interaction between human users and agents trained to control fighter jets in simulated 3D air combat scenarios. The agents are trained in a dedicated environment using Multi-Agent Reinforcement Learning. A communication link is developed to allow seamless deployment of trained agents into VR-Forces, a widely used defense simulation tool for realistic tactical scenarios. This integration allows mixed simulations where human-controlled entities engage with intelligent agents exhibiting distinct combat behaviors. Our interaction model creates new opportunities for human-agent teaming, immersive training, and the exploration of innovative tactics in defense contexts.
<div id='section'>Paperid: <span id='pid'>600, <a href='https://arxiv.org/pdf/2509.26002.pdf' target='_blank'>https://arxiv.org/pdf/2509.26002.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ardian Selmonaj, Giacomo Del Rio, Adrian Schneider, Alessandro Antonucci
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.26002">Towards Human Engagement with Realistic AI Combat Pilots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a system that enables real-time interaction between human users and agents trained to control fighter jets in simulated 3D air combat scenarios. The agents are trained in a dedicated environment using Multi-Agent Reinforcement Learning. A communication link is developed to allow seamless deployment of trained agents into VR-Forces, a widely used defense simulation tool for realistic tactical scenarios. This integration allows mixed simulations where human-controlled entities engage with intelligent agents exhibiting distinct combat behaviors. Our interaction model creates new opportunities for human-agent teaming, immersive training, and the exploration of innovative tactics in defense contexts.
<div id='section'>Paperid: <span id='pid'>601, <a href='https://arxiv.org/pdf/2509.26002.pdf' target='_blank'>https://arxiv.org/pdf/2509.26002.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ardian Selmonaj, Giacomo Del Rio, Adrian Schneider, Alessandro Antonucci
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.26002">Towards Human Engagement with Realistic AI Combat Pilots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a system that enables real-time interaction between human users and agents trained to control fighter jets in simulated 3D air combat scenarios. The agents are trained in a dedicated environment using Multi-Agent Reinforcement Learning. A communication link is developed to allow seamless deployment of trained agents into VR-Forces, a widely used defense simulation tool for realistic tactical scenarios. This integration allows mixed simulations where human-controlled entities engage with intelligent agents exhibiting distinct combat behaviors. Our interaction model creates new opportunities for human-agent teaming, immersive training, and the exploration of innovative tactics in defense contexts.
<div id='section'>Paperid: <span id='pid'>602, <a href='https://arxiv.org/pdf/2509.26002.pdf' target='_blank'>https://arxiv.org/pdf/2509.26002.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ardian Selmonaj, Giacomo Del Rio, Adrian Schneider, Alessandro Antonucci
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.26002">Towards Human Engagement with Realistic AI Combat Pilots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a system that enables real-time interaction between human users and agents trained to control fighter jets in simulated 3D air combat scenarios. The agents are trained in a dedicated environment using Multi-Agent Reinforcement Learning. A communication link is developed to allow seamless deployment of trained agents into VR-Forces, a widely used defense simulation tool for realistic tactical scenarios. This integration allows mixed simulations where human-controlled entities engage with intelligent agents exhibiting distinct combat behaviors. Our interaction model creates new opportunities for human-agent teaming, immersive training, and the exploration of innovative tactics in defense contexts.
<div id='section'>Paperid: <span id='pid'>603, <a href='https://arxiv.org/pdf/2509.18088.pdf' target='_blank'>https://arxiv.org/pdf/2509.18088.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chuhao Qin, Evangelos Pournaras
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.18088">Strategic Coordination for Evolving Multi-agent Systems: A Hierarchical Reinforcement and Collective Learning Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Decentralized combinatorial optimization in evolving multi-agent systems poses significant challenges, requiring agents to balance long-term decision-making, short-term optimized collective outcomes, while preserving autonomy of interactive agents under unanticipated changes. Reinforcement learning offers a way to model sequential decision-making through dynamic programming to anticipate future environmental changes. However, applying multi-agent reinforcement learning (MARL) to decentralized combinatorial optimization problems remains an open challenge due to the exponential growth of the joint state-action space, high communication overhead, and privacy concerns in centralized training. To address these limitations, this paper proposes Hierarchical Reinforcement and Collective Learning (HRCL), a novel approach that leverages both MARL and decentralized collective learning based on a hierarchical framework. Agents take high-level strategies using MARL to group possible plans for action space reduction and constrain the agent behavior for Pareto optimality. Meanwhile, the low-level collective learning layer ensures efficient and decentralized coordinated decisions among agents with minimal communication. Extensive experiments in a synthetic scenario and real-world smart city application models, including energy self-management and drone swarm sensing, demonstrate that HRCL significantly improves performance, scalability, and adaptability compared to the standalone MARL and collective learning approaches, achieving a win-win synthesis solution.
<div id='section'>Paperid: <span id='pid'>604, <a href='https://arxiv.org/pdf/2509.18088.pdf' target='_blank'>https://arxiv.org/pdf/2509.18088.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chuhao Qin, Evangelos Pournaras
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.18088">Strategic Coordination for Evolving Multi-agent Systems: A Hierarchical Reinforcement and Collective Learning Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Decentralized combinatorial optimization in evolving multi-agent systems poses significant challenges, requiring agents to balance long-term decision-making, short-term optimized collective outcomes, while preserving autonomy of interactive agents under unanticipated changes. Reinforcement learning offers a way to model sequential decision-making through dynamic programming to anticipate future environmental changes. However, applying multi-agent reinforcement learning (MARL) to decentralized combinatorial optimization problems remains an open challenge due to the exponential growth of the joint state-action space, high communication overhead, and privacy concerns in centralized training. To address these limitations, this paper proposes Hierarchical Reinforcement and Collective Learning (HRCL), a novel approach that leverages both MARL and decentralized collective learning based on a hierarchical framework. Agents take high-level strategies using MARL to group possible plans for action space reduction and constrain the agent behavior for Pareto optimality. Meanwhile, the low-level collective learning layer ensures efficient and decentralized coordinated decisions among agents with minimal communication. Extensive experiments in a synthetic scenario and real-world smart city application models, including energy self-management and drone swarm sensing, demonstrate that HRCL significantly improves performance, scalability, and adaptability compared to the standalone MARL and collective learning approaches, achieving a win-win synthesis solution.
<div id='section'>Paperid: <span id='pid'>605, <a href='https://arxiv.org/pdf/2509.16606.pdf' target='_blank'>https://arxiv.org/pdf/2509.16606.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wei Duan, Jie Lu, Junyu Xuan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.16606">Bayesian Ego-graph inference for Networked Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In networked multi-agent reinforcement learning (Networked-MARL), decentralized agents must act under local observability and constrained communication over fixed physical graphs. Existing methods often assume static neighborhoods, limiting adaptability to dynamic or heterogeneous environments. While centralized frameworks can learn dynamic graphs, their reliance on global state access and centralized infrastructure is impractical in real-world decentralized systems. We propose a stochastic graph-based policy for Networked-MARL, where each agent conditions its decision on a sampled subgraph over its local physical neighborhood. Building on this formulation, we introduce BayesG, a decentralized actor-framework that learns sparse, context-aware interaction structures via Bayesian variational inference. Each agent operates over an ego-graph and samples a latent communication mask to guide message passing and policy computation. The variational distribution is trained end-to-end alongside the policy using an evidence lower bound (ELBO) objective, enabling agents to jointly learn both interaction topology and decision-making strategies. BayesG outperforms strong MARL baselines on large-scale traffic control tasks with up to 167 agents, demonstrating superior scalability, efficiency, and performance.
<div id='section'>Paperid: <span id='pid'>606, <a href='https://arxiv.org/pdf/2509.16606.pdf' target='_blank'>https://arxiv.org/pdf/2509.16606.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wei Duan, Jie Lu, Junyu Xuan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.16606">Bayesian Ego-graph inference for Networked Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In networked multi-agent reinforcement learning (Networked-MARL), decentralized agents must act under local observability and constrained communication over fixed physical graphs. Existing methods often assume static neighborhoods, limiting adaptability to dynamic or heterogeneous environments. While centralized frameworks can learn dynamic graphs, their reliance on global state access and centralized infrastructure is impractical in real-world decentralized systems. We propose a stochastic graph-based policy for Networked-MARL, where each agent conditions its decision on a sampled subgraph over its local physical neighborhood. Building on this formulation, we introduce BayesG, a decentralized actor-framework that learns sparse, context-aware interaction structures via Bayesian variational inference. Each agent operates over an ego-graph and samples a latent communication mask to guide message passing and policy computation. The variational distribution is trained end-to-end alongside the policy using an evidence lower bound (ELBO) objective, enabling agents to jointly learn both interaction topology and decision-making strategies. BayesG outperforms strong MARL baselines on large-scale traffic control tasks with up to 167 agents, demonstrating superior scalability, efficiency, and performance.
<div id='section'>Paperid: <span id='pid'>607, <a href='https://arxiv.org/pdf/2508.20018.pdf' target='_blank'>https://arxiv.org/pdf/2508.20018.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Quanfeng Lu, Zhantao Ma, Shuai Zhong, Jin Wang, Dahai Yu, Michael K. Ng, Ping Luo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.20018">SWIRL: A Staged Workflow for Interleaved Reinforcement Learning in Mobile GUI Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid advancement of large vision language models (LVLMs) and agent systems has heightened interest in mobile GUI agents that can reliably translate natural language into interface operations. Existing single-agent approaches, however, remain limited by structural constraints. Although multi-agent systems naturally decouple different competencies, recent progress in multi-agent reinforcement learning (MARL) has often been hindered by inefficiency and remains incompatible with current LVLM architectures. To address these challenges, we introduce SWIRL, a staged workflow for interleaved reinforcement learning designed for multi-agent systems. SWIRL reformulates MARL into a sequence of single-agent reinforcement learning tasks, updating one agent at a time while keeping the others fixed. This formulation enables stable training and promotes efficient coordination across agents. Theoretically, we provide a stepwise safety bound, a cross-round monotonic improvement theorem, and convergence guarantees on return, ensuring robust and principled optimization. In application to mobile GUI control, SWIRL instantiates a Navigator that converts language and screen context into structured plans, and an Interactor that grounds these plans into executable atomic actions. Extensive experiments demonstrate superior performance on both high-level and low-level GUI benchmarks. Beyond GUI tasks, SWIRL also demonstrates strong capability in multi-agent mathematical reasoning, underscoring its potential as a general framework for developing efficient and robust multi-agent systems.
<div id='section'>Paperid: <span id='pid'>608, <a href='https://arxiv.org/pdf/2508.15652.pdf' target='_blank'>https://arxiv.org/pdf/2508.15652.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ardian Selmonaj, Miroslav Strupl, Oleg Szehr, Alessandro Antonucci
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.15652">Understanding Action Effects through Instrumental Empowerment in Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To reliably deploy Multi-Agent Reinforcement Learning (MARL) systems, it is crucial to understand individual agent behaviors. While prior work typically evaluates overall team performance based on explicit reward signals, it is unclear how to infer agent contributions in the absence of any value feedback. In this work, we investigate whether meaningful insights into agent behaviors can be extracted solely by analyzing the policy distribution. Inspired by the phenomenon that intelligent agents tend to pursue convergent instrumental values, we introduce Intended Cooperation Values (ICVs), a method based on information-theoretic Shapley values for quantifying each agent's causal influence on their co-players' instrumental empowerment. Specifically, ICVs measure an agent's action effect on its teammates' policies by assessing their decision (un)certainty and preference alignment. By analyzing action effects on policies and value functions across cooperative and competitive MARL tasks, our method identifies which agent behaviors are beneficial to team success, either by fostering deterministic decisions or by preserving flexibility for future action choices, while also revealing the extent to which agents adopt similar or diverse strategies. Our proposed method offers novel insights into cooperation dynamics and enhances explainability in MARL systems.
<div id='section'>Paperid: <span id='pid'>609, <a href='https://arxiv.org/pdf/2508.12845.pdf' target='_blank'>https://arxiv.org/pdf/2508.12845.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Artem Pshenitsyn, Aleksandr Panov, Alexey Skrynnik
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.12845">CAMAR: Continuous Actions Multi-Agent Routing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent reinforcement learning (MARL) is a powerful paradigm for solving cooperative and competitive decision-making problems. While many MARL benchmarks have been proposed, few combine continuous state and action spaces with challenging coordination and planning tasks. We introduce CAMAR, a new MARL benchmark designed explicitly for multi-agent pathfinding in environments with continuous actions. CAMAR supports cooperative and competitive interactions between agents and runs efficiently at up to 100,000 environment steps per second. We also propose a three-tier evaluation protocol to better track algorithmic progress and enable deeper analysis of performance. In addition, CAMAR allows the integration of classical planning methods such as RRT and RRT* into MARL pipelines. We use them as standalone baselines and combine RRT* with popular MARL algorithms to create hybrid approaches. We provide a suite of test scenarios and benchmarking tools to ensure reproducibility and fair comparison. Experiments show that CAMAR presents a challenging and realistic testbed for the MARL community.
<div id='section'>Paperid: <span id='pid'>610, <a href='https://arxiv.org/pdf/2507.17433.pdf' target='_blank'>https://arxiv.org/pdf/2507.17433.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hugh Adams, Srijoni Majumdar, Evangelos Pournaras
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.17433">Fair Compromises in Participatory Budgeting: a Multi-Agent Deep Reinforcement Learning Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Participatory budgeting is a method of collectively understanding and addressing spending priorities where citizens vote on how a budget is spent, it is regularly run to improve the fairness of the distribution of public funds. Participatory budgeting requires voters to make decisions on projects which can lead to ``choice overload". A multi-agent reinforcement learning approach to decision support can make decision making easier for voters by identifying voting strategies that increase the winning proportion of their vote. This novel approach can also support policymakers by highlighting aspects of election design that enable fair compromise on projects. This paper presents a novel, ethically aligned approach to decision support using multi-agent deep reinforcement learning modelling. This paper introduces a novel use of a branching neural network architecture to overcome scalability challenges of multi-agent reinforcement learning in a decentralized way. Fair compromises are found through optimising voter actions towards greater representation of voter preferences in the winning set. Experimental evaluation with real-world participatory budgeting data reveals a pattern in fair compromise: that it is achievable through projects with smaller cost.
<div id='section'>Paperid: <span id='pid'>611, <a href='https://arxiv.org/pdf/2507.16249.pdf' target='_blank'>https://arxiv.org/pdf/2507.16249.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Srivatsan Krishnan, Jason Jabbour, Dan Zhang, Natasha Jaques, Aleksandra Faust, Shayegan Omidshafiei, Vijay Janapa Reddi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.16249">Multi-Agent Reinforcement Learning for Sample-Efficient Deep Neural Network Mapping</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Mapping deep neural networks (DNNs) to hardware is critical for optimizing latency, energy consumption, and resource utilization, making it a cornerstone of high-performance accelerator design. Due to the vast and complex mapping space, reinforcement learning (RL) has emerged as a promising approach-but its effectiveness is often limited by sample inefficiency. We present a decentralized multi-agent reinforcement learning (MARL) framework designed to overcome this challenge. By distributing the search across multiple agents, our framework accelerates exploration. To avoid inefficiencies from training multiple agents in parallel, we introduce an agent clustering algorithm that assigns similar mapping parameters to the same agents based on correlation analysis. This enables a decentralized, parallelized learning process that significantly improves sample efficiency. Experimental results show our MARL approach improves sample efficiency by 30-300x over standard single-agent RL, achieving up to 32.61x latency reduction and 16.45x energy-delay product (EDP) reduction under iso-sample conditions.
<div id='section'>Paperid: <span id='pid'>612, <a href='https://arxiv.org/pdf/2507.14850.pdf' target='_blank'>https://arxiv.org/pdf/2507.14850.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>H. M. Sabbir Ahmad, Ehsan Sabouni, Alexander Wasilkoff, Param Budhraja, Zijian Guo, Songyuan Zhang, Chuchu Fan, Christos Cassandras, Wenchao Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.14850">Hierarchical Multi-Agent Reinforcement Learning with Control Barrier Functions for Safety-Critical Autonomous Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We address the problem of safe policy learning in multi-agent safety-critical autonomous systems. In such systems, it is necessary for each agent to meet the safety requirements at all times while also cooperating with other agents to accomplish the task. Toward this end, we propose a safe Hierarchical Multi-Agent Reinforcement Learning (HMARL) approach based on Control Barrier Functions (CBFs). Our proposed hierarchical approach decomposes the overall reinforcement learning problem into two levels learning joint cooperative behavior at the higher level and learning safe individual behavior at the lower or agent level conditioned on the high-level policy. Specifically, we propose a skill-based HMARL-CBF algorithm in which the higher level problem involves learning a joint policy over the skills for all the agents and the lower-level problem involves learning policies to execute the skills safely with CBFs. We validate our approach on challenging environment scenarios whereby a large number of agents have to safely navigate through conflicting road networks. Compared with existing state of the art methods, our approach significantly improves the safety achieving near perfect (within 5%) success/safety rate while also improving performance across all the environments.
<div id='section'>Paperid: <span id='pid'>613, <a href='https://arxiv.org/pdf/2507.09989.pdf' target='_blank'>https://arxiv.org/pdf/2507.09989.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaoyang Yu, Youfang Lin, Shuo Wang, Sheng Han
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.09989">Improving monotonic optimization in heterogeneous multi-agent reinforcement learning with optimal marginal deterministic policy gradient</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In heterogeneous multi-agent reinforcement learning (MARL), achieving monotonic improvement plays a pivotal role in enhancing performance. The HAPPO algorithm proposes a feasible solution by introducing a sequential update scheme, which requires independent learning with No Parameter-sharing (NoPS). However, heterogeneous MARL generally requires Partial Parameter-sharing (ParPS) based on agent grouping to achieve high cooperative performance. Our experiments prove that directly combining ParPS with the sequential update scheme leads to the policy updating baseline drift problem, thereby failing to achieve improvement. To solve the conflict between monotonic improvement and ParPS, we propose the Optimal Marginal Deterministic Policy Gradient (OMDPG) algorithm. First, we replace the sequentially computed $Q_Ï^s(s,a_{1:i})$ with the Optimal Marginal Q (OMQ) function $Ï_Ï^*(s,a_{1:i})$ derived from Q-functions. This maintains MAAD's monotonic improvement while eliminating the conflict through optimal joint action sequences instead of sequential policy ratio calculations. Second, we introduce the Generalized Q Critic (GQC) as the critic function, employing pessimistic uncertainty-constrained loss to optimize different Q-value estimations. This provides the required Q-values for OMQ computation and stable baselines for actor updates. Finally, we implement a Centralized Critic Grouped Actor (CCGA) architecture that simultaneously achieves ParPS in local policy networks and accurate global Q-function computation. Experimental results in SMAC and MAMuJoCo environments demonstrate that OMDPG outperforms various state-of-the-art MARL baselines.
<div id='section'>Paperid: <span id='pid'>614, <a href='https://arxiv.org/pdf/2507.06004.pdf' target='_blank'>https://arxiv.org/pdf/2507.06004.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chang Yao, Youfang Lin, Shoucheng Song, Hao Wu, Yuqing Ma, Shang Han, Kai Lv
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.06004">From General Relation Patterns to Task-Specific Decision-Making in Continual Multi-Agent Coordination</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Continual Multi-Agent Reinforcement Learning (Co-MARL) requires agents to address catastrophic forgetting issues while learning new coordination policies with the dynamics team. In this paper, we delve into the core of Co-MARL, namely Relation Patterns, which refer to agents' general understanding of interactions. In addition to generality, relation patterns exhibit task-specificity when mapped to different action spaces. To this end, we propose a novel method called General Relation Patterns-Guided Task-Specific Decision-Maker (RPG). In RPG, agents extract relation patterns from dynamic observation spaces using a relation capturer. These task-agnostic relation patterns are then mapped to different action spaces via a task-specific decision-maker generated by a conditional hypernetwork. To combat forgetting, we further introduce regularization items on both the relation capturer and the conditional hypernetwork. Results on SMAC and LBF demonstrate that RPG effectively prevents catastrophic forgetting when learning new tasks and achieves zero-shot generalization to unseen tasks.
<div id='section'>Paperid: <span id='pid'>615, <a href='https://arxiv.org/pdf/2506.09390.pdf' target='_blank'>https://arxiv.org/pdf/2506.09390.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kehan Zheng, Jinfeng Zhou, Hongning Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.09390">Beyond Nash Equilibrium: Bounded Rationality of LLMs and humans in Strategic Decision-making</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large language models are increasingly used in strategic decision-making settings, yet evidence shows that, like humans, they often deviate from full rationality. In this study, we compare LLMs and humans using experimental paradigms directly adapted from behavioral game-theory research. We focus on two well-studied strategic games, Rock-Paper-Scissors and the Prisoner's Dilemma, which are well known for revealing systematic departures from rational play in human subjects. By placing LLMs in identical experimental conditions, we evaluate whether their behaviors exhibit the bounded rationality characteristic of humans. Our findings show that LLMs reproduce familiar human heuristics, such as outcome-based strategy switching and increased cooperation when future interaction is possible, but they apply these rules more rigidly and demonstrate weaker sensitivity to the dynamic changes in the game environment. Model-level analyses reveal distinctive architectural signatures in strategic behavior, and even reasoning models sometimes struggle to find effective strategies in adaptive situations. These results indicate that current LLMs capture only a partial form of human-like bounded rationality and highlight the need for training methods that encourage flexible opponent modeling and stronger context awareness.
<div id='section'>Paperid: <span id='pid'>616, <a href='https://arxiv.org/pdf/2506.00982.pdf' target='_blank'>https://arxiv.org/pdf/2506.00982.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Keshawn Smith, Zhili Zhang, H M Sabbir Ahmad, Ehsan Sabouni, Maniak Mondal, Song Han, Wenchao Li, Fei Miao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.00982">Robust and Safe Multi-Agent Reinforcement Learning Framework with Communication for Autonomous Vehicles</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep multi-agent reinforcement learning (MARL) has been demonstrated effectively in simulations for many multi-robot problems. For autonomous vehicles, the development of vehicle-to-vehicle (V2V) communication technologies provide opportunities to further enhance safety of the system. However, zero-shot transfer of simulator-trained MARL policies to hardware dynamic systems remains challenging, and how to leverage communication and shared information for MARL has limited demonstrations on hardware. This problem is challenged by discrepancies between simulated and physical states, system state and model uncertainties, practical shared information design, and the need for safety guarantees in both simulation and hardware. This paper introduces RSR-RSMARL, a novel Robust and Safe MARL framework that supports Real-Sim-Real (RSR) policy adaptation for multi-agent systems with communication among agents, with both simulation and hardware demonstrations. RSR-RSMARL leverages state (includes shared state information among agents) and action representations considering real system complexities for MARL formulation. The MARL policy is trained with robust MARL algorithm to enable zero-shot transfer to hardware considering the sim-to-real gap. A safety shield module using Control Barrier Functions (CBFs) provides safety guarantee for each individual agent. Experiment results on F1/10th-scale autonomous vehicles with V2V communication demonstrate the ability of RSR-RSMARL framework to enhance driving safety and coordination across multiple configurations. These findings emphasize the importance of jointly designing robust policy representations and modular safety architectures to enable scalable, generalizable RSR transfer in multi-agent autonomy.
<div id='section'>Paperid: <span id='pid'>617, <a href='https://arxiv.org/pdf/2506.00982.pdf' target='_blank'>https://arxiv.org/pdf/2506.00982.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Keshawn Smith, Zhili Zhang, H M Sabbir Ahmad, Ehsan Sabouni, Maniak Mondal, Song Han, Wenchao Li, Fei Miao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.00982">Robust and Safe Multi-Agent Reinforcement Learning with Communication for Autonomous Vehicles: From Simulation to Hardware</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep multi-agent reinforcement learning (MARL) has been demonstrated effectively in simulations for multi-robot problems. For autonomous vehicles, the development of vehicle-to-vehicle (V2V) communication technologies provide opportunities to further enhance system safety. However, zero-shot transfer of simulator-trained MARL policies to dynamic hardware systems remains challenging, and how to leverage communication and shared information for MARL has limited demonstrations on hardware. This problem is challenged by discrepancies between simulated and physical states, system state and model uncertainties, practical shared information design, and the need for safety guarantees in both simulation and hardware. This paper designs RSR-RSMARL, a novel Robust and Safe MARL framework that supports Real-Sim-Real (RSR) policy adaptation for multi-agent systems with communication among agents, with both simulation and hardware demonstrations. RSR-RSMARL leverages state (includes shared state information among agents) and action representations considering real system complexities for MARL formulation. The MARL policy is trained with robust MARL algorithm to enable zero-shot transfer to hardware considering the sim-to-real gap. A safety shield module using Control Barrier Functions (CBFs) provides safety guarantee for each individual agent. Experimental results on 1/10th-scale autonomous vehicles with V2V communication demonstrate the ability of RSR-RSMARL framework to enhance driving safety and coordination across multiple configurations. These findings emphasize the importance of jointly designing robust policy representations and modular safety architectures to enable scalable, generalizable RSR transfer in multi-agent autonomy.
<div id='section'>Paperid: <span id='pid'>618, <a href='https://arxiv.org/pdf/2505.11311.pdf' target='_blank'>https://arxiv.org/pdf/2505.11311.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ardian Selmonaj, Alessandro Antonucci, Adrian Schneider, Michael RÃ¼egsegger, Matthias Sommer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.11311">Explaining Strategic Decisions in Multi-Agent Reinforcement Learning for Aerial Combat Tactics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Artificial intelligence (AI) is reshaping strategic planning, with Multi-Agent Reinforcement Learning (MARL) enabling coordination among autonomous agents in complex scenarios. However, its practical deployment in sensitive military contexts is constrained by the lack of explainability, which is an essential factor for trust, safety, and alignment with human strategies. This work reviews and assesses current advances in explainability methods for MARL with a focus on simulated air combat scenarios. We proceed by adapting various explainability techniques to different aerial combat scenarios to gain explanatory insights about the model behavior. By linking AI-generated tactics with human-understandable reasoning, we emphasize the need for transparency to ensure reliable deployment and meaningful human-machine interaction. By illuminating the crucial importance of explainability in advancing MARL for operational defense, our work supports not only strategic planning but also the training of military personnel with insightful and comprehensible analyses.
<div id='section'>Paperid: <span id='pid'>619, <a href='https://arxiv.org/pdf/2505.08995.pdf' target='_blank'>https://arxiv.org/pdf/2505.08995.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ardian Selmonaj, Oleg Szehr, Giacomo Del Rio, Alessandro Antonucci, Adrian Schneider, Michael RÃ¼egsegger
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.08995">Enhancing Aerial Combat Tactics through Hierarchical Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work presents a Hierarchical Multi-Agent Reinforcement Learning framework for analyzing simulated air combat scenarios involving heterogeneous agents. The objective is to identify effective Courses of Action that lead to mission success within preset simulations, thereby enabling the exploration of real-world defense scenarios at low cost and in a safe-to-fail setting. Applying deep Reinforcement Learning in this context poses specific challenges, such as complex flight dynamics, the exponential size of the state and action spaces in multi-agent systems, and the capability to integrate real-time control of individual units with look-ahead planning. To address these challenges, the decision-making process is split into two levels of abstraction: low-level policies control individual units, while a high-level commander policy issues macro commands aligned with the overall mission targets. This hierarchical structure facilitates the training process by exploiting policy symmetries of individual agents and by separating control from command tasks. The low-level policies are trained for individual combat control in a curriculum of increasing complexity. The high-level commander is then trained on mission targets given pre-trained control policies. The empirical validation confirms the advantages of the proposed framework.
<div id='section'>Paperid: <span id='pid'>620, <a href='https://arxiv.org/pdf/2504.21164.pdf' target='_blank'>https://arxiv.org/pdf/2504.21164.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bhavini Jeloka, Yue Guan, Panagiotis Tsiotras
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.21164">Learning Large-Scale Competitive Team Behaviors with Mean-Field Interactions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>State-of-the-art multi-agent reinforcement learning (MARL) algorithms such as MADDPG and MAAC fail to scale in situations where the number of agents becomes large. Mean-field theory has shown encouraging results in modeling macroscopic agent behavior for teams with a large number of agents through a continuum approximation of the agent population and its interaction with the environment. In this work, we extend proximal policy optimization (PPO) to the mean-field domain by introducing the Mean-Field Multi-Agent Proximal Policy Optimization (MF-MAPPO), a novel algorithm that utilizes the effectiveness of the finite-population mean-field approximation in the context of zero-sum competitive multi-agent games between two teams. The proposed algorithm can be easily scaled to hundreds and thousands of agents in each team as shown through numerical experiments. In particular, the algorithm is applied to realistic applications such as large-scale offense-defense battlefield scenarios.
<div id='section'>Paperid: <span id='pid'>621, <a href='https://arxiv.org/pdf/2504.21164.pdf' target='_blank'>https://arxiv.org/pdf/2504.21164.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bhavini Jeloka, Yue Guan, Panagiotis Tsiotras
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.21164">Learning Large-Scale Competitive Team Behaviors with Mean-Field Interactions and Online Opponent Modeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While multi-agent reinforcement learning (MARL) has been proven effective across both collaborative and competitive tasks, existing algorithms often struggle to scale to large populations of agents. Recent advancements in mean-field (MF) theory provide scalable solutions by approximating population interactions as a continuum, yet most existing frameworks focus exclusively on either fully cooperative or purely competitive settings. To bridge this gap, we introduce MF-MAPPO, a mean-field extension of PPO designed for zero-sum team games that integrate intra-team cooperation with inter-team competition. MF-MAPPO employs a shared actor and a minimally informed critic per team and is trained directly on finite-population simulators, thereby enabling deployment to realistic scenarios with thousands of agents. We further show that MF-MAPPO naturally extends to partially observable settings through a simple gradient-regularized training scheme. Our evaluation utilizes large-scale benchmark scenarios using our own testing simulation platform for MF team games (MFEnv), including offense-defense battlefield tasks as well as variants of population-based rock-paper-scissors games that admit analytical solutions, for benchmarking. Across these benchmarks, MF-MAPPO outperforms existing methods and exhibits complex, heterogeneous behaviors, demonstrating the effectiveness of combining mean-field theory and MARL techniques at scale.
<div id='section'>Paperid: <span id='pid'>622, <a href='https://arxiv.org/pdf/2504.21164.pdf' target='_blank'>https://arxiv.org/pdf/2504.21164.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bhavini Jeloka, Yue Guan, Panagiotis Tsiotras
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.21164">Learning Large-Scale Competitive Team Behaviors with Mean-Field Interactions and Online Opponent Modeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While multi-agent reinforcement learning (MARL) has been proven effective across both collaborative and competitive tasks, existing algorithms often struggle to scale to large populations of agents. Recent advancements in mean-field (MF) theory provide scalable solutions by approximating population interactions as a continuum, yet most existing frameworks focus exclusively on either fully cooperative or purely competitive settings. To bridge this gap, we introduce MF-MAPPO, a mean-field extension of PPO designed for zero-sum team games that integrate intra-team cooperation with inter-team competition. MF-MAPPO employs a shared actor and a minimally informed critic per team and is trained directly on finite-population simulators, thereby enabling deployment to realistic scenarios with thousands of agents. We further show that MF-MAPPO naturally extends to partially observable settings through a simple gradient-regularized training scheme. Our evaluation utilizes large-scale benchmark scenarios using our own testing simulation platform for MF team games (MFEnv), including offense-defense battlefield tasks as well as variants of population-based rock-paper-scissors games that admit analytical solutions, for benchmarking. Across these benchmarks, MF-MAPPO outperforms existing methods and exhibits complex, heterogeneous behaviors, demonstrating the effectiveness of combining mean-field theory and MARL techniques at scale.
<div id='section'>Paperid: <span id='pid'>623, <a href='https://arxiv.org/pdf/2503.13077.pdf' target='_blank'>https://arxiv.org/pdf/2503.13077.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Amir Baghi, Jens SjÃ¶lund, Joakim Bergdahl, Linus GisslÃ©n, Alessandro Sestini
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.13077">Towards Better Sample Efficiency in Multi-Agent Reinforcement Learning via Exploration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent reinforcement learning has shown promise in learning cooperative behaviors in team-based environments. However, such methods often demand extensive training time. For instance, the state-of-the-art method TiZero takes 40 days to train high-quality policies for a football environment. In this paper, we hypothesize that better exploration mechanisms can improve the sample efficiency of multi-agent methods. We propose two different approaches for better exploration in TiZero: a self-supervised intrinsic reward and a random network distillation bonus. Additionally, we introduce architectural modifications to the original algorithm to enhance TiZero's computational efficiency. We evaluate the sample efficiency of these approaches through extensive experiments. Our results show that random network distillation improves training sample efficiency by 18.8% compared to the original TiZero. Furthermore, we evaluate the qualitative behavior of the models produced by both variants against a heuristic AI, with the self-supervised reward encouraging possession and random network distillation leading to a more offensive performance. Our results highlights the applicability of our random network distillation variant in practical settings. Lastly, due to the nature of the proposed method, we acknowledge its use beyond football simulation, especially in environments with strong multi-agent and strategic aspects.
<div id='section'>Paperid: <span id='pid'>624, <a href='https://arxiv.org/pdf/2503.02077.pdf' target='_blank'>https://arxiv.org/pdf/2503.02077.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziyan Wang, Zhicheng Zhang, Fei Fang, Yali Du
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.02077">M3HF: Multi-agent Reinforcement Learning from Multi-phase Human Feedback of Mixed Quality</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Designing effective reward functions in multi-agent reinforcement learning (MARL) is a significant challenge, often leading to suboptimal or misaligned behaviors in complex, coordinated environments. We introduce Multi-agent Reinforcement Learning from Multi-phase Human Feedback of Mixed Quality ($\text{M}^3\text{HF}$), a novel framework that integrates multi-phase human feedback of mixed quality into the MARL training process. By involving humans with diverse expertise levels to provide iterative guidance, $\text{M}^3\text{HF}$ leverages both expert and non-expert feedback to continuously refine agents' policies. During training, we strategically pause agent learning for human evaluation, parse feedback using large language models to assign it appropriately and update reward functions through predefined templates and adaptive weights by using weight decay and performance-based adjustments. Our approach enables the integration of nuanced human insights across various levels of quality, enhancing the interpretability and robustness of multi-agent cooperation. Empirical results in challenging environments demonstrate that $\text{M}^3\text{HF}$ significantly outperforms state-of-the-art methods, effectively addressing the complexities of reward design in MARL and enabling broader human participation in the training process.
<div id='section'>Paperid: <span id='pid'>625, <a href='https://arxiv.org/pdf/2502.10148.pdf' target='_blank'>https://arxiv.org/pdf/2502.10148.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiyuan Li, Wenshuai Zhao, Joni Pajarinen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.10148">Cooperative Multi-Agent Planning with Adaptive Skill Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite much progress in training distributed artificial intelligence (AI), building cooperative multi-agent systems with multi-agent reinforcement learning (MARL) faces challenges in sample efficiency, interpretability, and transferability. Unlike traditional learning-based methods that require extensive interaction with the environment, large language models (LLMs) demonstrate remarkable capabilities in zero-shot planning and complex reasoning. However, existing LLM-based approaches heavily rely on text-based observations and struggle with the non-Markovian nature of multi-agent interactions under partial observability. We present COMPASS, a novel multi-agent architecture that integrates vision-language models (VLMs) with a dynamic skill library and structured communication for decentralized closed-loop decision-making. The skill library, bootstrapped from demonstrations, evolves via planner-guided tasks to enable adaptive strategies. COMPASS propagates entity information through multi-hop communication under partial observability. Evaluations on the improved StarCraft Multi-Agent Challenge (SMACv2) demonstrate COMPASS's strong performance against state-of-the-art MARL baselines across both symmetric and asymmetric scenarios. Notably, in the symmetric Protoss 5v5 task, COMPASS achieved a 57\% win rate, representing a 30 percentage point advantage over QMIX (27\%). Project page can be found at https://stellar-entremet-1720bb.netlify.app/.
<div id='section'>Paperid: <span id='pid'>626, <a href='https://arxiv.org/pdf/2502.07635.pdf' target='_blank'>https://arxiv.org/pdf/2502.07635.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Guilherme S. Varela, Alberto Sardinha, Francisco S. Melo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.07635">Distributed Value Decomposition Networks with Networked Agents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We investigate the problem of distributed training under partial observability, whereby cooperative multi-agent reinforcement learning agents (MARL) maximize the expected cumulative joint reward. We propose distributed value decomposition networks (DVDN) that generate a joint Q-function that factorizes into agent-wise Q-functions. Whereas the original value decomposition networks rely on centralized training, our approach is suitable for domains where centralized training is not possible and agents must learn by interacting with the physical environment in a decentralized manner while communicating with their peers. DVDN overcomes the need for centralized training by locally estimating the shared objective. We contribute with two innovative algorithms, DVDN and DVDN (GT), for the heterogeneous and homogeneous agents settings respectively. Empirically, both algorithms approximate the performance of value decomposition networks, in spite of the information loss during communication, as demonstrated in ten MARL tasks in three standard environments.
<div id='section'>Paperid: <span id='pid'>627, <a href='https://arxiv.org/pdf/2501.15695.pdf' target='_blank'>https://arxiv.org/pdf/2501.15695.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hung Du, Srikanth Thudumu, Hy Nguyen, Rajesh Vasa, Kon Mouzakis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.15695">Contextual Knowledge Sharing in Multi-Agent Reinforcement Learning with Decentralized Communication and Coordination</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Decentralized Multi-Agent Reinforcement Learning (Dec-MARL) has emerged as a pivotal approach for addressing complex tasks in dynamic environments. Existing Multi-Agent Reinforcement Learning (MARL) methodologies typically assume a shared objective among agents and rely on centralized control. However, many real-world scenarios feature agents with individual goals and limited observability of other agents, complicating coordination and hindering adaptability. Existing Dec-MARL strategies prioritize either communication or coordination, lacking an integrated approach that leverages both. This paper presents a novel Dec-MARL framework that integrates peer-to-peer communication and coordination, incorporating goal-awareness and time-awareness into the agents' knowledge-sharing processes. Our framework equips agents with the ability to (i) share contextually relevant knowledge to assist other agents, and (ii) reason based on information acquired from multiple agents, while considering their own goals and the temporal context of prior knowledge. We evaluate our approach through several complex multi-agent tasks in environments with dynamically appearing obstacles. Our work demonstrates that incorporating goal-aware and time-aware knowledge sharing significantly enhances overall performance.
<div id='section'>Paperid: <span id='pid'>628, <a href='https://arxiv.org/pdf/2501.08778.pdf' target='_blank'>https://arxiv.org/pdf/2501.08778.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Guilherme S. Varela, Alberto Sardinha, Francisco S. Melo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.08778">Networked Agents in the Dark: Team Value Learning under Partial Observability</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a novel cooperative multi-agent reinforcement learning (MARL) approach for networked agents. In contrast to previous methods that rely on complete state information or joint observations, our agents must learn how to reach shared objectives under partial observability. During training, they collect individual rewards and approximate a team value function through local communication, resulting in cooperative behavior. To describe our problem, we introduce the networked dynamic partially observable Markov game framework, where agents communicate over a switching topology communication network. Our distributed method, DNA-MARL, uses a consensus mechanism for local communication and gradient descent for local computation. DNA-MARL increases the range of the possible applications of networked agents, being well-suited for real world domains that impose privacy and where the messages may not reach their recipients. We evaluate DNA-MARL across benchmark MARL scenarios. Our results highlight the superior performance of DNA-MARL over previous methods.
<div id='section'>Paperid: <span id='pid'>629, <a href='https://arxiv.org/pdf/2501.08600.pdf' target='_blank'>https://arxiv.org/pdf/2501.08600.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tyler Stennett, Myeongsoo Kim, Saurabh Sinha, Alessandro Orso
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.08600">AutoRestTest: A Tool for Automated REST API Testing Using LLMs and MARL</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As REST APIs have become widespread in modern web services, comprehensive testing of these APIs is increasingly crucial. Because of the vast search space of operations, parameters, and parameter values, along with their dependencies and constraints, current testing tools often achieve low code coverage, resulting in suboptimal fault detection. To address this limitation, we present AutoRestTest, a novel tool that integrates the Semantic Property Dependency Graph (SPDG) with Multi-Agent Reinforcement Learning (MARL) and large language models (LLMs) for effective REST API testing. AutoRestTest determines operation-dependent parameters using the SPDG and employs five specialized agents (operation, parameter, value, dependency, and header) to identify dependencies of operations and generate operation sequences, parameter combinations, and values. Through an intuitive command-line interface, users can easily configure and monitor tests with successful operation count, unique server errors detected, and time elapsed. Upon completion, AutoRestTest generates a detailed report highlighting errors detected and operations exercised. In this paper, we introduce our tool and present preliminary findings, with a demonstration video available at https://www.youtube.com/watch?v=VVus2W8rap8.
<div id='section'>Paperid: <span id='pid'>630, <a href='https://arxiv.org/pdf/2412.15461.pdf' target='_blank'>https://arxiv.org/pdf/2412.15461.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Desmond Chan, Bart De Keijzer, Tobias Galla, Stefanos Leonardos, Carmine Ventre
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.15461">Asymptotic Extinction in Large Coordination Games</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We study the exploration-exploitation trade-off for large multiplayer coordination games where players strategise via Q-Learning, a common learning framework in multi-agent reinforcement learning. Q-Learning is known to have two shortcomings, namely non-convergence and potential equilibrium selection problems, when there are multiple fixed points, called Quantal Response Equilibria (QRE). Furthermore, whilst QRE have full support for finite games, it is not clear how Q-Learning behaves as the game becomes large. In this paper, we characterise the critical exploration rate that guarantees convergence to a unique fixed point, addressing the two shortcomings above. Using a generating-functional method, we show that this rate increases with the number of players and the alignment of their payoffs. For many-player coordination games with perfectly aligned payoffs, this exploration rate is roughly twice that of $p$-player zero-sum games. As for large games, we provide a structural result for QRE, which suggests that as the game size increases, Q-Learning converges to a QRE near the boundary of the simplex of the action space, a phenomenon we term asymptotic extinction, where a constant fraction of the actions are played with zero probability at a rate $o(1/N)$ for an $N$-action game.
<div id='section'>Paperid: <span id='pid'>631, <a href='https://arxiv.org/pdf/2412.00534.pdf' target='_blank'>https://arxiv.org/pdf/2412.00534.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuchen Shi, Huaxin Pei, Liang Feng, Yi Zhang, Danya Yao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.00534">Towards Fault Tolerance in Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Agent faults pose a significant threat to the performance of multi-agent reinforcement learning (MARL) algorithms, introducing two key challenges. First, agents often struggle to extract critical information from the chaotic state space created by unexpected faults. Second, transitions recorded before and after faults in the replay buffer affect training unevenly, leading to a sample imbalance problem. To overcome these challenges, this paper enhances the fault tolerance of MARL by combining optimized model architecture with a tailored training data sampling strategy. Specifically, an attention mechanism is incorporated into the actor and critic networks to automatically detect faults and dynamically regulate the attention given to faulty agents. Additionally, a prioritization mechanism is introduced to selectively sample transitions critical to current training needs. To further support research in this area, we design and open-source a highly decoupled code platform for fault-tolerant MARL, aimed at improving the efficiency of studying related problems. Experimental results demonstrate the effectiveness of our method in handling various types of faults, faults occurring in any agent, and faults arising at random times.
<div id='section'>Paperid: <span id='pid'>632, <a href='https://arxiv.org/pdf/2411.07098.pdf' target='_blank'>https://arxiv.org/pdf/2411.07098.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Myeongsoo Kim, Tyler Stennett, Saurabh Sinha, Alessandro Orso
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.07098">A Multi-Agent Approach for REST API Testing with Semantic Graphs and LLM-Driven Inputs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As modern web services increasingly rely on REST APIs, their thorough testing has become crucial. Furthermore, the advent of REST API documentation languages, such as the OpenAPI Specification, has led to the emergence of many black-box REST API testing tools. However, these tools often focus on individual test elements in isolation (e.g., APIs, parameters, values), resulting in lower coverage and less effectiveness in fault detection. To address these limitations, we present AutoRestTest, the first black-box tool to adopt a dependency-embedded multi-agent approach for REST API testing that integrates multi-agent reinforcement learning (MARL) with a semantic property dependency graph (SPDG) and Large Language Models (LLMs). Our approach treats REST API testing as a separable problem, where four agents -- API, dependency, parameter, and value agents -- collaborate to optimize API exploration. LLMs handle domain-specific value generation, the SPDG model simplifies the search space for dependencies using a similarity score between API operations, and MARL dynamically optimizes the agents' behavior. Our evaluation of AutoRestTest on 12 real-world REST services shows that it outperforms the four leading black-box REST API testing tools, including those assisted by RESTGPT (which generates realistic test inputs using LLMs), in terms of code coverage, operation coverage, and fault detection. Notably, AutoRestTest is the only tool able to trigger an internal server error in the Spotify service. Our ablation study illustrates that each component of AutoRestTest -- the SPDG, the LLM, and the agent-learning mechanism -- contributes to its overall effectiveness.
<div id='section'>Paperid: <span id='pid'>633, <a href='https://arxiv.org/pdf/2410.02664.pdf' target='_blank'>https://arxiv.org/pdf/2410.02664.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zeyang Liu, Xinrui Yang, Shiguang Sun, Long Qian, Lipeng Wan, Xingyu Chen, Xuguang Lan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.02664">Grounded Answers for Multi-agent Decision-making Problem through Generative World Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent progress in generative models has stimulated significant innovations in many fields, such as image generation and chatbots. Despite their success, these models often produce sketchy and misleading solutions for complex multi-agent decision-making problems because they miss the trial-and-error experience and reasoning as humans. To address this limitation, we explore a paradigm that integrates a language-guided simulator into the multi-agent reinforcement learning pipeline to enhance the generated answer. The simulator is a world model that separately learns dynamics and reward, where the dynamics model comprises an image tokenizer as well as a causal transformer to generate interaction transitions autoregressively, and the reward model is a bidirectional transformer learned by maximizing the likelihood of trajectories in the expert demonstrations under language guidance. Given an image of the current state and the task description, we use the world model to train the joint policy and produce the image sequence as the answer by running the converged policy on the dynamics model. The empirical results demonstrate that this framework can improve the answers for multi-agent decision-making problems by showing superior performance on the training and unseen tasks of the StarCraft Multi-Agent Challenge benchmark. In particular, it can generate consistent interaction sequences and explainable reward functions at interaction states, opening the path for training generative models of the future.
<div id='section'>Paperid: <span id='pid'>634, <a href='https://arxiv.org/pdf/2409.11852.pdf' target='_blank'>https://arxiv.org/pdf/2409.11852.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianye Xu, Omar Sobhy, Bassam Alrifaee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.11852">XP-MARL: Auxiliary Prioritization in Multi-Agent Reinforcement Learning to Address Non-Stationarity</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Non-stationarity poses a fundamental challenge in Multi-Agent Reinforcement Learning (MARL), arising from agents simultaneously learning and altering their policies. This creates a non-stationary environment from the perspective of each individual agent, often leading to suboptimal or even unconverged learning outcomes. We propose an open-source framework named XP-MARL, which augments MARL with auxiliary prioritization to address this challenge in cooperative settings. XP-MARL is 1) founded upon our hypothesis that prioritizing agents and letting higher-priority agents establish their actions first would stabilize the learning process and thus mitigate non-stationarity and 2) enabled by our proposed mechanism called action propagation, where higher-priority agents act first and communicate their actions, providing a more stationary environment for others. Moreover, instead of using a predefined or heuristic priority assignment, XP-MARL learns priority-assignment policies with an auxiliary MARL problem, leading to a joint learning scheme. Experiments in a motion-planning scenario involving Connected and Automated Vehicles (CAVs) demonstrate that XP-MARL improves the safety of a baseline model by 84.4% and outperforms a state-of-the-art approach, which improves the baseline by only 12.8%. Code: github.com/cas-lab-munich/sigmarl
<div id='section'>Paperid: <span id='pid'>635, <a href='https://arxiv.org/pdf/2408.07644.pdf' target='_blank'>https://arxiv.org/pdf/2408.07644.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianye Xu, Pan Hu, Bassam Alrifaee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.07644">SigmaRL: A Sample-Efficient and Generalizable Multi-Agent Reinforcement Learning Framework for Motion Planning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces an open-source, decentralized framework named SigmaRL, designed to enhance both sample efficiency and generalization of multi-agent Reinforcement Learning (RL) for motion planning of connected and automated vehicles. Most RL agents exhibit a limited capacity to generalize, often focusing narrowly on specific scenarios, and are usually evaluated in similar or even the same scenarios seen during training. Various methods have been proposed to address these challenges, including experience replay and regularization. However, how observation design in RL affects sample efficiency and generalization remains an under-explored area. We address this gap by proposing five strategies to design information-dense observations, focusing on general features that are applicable to most traffic scenarios. We train our RL agents using these strategies on an intersection and evaluate their generalization through numerical experiments across completely unseen traffic scenarios, including a new intersection, an on-ramp, and a roundabout. Incorporating these information-dense observations reduces training times to under one hour on a single CPU, and the evaluation results reveal that our RL agents can effectively zero-shot generalize. Code: github.com/bassamlab/SigmaRL
<div id='section'>Paperid: <span id='pid'>636, <a href='https://arxiv.org/pdf/2408.07395.pdf' target='_blank'>https://arxiv.org/pdf/2408.07395.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaoyang Yu, Youfang Lin, Shuo Wang, Kai Lv, Sheng Han
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.07395">Improving Global Parameter-sharing in Physically Heterogeneous Multi-agent Reinforcement Learning with Unified Action Space</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In a multi-agent system (MAS), action semantics indicates the different influences of agents' actions toward other entities, and can be used to divide agents into groups in a physically heterogeneous MAS. Previous multi-agent reinforcement learning (MARL) algorithms apply global parameter-sharing across different types of heterogeneous agents without careful discrimination of different action semantics. This common implementation decreases the cooperation and coordination between agents in complex situations. However, fully independent agent parameters dramatically increase the computational cost and training difficulty. In order to benefit from the usage of different action semantics while also maintaining a proper parameter-sharing structure, we introduce the Unified Action Space (UAS) to fulfill the requirement. The UAS is the union set of all agent actions with different semantics. All agents first calculate their unified representation in the UAS, and then generate their heterogeneous action policies using different available-action-masks. To further improve the training of extra UAS parameters, we introduce a Cross-Group Inverse (CGI) loss to predict other groups' agent policies with the trajectory information. As a universal method for solving the physically heterogeneous MARL problem, we implement the UAS adding to both value-based and policy-based MARL algorithms, and propose two practical algorithms: U-QMIX and U-MAPPO. Experimental results in the SMAC environment prove the effectiveness of both U-QMIX and U-MAPPO compared with several state-of-the-art MARL methods.
<div id='section'>Paperid: <span id='pid'>637, <a href='https://arxiv.org/pdf/2407.14931.pdf' target='_blank'>https://arxiv.org/pdf/2407.14931.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alexey Skrynnik, Anton Andreychuk, Anatolii Borzilov, Alexander Chernyavskiy, Konstantin Yakovlev, Aleksandr Panov
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.14931">POGEMA: A Benchmark Platform for Cooperative Multi-Agent Pathfinding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent reinforcement learning (MARL) has recently excelled in solving challenging cooperative and competitive multi-agent problems in various environments, typically involving a small number of agents and full observability. Moreover, a range of crucial robotics-related tasks, such as multi-robot pathfinding, which have traditionally been approached with classical non-learnable methods (e.g., heuristic search), are now being suggested for solution using learning-based or hybrid methods. However, in this domain, it remains difficult, if not impossible, to conduct a fair comparison between classical, learning-based, and hybrid approaches due to the lack of a unified framework that supports both learning and evaluation. To address this, we introduce POGEMA, a comprehensive set of tools that includes a fast environment for learning, a problem instance generator, a collection of predefined problem instances, a visualization toolkit, and a benchmarking tool for automated evaluation. We also introduce and define an evaluation protocol that specifies a range of domain-related metrics, computed based on primary evaluation indicators (such as success rate and path length), enabling a fair multi-fold comparison. The results of this comparison, which involves a variety of state-of-the-art MARL, search-based, and hybrid methods, are presented.
<div id='section'>Paperid: <span id='pid'>638, <a href='https://arxiv.org/pdf/2407.08164.pdf' target='_blank'>https://arxiv.org/pdf/2407.08164.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pu Feng, Junkang Liang, Size Wang, Xin Yu, Xin Ji, Yiting Chen, Kui Zhang, Rongye Shi, Wenjun Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.08164">Hierarchical Consensus-Based Multi-Agent Reinforcement Learning for Multi-Robot Cooperation Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In multi-agent reinforcement learning (MARL), the Centralized Training with Decentralized Execution (CTDE) framework is pivotal but struggles due to a gap: global state guidance in training versus reliance on local observations in execution, lacking global signals. Inspired by human societal consensus mechanisms, we introduce the Hierarchical Consensus-based Multi-Agent Reinforcement Learning (HC-MARL) framework to address this limitation. HC-MARL employs contrastive learning to foster a global consensus among agents, enabling cooperative behavior without direct communication. This approach enables agents to form a global consensus from local observations, using it as an additional piece of information to guide collaborative actions during execution. To cater to the dynamic requirements of various tasks, consensus is divided into multiple layers, encompassing both short-term and long-term considerations. Short-term observations prompt the creation of an immediate, low-layer consensus, while long-term observations contribute to the formation of a strategic, high-layer consensus. This process is further refined through an adaptive attention mechanism that dynamically adjusts the influence of each consensus layer. This mechanism optimizes the balance between immediate reactions and strategic planning, tailoring it to the specific demands of the task at hand. Extensive experiments and real-world applications in multi-robot systems showcase our framework's superior performance, marking significant advancements over baselines.
<div id='section'>Paperid: <span id='pid'>639, <a href='https://arxiv.org/pdf/2406.02081.pdf' target='_blank'>https://arxiv.org/pdf/2406.02081.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenzhe Li, Zihan Ding, Seth Karten, Chi Jin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.02081">FightLadder: A Benchmark for Competitive Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in reinforcement learning (RL) heavily rely on a variety of well-designed benchmarks, which provide environmental platforms and consistent criteria to evaluate existing and novel algorithms. Specifically, in multi-agent RL (MARL), a plethora of benchmarks based on cooperative games have spurred the development of algorithms that improve the scalability of cooperative multi-agent systems. However, for the competitive setting, a lightweight and open-sourced benchmark with challenging gaming dynamics and visual inputs has not yet been established. In this work, we present FightLadder, a real-time fighting game platform, to empower competitive MARL research. Along with the platform, we provide implementations of state-of-the-art MARL algorithms for competitive games, as well as a set of evaluation metrics to characterize the performance and exploitability of agents. We demonstrate the feasibility of this platform by training a general agent that consistently defeats 12 built-in characters in single-player mode, and expose the difficulty of training a non-exploitable agent without human knowledge and demonstrations in two-player mode. FightLadder provides meticulously designed environments to address critical challenges in competitive MARL research, aiming to catalyze a new era of discovery and advancement in the field. Videos and code at https://sites.google.com/view/fightladder/home.
<div id='section'>Paperid: <span id='pid'>640, <a href='https://arxiv.org/pdf/2404.14649.pdf' target='_blank'>https://arxiv.org/pdf/2404.14649.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zechen Hu, Daigo Shishika, Xuesu Xiao, Xuan Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.14649">Bi-CL: A Reinforcement Learning Framework for Robots Coordination Through Bi-level Optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In multi-robot systems, achieving coordinated missions remains a significant challenge due to the coupled nature of coordination behaviors and the lack of global information for individual robots. To mitigate these challenges, this paper introduces a novel approach, Bi-level Coordination Learning (Bi-CL), that leverages a bi-level optimization structure within a centralized training and decentralized execution paradigm. Our bi-level reformulation decomposes the original problem into a reinforcement learning level with reduced action space, and an imitation learning level that gains demonstrations from a global optimizer. Both levels contribute to improved learning efficiency and scalability. We note that robots' incomplete information leads to mismatches between the two levels of learning models. To address this, Bi-CL further integrates an alignment penalty mechanism, aiming to minimize the discrepancy between the two levels without degrading their training efficiency. We introduce a running example to conceptualize the problem formulation and apply Bi-CL to two variations of this example: route-based and graph-based scenarios. Simulation results demonstrate that Bi-CL can learn more efficiently and achieve comparable performance with traditional multi-agent reinforcement learning baselines for multi-robot coordination.
<div id='section'>Paperid: <span id='pid'>641, <a href='https://arxiv.org/pdf/2404.10976.pdf' target='_blank'>https://arxiv.org/pdf/2404.10976.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wei Duan, Jie Lu, Junyu Xuan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.10976">Group-Aware Coordination Graph for Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cooperative Multi-Agent Reinforcement Learning (MARL) necessitates seamless collaboration among agents, often represented by an underlying relation graph. Existing methods for learning this graph primarily focus on agent-pair relations, neglecting higher-order relationships. While several approaches attempt to extend cooperation modelling to encompass behaviour similarities within groups, they commonly fall short in concurrently learning the latent graph, thereby constraining the information exchange among partially observed agents. To overcome these limitations, we present a novel approach to infer the Group-Aware Coordination Graph (GACG), which is designed to capture both the cooperation between agent pairs based on current observations and group-level dependencies from behaviour patterns observed across trajectories. This graph is further used in graph convolution for information exchange between agents during decision-making. To further ensure behavioural consistency among agents within the same group, we introduce a group distance loss, which promotes group cohesion and encourages specialization between groups. Our evaluations, conducted on StarCraft II micromanagement tasks, demonstrate GACG's superior performance. An ablation study further provides experimental evidence of the effectiveness of each component of our method.
<div id='section'>Paperid: <span id='pid'>642, <a href='https://arxiv.org/pdf/2404.03101.pdf' target='_blank'>https://arxiv.org/pdf/2404.03101.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Weizhe Chen, Sven Koenig, Bistra Dilkina
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.03101">MARL-LNS: Cooperative Multi-agent Reinforcement Learning via Large Neighborhoods Search</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cooperative multi-agent reinforcement learning (MARL) has been an increasingly important research topic in the last half-decade because of its great potential for real-world applications. Because of the curse of dimensionality, the popular "centralized training decentralized execution" framework requires a long time in training, yet still cannot converge efficiently. In this paper, we propose a general training framework, MARL-LNS, to algorithmically address these issues by training on alternating subsets of agents using existing deep MARL algorithms as low-level trainers, while not involving any additional parameters to be trained. Based on this framework, we provide three algorithm variants based on the framework: random large neighborhood search (RLNS), batch large neighborhood search (BLNS), and adaptive large neighborhood search (ALNS), which alternate the subsets of agents differently. We test our algorithms on both the StarCraft Multi-Agent Challenge and Google Research Football, showing that our algorithms can automatically reduce at least 10% of training time while reaching the same final skill level as the original algorithm.
<div id='section'>Paperid: <span id='pid'>643, <a href='https://arxiv.org/pdf/2403.19253.pdf' target='_blank'>https://arxiv.org/pdf/2403.19253.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wei Duan, Jie Lu, Junyu Xuan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.19253">Inferring Latent Temporal Sparse Coordination Graph for Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Effective agent coordination is crucial in cooperative Multi-Agent Reinforcement Learning (MARL). While agent cooperation can be represented by graph structures, prevailing graph learning methods in MARL are limited. They rely solely on one-step observations, neglecting crucial historical experiences, leading to deficient graphs that foster redundant or detrimental information exchanges. Additionally, high computational demands for action-pair calculations in dense graphs impede scalability. To address these challenges, we propose inferring a Latent Temporal Sparse Coordination Graph (LTS-CG) for MARL. The LTS-CG leverages agents' historical observations to calculate an agent-pair probability matrix, where a sparse graph is sampled from and used for knowledge exchange between agents, thereby simultaneously capturing agent dependencies and relation uncertainty. The computational complexity of this procedure is only related to the number of agents. This graph learning process is further augmented by two innovative characteristics: Predict-Future, which enables agents to foresee upcoming observations, and Infer-Present, ensuring a thorough grasp of the environmental context from limited data. These features allow LTS-CG to construct temporal graphs from historical and real-time information, promoting knowledge exchange during policy learning and effective collaboration. Graph learning and agent training occur simultaneously in an end-to-end manner. Our demonstrated results on the StarCraft II benchmark underscore LTS-CG's superior performance.
<div id='section'>Paperid: <span id='pid'>644, <a href='https://arxiv.org/pdf/2402.17978.pdf' target='_blank'>https://arxiv.org/pdf/2402.17978.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zeyang Liu, Lipeng Wan, Xinrui Yang, Zhuoran Chen, Xingyu Chen, Xuguang Lan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.17978">Imagine, Initialize, and Explore: An Effective Exploration Method in Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Effective exploration is crucial to discovering optimal strategies for multi-agent reinforcement learning (MARL) in complex coordination tasks. Existing methods mainly utilize intrinsic rewards to enable committed exploration or use role-based learning for decomposing joint action spaces instead of directly conducting a collective search in the entire action-observation space. However, they often face challenges obtaining specific joint action sequences to reach successful states in long-horizon tasks. To address this limitation, we propose Imagine, Initialize, and Explore (IIE), a novel method that offers a promising solution for efficient multi-agent exploration in complex scenarios. IIE employs a transformer model to imagine how the agents reach a critical state that can influence each other's transition functions. Then, we initialize the environment at this state using a simulator before the exploration phase. We formulate the imagination as a sequence modeling problem, where the states, observations, prompts, actions, and rewards are predicted autoregressively. The prompt consists of timestep-to-go, return-to-go, influence value, and one-shot demonstration, specifying the desired state and trajectory as well as guiding the action generation. By initializing agents at the critical states, IIE significantly increases the likelihood of discovering potentially important under-explored regions. Despite its simplicity, empirical results demonstrate that our method outperforms multi-agent exploration baselines on the StarCraft Multi-Agent Challenge (SMAC) and SMACv2 environments. Particularly, IIE shows improved performance in the sparse-reward SMAC tasks and produces more effective curricula over the initialized states than other generative methods, such as CVAE-GAN and diffusion models.
<div id='section'>Paperid: <span id='pid'>645, <a href='https://arxiv.org/pdf/2401.12574.pdf' target='_blank'>https://arxiv.org/pdf/2401.12574.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiyuan Li, Wenshuai Zhao, Lijun Wu, Joni Pajarinen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.12574">Backpropagation Through Agents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A fundamental challenge in multi-agent reinforcement learning (MARL) is to learn the joint policy in an extremely large search space, which grows exponentially with the number of agents. Moreover, fully decentralized policy factorization significantly restricts the search space, which may lead to sub-optimal policies. In contrast, the auto-regressive joint policy can represent a much richer class of joint policies by factorizing the joint policy into the product of a series of conditional individual policies. While such factorization introduces the action dependency among agents explicitly in sequential execution, it does not take full advantage of the dependency during learning. In particular, the subsequent agents do not give the preceding agents feedback about their decisions. In this paper, we propose a new framework Back-Propagation Through Agents (BPTA) that directly accounts for both agents' own policy updates and the learning of their dependent counterparts. This is achieved by propagating the feedback through action chains. With the proposed framework, our Bidirectional Proximal Policy Optimisation (BPPO) outperforms the state-of-the-art methods. Extensive experiments on matrix games, StarCraftII v2, Multi-agent MuJoCo, and Google Research Football demonstrate the effectiveness of the proposed method.
<div id='section'>Paperid: <span id='pid'>646, <a href='https://arxiv.org/pdf/2401.08728.pdf' target='_blank'>https://arxiv.org/pdf/2401.08728.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiyuan Li, Wenshuai Zhao, Lijun Wu, Joni Pajarinen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.08728">AgentMixer: Multi-Agent Correlated Policy Factorization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In multi-agent reinforcement learning, centralized training with decentralized execution (CTDE) methods typically assume that agents make decisions based on their local observations independently, which may not lead to a correlated joint policy with coordination. Coordination can be explicitly encouraged during training and individual policies can be trained to imitate the correlated joint policy. However, this may lead to an \textit{asymmetric learning failure} due to the observation mismatch between the joint and individual policies. Inspired by the concept of correlated equilibrium, we introduce a \textit{strategy modification} called AgentMixer that allows agents to correlate their policies. AgentMixer combines individual partially observable policies into a joint fully observable policy non-linearly. To enable decentralized execution, we introduce \textit{Individual-Global-Consistency} to guarantee mode consistency during joint training of the centralized and decentralized policies and prove that AgentMixer converges to an $Îµ$-approximate Correlated Equilibrium. In the Multi-Agent MuJoCo, SMAC-v2, Matrix Game, and Predator-Prey benchmarks, AgentMixer outperforms or matches state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>647, <a href='https://arxiv.org/pdf/2401.00167.pdf' target='_blank'>https://arxiv.org/pdf/2401.00167.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xin Yu, Rongye Shi, Pu Feng, Yongkai Tian, Simin Li, Shuhao Liao, Wenjun Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.00167">Leveraging Partial Symmetry for Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Incorporating symmetry as an inductive bias into multi-agent reinforcement learning (MARL) has led to improvements in generalization, data efficiency, and physical consistency. While prior research has succeeded in using perfect symmetry prior, the realm of partial symmetry in the multi-agent domain remains unexplored. To fill in this gap, we introduce the partially symmetric Markov game, a new subclass of the Markov game. We then theoretically show that the performance error introduced by utilizing symmetry in MARL is bounded, implying that the symmetry prior can still be useful in MARL even in partial symmetry situations. Motivated by this insight, we propose the Partial Symmetry Exploitation (PSE) framework that is able to adaptively incorporate symmetry prior in MARL under different symmetry-breaking conditions. Specifically, by adaptively adjusting the exploitation of symmetry, our framework is able to achieve superior sample efficiency and overall performance of MARL algorithms. Extensive experiments are conducted to demonstrate the superior performance of the proposed framework over baselines. Finally, we implement the proposed framework in real-world multi-robot testbed to show its superiority.
<div id='section'>Paperid: <span id='pid'>648, <a href='https://arxiv.org/pdf/2312.09120.pdf' target='_blank'>https://arxiv.org/pdf/2312.09120.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Martin Riedmiller, Tim Hertweck, Roland Hafner
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.09120">Less is more -- the Dispatcher/ Executor principle for multi-task Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humans instinctively know how to neglect details when it comes to solve complex decision making problems in environments with unforeseeable variations. This abstraction process seems to be a vital property for most biological systems and helps to 'abstract away' unnecessary details and boost generalisation. In this work we introduce the dispatcher/ executor principle for the design of multi-task Reinforcement Learning controllers. It suggests to partition the controller in two entities, one that understands the task (the dispatcher) and one that computes the controls for the specific device (the executor) - and to connect these two by a strongly regularizing communication channel. The core rationale behind this position paper is that changes in structure and design principles can improve generalisation properties and drastically enforce data-efficiency. It is in some sense a 'yes, and ...' response to the current trend of using large neural networks trained on vast amounts of data and bet on emerging generalisation properties. While we agree on the power of scaling - in the sense of Sutton's 'bitter lesson' - we will give some evidence, that considering structure and adding design principles can be a valuable and critical component in particular when data is not abundant and infinite, but is a precious resource.
<div id='section'>Paperid: <span id='pid'>649, <a href='https://arxiv.org/pdf/2312.04767.pdf' target='_blank'>https://arxiv.org/pdf/2312.04767.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mi Zhou, Jiazhi Li, Masood Mortazavi, Ning Yan, Chaouki Abdallah
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.04767">Finite Horizon Multi-Agent Reinforcement Learning in Solving Optimal Control of State-Dependent Switched Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this article, a \underline{S}tate-dependent \underline{M}ulti-\underline{A}gent \underline{D}eep \underline{D}eterministic \underline{P}olicy \underline{G}radient (\textbf{SMADDPG}) method is proposed in order to learn an optimal control policy for regionally switched systems. We observe good performance of this method and explain it in a rigorous mathematical language using some simplifying assumptions in order to motivate the ideas and to apply them to some canonical examples. Using reinforcement learning, the performance of the switched learning-based multi-agent method is compared with the vanilla DDPG in two customized demonstrative environments with one and two-dimensional state spaces.
<div id='section'>Paperid: <span id='pid'>650, <a href='https://arxiv.org/pdf/2310.12359.pdf' target='_blank'>https://arxiv.org/pdf/2310.12359.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuhang Zhang, Marcos Quinones-Grueiro, Zhiyao Zhang, Yanbing Wang, William Barbour, Gautam Biswas, Daniel Work
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.12359">MARVEL: Multi-Agent Reinforcement-Learning for Large-Scale Variable Speed Limits</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Variable Speed Limit (VSL) control acts as a promising highway traffic management strategy with worldwide deployment, which can enhance traffic safety by dynamically adjusting speed limits according to real-time traffic conditions. Most of the deployed VSL control algorithms so far are rule-based, lacking generalizability under varying and complex traffic scenarios. In this work, we propose MARVEL (Multi-Agent Reinforcement-learning for large-scale Variable spEed Limits), a novel framework for large-scale VSL control on highway corridors with real-world deployment settings. MARVEL utilizes only sensing information observable in the real world as state input and learns through a reward structure that incorporates adaptability to traffic conditions, safety, and mobility, thereby enabling multi-agent coordination. With parameter sharing among all VSL agents, the proposed framework scales to cover corridors with many agents. The policies are trained in a microscopic traffic simulation environment, focusing on a short freeway stretch with 8 VSL agents spanning 7 miles. For testing, these policies are applied to a more extensive network with 34 VSL agents spanning 17 miles of I-24 near Nashville, TN, USA. MARVEL-based method improves traffic safety by 63.4% compared to the no control scenario and enhances traffic mobility by 58.6% compared to a state-of-the-practice algorithm that has been deployed on I-24. Besides, we conduct an explainability analysis to examine the decision-making process of the agents and explore the learned policy under different traffic conditions. Finally, we test the response of the policy learned from the simulation-based experiments with real-world data collected from I-24 and illustrate its deployment capability.
<div id='section'>Paperid: <span id='pid'>651, <a href='https://arxiv.org/pdf/2309.14792.pdf' target='_blank'>https://arxiv.org/pdf/2309.14792.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenshuai Zhao, Eetu-Aleksi Rantala, Sahar Salimpour, Zhiyuan Li, Joni Pajarinen, Jorge PeÃ±a Queralta
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.14792">Exploiting Local Observations for Robust Robot Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While many robotic tasks can be addressed using either centralized single-agent control with full state observation or decentralized multi-agent control, clear criteria for choosing between these approaches remain underexplored. This paper systematically investigates how multi-agent reinforcement learning (MARL) with local observations can improve robustness in complex robotic systems compared to traditional centralized control. Through theoretical analysis and empirical validation, we show that in certain tasks, decentralized MARL can achieve performance comparable to centralized methods while exhibiting greater resilience to perturbations and agent failures. By analytically demonstrating the equivalence of single-agent reinforcement learning (SARL) and MARL under full observability, we identify observability as the critical factor distinguishing the two paradigms. We further derive bounds quantifying performance degradation under external perturbations for locally observable policies. Empirical results on standard MARL benchmarks confirm that MARL with limited observations can maintain competitive performance. Finally, real-world experiments with a mobile manipulator demonstrate that decentralized MARL controllers achieve markedly improved robustness to agent malfunctions and environmental disturbances relative to centralized baselines. Together, these findings highlight MARL with local observations as a robust and practical alternative to conventional centralized control in complex robotic systems.
<div id='section'>Paperid: <span id='pid'>652, <a href='https://arxiv.org/pdf/2309.11247.pdf' target='_blank'>https://arxiv.org/pdf/2309.11247.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ardian Selmonaj, Oleg Szehr, Giacomo Del Rio, Alessandro Antonucci, Adrian Schneider, Michael RÃ¼egsegger
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.11247">Hierarchical Multi-Agent Reinforcement Learning for Air Combat Maneuvering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The application of artificial intelligence to simulate air-to-air combat scenarios is attracting increasing attention. To date the high-dimensional state and action spaces, the high complexity of situation information (such as imperfect and filtered information, stochasticity, incomplete knowledge about mission targets) and the nonlinear flight dynamics pose significant challenges for accurate air combat decision-making. These challenges are exacerbated when multiple heterogeneous agents are involved. We propose a hierarchical multi-agent reinforcement learning framework for air-to-air combat with multiple heterogeneous agents. In our framework, the decision-making process is divided into two stages of abstraction, where heterogeneous low-level policies control the action of individual units, and a high-level commander policy issues macro commands given the overall mission targets. Low-level policies are trained for accurate unit combat control. Their training is organized in a learning curriculum with increasingly complex training scenarios and league-based self-play. The commander policy is trained on mission targets given pre-trained low-level policies. The empirical validation advocates the advantages of our design choices.
<div id='section'>Paperid: <span id='pid'>653, <a href='https://arxiv.org/pdf/2308.09902.pdf' target='_blank'>https://arxiv.org/pdf/2308.09902.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Canzhe Zhao, Yanjie Ze, Jing Dong, Baoxiang Wang, Shuai Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.09902">DPMAC: Differentially Private Communication for Cooperative Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Communication lays the foundation for cooperation in human society and in multi-agent reinforcement learning (MARL). Humans also desire to maintain their privacy when communicating with others, yet such privacy concern has not been considered in existing works in MARL. To this end, we propose the \textit{differentially private multi-agent communication} (DPMAC) algorithm, which protects the sensitive information of individual agents by equipping each agent with a local message sender with rigorous $(Îµ, Î´)$-differential privacy (DP) guarantee. In contrast to directly perturbing the messages with predefined DP noise as commonly done in privacy-preserving scenarios, we adopt a stochastic message sender for each agent respectively and incorporate the DP requirement into the sender, which automatically adjusts the learned message distribution to alleviate the instability caused by DP noise. Further, we prove the existence of a Nash equilibrium in cooperative MARL with privacy-preserving communication, which suggests that this problem is game-theoretically learnable. Extensive experiments demonstrate a clear advantage of DPMAC over baseline methods in privacy-preserving scenarios.
<div id='section'>Paperid: <span id='pid'>654, <a href='https://arxiv.org/pdf/2308.03643.pdf' target='_blank'>https://arxiv.org/pdf/2308.03643.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yueheng Li, Zicheng Zhang, Hao Chen, Zhan Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.03643">Mamba: Bringing Multi-Dimensional ABR to WebRTC</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Contemporary real-time video communication systems, such as WebRTC, use an adaptive bitrate (ABR) algorithm to assure high-quality and low-delay services, e.g., promptly adjusting video bitrate according to the instantaneous network bandwidth. However, target bitrate decisions in the network and bitrate control in the codec are typically incoordinated and simply ignoring the effect of inappropriate resolution and frame rate settings also leads to compromised results in bitrate control, thus devastatingly deteriorating the quality of experience (QoE). To tackle these challenges, Mamba, an end-to-end multi-dimensional ABR algorithm is proposed, which utilizes multi-agent reinforcement learning (MARL) to maximize the user's QoE by adaptively and collaboratively adjusting encoding factors including the quantization parameters (QP), resolution, and frame rate based on observed states such as network conditions and video complexity information in a video conferencing system. We also introduce curriculum learning to improve the training efficiency of MARL. Both the in-lab and real-world evaluation results demonstrate the remarkable efficacy of Mamba.
<div id='section'>Paperid: <span id='pid'>655, <a href='https://arxiv.org/pdf/2307.16186.pdf' target='_blank'>https://arxiv.org/pdf/2307.16186.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xin Yu, Rongye Shi, Pu Feng, Yongkai Tian, Jie Luo, Wenjun Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.16186">ESP: Exploiting Symmetry Prior for Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent reinforcement learning (MARL) has achieved promising results in recent years. However, most existing reinforcement learning methods require a large amount of data for model training. In addition, data-efficient reinforcement learning requires the construction of strong inductive biases, which are ignored in the current MARL approaches. Inspired by the symmetry phenomenon in multi-agent systems, this paper proposes a framework for exploiting prior knowledge by integrating data augmentation and a well-designed consistency loss into the existing MARL methods. In addition, the proposed framework is model-agnostic and can be applied to most of the current MARL algorithms. Experimental tests on multiple challenging tasks demonstrate the effectiveness of the proposed framework. Moreover, the proposed framework is applied to a physical multi-robot testbed to show its superiority.
<div id='section'>Paperid: <span id='pid'>656, <a href='https://arxiv.org/pdf/2307.12915.pdf' target='_blank'>https://arxiv.org/pdf/2307.12915.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Srijoni Majumdar, Evangelos Pournaras
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.12915">Consensus-based Participatory Budgeting for Legitimacy: Decision Support via Multi-agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The legitimacy of bottom-up democratic processes for the distribution of public funds by policy-makers is challenging and complex. Participatory budgeting is such a process, where voting outcomes may not always be fair or inclusive. Deliberation for which project ideas to put for voting and choose for implementation lack systematization and do not scale. This paper addresses these grand challenges by introducing a novel and legitimate iterative consensus-based participatory budgeting process. Consensus is designed to be a result of decision support via an innovative multi-agent reinforcement learning approach. Voters are assisted to interact with each other to make viable compromises. Extensive experimental evaluation with real-world participatory budgeting data from Poland reveal striking findings: Consensus is reachable, efficient and robust. Compromise is required, which is though comparable to the one of existing voting aggregation methods that promote fairness and inclusion without though attaining consensus.
<div id='section'>Paperid: <span id='pid'>657, <a href='https://arxiv.org/pdf/2306.11168.pdf' target='_blank'>https://arxiv.org/pdf/2306.11168.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sean Ye, Manisha Natarajan, Zixuan Wu, Rohan Paleja, Letian Chen, Matthew C. Gombolay
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.11168">Learning Models of Adversarial Agent Behavior under Partial Observability</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The need for opponent modeling and tracking arises in several real-world scenarios, such as professional sports, video game design, and drug-trafficking interdiction. In this work, we present Graph based Adversarial Modeling with Mutal Information (GrAMMI) for modeling the behavior of an adversarial opponent agent. GrAMMI is a novel graph neural network (GNN) based approach that uses mutual information maximization as an auxiliary objective to predict the current and future states of an adversarial opponent with partial observability. To evaluate GrAMMI, we design two large-scale, pursuit-evasion domains inspired by real-world scenarios, where a team of heterogeneous agents is tasked with tracking and interdicting a single adversarial agent, and the adversarial agent must evade detection while achieving its own objectives. With the mutual information formulation, GrAMMI outperforms all baselines in both domains and achieves 31.68% higher log-likelihood on average for future adversarial state predictions across both domains.
<div id='section'>Paperid: <span id='pid'>658, <a href='https://arxiv.org/pdf/2306.02430.pdf' target='_blank'>https://arxiv.org/pdf/2306.02430.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wei-Fang Sun, Cheng-Kuang Lee, Simon See, Chun-Yi Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.02430">A Unified Framework for Factorizing Distributional Value Functions for Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In fully cooperative multi-agent reinforcement learning (MARL) settings, environments are highly stochastic due to the partial observability of each agent and the continuously changing policies of other agents. To address the above issues, we proposed a unified framework, called DFAC, for integrating distributional RL with value function factorization methods. This framework generalizes expected value function factorization methods to enable the factorization of return distributions. To validate DFAC, we first demonstrate its ability to factorize the value functions of a simple matrix game with stochastic rewards. Then, we perform experiments on all Super Hard maps of the StarCraft Multi-Agent Challenge and six self-designed Ultra Hard maps, showing that DFAC is able to outperform a number of baselines.
<div id='section'>Paperid: <span id='pid'>659, <a href='https://arxiv.org/pdf/2304.04637.pdf' target='_blank'>https://arxiv.org/pdf/2304.04637.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yueheng Li, Qianyuan Zheng, Zicheng Zhang, Hao Chen, Zhan Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.04637">Improving ABR Performance for Short Video Streaming Using Multi-Agent Reinforcement Learning with Expert Guidance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the realm of short video streaming, popular adaptive bitrate (ABR) algorithms developed for classical long video applications suffer from catastrophic failures because they are tuned to solely adapt bitrates. Instead, short video adaptive bitrate (SABR) algorithms have to properly determine which video at which bitrate level together for content prefetching, without sacrificing the users' quality of experience (QoE) and yielding noticeable bandwidth wastage jointly. Unfortunately, existing SABR methods are inevitably entangled with slow convergence and poor generalization. Thus, in this paper, we propose Incendio, a novel SABR framework that applies Multi-Agent Reinforcement Learning (MARL) with Expert Guidance to separate the decision of video ID and video bitrate in respective buffer management and bitrate adaptation agents to maximize the system-level utilized score modeled as a compound function of QoE and bandwidth wastage metrics. To train Incendio, it is first initialized by imitating the hand-crafted expert rules and then fine-tuned through the use of MARL. Results from extensive experiments indicate that Incendio outperforms the current state-of-the-art SABR algorithm with a 53.2% improvement measured by the utility score while maintaining low training complexity and inference time.
<div id='section'>Paperid: <span id='pid'>660, <a href='https://arxiv.org/pdf/2303.01768.pdf' target='_blank'>https://arxiv.org/pdf/2303.01768.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jihwan Oh, Joonkee Kim, Minchan Jeong, Se-Young Yun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.01768">Toward Risk-based Optimistic Exploration for Cooperative Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The multi-agent setting is intricate and unpredictable since the behaviors of multiple agents influence one another. To address this environmental uncertainty, distributional reinforcement learning algorithms that incorporate uncertainty via distributional output have been integrated with multi-agent reinforcement learning (MARL) methods, achieving state-of-the-art performance. However, distributional MARL algorithms still rely on the traditional $Îµ$-greedy, which does not take cooperative strategy into account. In this paper, we present a risk-based exploration that leads to collaboratively optimistic behavior by shifting the sampling region of distribution. Initially, we take expectations from the upper quantiles of state-action values for exploration, which are optimistic actions, and gradually shift the sampling region of quantiles to the full distribution for exploitation. By ensuring that each agent is exposed to the same level of risk, we can force them to take cooperatively optimistic actions. Our method shows remarkable performance in multi-agent settings requiring cooperative exploration based on quantile regression appropriately controlling the level of risk.
<div id='section'>Paperid: <span id='pid'>661, <a href='https://arxiv.org/pdf/2303.01070.pdf' target='_blank'>https://arxiv.org/pdf/2303.01070.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaoyang Yu, Youfang Lin, Xiangsen Wang, Sheng Han, Kai Lv
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.01070">GHQ: Grouped Hybrid Q Learning for Heterogeneous Cooperative Multi-agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Previous deep multi-agent reinforcement learning (MARL) algorithms have achieved impressive results, typically in homogeneous scenarios. However, heterogeneous scenarios are also very common and usually harder to solve. In this paper, we mainly discuss cooperative heterogeneous MARL problems in Starcraft Multi-Agent Challenges (SMAC) environment. We firstly define and describe the heterogeneous problems in SMAC. In order to comprehensively reveal and study the problem, we make new maps added to the original SMAC maps. We find that baseline algorithms fail to perform well in those heterogeneous maps. To address this issue, we propose the Grouped Individual-Global-Max Consistency (GIGM) and a novel MARL algorithm, Grouped Hybrid Q Learning (GHQ). GHQ separates agents into several groups and keeps individual parameters for each group, along with a novel hybrid structure for factorization. To enhance coordination between groups, we maximize the Inter-group Mutual Information (IGMI) between groups' trajectories. Experiments on original and new heterogeneous maps show the fabulous performance of GHQ compared to other state-of-the-art algorithms.
<div id='section'>Paperid: <span id='pid'>662, <a href='https://arxiv.org/pdf/2210.06274.pdf' target='_blank'>https://arxiv.org/pdf/2210.06274.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pedro P. Santos, Diogo S. Carvalho, Miguel Vasco, Alberto Sardinha, Pedro A. Santos, Ana Paiva, Francisco S. Melo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2210.06274">Centralized Training with Hybrid Execution in Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce hybrid execution in multi-agent reinforcement learning (MARL), a new paradigm in which agents aim to successfully complete cooperative tasks with arbitrary communication levels at execution time by taking advantage of information-sharing among the agents. Under hybrid execution, the communication level can range from a setting in which no communication is allowed between agents (fully decentralized), to a setting featuring full communication (fully centralized), but the agents do not know beforehand which communication level they will encounter at execution time. To formalize our setting, we define a new class of multi-agent partially observable Markov decision processes (POMDPs) that we name hybrid-POMDPs, which explicitly model a communication process between the agents. We contribute MARO, an approach that makes use of an auto-regressive predictive model, trained in a centralized manner, to estimate missing agents' observations at execution time. We evaluate MARO on standard scenarios and extensions of previous benchmarks tailored to emphasize the negative impact of partial observability in MARL. Experimental results show that our method consistently outperforms relevant baselines, allowing agents to act with faulty communication while successfully exploiting shared information.
<div id='section'>Paperid: <span id='pid'>663, <a href='https://arxiv.org/pdf/2205.10016.pdf' target='_blank'>https://arxiv.org/pdf/2205.10016.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenshuai Zhao, Zhiyuan Li, Joni Pajarinen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2205.10016">Learning Progress Driven Multi-Agent Curriculum</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The number of agents can be an effective curriculum variable for controlling the difficulty of multi-agent reinforcement learning (MARL) tasks. Existing work typically uses manually defined curricula such as linear schemes. We identify two potential flaws while applying existing reward-based automatic curriculum learning methods in MARL: (1) The expected episode return used to measure task difficulty has high variance; (2) Credit assignment difficulty can be exacerbated in tasks where increasing the number of agents yields higher returns which is common in many MARL tasks. To address these issues, we propose to control the curriculum by using a TD-error based *learning progress* measure and by letting the curriculum proceed from an initial context distribution to the final task specific one. Since our approach maintains a distribution over the number of agents and measures learning progress rather than absolute performance, which often increases with the number of agents, we alleviate problem (2). Moreover, the learning progress measure naturally alleviates problem (1) by aggregating returns. In three challenging sparse-reward MARL benchmarks, our approach outperforms state-of-the-art baselines.
<div id='section'>Paperid: <span id='pid'>664, <a href='https://arxiv.org/pdf/2509.22216.pdf' target='_blank'>https://arxiv.org/pdf/2509.22216.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ahmet Onur Akman, Anastasia Psarou, ZoltÃ¡n GyÃ¶rgy Varga, Grzegorz JamrÃ³z, RafaÅ Kucharski
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.22216">Impact of Collective Behaviors of Autonomous Vehicles on Urban Traffic Dynamics: A Multi-Agent Reinforcement Learning Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This study examines the potential impact of reinforcement learning (RL)-enabled autonomous vehicles (AV) on urban traffic flow in a mixed traffic environment. We focus on a simplified day-to-day route choice problem in a multi-agent setting. We consider a city network where human drivers travel through their chosen routes to reach their destinations in minimum travel time. Then, we convert one-third of the population into AVs, which are RL agents employing Deep Q-learning algorithm. We define a set of optimization targets, or as we call them behaviors, namely selfish, collaborative, competitive, social, altruistic, and malicious. We impose a selected behavior on AVs through their rewards. We run our simulations using our in-house developed RL framework PARCOUR. Our simulations reveal that AVs optimize their travel times by up to 5\%, with varying impacts on human drivers' travel times depending on the AV behavior. In all cases where AVs adopt a self-serving behavior, they achieve shorter travel times than human drivers. Our findings highlight the complexity differences in learning tasks of each target behavior. We demonstrate that the multi-agent RL setting is applicable for collective routing on traffic networks, though their impact on coexisting parties greatly varies with the behaviors adopted.
<div id='section'>Paperid: <span id='pid'>665, <a href='https://arxiv.org/pdf/2509.22216.pdf' target='_blank'>https://arxiv.org/pdf/2509.22216.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ahmet Onur Akman, Anastasia Psarou, ZoltÃ¡n GyÃ¶rgy Varga, Grzegorz JamrÃ³z, RafaÅ Kucharski
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.22216">Impact of Collective Behaviors of Autonomous Vehicles on Urban Traffic Dynamics: A Multi-Agent Reinforcement Learning Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This study examines the potential impact of reinforcement learning (RL)-enabled autonomous vehicles (AV) on urban traffic flow in a mixed traffic environment. We focus on a simplified day-to-day route choice problem in a multi-agent setting. We consider a city network where human drivers travel through their chosen routes to reach their destinations in minimum travel time. Then, we convert one-third of the population into AVs, which are RL agents employing Deep Q-learning algorithm. We define a set of optimization targets, or as we call them behaviors, namely selfish, collaborative, competitive, social, altruistic, and malicious. We impose a selected behavior on AVs through their rewards. We run our simulations using our in-house developed RL framework PARCOUR. Our simulations reveal that AVs optimize their travel times by up to 5\%, with varying impacts on human drivers' travel times depending on the AV behavior. In all cases where AVs adopt a self-serving behavior, they achieve shorter travel times than human drivers. Our findings highlight the complexity differences in learning tasks of each target behavior. We demonstrate that the multi-agent RL setting is applicable for collective routing on traffic networks, though their impact on coexisting parties greatly varies with the behaviors adopted.
<div id='section'>Paperid: <span id='pid'>666, <a href='https://arxiv.org/pdf/2507.09094.pdf' target='_blank'>https://arxiv.org/pdf/2507.09094.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaoren Xu, Hao Xu, Dongyu Wei, Walid Saad, Mehdi Bennis, Mingzhe Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.09094">Transformer based Collaborative Reinforcement Learning for Fluid Antenna System (FAS)-enabled 3D UAV Positioning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, a novel Three dimensional (3D) positioning framework of fluid antenna system (FAS)-enabled unmanned aerial vehicles (UAVs) is developed. In the proposed framework, a set of controlled UAVs cooperatively estimate the real-time 3D position of a target UAV. Here, the active UAV transmits a measurement signal to the passive UAVs via the reflection from the target UAV. Each passive UAV estimates the distance of the active-target-passive UAV link and selects an antenna port to share the distance information with the base station (BS) that calculates the real-time position of the target UAV. As the target UAV is moving due to its task operation, the controlled UAVs must optimize their trajectories and select optimal antenna port, aiming to estimate the real-time position of the target UAV. We formulate this problem as an optimization problem to minimize the target UAV positioning error via optimizing the trajectories of all controlled UAVs and antenna port selection of passive UAVs. Here, an attention-based recurrent multi-agent reinforcement learning (AR-MARL) scheme is proposed, which enables each controlled UAV to use the local Q function to determine its trajectory and antenna port while optimizing the target UAV positioning performance without knowing the trajectories and antenna port selections of other controlled UAVs. Different from current MARL methods, the proposed method uses a recurrent neural network (RNN) that incorporates historical state-action pairs of each controlled UAV, and an attention mechanism to analyze the importance of these historical state-action pairs, thus improving the global Q function approximation accuracy and the target UAV positioning accuracy. Simulation results show that the proposed AR-MARL scheme can reduce the average positioning error by up to 17.5% and 58.5% compared to the VD-MARL scheme and the proposed method without FAS.
<div id='section'>Paperid: <span id='pid'>667, <a href='https://arxiv.org/pdf/2506.20664.pdf' target='_blank'>https://arxiv.org/pdf/2506.20664.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Andrei Lupu, Timon Willi, Jakob Foerster
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.20664">The Decrypto Benchmark for Multi-Agent Reasoning and Theory of Mind</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As Large Language Models (LLMs) gain agentic abilities, they will have to navigate complex multi-agent scenarios, interacting with human users and other agents in cooperative and competitive settings. This will require new reasoning skills, chief amongst them being theory of mind (ToM), or the ability to reason about the "mental" states of other agents. However, ToM and other multi-agent abilities in LLMs are poorly understood, since existing benchmarks suffer from narrow scope, data leakage, saturation, and lack of interactivity. We thus propose Decrypto, a game-based benchmark for multi-agent reasoning and ToM drawing inspiration from cognitive science, computational pragmatics and multi-agent reinforcement learning. It is designed to be as easy as possible in all other dimensions, eliminating confounding factors commonly found in other benchmarks. To our knowledge, it is also the first platform for designing interactive ToM experiments.
  We validate the benchmark design through comprehensive empirical evaluations of frontier LLMs, robustness studies, and human-AI cross-play experiments. We find that LLM game-playing abilities lag behind humans and simple word-embedding baselines. We then create variants of two classic cognitive science experiments within Decrypto to evaluate three key ToM abilities. Surprisingly, we find that state-of-the-art reasoning models are significantly worse at those tasks than their older counterparts. This demonstrates that Decrypto addresses a crucial gap in current reasoning and ToM evaluations, and paves the path towards better artificial agents.
<div id='section'>Paperid: <span id='pid'>668, <a href='https://arxiv.org/pdf/2506.17029.pdf' target='_blank'>https://arxiv.org/pdf/2506.17029.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Leizhen Wang, Peibo Duan, Cheng Lyu, Zewen Wang, Zhiqiang He, Nan Zheng, Zhenliang Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.17029">Scalable and Reliable Multi-agent Reinforcement Learning for Traffic Assignment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The evolution of metropolitan cities and the increase in travel demands impose stringent requirements on traffic assignment methods. Multi-agent reinforcement learning (MARL) approaches outperform traditional methods in modeling adaptive routing behavior without requiring explicit system dynamics, which is beneficial for real-world deployment. However, MARL frameworks face challenges in scalability and reliability when managing extensive networks with substantial travel demand, which limiting their practical applicability in solving large-scale traffic assignment problems. To address these challenges, this study introduces MARL-OD-DA, a new MARL framework for the traffic assignment problem, which redefines agents as origin-destination (OD) pair routers rather than individual travelers, significantly enhancing scalability. Additionally, a Dirichlet-based action space with action pruning and a reward function based on the local relative gap are designed to enhance solution reliability and improve convergence efficiency. Experiments demonstrate that the proposed MARL framework effectively handles medium-sized networks with extensive and varied city-level OD demand, surpassing existing MARL methods. When implemented in the SiouxFalls network, MARL-OD-DA achieves better assignment solutions in 10 steps, with a relative gap that is 94.99% lower than that of conventional methods.
<div id='section'>Paperid: <span id='pid'>669, <a href='https://arxiv.org/pdf/2505.24265.pdf' target='_blank'>https://arxiv.org/pdf/2505.24265.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Harsh Goel, Mohammad Omama, Behdad Chalaki, Vaishnav Tadiparthi, Ehsan Moradi Pari, Sandeep Chinchali
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.24265">R3DM: Enabling Role Discovery and Diversity Through Dynamics Models in Multi-agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent reinforcement learning (MARL) has achieved significant progress in large-scale traffic control, autonomous vehicles, and robotics. Drawing inspiration from biological systems where roles naturally emerge to enable coordination, role-based MARL methods have been proposed to enhance cooperation learning for complex tasks. However, existing methods exclusively derive roles from an agent's past experience during training, neglecting their influence on its future trajectories. This paper introduces a key insight: an agent's role should shape its future behavior to enable effective coordination. Hence, we propose Role Discovery and Diversity through Dynamics Models (R3DM), a novel role-based MARL framework that learns emergent roles by maximizing the mutual information between agents' roles, observed trajectories, and expected future behaviors. R3DM optimizes the proposed objective through contrastive learning on past trajectories to first derive intermediate roles that shape intrinsic rewards to promote diversity in future behaviors across different roles through a learned dynamics model. Benchmarking on SMAC and SMACv2 environments demonstrates that R3DM outperforms state-of-the-art MARL approaches, improving multi-agent coordination to increase win rates by up to 20%.
<div id='section'>Paperid: <span id='pid'>670, <a href='https://arxiv.org/pdf/2505.05262.pdf' target='_blank'>https://arxiv.org/pdf/2505.05262.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Andreas Kontogiannis, Konstantinos Papathanasiou, Yi Shen, Giorgos Stamou, Michael M. Zavlanos, George Vouros
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.05262">Enhancing Cooperative Multi-Agent Reinforcement Learning with State Modelling and Adversarial Exploration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Learning to cooperate in distributed partially observable environments with no communication abilities poses significant challenges for multi-agent deep reinforcement learning (MARL). This paper addresses key concerns in this domain, focusing on inferring state representations from individual agent observations and leveraging these representations to enhance agents' exploration and collaborative task execution policies. To this end, we propose a novel state modelling framework for cooperative MARL, where agents infer meaningful belief representations of the non-observable state, with respect to optimizing their own policies, while filtering redundant and less informative joint state information. Building upon this framework, we propose the MARL SMPE algorithm. In SMPE, agents enhance their own policy's discriminative abilities under partial observability, explicitly by incorporating their beliefs into the policy network, and implicitly by adopting an adversarial type of exploration policies which encourages agents to discover novel, high-value states while improving the discriminative abilities of others. Experimentally, we show that SMPE outperforms state-of-the-art MARL algorithms in complex fully cooperative tasks from the MPE, LBF, and RWARE benchmarks.
<div id='section'>Paperid: <span id='pid'>671, <a href='https://arxiv.org/pdf/2504.17590.pdf' target='_blank'>https://arxiv.org/pdf/2504.17590.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sihem Bakri, Indrakshi Dey, Harun Siljak, Marco Ruffini, Nicola Marchetti
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.17590">Mitigating xApp conflicts for efficient network slicing in 6G O-RAN: a graph convolutional-based attention network approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>O-RAN (Open-Radio Access Network) offers a flexible, open architecture for next-generation wireless networks. Network slicing within O-RAN allows network operators to create customized virtual networks, each tailored to meet the specific needs of a particular application or service. Efficiently managing these slices is crucial for future 6G networks. O-RAN introduces specialized software applications called xApps that manage different network functions. In network slicing, an xApp can be responsible for managing a separate network slice. To optimize resource allocation across numerous network slices, these xApps must coordinate. Traditional methods where all xApps communicate freely can lead to excessive overhead, hindering network performance. In this paper, we address the issue of xApp conflict mitigation by proposing an innovative Zero-Touch Management (ZTM) solution for radio resource management in O-RAN. Our approach leverages Multi-Agent Reinforcement Learning (MARL) to enable xApps to learn and optimize resource allocation without the need for constant manual intervention. We introduce a Graph Convolutional Network (GCN)-based attention mechanism to streamline communication among xApps, reducing overhead and improving overall system efficiency. Our results compare traditional MARL, where all xApps communicate, against our MARL GCN-based attention method. The findings demonstrate the superiority of our approach, especially as the number of xApps increases, ultimately providing a scalable and efficient solution for optimal network slicing management in O-RAN.
<div id='section'>Paperid: <span id='pid'>672, <a href='https://arxiv.org/pdf/2503.15947.pdf' target='_blank'>https://arxiv.org/pdf/2503.15947.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianyi Hu, Qingxu Fu, Zhiqiang Pu, Yuan Wang, Tenghai Qiu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.15947">Unreal-MAP: Unreal-Engine-Based General Platform for Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we propose Unreal Multi-Agent Playground (Unreal-MAP), an MARL general platform based on the Unreal-Engine (UE). Unreal-MAP allows users to freely create multi-agent tasks using the vast visual and physical resources available in the UE community, and deploy state-of-the-art (SOTA) MARL algorithms within them. Unreal-MAP is user-friendly in terms of deployment, modification, and visualization, and all its components are open-source. We also develop an experimental framework compatible with algorithms ranging from rule-based to learning-based provided by third-party frameworks. Lastly, we deploy several SOTA algorithms in example tasks developed via Unreal-MAP, and conduct corresponding experimental analyses. We believe Unreal-MAP can play an important role in the MARL field by closely integrating existing algorithms with user-customized tasks, thus advancing the field of MARL.
<div id='section'>Paperid: <span id='pid'>673, <a href='https://arxiv.org/pdf/2503.11488.pdf' target='_blank'>https://arxiv.org/pdf/2503.11488.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yifeng Zhang, Yilin Liu, Ping Gong, Peizhuo Li, Mingfeng Fan, Guillaume Sartoretti
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.11488">Unicorn: A Universal and Collaborative Reinforcement Learning Approach Towards Generalizable Network-Wide Traffic Signal Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Adaptive traffic signal control (ATSC) is crucial in reducing congestion, maximizing throughput, and improving mobility in rapidly growing urban areas. Recent advancements in parameter-sharing multi-agent reinforcement learning (MARL) have greatly enhanced the scalable and adaptive optimization of complex, dynamic flows in large-scale homogeneous networks. However, the inherent heterogeneity of real-world traffic networks, with their varied intersection topologies and interaction dynamics, poses substantial challenges to achieving scalable and effective ATSC across different traffic scenarios. To address these challenges, we present Unicorn, a universal and collaborative MARL framework designed for efficient and adaptable network-wide ATSC. Specifically, we first propose a unified approach to map the states and actions of intersections with varying topologies into a common structure based on traffic movements. Next, we design a Universal Traffic Representation (UTR) module with a decoder-only network for general feature extraction, enhancing the model's adaptability to diverse traffic scenarios. Additionally, we incorporate an Intersection Specifics Representation (ISR) module, designed to identify key latent vectors that represent the unique intersection's topology and traffic dynamics through variational inference techniques. To further refine these latent representations, we employ a contrastive learning approach in a self-supervised manner, which enables better differentiation of intersection-specific features. Moreover, we integrate the state-action dependencies of neighboring agents into policy optimization, which effectively captures dynamic agent interactions and facilitates efficient regional collaboration. Our results show that Unicorn outperforms other methods across various evaluation metrics, highlighting its potential in complex, dynamic traffic networks.
<div id='section'>Paperid: <span id='pid'>674, <a href='https://arxiv.org/pdf/2503.03796.pdf' target='_blank'>https://arxiv.org/pdf/2503.03796.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hyeonjun Kim, Kanghoon Lee, Junho Park, Jiachen Li, Jinkyoo Park
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.03796">Human Implicit Preference-Based Policy Fine-tuning for Multi-Agent Reinforcement Learning in USV Swarm</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-Agent Reinforcement Learning (MARL) has shown promise in solving complex problems involving cooperation and competition among agents, such as an Unmanned Surface Vehicle (USV) swarm used in search and rescue, surveillance, and vessel protection. However, aligning system behavior with user preferences is challenging due to the difficulty of encoding expert intuition into reward functions. To address the issue, we propose a Reinforcement Learning with Human Feedback (RLHF) approach for MARL that resolves credit-assignment challenges through an Agent-Level Feedback system categorizing feedback into intra-agent, inter-agent, and intra-team types. To overcome the challenges of direct human feedback, we employ a Large Language Model (LLM) evaluator to validate our approach using feedback scenarios such as region constraints, collision avoidance, and task allocation. Our method effectively refines USV swarm policies, addressing key challenges in multi-agent systems while maintaining fairness and performance consistency.
<div id='section'>Paperid: <span id='pid'>675, <a href='https://arxiv.org/pdf/2502.20065.pdf' target='_blank'>https://arxiv.org/pdf/2502.20065.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ahmet Onur Akman, Anastasia Psarou, Åukasz Gorczyca, ZoltÃ¡n GyÃ¶rgy Varga, Grzegorz JamrÃ³z, RafaÅ Kucharski
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.20065">RouteRL: Multi-agent reinforcement learning framework for urban route choice with autonomous vehicles</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>RouteRL is a novel framework that integrates multi-agent reinforcement learning (MARL) with a microscopic traffic simulation, facilitating the testing and development of efficient route choice strategies for autonomous vehicles (AVs). The proposed framework simulates the daily route choices of driver agents in a city, including two types: human drivers, emulated using behavioral route choice models, and AVs, modeled as MARL agents optimizing their policies for a predefined objective. RouteRL aims to advance research in MARL, transport modeling, and human-AI interaction for transportation applications. This study presents a technical report on RouteRL, outlines its potential research contributions, and showcases its impact via illustrative examples.
<div id='section'>Paperid: <span id='pid'>676, <a href='https://arxiv.org/pdf/2502.16608.pdf' target='_blank'>https://arxiv.org/pdf/2502.16608.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuli Zhang, Shangbo Wang, Dongyao Jia, Pengfei Fan, Ruiyuan Jiang, Hankang Gu, Andy H. F. Chow
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.16608">Toward Dependency Dynamics in Multi-Agent Reinforcement Learning for Traffic Signal Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reinforcement learning (RL) emerges as a promising data-driven approach for adaptive traffic signal control (ATSC) in complex urban traffic networks, with deep neural networks substantially augmenting its learning capabilities. However, centralized RL becomes impractical for ATSC involving multiple agents due to the exceedingly high dimensionality of the joint action space. Multi-agent RL (MARL) mitigates this scalability issue by decentralizing control to local RL agents. Nevertheless, this decentralized method introduces new challenges: the environment becomes partially observable from the perspective of each local agent due to constrained inter-agent communication. Both centralized RL and MARL exhibit distinct strengths and weaknesses, particularly under heavy intersectional traffic conditions. In this paper, we justify that MARL can achieve the optimal global Q-value by separating into multiple IRL (Independent Reinforcement Learning) processes when no spill-back congestion occurs (no agent dependency) among agents (intersections). In the presence of spill-back congestion (with agent dependency), the maximum global Q-value can be achieved by using centralized RL. Building upon the conclusions, we propose a novel Dynamic Parameter Update Strategy for Deep Q-Network (DQN-DPUS), which updates the weights and bias based on the dependency dynamics among agents, i.e. updating only the diagonal sub-matrices for the scenario without spill-back congestion. We validate the DQN-DPUS in a simple network with two intersections under varying traffic, and show that the proposed strategy can speed up the convergence rate without sacrificing optimal exploration. The results corroborate our theoretical findings, demonstrating the efficacy of DQN-DPUS in optimizing traffic signal control.
<div id='section'>Paperid: <span id='pid'>677, <a href='https://arxiv.org/pdf/2502.13188.pdf' target='_blank'>https://arxiv.org/pdf/2502.13188.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Anastasia Psarou, Ahmet Onur Akman, Åukasz Gorczyca, MichaÅ Hoffmann, Grzegorz JamrÃ³z, RafaÅ Kucharski
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.13188">Collaboration Between the City and Machine Learning Community is Crucial to Efficient Autonomous Vehicles Routing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autonomous vehicles (AVs), possibly using Multi-Agent Reinforcement Learning (MARL) for simultaneous route optimization, may destabilize traffic networks, with human drivers potentially experiencing longer travel times. We study this interaction by simulating human drivers and AVs. Our experiments with standard MARL algorithms reveal that, both in simplified and complex networks, policies often fail to converge to an optimal solution or require long training periods. This problem is amplified by the fact that we cannot rely entirely on simulated training, as there are no accurate models of human routing behavior. At the same time, real-world training in cities risks destabilizing urban traffic systems, increasing externalities, such as $CO_2$ emissions, and introducing non-stationarity as human drivers will adapt unpredictably to AV behaviors. In this position paper, we argue that city authorities must collaborate with the ML community to monitor and critically evaluate the routing algorithms proposed by car companies toward fair and system-efficient routing algorithms and regulatory standards.
<div id='section'>Paperid: <span id='pid'>678, <a href='https://arxiv.org/pdf/2410.03997.pdf' target='_blank'>https://arxiv.org/pdf/2410.03997.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuan Zhuang, Yi Shen, Zhili Zhang, Yuxiao Chen, Fei Miao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.03997">YOLO-MARL: You Only LLM Once for Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Advancements in deep multi-agent reinforcement learning (MARL) have positioned it as a promising approach for decision-making in cooperative games. However, it still remains challenging for MARL agents to learn cooperative strategies for some game environments. Recently, large language models (LLMs) have demonstrated emergent reasoning capabilities, making them promising candidates for enhancing coordination among the agents. However, due to the model size of LLMs, it can be expensive to frequently infer LLMs for actions that agents can take. In this work, we propose You Only LLM Once for MARL (YOLO-MARL), a novel framework that leverages the high-level task planning capabilities of LLMs to improve the policy learning process of multi-agents in cooperative games. Notably, for each game environment, YOLO-MARL only requires one time interaction with LLMs in the proposed strategy generation, state interpretation and planning function generation modules, before the MARL policy training process. This avoids the ongoing costs and computational time associated with frequent LLMs API calls during training. Moreover, trained decentralized policies based on normal-sized neural networks operate independently of the LLM. We evaluate our method across two different environments and demonstrate that YOLO-MARL outperforms traditional MARL algorithms.
<div id='section'>Paperid: <span id='pid'>679, <a href='https://arxiv.org/pdf/2409.20326.pdf' target='_blank'>https://arxiv.org/pdf/2409.20326.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zichong Li, Filip Bjelonic, Victor Klemm, Marco Hutter
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.20326">MARLadona -- Towards Cooperative Team Play Using Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robot soccer, in its full complexity, poses an unsolved research challenge. Current solutions heavily rely on engineered heuristic strategies, which lack robustness and adaptability. Deep reinforcement learning has gained significant traction in various complex robotics tasks such as locomotion, manipulation, and competitive games (e.g., AlphaZero, OpenAI Five), making it a promising solution to the robot soccer problem. This paper introduces MARLadona. A decentralized multi-agent reinforcement learning (MARL) training pipeline capable of producing agents with sophisticated team play behavior, bridging the shortcomings of heuristic methods. Furthermore, we created an open-source multi-agent soccer environment. Utilizing our MARL framework and a modified global entity encoder (GEE) as our core architecture, our approach achieves a 66.8% win rate against HELIOS agent, which employs a state-of-the-art heuristic strategy. In addition, we provided an in-depth analysis of the policy behavior and interpreted the agent's intention using the critic network.
<div id='section'>Paperid: <span id='pid'>680, <a href='https://arxiv.org/pdf/2409.16460.pdf' target='_blank'>https://arxiv.org/pdf/2409.16460.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hang Liu, Yi Cheng, Rankun Li, Xiaowen Hu, Linqi Ye, Houde Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.16460">MBC: Multi-Brain Collaborative Control for Quadruped Robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the field of locomotion task of quadruped robots, Blind Policy and Perceptive Policy each have their own advantages and limitations. The Blind Policy relies on preset sensor information and algorithms, suitable for known and structured environments, but it lacks adaptability in complex or unknown environments. The Perceptive Policy uses visual sensors to obtain detailed environmental information, allowing it to adapt to complex terrains, but its effectiveness is limited under occluded conditions, especially when perception fails. Unlike the Blind Policy, the Perceptive Policy is not as robust under these conditions. To address these challenges, we propose a MBC:Multi-Brain collaborative system that incorporates the concepts of Multi-Agent Reinforcement Learning and introduces collaboration between the Blind Policy and the Perceptive Policy. By applying this multi-policy collaborative model to a quadruped robot, the robot can maintain stable locomotion even when the perceptual system is impaired or observational data is incomplete. Our simulations and real-world experiments demonstrate that this system significantly improves the robot's passability and robustness against perception failures in complex environments, validating the effectiveness of multi-policy collaboration in enhancing robotic motion performance.
<div id='section'>Paperid: <span id='pid'>681, <a href='https://arxiv.org/pdf/2409.12882.pdf' target='_blank'>https://arxiv.org/pdf/2409.12882.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hairi, Minghong Fang, Zifan Zhang, Alvaro Velasquez, Jia Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.12882">On the Hardness of Decentralized Multi-Agent Policy Evaluation under Byzantine Attacks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we study a fully-decentralized multi-agent policy evaluation problem, which is an important sub-problem in cooperative multi-agent reinforcement learning, in the presence of up to $f$ faulty agents. In particular, we focus on the so-called Byzantine faulty model with model poisoning setting. In general, policy evaluation is to evaluate the value function of any given policy. In cooperative multi-agent system, the system-wide rewards are usually modeled as the uniform average of rewards from all agents. We investigate the multi-agent policy evaluation problem in the presence of Byzantine agents, particularly in the setting of heterogeneous local rewards. Ideally, the goal of the agents is to evaluate the accumulated system-wide rewards, which are uniform average of rewards of the normal agents for a given policy. It means that all agents agree upon common values (the consensus part) and furthermore, the consensus values are the value functions (the convergence part). However, we prove that this goal is not achievable. Instead, we consider a relaxed version of the problem, where the goal of the agents is to evaluate accumulated system-wide reward, which is an appropriately weighted average reward of the normal agents. We further prove that there is no correct algorithm that can guarantee that the total number of positive weights exceeds $|\mathcal{N}|-f $, where $|\mathcal{N}|$ is the number of normal agents. Towards the end, we propose a Byzantine-tolerant decentralized temporal difference algorithm that can guarantee asymptotic consensus under scalar function approximation. We then empirically test the effective of the proposed algorithm.
<div id='section'>Paperid: <span id='pid'>682, <a href='https://arxiv.org/pdf/2407.16312.pdf' target='_blank'>https://arxiv.org/pdf/2407.16312.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Florian Felten, Umut Ucak, Hicham Azmani, Gao Peng, Willem RÃ¶pke, Hendrik Baier, Patrick Mannion, Diederik M. Roijers, Jordan K. Terry, El-Ghazali Talbi, GrÃ©goire Danoy, Ann NowÃ©, Roxana RÄdulescu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.16312">MOMAland: A Set of Benchmarks for Multi-Objective Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Many challenging tasks such as managing traffic systems, electricity grids, or supply chains involve complex decision-making processes that must balance multiple conflicting objectives and coordinate the actions of various independent decision-makers (DMs). One perspective for formalising and addressing such tasks is multi-objective multi-agent reinforcement learning (MOMARL). MOMARL broadens reinforcement learning (RL) to problems with multiple agents each needing to consider multiple objectives in their learning process. In reinforcement learning research, benchmarks are crucial in facilitating progress, evaluation, and reproducibility. The significance of benchmarks is underscored by the existence of numerous benchmark frameworks developed for various RL paradigms, including single-agent RL (e.g., Gymnasium), multi-agent RL (e.g., PettingZoo), and single-agent multi-objective RL (e.g., MO-Gymnasium). To support the advancement of the MOMARL field, we introduce MOMAland, the first collection of standardised environments for multi-objective multi-agent reinforcement learning. MOMAland addresses the need for comprehensive benchmarking in this emerging field, offering over 10 diverse environments that vary in the number of agents, state representations, reward structures, and utility considerations. To provide strong baselines for future research, MOMAland also includes algorithms capable of learning policies in such settings.
<div id='section'>Paperid: <span id='pid'>683, <a href='https://arxiv.org/pdf/2407.03302.pdf' target='_blank'>https://arxiv.org/pdf/2407.03302.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Brendon Boldt, David Mortensen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.03302">A Review of the Applications of Deep Learning-Based Emergent Communication</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Emergent communication, or emergent language, is the field of research which studies how human language-like communication systems emerge de novo in deep multi-agent reinforcement learning environments. The possibilities of replicating the emergence of a complex behavior like language have strong intuitive appeal, yet it is necessary to complement this with clear notions of how such research can be applicable to other fields of science, technology, and engineering. This paper comprehensively reviews the applications of emergent communication research across machine learning, natural language processing, linguistics, and cognitive science. Each application is illustrated with a description of its scope, an explication of emergent communication's unique role in addressing it, a summary of the extant literature working towards the application, and brief recommendations for near-term research directions.
<div id='section'>Paperid: <span id='pid'>684, <a href='https://arxiv.org/pdf/2406.11265.pdf' target='_blank'>https://arxiv.org/pdf/2406.11265.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuanzhe Geng, Erwu Liu, Wei Ni, Rui Wang, Yan Liu, Hao Xu, Chen Cai, Abbas Jamalipour
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.11265">Balancing Performance and Cost for Two-Hop Cooperative Communications: Stackelberg Game and Distributed Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper aims to balance performance and cost in a two-hop wireless cooperative communication network where the source and relays have contradictory optimization goals and make decisions in a distributed manner. This differs from most existing works that have typically assumed that source and relay nodes follow a schedule created implicitly by a central controller. We propose that the relays form an alliance in an attempt to maximize the benefit of relaying while the source aims to increase the channel capacity cost-effectively. To this end, we establish the trade problem as a Stackelberg game, and prove the existence of its equilibrium. Another important aspect is that we use multi-agent reinforcement learning (MARL) to approach the equilibrium in a situation where the instantaneous channel state information (CSI) is unavailable, and the source and relays do not have knowledge of each other's goal. A multi-agent deep deterministic policy gradient-based framework is designed, where the relay alliance and the source act as agents. Experiments demonstrate that the proposed method can obtain an acceptable performance that is close to the game-theoretic equilibrium for all players under time-invariant environments, which considerably outperforms its potential alternatives and is only about 2.9% away from the optimal solution.
<div id='section'>Paperid: <span id='pid'>685, <a href='https://arxiv.org/pdf/2406.09835.pdf' target='_blank'>https://arxiv.org/pdf/2406.09835.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Malio Li, Elia Piccoli, Vincenzo Lomonaco, Davide Bacciu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.09835">I Know How: Combining Prior Policies to Solve New Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-Task Reinforcement Learning aims at developing agents that are able to continually evolve and adapt to new scenarios. However, this goal is challenging to achieve due to the phenomenon of catastrophic forgetting and the high demand of computational resources. Learning from scratch for each new task is not a viable or sustainable option, and thus agents should be able to collect and exploit prior knowledge while facing new problems. While several methodologies have attempted to address the problem from different perspectives, they lack a common structure. In this work, we propose a new framework, I Know How (IKH), which provides a common formalization. Our methodology focuses on modularity and compositionality of knowledge in order to achieve and enhance agent's ability to learn and adapt efficiently to dynamic environments. To support our framework definition, we present a simple application of it in a simulated driving environment and compare its performance with that of state-of-the-art approaches.
<div id='section'>Paperid: <span id='pid'>686, <a href='https://arxiv.org/pdf/2404.12824.pdf' target='_blank'>https://arxiv.org/pdf/2404.12824.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shaohao Zhu, Jiacheng Zhou, Anjun Chen, Mingming Bai, Jiming Chen, Jinming Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.12824">MAexp: A Generic Platform for RL-based Multi-Agent Exploration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The sim-to-real gap poses a significant challenge in RL-based multi-agent exploration due to scene quantization and action discretization. Existing platforms suffer from the inefficiency in sampling and the lack of diversity in Multi-Agent Reinforcement Learning (MARL) algorithms across different scenarios, restraining their widespread applications. To fill these gaps, we propose MAexp, a generic platform for multi-agent exploration that integrates a broad range of state-of-the-art MARL algorithms and representative scenarios. Moreover, we employ point clouds to represent our exploration scenarios, leading to high-fidelity environment mapping and a sampling speed approximately 40 times faster than existing platforms. Furthermore, equipped with an attention-based Multi-Agent Target Generator and a Single-Agent Motion Planner, MAexp can work with arbitrary numbers of agents and accommodate various types of robots. Extensive experiments are conducted to establish the first benchmark featuring several high-performance MARL algorithms across typical scenarios for robots with continuous actions, which highlights the distinct strengths of each algorithm in different scenarios.
<div id='section'>Paperid: <span id='pid'>687, <a href='https://arxiv.org/pdf/2404.05950.pdf' target='_blank'>https://arxiv.org/pdf/2404.05950.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinyuan Feng, Min Chen, Zhiqiang Pu, Tenghai Qiu, Jianqiang Yi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.05950">Efficient Multi-Task Reinforcement Learning via Task-Specific Action Correction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-task reinforcement learning (MTRL) demonstrate potential for enhancing the generalization of a robot, enabling it to perform multiple tasks concurrently. However, the performance of MTRL may still be susceptible to conflicts between tasks and negative interference. To facilitate efficient MTRL, we propose Task-Specific Action Correction (TSAC), a general and complementary approach designed for simultaneous learning of multiple tasks. TSAC decomposes policy learning into two separate policies: a shared policy (SP) and an action correction policy (ACP). To alleviate conflicts resulting from excessive focus on specific tasks' details in SP, ACP incorporates goal-oriented sparse rewards, enabling an agent to adopt a long-term perspective and achieve generalization across tasks. Additional rewards transform the original problem into a multi-objective MTRL problem. Furthermore, to convert the multi-objective MTRL into a single-objective formulation, TSAC assigns a virtual expected budget to the sparse rewards and employs Lagrangian method to transform a constrained single-objective optimization into an unconstrained one. Experimental evaluations conducted on Meta-World's MT10 and MT50 benchmarks demonstrate that TSAC outperforms existing state-of-the-art methods, achieving significant improvements in both sample efficiency and effective action execution.
<div id='section'>Paperid: <span id='pid'>688, <a href='https://arxiv.org/pdf/2403.18056.pdf' target='_blank'>https://arxiv.org/pdf/2403.18056.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qingxu Fu, Tenghai Qiu, Jianqiang Yi, Zhiqiang Pu, Xiaolin Ai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.18056">Self-Clustering Hierarchical Multi-Agent Reinforcement Learning with Extensible Cooperation Graph</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-Agent Reinforcement Learning (MARL) has been successful in solving many cooperative challenges. However, classic non-hierarchical MARL algorithms still cannot address various complex multi-agent problems that require hierarchical cooperative behaviors. The cooperative knowledge and policies learned in non-hierarchical algorithms are implicit and not interpretable, thereby restricting the integration of existing knowledge. This paper proposes a novel hierarchical MARL model called Hierarchical Cooperation Graph Learning (HCGL) for solving general multi-agent problems. HCGL has three components: a dynamic Extensible Cooperation Graph (ECG) for achieving self-clustering cooperation; a group of graph operators for adjusting the topology of ECG; and an MARL optimizer for training these graph operators. HCGL's key distinction from other MARL models is that the behaviors of agents are guided by the topology of ECG instead of policy neural networks. ECG is a three-layer graph consisting of an agent node layer, a cluster node layer, and a target node layer. To manipulate the ECG topology in response to changing environmental conditions, four graph operators are trained to adjust the edge connections of ECG dynamically. The hierarchical feature of ECG provides a unique approach to merge primitive actions (actions executed by the agents) and cooperative actions (actions executed by the clusters) into a unified action space, allowing us to integrate fundamental cooperative knowledge into an extensible interface. In our experiments, the HCGL model has shown outstanding performance in multi-agent benchmarks with sparse rewards. We also verify that HCGL can easily be transferred to large-scale scenarios with high zero-shot transfer success rates.
<div id='section'>Paperid: <span id='pid'>689, <a href='https://arxiv.org/pdf/2401.11257.pdf' target='_blank'>https://arxiv.org/pdf/2401.11257.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianyi Hu, Zhiqiang Pu, Xiaolin Ai, Tenghai Qiu, Jianqiang Yi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.11257">Measuring Policy Distance for Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Diversity plays a crucial role in improving the performance of multi-agent reinforcement learning (MARL). Currently, many diversity-based methods have been developed to overcome the drawbacks of excessive parameter sharing in traditional MARL. However, there remains a lack of a general metric to quantify policy differences among agents. Such a metric would not only facilitate the evaluation of the diversity evolution in multi-agent systems, but also provide guidance for the design of diversity-based MARL algorithms. In this paper, we propose the multi-agent policy distance (MAPD), a general tool for measuring policy differences in MARL. By learning the conditional representations of agents' decisions, MAPD can computes the policy distance between any pair of agents. Furthermore, we extend MAPD to a customizable version, which can quantify differences among agent policies on specified aspects. Based on the online deployment of MAPD, we design a multi-agent dynamic parameter sharing (MADPS) algorithm as an example of the MAPD's applications. Extensive experiments demonstrate that our method is effective in measuring differences in agent policies and specific behavioral tendencies. Moreover, in comparison to other methods of parameter sharing, MADPS exhibits superior performance.
<div id='section'>Paperid: <span id='pid'>690, <a href='https://arxiv.org/pdf/2401.06470.pdf' target='_blank'>https://arxiv.org/pdf/2401.06470.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gengrui Zhang, Yao Wang, Xiaoshuang Chen, Hongyi Qian, Kaiqiao Zhan, Ben Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.06470">UNEX-RL: Reinforcing Long-Term Rewards in Multi-Stage Recommender Systems with UNidirectional EXecution</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, there has been a growing interest in utilizing reinforcement learning (RL) to optimize long-term rewards in recommender systems. Since industrial recommender systems are typically designed as multi-stage systems, RL methods with a single agent face challenges when optimizing multiple stages simultaneously. The reason is that different stages have different observation spaces, and thus cannot be modeled by a single agent. To address this issue, we propose a novel UNidirectional-EXecution-based multi-agent Reinforcement Learning (UNEX-RL) framework to reinforce the long-term rewards in multi-stage recommender systems. We show that the unidirectional execution is a key feature of multi-stage recommender systems, bringing new challenges to the applications of multi-agent reinforcement learning (MARL), namely the observation dependency and the cascading effect. To tackle these challenges, we provide a cascading information chain (CIC) method to separate the independent observations from action-dependent observations and use CIC to train UNEX-RL effectively. We also discuss practical variance reduction techniques for UNEX-RL. Finally, we show the effectiveness of UNEX-RL on both public datasets and an online recommender system with over 100 million users. Specifically, UNEX-RL reveals a 0.558% increase in users' usage time compared with single-agent RL algorithms in online A/B experiments, highlighting the effectiveness of UNEX-RL in industrial recommender systems.
<div id='section'>Paperid: <span id='pid'>691, <a href='https://arxiv.org/pdf/2311.11204.pdf' target='_blank'>https://arxiv.org/pdf/2311.11204.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zheng Wang, Cheng Long, Gao Cong, Christian S. Jensen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.11204">Collectively Simplifying Trajectories in a Database: A Query Accuracy Driven Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Increasing and massive volumes of trajectory data are being accumulated that may serve a variety of applications, such as mining popular routes or identifying ridesharing candidates. As storing and querying massive trajectory data is costly, trajectory simplification techniques have been introduced that intuitively aim to reduce the sizes of trajectories, thus reducing storage and speeding up querying, while preserving as much information as possible. Existing techniques rely mainly on hand-crafted error measures when deciding which point to drop when simplifying a trajectory. While the hope may be that such simplification affects the subsequent usability of the data only minimally, the usability of the simplified data remains largely unexplored. Instead of using error measures that indirectly may to some extent yield simplified trajectories with high usability, we adopt a direct approach to simplification and present the first study of query accuracy driven trajectory simplification, where the direct objective is to achieve a simplified trajectory database that preserves the query accuracy of the original database as much as possible. Specifically, we propose a multi-agent reinforcement learning based solution with two agents working cooperatively to collectively simplify trajectories in a database while optimizing query usability. Extensive experiments on four real-world trajectory datasets show that the solution is capable of consistently outperforming baseline solutions over various query types and dynamics.
<div id='section'>Paperid: <span id='pid'>692, <a href='https://arxiv.org/pdf/2310.12909.pdf' target='_blank'>https://arxiv.org/pdf/2310.12909.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yasin Findik, Paul Robinette, Kshitij Jerath, S. Reza Ahmadzadeh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.12909">Collaborative Adaptation: Learning to Recover from Unforeseen Malfunctions in Multi-Robot Teams</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cooperative multi-agent reinforcement learning (MARL) approaches tackle the challenge of finding effective multi-agent cooperation strategies for accomplishing individual or shared objectives in multi-agent teams. In real-world scenarios, however, agents may encounter unforeseen failures due to constraints like battery depletion or mechanical issues. Existing state-of-the-art methods in MARL often recover slowly -- if at all -- from such malfunctions once agents have already converged on a cooperation strategy. To address this gap, we present the Collaborative Adaptation (CA) framework. CA introduces a mechanism that guides collaboration and accelerates adaptation from unforeseen failures by leveraging inter-agent relationships. Our findings demonstrate that CA enables agents to act on the knowledge of inter-agent relations, recovering from unforeseen agent failures and selecting appropriate cooperative strategies.
<div id='section'>Paperid: <span id='pid'>693, <a href='https://arxiv.org/pdf/2310.10549.pdf' target='_blank'>https://arxiv.org/pdf/2310.10549.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mai Le, Thien Huynh-The, Tan Do-Duy, Thai-Hoc Vu, Won-Joo Hwang, Quoc-Viet Pham
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.10549">Applications of Distributed Machine Learning for the Internet-of-Things: A Comprehensive Survey</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The emergence of new services and applications in emerging wireless networks (e.g., beyond 5G and 6G) has shown a growing demand for the usage of artificial intelligence (AI) in the Internet of Things (IoT). However, the proliferation of massive IoT connections and the availability of computing resources distributed across future IoT systems have strongly demanded the development of distributed AI for better IoT services and applications. Therefore, existing AI-enabled IoT systems can be enhanced by implementing distributed machine learning (aka distributed learning) approaches. This work aims to provide a comprehensive survey on distributed learning for IoT services and applications in emerging networks. In particular, we first provide a background of machine learning and present a preliminary to typical distributed learning approaches, such as federated learning, multi-agent reinforcement learning, and distributed inference. Then, we provide an extensive review of distributed learning for critical IoT services (e.g., data sharing and computation offloading, localization, mobile crowdsensing, and security and privacy) and IoT applications (e.g., smart healthcare, smart grid, autonomous vehicle, aerial IoT networks, and smart industry). From the reviewed literature, we also present critical challenges of distributed learning for IoT and propose several promising solutions and research directions in this emerging area.
<div id='section'>Paperid: <span id='pid'>694, <a href='https://arxiv.org/pdf/2310.09833.pdf' target='_blank'>https://arxiv.org/pdf/2310.09833.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Simin Li, Ruixiao Xu, Jingqiao Xiu, Yuwei Zheng, Pu Feng, Yaodong Yang, Xianglong Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.09833">Robust Multi-Agent Reinforcement Learning by Mutual Information Regularization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In multi-agent reinforcement learning (MARL), ensuring robustness against unpredictable or worst-case actions by allies is crucial for real-world deployment. Existing robust MARL methods either approximate or enumerate all possible threat scenarios against worst-case adversaries, leading to computational intensity and reduced robustness. In contrast, human learning efficiently acquires robust behaviors in daily life without preparing for every possible threat. Inspired by this, we frame robust MARL as an inference problem, with worst-case robustness implicitly optimized under all threat scenarios via off-policy evaluation. Within this framework, we demonstrate that Mutual Information Regularization as Robust Regularization (MIR3) during routine training is guaranteed to maximize a lower bound on robustness, without the need for adversaries. Further insights show that MIR3 acts as an information bottleneck, preventing agents from over-reacting to others and aligning policies with robust action priors. In the presence of worst-case adversaries, our MIR3 significantly surpasses baseline methods in robustness and training efficiency while maintaining cooperative performance in StarCraft II and robot swarm control. When deploying the robot swarm control algorithm in the real world, our method also outperforms the best baseline by 14.29%.
<div id='section'>Paperid: <span id='pid'>695, <a href='https://arxiv.org/pdf/2307.16228.pdf' target='_blank'>https://arxiv.org/pdf/2307.16228.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sihong He, Shuo Han, Fei Miao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.16228">Robust Electric Vehicle Balancing of Autonomous Mobility-On-Demand System: A Multi-Agent Reinforcement Learning Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Electric autonomous vehicles (EAVs) are getting attention in future autonomous mobility-on-demand (AMoD) systems due to their economic and societal benefits. However, EAVs' unique charging patterns (long charging time, high charging frequency, unpredictable charging behaviors, etc.) make it challenging to accurately predict the EAVs supply in E-AMoD systems. Furthermore, the mobility demand's prediction uncertainty makes it an urgent and challenging task to design an integrated vehicle balancing solution under supply and demand uncertainties. Despite the success of reinforcement learning-based E-AMoD balancing algorithms, state uncertainties under the EV supply or mobility demand remain unexplored. In this work, we design a multi-agent reinforcement learning (MARL)-based framework for EAVs balancing in E-AMoD systems, with adversarial agents to model both the EAVs supply and mobility demand uncertainties that may undermine the vehicle balancing solutions. We then propose a robust E-AMoD Balancing MARL (REBAMA) algorithm to train a robust EAVs balancing policy to balance both the supply-demand ratio and charging utilization rate across the whole city. Experiments show that our proposed robust method performs better compared with a non-robust MARL method that does not consider state uncertainties; it improves the reward, charging utilization fairness, and supply-demand fairness by 19.28%, 28.18%, and 3.97%, respectively. Compared with a robust optimization-based method, the proposed MARL algorithm can improve the reward, charging utilization fairness, and supply-demand fairness by 8.21%, 8.29%, and 9.42%, respectively.
<div id='section'>Paperid: <span id='pid'>696, <a href='https://arxiv.org/pdf/2306.16316.pdf' target='_blank'>https://arxiv.org/pdf/2306.16316.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shengchao Yan, Baohe Zhang, Yuan Zhang, Joschka Boedecker, Wolfram Burgard
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.16316">Learning Continuous Control with Geometric Regularity from Robot Intrinsic Symmetry</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Geometric regularity, which leverages data symmetry, has been successfully incorporated into deep learning architectures such as CNNs, RNNs, GNNs, and Transformers. While this concept has been widely applied in robotics to address the curse of dimensionality when learning from high-dimensional data, the inherent reflectional and rotational symmetry of robot structures has not been adequately explored. Drawing inspiration from cooperative multi-agent reinforcement learning, we introduce novel network structures for single-agent control learning that explicitly capture these symmetries. Moreover, we investigate the relationship between the geometric prior and the concept of Parameter Sharing in multi-agent reinforcement learning. Last but not the least, we implement the proposed framework in online and offline learning methods to demonstrate its ease of use. Through experiments conducted on various challenging continuous control tasks on simulators and real robots, we highlight the significant potential of the proposed geometric regularity in enhancing robot learning capabilities.
<div id='section'>Paperid: <span id='pid'>697, <a href='https://arxiv.org/pdf/2306.08041.pdf' target='_blank'>https://arxiv.org/pdf/2306.08041.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Young Wu, Jeremy McMahan, Xiaojin Zhu, Qiaomin Xie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.08041">Data Poisoning to Fake a Nash Equilibrium in Markov Games</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We characterize offline data poisoning attacks on Multi-Agent Reinforcement Learning (MARL), where an attacker may change a data set in an attempt to install a (potentially fictitious) unique Markov-perfect Nash equilibrium for a two-player zero-sum Markov game. We propose the unique Nash set, namely the set of games, specified by their Q functions, with a specific joint policy being the unique Nash equilibrium. The unique Nash set is central to poisoning attacks because the attack is successful if and only if data poisoning pushes all plausible games inside the set. The unique Nash set generalizes the reward polytope commonly used in inverse reinforcement learning to MARL. For zero-sum Markov games, both the inverse Nash set and the set of plausible games induced by data are polytopes in the Q function space. We exhibit a linear program to efficiently compute the optimal poisoning attack. Our work sheds light on the structure of data poisoning attacks on offline MARL, a necessary step before one can design more robust MARL algorithms.
<div id='section'>Paperid: <span id='pid'>698, <a href='https://arxiv.org/pdf/2306.06382.pdf' target='_blank'>https://arxiv.org/pdf/2306.06382.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jian Tao, Yang Zhang, Yangkun Chen, Xiu Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.06382">Multi-agent Exploration with Sub-state Entropy Estimation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Researchers have integrated exploration techniques into multi-agent reinforcement learning (MARL) algorithms, drawing on their remarkable success in deep reinforcement learning. Nonetheless, exploration in MARL presents a more substantial challenge, as agents need to coordinate their efforts in order to achieve comprehensive state coverage. Reaching a unanimous agreement on which kinds of states warrant exploring can be a struggle for agents in this context. We introduce \textbf{M}ulti-agent \textbf{E}xploration based on \textbf{S}ub-state \textbf{E}ntropy (MESE) to address this limitation. This novel approach incentivizes agents to explore states cooperatively by directing them to achieve consensus via an extra team reward. Calculating the additional reward is based on the novelty of the current sub-state that merits cooperative exploration. MESE employs a conditioned entropy approach to select the sub-state, using particle-based entropy estimation to calculate the entropy. MESE is a plug-and-play module that can be seamlessly integrated into most existing MARL algorithms, which makes it a highly effective tool for reinforcement learning. Our experiments demonstrate that MESE can substantially improve the MAPPO's performance on various tasks in the StarCraft multi-agent challenge (SMAC).
<div id='section'>Paperid: <span id='pid'>699, <a href='https://arxiv.org/pdf/2305.17568.pdf' target='_blank'>https://arxiv.org/pdf/2305.17568.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Donghao Ying, Yunkai Zhang, Yuhao Ding, Alec Koppel, Javad Lavaei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.17568">Scalable Primal-Dual Actor-Critic Method for Safe Multi-Agent RL with General Utilities</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We investigate safe multi-agent reinforcement learning, where agents seek to collectively maximize an aggregate sum of local objectives while satisfying their own safety constraints. The objective and constraints are described by {\it general utilities}, i.e., nonlinear functions of the long-term state-action occupancy measure, which encompass broader decision-making goals such as risk, exploration, or imitations. The exponential growth of the state-action space size with the number of agents presents challenges for global observability, further exacerbated by the global coupling arising from agents' safety constraints. To tackle this issue, we propose a primal-dual method utilizing shadow reward and $Îº$-hop neighbor truncation under a form of correlation decay property, where $Îº$ is the communication radius. In the exact setting, our algorithm converges to a first-order stationary point (FOSP) at the rate of $\mathcal{O}\left(T^{-2/3}\right)$. In the sample-based setting, we demonstrate that, with high probability, our algorithm requires $\widetilde{\mathcal{O}}\left(Îµ^{-3.5}\right)$ samples to achieve an $Îµ$-FOSP with an approximation error of $\mathcal{O}(Ï_0^{2Îº})$, where $Ï_0\in (0,1)$. Finally, we demonstrate the effectiveness of our model through extensive numerical experiments.
<div id='section'>Paperid: <span id='pid'>700, <a href='https://arxiv.org/pdf/2305.16145.pdf' target='_blank'>https://arxiv.org/pdf/2305.16145.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Harsh Goel, Yifeng Zhang, Mehul Damani, Guillaume Sartoretti
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.16145">SocialLight: Distributed Cooperation Learning towards Network-Wide Traffic Signal Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Many recent works have turned to multi-agent reinforcement learning (MARL) for adaptive traffic signal control to optimize the travel time of vehicles over large urban networks. However, achieving effective and scalable cooperation among junctions (agents) remains an open challenge, as existing methods often rely on extensive, non-generalizable reward shaping or on non-scalable centralized learning. To address these problems, we propose a new MARL method for traffic signal control, SocialLight, which learns cooperative traffic control policies by distributedly estimating the individual marginal contribution of agents on their local neighborhood. SocialLight relies on the Asynchronous Actor Critic (A3C) framework, and makes learning scalable by learning a locally-centralized critic conditioned over the states and actions of neighboring agents, used by agents to estimate individual contributions by counterfactual reasoning. We further introduce important modifications to the advantage calculation that help stabilize policy updates. These modifications decouple the impact of the neighbors' actions on the computed advantages, thereby reducing the variance in the gradient updates. We benchmark our trained network against state-of-the-art traffic signal control methods on standard benchmarks in two traffic simulators, SUMO and CityFlow. Our results show that SocialLight exhibits improved scalability to larger road networks and better performance across usual traffic metrics.
<div id='section'>Paperid: <span id='pid'>701, <a href='https://arxiv.org/pdf/2303.10733.pdf' target='_blank'>https://arxiv.org/pdf/2303.10733.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yat Long Lo, Christian Schroeder de Witt, Samuel Sokota, Jakob Nicolaus Foerster, Shimon Whiteson
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.10733">Cheap Talk Discovery and Utilization in Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>By enabling agents to communicate, recent cooperative multi-agent reinforcement learning (MARL) methods have demonstrated better task performance and more coordinated behavior. Most existing approaches facilitate inter-agent communication by allowing agents to send messages to each other through free communication channels, i.e., cheap talk channels. Current methods require these channels to be constantly accessible and known to the agents a priori. In this work, we lift these requirements such that the agents must discover the cheap talk channels and learn how to use them. Hence, the problem has two main parts: cheap talk discovery (CTD) and cheap talk utilization (CTU). We introduce a novel conceptual framework for both parts and develop a new algorithm based on mutual information maximization that outperforms existing algorithms in CTD/CTU settings. We also release a novel benchmark suite to stimulate future research in CTD/CTU.
<div id='section'>Paperid: <span id='pid'>702, <a href='https://arxiv.org/pdf/2302.07938.pdf' target='_blank'>https://arxiv.org/pdf/2302.07938.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Donghao Ying, Yuhao Ding, Alec Koppel, Javad Lavaei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.07938">Scalable Multi-Agent Reinforcement Learning with General Utilities</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We study the scalable multi-agent reinforcement learning (MARL) with general utilities, defined as nonlinear functions of the team's long-term state-action occupancy measure. The objective is to find a localized policy that maximizes the average of the team's local utility functions without the full observability of each agent in the team. By exploiting the spatial correlation decay property of the network structure, we propose a scalable distributed policy gradient algorithm with shadow reward and localized policy that consists of three steps: (1) shadow reward estimation, (2) truncated shadow Q-function estimation, and (3) truncated policy gradient estimation and policy update. Our algorithm converges, with high probability, to $Îµ$-stationarity with $\widetilde{\mathcal{O}}(Îµ^{-2})$ samples up to some approximation error that decreases exponentially in the communication radius. This is the first result in the literature on multi-agent RL with general utilities that does not require the full observability.
<div id='section'>Paperid: <span id='pid'>703, <a href='https://arxiv.org/pdf/2302.07493.pdf' target='_blank'>https://arxiv.org/pdf/2302.07493.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shijing Yuan, Hongze Liu, Hongtao Lv, Zhanbo Feng, Jie Li, Hongyang Chen, Chentao Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.07493">Adaptive incentive for cross-silo federated learning: A multi-agent reinforcement learning approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cross-silo federated learning (FL) is a typical FL that enables organizations(e.g., financial or medical entities) to train global models on isolated data. Reasonable incentive is key to encouraging organizations to contribute data. However, existing works on incentivizing cross-silo FL lack consideration of the environmental dynamics (e.g., precision of the trained global model and data owned by uncertain clients during the training processes). Moreover, most of them assume that organizations share private information, which is unrealistic. To overcome these limitations, we propose a novel adaptive mechanism for cross-silo FL, towards incentivizing organizations to contribute data to maximize their long-term payoffs in a real dynamic training environment. The mechanism is based on multi-agent reinforcement learning, which learns near-optimal data contribution strategy from the history of potential games without organizations' private information. Experiments demonstrate that our mechanism achieves adaptive incentive and effectively improves the long-term payoffs for organizations.
<div id='section'>Paperid: <span id='pid'>704, <a href='https://arxiv.org/pdf/2302.04321.pdf' target='_blank'>https://arxiv.org/pdf/2302.04321.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Songyang Han, Shanglin Zhou, Lynn Pepin, Jiangwei Wang, Caiwen Ding, Fei Miao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.04321">Shared Information-Based Safe And Efficient Behavior Planning For Connected Autonomous Vehicles</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The recent advancements in wireless technology enable connected autonomous vehicles (CAVs) to gather data via vehicle-to-vehicle (V2V) communication, such as processed LIDAR and camera data from other vehicles. In this work, we design an integrated information sharing and safe multi-agent reinforcement learning (MARL) framework for CAVs, to take advantage of the extra information when making decisions to improve traffic efficiency and safety. We first use weight pruned convolutional neural networks (CNN) to process the raw image and point cloud LIDAR data locally at each autonomous vehicle, and share CNN-output data with neighboring CAVs. We then design a safe actor-critic algorithm that utilizes both a vehicle's local observation and the information received via V2V communication to explore an efficient behavior planning policy with safety guarantees. Using the CARLA simulator for experiments, we show that our approach improves the CAV system's efficiency in terms of average velocity and comfort under different CAV ratios and different traffic densities. We also show that our approach avoids the execution of unsafe actions and always maintains a safe distance from other vehicles. We construct an obstacle-at-corner scenario to show that the shared vision can help CAVs to observe obstacles earlier and take action to avoid traffic jams.
<div id='section'>Paperid: <span id='pid'>705, <a href='https://arxiv.org/pdf/2301.11014.pdf' target='_blank'>https://arxiv.org/pdf/2301.11014.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yan Lin, Jinming Bao, Yijin Zhang, Jun Li, Feng Shu, Lajos Hanzo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.11014">Privacy-Preserving Joint Edge Association and Power Optimization for the Internet of Vehicles via Federated Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Proactive edge association is capable of improving wireless connectivity at the cost of increased handover (HO) frequency and energy consumption, while relying on a large amount of private information sharing required for decision making. In order to improve the connectivity-cost trade-off without privacy leakage, we investigate the privacy-preserving joint edge association and power allocation (JEAPA) problem in the face of the environmental uncertainty and the infeasibility of individual learning. Upon modelling the problem by a decentralized partially observable Markov Decision Process (Dec-POMDP), it is solved by federated multi-agent reinforcement learning (FMARL) through only sharing encrypted training data for federatively learning the policy sought. Our simulation results show that the proposed solution strikes a compelling trade-off, while preserving a higher privacy level than the state-of-the-art solutions.
<div id='section'>Paperid: <span id='pid'>706, <a href='https://arxiv.org/pdf/2210.02300.pdf' target='_blank'>https://arxiv.org/pdf/2210.02300.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhili Zhang, Songyang Han, Jiangwei Wang, Fei Miao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2210.02300">Spatial-Temporal-Aware Safe Multi-Agent Reinforcement Learning of Connected Autonomous Vehicles in Challenging Scenarios</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Communication technologies enable coordination among connected and autonomous vehicles (CAVs). However, it remains unclear how to utilize shared information to improve the safety and efficiency of the CAV system in dynamic and complicated driving scenarios. In this work, we propose a framework of constrained multi-agent reinforcement learning (MARL) with a parallel Safety Shield for CAVs in challenging driving scenarios that includes unconnected hazard vehicles. The coordination mechanisms of the proposed MARL include information sharing and cooperative policy learning, with Graph Convolutional Network (GCN)-Transformer as a spatial-temporal encoder that enhances the agent's environment awareness. The Safety Shield module with Control Barrier Functions (CBF)-based safety checking protects the agents from taking unsafe actions. We design a constrained multi-agent advantage actor-critic (CMAA2C) algorithm to train safe and cooperative policies for CAVs. With the experiment deployed in the CARLA simulator, we verify the performance of the safety checking, spatial-temporal encoder, and coordination mechanisms designed in our method by comparative experiments in several challenging scenarios with unconnected hazard vehicles. Results show that our proposed methodology significantly increases system safety and efficiency in challenging scenarios.
<div id='section'>Paperid: <span id='pid'>707, <a href='https://arxiv.org/pdf/2209.08244.pdf' target='_blank'>https://arxiv.org/pdf/2209.08244.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kefan Su, Siyuan Zhou, Jiechuan Jiang, Chuang Gan, Xiangjun Wang, Zongqing Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2209.08244">MA2QL: A Minimalist Approach to Fully Decentralized Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Decentralized learning has shown great promise for cooperative multi-agent reinforcement learning (MARL). However, non-stationarity remains a significant challenge in fully decentralized learning. In the paper, we tackle the non-stationarity problem in the simplest and fundamental way and propose multi-agent alternate Q-learning (MA2QL), where agents take turns updating their Q-functions by Q-learning. MA2QL is a minimalist approach to fully decentralized cooperative MARL but is theoretically grounded. We prove that when each agent guarantees $\varepsilon$-convergence at each turn, their joint policy converges to a Nash equilibrium. In practice, MA2QL only requires minimal changes to independent Q-learning (IQL). We empirically evaluate MA2QL on a variety of cooperative multi-agent tasks. Results show MA2QL consistently outperforms IQL, which verifies the effectiveness of MA2QL, despite such minimal changes.
<div id='section'>Paperid: <span id='pid'>708, <a href='https://arxiv.org/pdf/2209.08230.pdf' target='_blank'>https://arxiv.org/pdf/2209.08230.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sihong He, Yue Wang, Shuo Han, Shaofeng Zou, Fei Miao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2209.08230">A Robust and Constrained Multi-Agent Reinforcement Learning Electric Vehicle Rebalancing Method in AMoD Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Electric vehicles (EVs) play critical roles in autonomous mobility-on-demand (AMoD) systems, but their unique charging patterns increase the model uncertainties in AMoD systems (e.g. state transition probability). Since there usually exists a mismatch between the training and test/true environments, incorporating model uncertainty into system design is of critical importance in real-world applications. However, model uncertainties have not been considered explicitly in EV AMoD system rebalancing by existing literature yet, and the coexistence of model uncertainties and constraints that the decision should satisfy makes the problem even more challenging. In this work, we design a robust and constrained multi-agent reinforcement learning (MARL) framework with state transition kernel uncertainty for EV AMoD systems. We then propose a robust and constrained MARL algorithm (ROCOMA) with robust natural policy gradients (RNPG) that trains a robust EV rebalancing policy to balance the supply-demand ratio and the charging utilization rate across the city under model uncertainty. Experiments show that the ROCOMA can learn an effective and robust rebalancing policy. It outperforms non-robust MARL methods in the presence of model uncertainties. It increases the system fairness by 19.6% and decreases the rebalancing costs by 75.8%.
<div id='section'>Paperid: <span id='pid'>709, <a href='https://arxiv.org/pdf/2209.06904.pdf' target='_blank'>https://arxiv.org/pdf/2209.06904.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Beomseok Kang, Saibal Mukhopadhyay
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2209.06904">Forecasting Evolution of Clusters in Game Agents with Hebbian Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large multi-agent systems such as real-time strategy games are often driven by collective behavior of agents. For example, in StarCraft II, human players group spatially near agents into a team and control the team to defeat opponents. In this light, clustering the agents in the game has been used for various purposes such as the efficient control of the agents in multi-agent reinforcement learning and game analytic tools for the game users. However, despite the useful information provided by clustering, learning the dynamics of multi-agent systems at a cluster level has been rarely studied yet. In this paper, we present a hybrid AI model that couples unsupervised and self-supervised learning to forecast evolution of the clusters in StarCraft II. We develop an unsupervised Hebbian learning method in a set-to-cluster module to efficiently create a variable number of the clusters with lower inference time complexity than K-means clustering. Also, a long short-term memory based prediction module is designed to recursively forecast state vectors generated by the set-to-cluster module to define cluster configuration. We experimentally demonstrate the proposed model successfully predicts complex movement of the clusters in the game.
<div id='section'>Paperid: <span id='pid'>710, <a href='https://arxiv.org/pdf/2206.01888.pdf' target='_blank'>https://arxiv.org/pdf/2206.01888.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Young Wu, Jeremy McMahan, Xiaojin Zhu, Qiaomin Xie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2206.01888">Reward Poisoning Attacks on Offline Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In offline multi-agent reinforcement learning (MARL), agents estimate policies from a given dataset. We study reward-poisoning attacks in this setting where an exogenous attacker modifies the rewards in the dataset before the agents see the dataset. The attacker wants to guide each agent into a nefarious target policy while minimizing the $L^p$ norm of the reward modification. Unlike attacks on single-agent RL, we show that the attacker can install the target policy as a Markov Perfect Dominant Strategy Equilibrium (MPDSE), which rational agents are guaranteed to follow. This attack can be significantly cheaper than separate single-agent attacks. We show that the attack works on various MARL agents including uncertainty-aware learners, and we exhibit linear programs to efficiently solve the attack problem. We also study the relationship between the structure of the datasets and the minimal attack cost. Our work paves the way for studying defense in offline MARL.
<div id='section'>Paperid: <span id='pid'>711, <a href='https://arxiv.org/pdf/2203.10603.pdf' target='_blank'>https://arxiv.org/pdf/2203.10603.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xihuai Wang, Zhicheng Zhang, Weinan Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2203.10603">Model-based Multi-agent Reinforcement Learning: Recent Progress and Prospects</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Significant advances have recently been achieved in Multi-Agent Reinforcement Learning (MARL) which tackles sequential decision-making problems involving multiple participants. However, MARL requires a tremendous number of samples for effective training. On the other hand, model-based methods have been shown to achieve provable advantages of sample efficiency. However, the attempts of model-based methods to MARL have just started very recently. This paper presents a review of the existing research on model-based MARL, including theoretical analyses, algorithms, and applications, and analyzes the advantages and potential of model-based MARL. Specifically, we provide a detailed taxonomy of the algorithms and point out the pros and cons for each algorithm according to the challenges inherent to multi-agent scenarios. We also outline promising directions for future development of this field.
<div id='section'>Paperid: <span id='pid'>712, <a href='https://arxiv.org/pdf/2202.03558.pdf' target='_blank'>https://arxiv.org/pdf/2202.03558.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nhan H. Pham, Lam M. Nguyen, Jie Chen, Hoang Thanh Lam, Subhro Das, Tsui-Wei Weng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2202.03558">Attacking c-MARL More Effectively: A Data Driven Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, a proliferation of methods were developed for cooperative multi-agent reinforcement learning (c-MARL). However, the robustness of c-MARL agents against adversarial attacks has been rarely explored. In this paper, we propose to evaluate the robustness of c-MARL agents via a model-based approach, named c-MBA. Our proposed formulation can craft much stronger adversarial state perturbations of c-MARL agents to lower total team rewards than existing model-free approaches. In addition, we propose the first victim-agent selection strategy and the first data-driven approach to define targeted failure states where each of them allows us to develop even stronger adversarial attack without the expert knowledge to the underlying environment. Our numerical experiments on two representative MARL benchmarks illustrate the advantage of our approach over other baselines: our model-based attack consistently outperforms other baselines in all tested environments.
<div id='section'>Paperid: <span id='pid'>713, <a href='https://arxiv.org/pdf/2112.12458.pdf' target='_blank'>https://arxiv.org/pdf/2112.12458.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>RaphaÃ«l Avalos, Mathieu Reymond, Ann NowÃ©, Diederik M. Roijers
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2112.12458">Local Advantage Networks for Cooperative Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Many recent successful off-policy multi-agent reinforcement learning (MARL) algorithms for cooperative partially observable environments focus on finding factorized value functions, leading to convoluted network structures. Building on the structure of independent Q-learners, our LAN algorithm takes a radically different approach, leveraging a dueling architecture to learn for each agent a decentralized best-response policies via individual advantage functions. The learning is stabilized by a centralized critic whose primary objective is to reduce the moving target problem of the individual advantages. The critic, whose network's size is independent of the number of agents, is cast aside after learning. Evaluation on the StarCraft II multi-agent challenge benchmark shows that LAN reaches state-of-the-art performance and is highly scalable with respect to the number of agents, opening up a promising alternative direction for MARL research.
<div id='section'>Paperid: <span id='pid'>714, <a href='https://arxiv.org/pdf/2509.21828.pdf' target='_blank'>https://arxiv.org/pdf/2509.21828.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>The Viet Bui, Tien Mai, Hong Thanh Nguyen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.21828">Preference-Guided Learning for Sparse-Reward Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We study the problem of online multi-agent reinforcement learning (MARL) in environments with sparse rewards, where reward feedback is not provided at each interaction but only revealed at the end of a trajectory. This setting, though realistic, presents a fundamental challenge: the lack of intermediate rewards hinders standard MARL algorithms from effectively guiding policy learning. To address this issue, we propose a novel framework that integrates online inverse preference learning with multi-agent on-policy optimization into a unified architecture. At its core, our approach introduces an implicit multi-agent reward learning model, built upon a preference-based value-decomposition network, which produces both global and local reward signals. These signals are further used to construct dual advantage streams, enabling differentiated learning targets for the centralized critic and decentralized actors. In addition, we demonstrate how large language models (LLMs) can be leveraged to provide preference labels that enhance the quality of the learned reward model. Empirical evaluations on state-of-the-art benchmarks, including MAMuJoCo and SMACv2, show that our method achieves superior performance compared to existing baselines, highlighting its effectiveness in addressing sparse-reward challenges in online MARL.
<div id='section'>Paperid: <span id='pid'>715, <a href='https://arxiv.org/pdf/2509.21828.pdf' target='_blank'>https://arxiv.org/pdf/2509.21828.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>The Viet Bui, Tien Mai, Hong Thanh Nguyen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.21828">Preference-Guided Learning for Sparse-Reward Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We study the problem of online multi-agent reinforcement learning (MARL) in environments with sparse rewards, where reward feedback is not provided at each interaction but only revealed at the end of a trajectory. This setting, though realistic, presents a fundamental challenge: the lack of intermediate rewards hinders standard MARL algorithms from effectively guiding policy learning. To address this issue, we propose a novel framework that integrates online inverse preference learning with multi-agent on-policy optimization into a unified architecture. At its core, our approach introduces an implicit multi-agent reward learning model, built upon a preference-based value-decomposition network, which produces both global and local reward signals. These signals are further used to construct dual advantage streams, enabling differentiated learning targets for the centralized critic and decentralized actors. In addition, we demonstrate how large language models (LLMs) can be leveraged to provide preference labels that enhance the quality of the learned reward model. Empirical evaluations on state-of-the-art benchmarks, including MAMuJoCo and SMACv2, show that our method achieves superior performance compared to existing baselines, highlighting its effectiveness in addressing sparse-reward challenges in online MARL.
<div id='section'>Paperid: <span id='pid'>716, <a href='https://arxiv.org/pdf/2509.14380.pdf' target='_blank'>https://arxiv.org/pdf/2509.14380.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Seoyeon Choi, Kanghyun Ryu, Jonghoon Ock, Negar Mehr
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.14380">CRAFT: Coaching Reinforcement Learning Autonomously using Foundation Models for Multi-Robot Coordination Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-Agent Reinforcement Learning (MARL) provides a powerful framework for learning coordination in multi-agent systems. However, applying MARL to robotics still remains challenging due to high-dimensional continuous joint action spaces, complex reward design, and non-stationary transitions inherent to decentralized settings. On the other hand, humans learn complex coordination through staged curricula, where long-horizon behaviors are progressively built upon simpler skills. Motivated by this, we propose CRAFT: Coaching Reinforcement learning Autonomously using Foundation models for multi-robot coordination Tasks, a framework that leverages the reasoning capabilities of foundation models to act as a "coach" for multi-robot coordination. CRAFT automatically decomposes long-horizon coordination tasks into sequences of subtasks using the planning capability of Large Language Models (LLMs). In what follows, CRAFT trains each subtask using reward functions generated by LLM, and refines them through a Vision Language Model (VLM)-guided reward-refinement loop. We evaluate CRAFT on multi-quadruped navigation and bimanual manipulation tasks, demonstrating its capability to learn complex coordination behaviors. In addition, we validate the multi-quadruped navigation policy in real hardware experiments.
<div id='section'>Paperid: <span id='pid'>717, <a href='https://arxiv.org/pdf/2509.14380.pdf' target='_blank'>https://arxiv.org/pdf/2509.14380.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Seoyeon Choi, Kanghyun Ryu, Jonghoon Ock, Negar Mehr
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.14380">CRAFT: Coaching Reinforcement Learning Autonomously using Foundation Models for Multi-Robot Coordination Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-Agent Reinforcement Learning (MARL) provides a powerful framework for learning coordination in multi-agent systems. However, applying MARL to robotics still remains challenging due to high-dimensional continuous joint action spaces, complex reward design, and non-stationary transitions inherent to decentralized settings. On the other hand, humans learn complex coordination through staged curricula, where long-horizon behaviors are progressively built upon simpler skills. Motivated by this, we propose CRAFT: Coaching Reinforcement learning Autonomously using Foundation models for multi-robot coordination Tasks, a framework that leverages the reasoning capabilities of foundation models to act as a "coach" for multi-robot coordination. CRAFT automatically decomposes long-horizon coordination tasks into sequences of subtasks using the planning capability of Large Language Models (LLMs). In what follows, CRAFT trains each subtask using reward functions generated by LLM, and refines them through a Vision Language Model (VLM)-guided reward-refinement loop. We evaluate CRAFT on multi-quadruped navigation and bimanual manipulation tasks, demonstrating its capability to learn complex coordination behaviors. In addition, we validate the multi-quadruped navigation policy in real hardware experiments.
<div id='section'>Paperid: <span id='pid'>718, <a href='https://arxiv.org/pdf/2507.20143.pdf' target='_blank'>https://arxiv.org/pdf/2507.20143.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhonghan Ge, Yuanyang Zhu, Chunlin Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.20143">Concept Learning for Cooperative Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite substantial progress in applying neural networks (NN) to multi-agent reinforcement learning (MARL) areas, they still largely suffer from a lack of transparency and interoperability. However, its implicit cooperative mechanism is not yet fully understood due to black-box networks. In this work, we study an interpretable value decomposition framework via concept bottleneck models, which promote trustworthiness by conditioning credit assignment on an intermediate level of human-like cooperation concepts. To address this problem, we propose a novel value-based method, named Concepts learning for Multi-agent Q-learning (CMQ), that goes beyond the current performance-vs-interpretability trade-off by learning interpretable cooperation concepts. CMQ represents each cooperation concept as a supervised vector, as opposed to existing models where the information flowing through their end-to-end mechanism is concept-agnostic. Intuitively, using individual action value conditioning on global state embeddings to represent each concept allows for extra cooperation representation capacity. Empirical evaluations on the StarCraft II micromanagement challenge and level-based foraging (LBF) show that CMQ achieves superior performance compared with the state-of-the-art counterparts. The results also demonstrate that CMQ provides more cooperation concept representation capturing meaningful cooperation modes, and supports test-time concept interventions for detecting potential biases of cooperation mode and identifying spurious artifacts that impact cooperation.
<div id='section'>Paperid: <span id='pid'>719, <a href='https://arxiv.org/pdf/2507.18867.pdf' target='_blank'>https://arxiv.org/pdf/2507.18867.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuefei Wu, Xiao Yin, Yuanyang Zhu, Chunlin Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.18867">Learning Individual Intrinsic Reward in Multi-Agent Reinforcement Learning via Incorporating Generalized Human Expertise</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Efficient exploration in multi-agent reinforcement learning (MARL) is a challenging problem when receiving only a team reward, especially in environments with sparse rewards. A powerful method to mitigate this issue involves crafting dense individual rewards to guide the agents toward efficient exploration. However, individual rewards generally rely on manually engineered shaping-reward functions that lack high-order intelligence, thus it behaves ineffectively than humans regarding learning and generalization in complex problems. To tackle these issues, we combine the above two paradigms and propose a novel framework, LIGHT (Learning Individual Intrinsic reward via Incorporating Generalized Human experTise), which can integrate human knowledge into MARL algorithms in an end-to-end manner. LIGHT guides each agent to avoid unnecessary exploration by considering both individual action distribution and human expertise preference distribution. Then, LIGHT designs individual intrinsic rewards for each agent based on actionable representational transformation relevant to Q-learning so that the agents align their action preferences with the human expertise while maximizing the joint action value. Experimental results demonstrate the superiority of our method over representative baselines regarding performance and better knowledge reusability across different sparse-reward tasks on challenging scenarios.
<div id='section'>Paperid: <span id='pid'>720, <a href='https://arxiv.org/pdf/2507.16306.pdf' target='_blank'>https://arxiv.org/pdf/2507.16306.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xingjian Zhang, Yizhuo Wang, Guillaume Sartoretti
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.16306">COMPASS: Cooperative Multi-Agent Persistent Monitoring using Spatio-Temporal Attention Network</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Persistent monitoring of dynamic targets is essential in real-world applications such as disaster response, environmental sensing, and wildlife conservation, where mobile agents must continuously gather information under uncertainty. We propose COMPASS, a multi-agent reinforcement learning (MARL) framework that enables decentralized agents to persistently monitor multiple moving targets efficiently. We model the environment as a graph, where nodes represent spatial locations and edges capture topological proximity, allowing agents to reason over structured layouts and revisit informative regions as needed. Each agent independently selects actions based on a shared spatio-temporal attention network that we design to integrate historical observations and spatial context. We model target dynamics using Gaussian Processes (GPs), which support principled belief updates and enable uncertainty-aware planning. We train COMPASS using centralized value estimation and decentralized policy execution under an adaptive reward setting. Our extensive experiments demonstrate that COMPASS consistently outperforms strong baselines in uncertainty reduction, target coverage, and coordination efficiency across dynamic multi-target scenarios.
<div id='section'>Paperid: <span id='pid'>721, <a href='https://arxiv.org/pdf/2507.15587.pdf' target='_blank'>https://arxiv.org/pdf/2507.15587.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yinsong Chen, Kaifeng Wang, Xiaoqiang Meng, Xueyuan Li, Zirui Li, Xin Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.15587">Red-Team Multi-Agent Reinforcement Learning for Emergency Braking Scenario</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current research on decision-making in safety-critical scenarios often relies on inefficient data-driven scenario generation or specific modeling approaches, which fail to capture corner cases in real-world contexts. To address this issue, we propose a Red-Team Multi-Agent Reinforcement Learning framework, where background vehicles with interference capabilities are treated as red-team agents. Through active interference and exploration, red-team vehicles can uncover corner cases outside the data distribution. The framework uses a Constraint Graph Representation Markov Decision Process, ensuring that red-team vehicles comply with safety rules while continuously disrupting the autonomous vehicles (AVs). A policy threat zone model is constructed to quantify the threat posed by red-team vehicles to AVs, inducing more extreme actions to increase the danger level of the scenario. Experimental results show that the proposed framework significantly impacts AVs decision-making safety and generates various corner cases. This method also offers a novel direction for research in safety-critical scenarios.
<div id='section'>Paperid: <span id='pid'>722, <a href='https://arxiv.org/pdf/2507.09179.pdf' target='_blank'>https://arxiv.org/pdf/2507.09179.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ronghua Shi, Yiou Liu, Xinyu Ying, Yang Tan, Yuchun Feng, Lynn Ai, Bill Shi, Xuhui Wang, Zhuang Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.09179">Hide-and-Shill: A Reinforcement Learning Framework for Market Manipulation Detection in Symphony-a Decentralized Multi-Agent System</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Decentralized finance (DeFi) has introduced a new era of permissionless financial innovation but also led to unprecedented market manipulation. Without centralized oversight, malicious actors coordinate shilling campaigns and pump-and-dump schemes across various platforms. We propose a Multi-Agent Reinforcement Learning (MARL) framework for decentralized manipulation detection, modeling the interaction between manipulators and detectors as a dynamic adversarial game. This framework identifies suspicious patterns using delayed token price reactions as financial indicators.Our method introduces three innovations: (1) Group Relative Policy Optimization (GRPO) to enhance learning stability in sparse-reward and partially observable settings; (2) a theory-based reward function inspired by rational expectations and information asymmetry, differentiating price discovery from manipulation noise; and (3) a multi-modal agent pipeline that integrates LLM-based semantic features, social graph signals, and on-chain market data for informed decision-making.The framework is integrated within the Symphony system, a decentralized multi-agent architecture enabling peer-to-peer agent execution and trust-aware learning through distributed logs, supporting chain-verifiable evaluation. Symphony promotes adversarial co-evolution among strategic actors and maintains robust manipulation detection without centralized oracles, enabling real-time surveillance across global DeFi ecosystems.Trained on 100,000 real-world discourse episodes and validated in adversarial simulations, Hide-and-Shill achieves top performance in detection accuracy and causal attribution. This work bridges multi-agent systems with financial surveillance, advancing a new paradigm for decentralized market intelligence. All resources are available at the Hide-and-Shill GitHub repository to promote open research and reproducibility.
<div id='section'>Paperid: <span id='pid'>723, <a href='https://arxiv.org/pdf/2507.09179.pdf' target='_blank'>https://arxiv.org/pdf/2507.09179.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ronghua Shi, Yiou Liu, Xinyu Ying, Yang Tan, Yuchun Feng, Lynn Ai, Bill Shi, Xuhui Wang, Zhuang Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.09179">Hide-and-Shill: A Reinforcement Learning Framework for Market Manipulation Detection in Symphony-a Decentralized Multi-Agent System</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Decentralized finance (DeFi) has introduced a new era of permissionless financial innovation but also led to unprecedented market manipulation. Without centralized oversight, malicious actors coordinate shilling campaigns and pump-and-dump schemes across various platforms. We propose a Multi-Agent Reinforcement Learning (MARL) framework for decentralized manipulation detection, modeling the interaction between manipulators and detectors as a dynamic adversarial game. This framework identifies suspicious patterns using delayed token price reactions as financial indicators.Our method introduces three innovations: (1) Group Relative Policy Optimization (GRPO) to enhance learning stability in sparse-reward and partially observable settings; (2) a theory-based reward function inspired by rational expectations and information asymmetry, differentiating price discovery from manipulation noise; and (3) a multi-modal agent pipeline that integrates LLM-based semantic features, social graph signals, and on-chain market data for informed decision-making.The framework is integrated within the Symphony system, a decentralized multi-agent architecture enabling peer-to-peer agent execution and trust-aware learning through distributed logs, supporting chain-verifiable evaluation. Symphony promotes adversarial co-evolution among strategic actors and maintains robust manipulation detection without centralized oracles, enabling real-time surveillance across global DeFi ecosystems.Trained on 100,000 real-world discourse episodes and validated in adversarial simulations, Hide-and-Shill achieves top performance in detection accuracy and causal attribution. This work bridges multi-agent systems with financial surveillance, advancing a new paradigm for decentralized market intelligence. All resources are available at the Hide-and-Shill GitHub repository to promote open research and reproducibility.
<div id='section'>Paperid: <span id='pid'>724, <a href='https://arxiv.org/pdf/2506.18679.pdf' target='_blank'>https://arxiv.org/pdf/2506.18679.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruicheng Zhang, Yu Sun, Zeyu Zhang, Jinai Li, Xiaofan Liu, Au Hoi Fan, Haowei Guo, Puxin Yan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.18679">MARL-MambaContour: Unleashing Multi-Agent Deep Reinforcement Learning for Active Contour Optimization in Medical Image Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce MARL-MambaContour, the first contour-based medical image segmentation framework based on Multi-Agent Reinforcement Learning (MARL). Our approach reframes segmentation as a multi-agent cooperation task focused on generate topologically consistent object-level contours, addressing the limitations of traditional pixel-based methods which could lack topological constraints and holistic structural awareness of anatomical regions. Each contour point is modeled as an autonomous agent that iteratively adjusts its position to align precisely with the target boundary, enabling adaptation to blurred edges and intricate morphologies common in medical images. This iterative adjustment process is optimized by a contour-specific Soft Actor-Critic (SAC) algorithm, further enhanced with the Entropy Regularization Adjustment Mechanism (ERAM) which dynamically balance agent exploration with contour smoothness. Furthermore, the framework incorporates a Mamba-based policy network featuring a novel Bidirectional Cross-attention Hidden-state Fusion Mechanism (BCHFM). This mechanism mitigates potential memory confusion limitations associated with long-range modeling in state space models, thereby facilitating more accurate inter-agent information exchange and informed decision-making. Extensive experiments on five diverse medical imaging datasets demonstrate the state-of-the-art performance of MARL-MambaContour, highlighting its potential as an accurate and robust clinical application.
<div id='section'>Paperid: <span id='pid'>725, <a href='https://arxiv.org/pdf/2506.18651.pdf' target='_blank'>https://arxiv.org/pdf/2506.18651.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuocun Yang, Huawen Hu, Enze Shi, Shu Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.18651">Dual-level Behavioral Consistency for Inter-group and Intra-group Coordination in Multi-Agent Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Behavioral diversity in Multi-agent reinforcement learning(MARL) represents an emerging and promising research area. Prior work has largely centered on intra-group behavioral consistency in multi-agent systems, with limited attention given to behavioral consistency in multi-agent grouping scenarios. In this paper, we introduce Dual-Level Behavioral Consistency (DLBC), a novel MARL control method designed to explicitly regulate agent behaviors at both intra-group and inter-group levels. DLBC partitions agents into distinct groups and dynamically modulates behavioral diversity both within and between these groups. By dynamically modulating behavioral diversity within and between these groups, DLBC achieves enhanced division of labor through inter-group consistency, which constrains behavioral strategies across different groups. Simultaneously, intra-group consistency, achieved by aligning behavioral strategies within each group, fosters stronger intra-group cooperation. Crucially, DLBC's direct constraint of agent policy functions ensures its broad applicability across various algorithmic frameworks. Experimental results in various grouping cooperation scenarios demonstrate that DLBC significantly enhances both intra-group cooperative performance and inter-group task specialization, yielding substantial performance improvements. DLBC provides new ideas for behavioral consistency control of multi-intelligent body systems, and its potential for application in more complex tasks and dynamic environments can be further explored in the future.
<div id='section'>Paperid: <span id='pid'>726, <a href='https://arxiv.org/pdf/2506.12600.pdf' target='_blank'>https://arxiv.org/pdf/2506.12600.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jie Pan, Tianyi Wang, Christian Claudel, Jing Shi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.12600">Trust-MARL: Trust-Based Multi-Agent Reinforcement Learning Framework for Cooperative On-Ramp Merging Control in Heterogeneous Traffic Flow</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Intelligent transportation systems require connected and automated vehicles (CAVs) to conduct safe and efficient cooperation with human-driven vehicles (HVs) in complex real-world traffic environments. However, the inherent unpredictability of human behaviour, especially at bottlenecks such as highway on-ramp merging areas, often disrupts traffic flow and compromises system performance. To address the challenge of cooperative on-ramp merging in heterogeneous traffic environments, this study proposes a trust-based multi-agent reinforcement learning (Trust-MARL) framework. At the macro level, Trust-MARL enhances global traffic efficiency by leveraging inter-agent trust to improve bottleneck throughput and mitigate traffic shockwave through emergent group-level coordination. At the micro level, a dynamic trust mechanism is designed to enable CAVs to adjust their cooperative strategies in response to real-time behaviors and historical interactions with both HVs and other CAVs. Furthermore, a trust-triggered game-theoretic decision-making module is integrated to guide each CAV in adapting its cooperation factor and executing context-aware lane-changing decisions under safety, comfort, and efficiency constraints. An extensive set of ablation studies and comparative experiments validates the effectiveness of the proposed Trust-MARL approach, demonstrating significant improvements in safety, efficiency, comfort, and adaptability across varying CAV penetration rates and traffic densities.
<div id='section'>Paperid: <span id='pid'>727, <a href='https://arxiv.org/pdf/2505.18750.pdf' target='_blank'>https://arxiv.org/pdf/2505.18750.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiarong Fan, Chenghao Huang, Hao Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.18750">Agent-Based Decentralized Energy Management of EV Charging Station with Solar Photovoltaics via Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the pursuit of energy net zero within smart cities, transportation electrification plays a pivotal role. The adoption of Electric Vehicles (EVs) keeps increasing, making energy management of EV charging stations critically important. While previous studies have managed to reduce energy cost of EV charging while maintaining grid stability, they often overlook the robustness of EV charging management against uncertainties of various forms, such as varying charging behaviors and possible faults in faults in some chargers. To address the gap, a novel Multi-Agent Reinforcement Learning (MARL) approach is proposed treating each charger to be an agent and coordinate all the agents in the EV charging station with solar photovoltaics in a more realistic scenario, where system faults may occur. A Long Short-Term Memory (LSTM) network is incorporated in the MARL algorithm to extract temporal features from time-series. Additionally, a dense reward mechanism is designed for training the agents in the MARL algorithm to improve EV charging experience. Through validation on a real-world dataset, we show that our approach is robust against system uncertainties and faults and also effective in minimizing EV charging costs and maximizing charging service satisfaction.
<div id='section'>Paperid: <span id='pid'>728, <a href='https://arxiv.org/pdf/2505.05967.pdf' target='_blank'>https://arxiv.org/pdf/2505.05967.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Uyoata E. Uyoata, Gilberto Berardinelli, Ramoni Adeogun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.05967">Learning Power Control Protocol for In-Factory 6G Subnetworks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In-X Subnetworks are envisioned to meet the stringent demands of short-range communication in diverse 6G use cases. In the context of In-Factory scenarios, effective power control is critical to mitigating the impact of interference resulting from potentially high subnetwork density. Existing approaches to power control in this domain have predominantly emphasized the data plane, often overlooking the impact of signaling overhead. Furthermore, prior work has typically adopted a network-centric perspective, relying on the assumption of complete and up-to-date channel state information (CSI) being readily available at the central controller. This paper introduces a novel multi-agent reinforcement learning (MARL) framework designed to enable access points to autonomously learn both signaling and power control protocols in an In-Factory Subnetwork environment. By formulating the problem as a partially observable Markov decision process (POMDP) and leveraging multi-agent proximal policy optimization (MAPPO), the proposed approach achieves significant advantages. The simulation results demonstrate that the learning-based method reduces signaling overhead by a factor of 8 while maintaining a buffer flush rate that lags the ideal "Genie" approach by only 5%.
<div id='section'>Paperid: <span id='pid'>729, <a href='https://arxiv.org/pdf/2504.10677.pdf' target='_blank'>https://arxiv.org/pdf/2504.10677.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Muhammad Al-Zafar Khan, Jamal Al-Karaki
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.10677">Achieving Optimal Tissue Repair Through MARL with Reward Shaping and Curriculum Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we present a multi-agent reinforcement learning (MARL) framework for optimizing tissue repair processes using engineered biological agents. Our approach integrates: (1) stochastic reaction-diffusion systems modeling molecular signaling, (2) neural-like electrochemical communication with Hebbian plasticity, and (3) a biologically informed reward function combining chemical gradient tracking, neural synchronization, and robust penalties. A curriculum learning scheme guides the agent through progressively complex repair scenarios. In silico experiments demonstrate emergent repair strategies, including dynamic secretion control and spatial coordination.
<div id='section'>Paperid: <span id='pid'>730, <a href='https://arxiv.org/pdf/2504.05553.pdf' target='_blank'>https://arxiv.org/pdf/2504.05553.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yongjie Fu, Lingyun Zhong, Zifan Li, Xuan Di
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.05553">Federated Hierarchical Reinforcement Learning for Adaptive Traffic Signal Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent reinforcement learning (MARL) has shown promise for adaptive traffic signal control (ATSC), enabling multiple intersections to coordinate signal timings in real time. However, in large-scale settings, MARL faces constraints due to extensive data sharing and communication requirements. Federated learning (FL) mitigates these challenges by training shared models without directly exchanging raw data, yet traditional FL methods such as FedAvg struggle with highly heterogeneous intersections. Different intersections exhibit varying traffic patterns, demands, and road structures, so performing FedAvg across all agents is inefficient. To address this gap, we propose Hierarchical Federated Reinforcement Learning (HFRL) for ATSC. HFRL employs clustering-based or optimization-based techniques to dynamically group intersections and perform FedAvg independently within groups of intersections with similar characteristics, enabling more effective coordination and scalability than standard FedAvg. Our experiments on synthetic and real-world traffic networks demonstrate that HFRL not only outperforms both decentralized and standard federated RL approaches but also identifies suitable grouping patterns based on network structure or traffic demand, resulting in a more robust framework for distributed, heterogeneous systems.
<div id='section'>Paperid: <span id='pid'>731, <a href='https://arxiv.org/pdf/2504.05045.pdf' target='_blank'>https://arxiv.org/pdf/2504.05045.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Huilin Yin, Zhikun Yang, Linchuan Zhang, Daniel Watzenig
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.05045">Attention-Augmented Inverse Reinforcement Learning with Graph Convolutions for Multi-Agent Task Allocation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.
  Multi-agent task allocation (MATA) plays a vital role in cooperative multi-agent systems, with significant implications for applications such as logistics, search and rescue, and robotic coordination. Although traditional deep reinforcement learning (DRL) methods have been shown to be promising, their effectiveness is hindered by a reliance on manually designed reward functions and inefficiencies in dynamic environments. In this paper, an inverse reinforcement learning (IRL)-based framework is proposed, in which multi-head self-attention (MHSA) and graph attention mechanisms are incorporated to enhance reward function learning and task execution efficiency. Expert demonstrations are utilized to infer optimal reward densities, allowing dependence on handcrafted designs to be reduced and adaptability to be improved. Extensive experiments validate the superiority of the proposed method over widely used multi-agent reinforcement learning (MARL) algorithms in terms of both cumulative rewards and task execution efficiency.
<div id='section'>Paperid: <span id='pid'>732, <a href='https://arxiv.org/pdf/2503.21807.pdf' target='_blank'>https://arxiv.org/pdf/2503.21807.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuan Wei, Xiaohan Shan, Jianmin Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.21807">LERO: LLM-driven Evolutionary framework with Hybrid Rewards and Enhanced Observation for Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent reinforcement learning (MARL) faces two critical bottlenecks distinct from single-agent RL: credit assignment in cooperative tasks and partial observability of environmental states. We propose LERO, a framework integrating Large language models (LLMs) with evolutionary optimization to address these MARL-specific challenges. The solution centers on two LLM-generated components: a hybrid reward function that dynamically allocates individual credit through reward decomposition, and an observation enhancement function that augments partial observations with inferred environmental context. An evolutionary algorithm optimizes these components through iterative MARL training cycles, where top-performing candidates guide subsequent LLM generations. Evaluations in Multi-Agent Particle Environments (MPE) demonstrate LERO's superiority over baseline methods, with improved task performance and training efficiency.
<div id='section'>Paperid: <span id='pid'>733, <a href='https://arxiv.org/pdf/2503.19699.pdf' target='_blank'>https://arxiv.org/pdf/2503.19699.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Muhammad Al-Zafar Khan, Jamal Al-Karaki
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.19699">Optimal Path Planning and Cost Minimization for a Drone Delivery System Via Model Predictive Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this study, we formulate the drone delivery problem as a control problem and solve it using Model Predictive Control. Two experiments are performed: The first is on a less challenging grid world environment with lower dimensionality, and the second is with a higher dimensionality and added complexity. The MPC method was benchmarked against three popular Multi-Agent Reinforcement Learning (MARL): Independent $Q$-Learning (IQL), Joint Action Learners (JAL), and Value-Decomposition Networks (VDN). It was shown that the MPC method solved the problem quicker and required fewer optimal numbers of drones to achieve a minimized cost and navigate the optimal path.
<div id='section'>Paperid: <span id='pid'>734, <a href='https://arxiv.org/pdf/2503.04262.pdf' target='_blank'>https://arxiv.org/pdf/2503.04262.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Vasilios Mavroudis, Gregory Palmer, Sara Farmer, Kez Smithson Whitehead, David Foster, Adam Price, Ian Miles, Alberto Caron, Stephen Pasteris
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.04262">Guidelines for Applying RL and MARL in Cybersecurity Applications</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reinforcement Learning (RL) and Multi-Agent Reinforcement Learning (MARL) have emerged as promising methodologies for addressing challenges in automated cyber defence (ACD). These techniques offer adaptive decision-making capabilities in high-dimensional, adversarial environments. This report provides a structured set of guidelines for cybersecurity professionals and researchers to assess the suitability of RL and MARL for specific use cases, considering factors such as explainability, exploration needs, and the complexity of multi-agent coordination. It also discusses key algorithmic approaches, implementation challenges, and real-world constraints, such as data scarcity and adversarial interference. The report further outlines open research questions, including policy optimality, agent cooperation levels, and the integration of MARL systems into operational cybersecurity frameworks. By bridging theoretical advancements and practical deployment, these guidelines aim to enhance the effectiveness of AI-driven cyber defence strategies.
<div id='section'>Paperid: <span id='pid'>735, <a href='https://arxiv.org/pdf/2503.02030.pdf' target='_blank'>https://arxiv.org/pdf/2503.02030.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yitao Bai, Sihan Zeng, Justin Romberg, Thinh T. Doan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.02030">Accelerating Multi-Task Temporal Difference Learning under Low-Rank Representation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We study policy evaluation problems in multi-task reinforcement learning (RL) under a low-rank representation setting. In this setting, we are given $N$ learning tasks where the corresponding value function of these tasks lie in an $r$-dimensional subspace, with $r<N$. One can apply the classic temporal-difference (TD) learning method for solving these problems where this method learns the value function of each task independently. In this paper, we are interested in understanding whether one can exploit the low-rank structure of the multi-task setting to accelerate the performance of TD learning. To answer this question, we propose a new variant of TD learning method, where we integrate the so-called truncated singular value decomposition step into the update of TD learning. This additional step will enable TD learning to exploit the dominant directions due to the low rank structure to update the iterates, therefore, improving its performance. Our empirical results show that the proposed method significantly outperforms the classic TD learning, where the performance gap increases as the rank $r$ decreases.
  From the theoretical point of view, introducing the truncated singular value decomposition step into TD learning might cause an instability on the updates. We provide a theoretical result showing that the instability does not happen. Specifically, we prove that the proposed method converges at a rate $\mathcal{O}(\frac{\ln(t)}{t})$, where $t$ is the number of iterations. This rate matches that of the standard TD learning.
<div id='section'>Paperid: <span id='pid'>736, <a href='https://arxiv.org/pdf/2502.13430.pdf' target='_blank'>https://arxiv.org/pdf/2502.13430.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao Ma, Shijie Wang, Zhiqiang Pu, Siyao Zhao, Xiaolin Ai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.13430">Vision-Based Generic Potential Function for Policy Alignment in Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Guiding the policy of multi-agent reinforcement learning to align with human common sense is a difficult problem, largely due to the complexity of modeling common sense as a reward, especially in complex and long-horizon multi-agent tasks. Recent works have shown the effectiveness of reward shaping, such as potential-based rewards, to enhance policy alignment. The existing works, however, primarily rely on experts to design rule-based rewards, which are often labor-intensive and lack a high-level semantic understanding of common sense. To solve this problem, we propose a hierarchical vision-based reward shaping method. At the bottom layer, a visual-language model (VLM) serves as a generic potential function, guiding the policy to align with human common sense through its intrinsic semantic understanding. To help the policy adapts to uncertainty and changes in long-horizon tasks, the top layer features an adaptive skill selection module based on a visual large language model (vLLM). The module uses instructions, video replays, and training records to dynamically select suitable potential function from a pre-designed pool. Besides, our method is theoretically proven to preserve the optimal policy. Extensive experiments conducted in the Google Research Football environment demonstrate that our method not only achieves a higher win rate but also effectively aligns the policy with human common sense.
<div id='section'>Paperid: <span id='pid'>737, <a href='https://arxiv.org/pdf/2502.09780.pdf' target='_blank'>https://arxiv.org/pdf/2502.09780.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tong Yang, Bo Dai, Lin Xiao, Yuejie Chi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.09780">Incentivize without Bonus: Provably Efficient Model-based Online Multi-agent RL for Markov Games</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent reinforcement learning (MARL) lies at the heart of a plethora of applications involving the interaction of a group of agents in a shared unknown environment. A prominent framework for studying MARL is Markov games, with the goal of finding various notions of equilibria in a sample-efficient manner, such as the Nash equilibrium (NE) and the coarse correlated equilibrium (CCE). However, existing sample-efficient approaches either require tailored uncertainty estimation under function approximation, or careful coordination of the players. In this paper, we propose a novel model-based algorithm, called VMG, that incentivizes exploration via biasing the empirical estimate of the model parameters towards those with a higher collective best-response values of all the players when fixing the other players' policies, thus encouraging the policy to deviate from its current equilibrium for more exploration. VMG is oblivious to different forms of function approximation, and permits simultaneous and uncoupled policy updates of all players. Theoretically, we also establish that VMG achieves a near-optimal regret for finding both the NEs of two-player zero-sum Markov games and CCEs of multi-player general-sum Markov games under linear function approximation in an online environment, which nearly match their counterparts with sophisticated uncertainty quantification.
<div id='section'>Paperid: <span id='pid'>738, <a href='https://arxiv.org/pdf/2502.06835.pdf' target='_blank'>https://arxiv.org/pdf/2502.06835.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziping Xu, Hinal Jajal, Sung Won Choi, Inbal Nahum-Shani, Guy Shani, Alexandra M. Psihogios, Pei-Yao Hung, Susan Murphy
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.06835">Reinforcement Learning on Dyads to Enhance Medication Adherence</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Medication adherence is critical for the recovery of adolescents and young adults (AYAs) who have undergone hematopoietic cell transplantation (HCT). However, maintaining adherence is challenging for AYAs after hospital discharge, who experience both individual (e.g. physical and emotional symptoms) and interpersonal barriers (e.g., relational difficulties with their care partner, who is often involved in medication management). To optimize the effectiveness of a three-component digital intervention targeting both members of the dyad as well as their relationship, we propose a novel Multi-Agent Reinforcement Learning (MARL) approach to personalize the delivery of interventions. By incorporating the domain knowledge, the MARL framework, where each agent is responsible for the delivery of one intervention component, allows for faster learning compared with a flattened agent. Evaluation using a dyadic simulator environment, based on real clinical data, shows a significant improvement in medication adherence (approximately 3%) compared to purely random intervention delivery. The effectiveness of this approach will be further evaluated in an upcoming trial.
<div id='section'>Paperid: <span id='pid'>739, <a href='https://arxiv.org/pdf/2502.02984.pdf' target='_blank'>https://arxiv.org/pdf/2502.02984.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dengyu Zhang, Chenghao, Feng Xue, Qingrui Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.02984">Learning Efficient Flocking Control based on Gibbs Random Fields</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Flocking control is essential for multi-robot systems in diverse applications, yet achieving efficient flocking in congested environments poses challenges regarding computation burdens, performance optimality, and motion safety. This paper addresses these challenges through a multi-agent reinforcement learning (MARL) framework built on Gibbs Random Fields (GRFs). With GRFs, a multi-robot system is represented by a set of random variables conforming to a joint probability distribution, thus offering a fresh perspective on flocking reward design. A decentralized training and execution mechanism, which enhances the scalability of MARL concerning robot quantity, is realized using a GRF-based credit assignment method. An action attention module is introduced to implicitly anticipate the motion intentions of neighboring robots, consequently mitigating potential non-stationarity issues in MARL. The proposed framework enables learning an efficient distributed control policy for multi-robot systems in challenging environments with success rate around $99\%$, as demonstrated through thorough comparisons with state-of-the-art solutions in simulations and experiments. Ablation studies are also performed to validate the efficiency of different framework modules.
<div id='section'>Paperid: <span id='pid'>740, <a href='https://arxiv.org/pdf/2501.08941.pdf' target='_blank'>https://arxiv.org/pdf/2501.08941.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Surya Murthy, John-Paul Clarke, Ufuk Topcu, Zhenyu Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.08941">A Reinforcement Learning Approach to Quiet and Safe UAM Traffic Management</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Urban air mobility (UAM) is a transformative system that operates various small aerial vehicles in urban environments to reshape urban transportation. However, integrating UAM into existing urban environments presents a variety of complex challenges. Recent analyses of UAM's operational constraints highlight aircraft noise and system safety as key hurdles to UAM system implementation. Future UAM air traffic management schemes must ensure that the system is both quiet and safe. We propose a multi-agent reinforcement learning approach to manage UAM traffic, aiming at both vertical separation assurance and noise mitigation. Through extensive training, the reinforcement learning agent learns to balance the two primary objectives by employing altitude adjustments in a multi-layer UAM network. The results reveal the tradeoffs among noise impact, traffic congestion, and separation. Overall, our findings demonstrate the potential of reinforcement learning in mitigating UAM's noise impact while maintaining safe separation using altitude adjustments
<div id='section'>Paperid: <span id='pid'>741, <a href='https://arxiv.org/pdf/2501.02888.pdf' target='_blank'>https://arxiv.org/pdf/2501.02888.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chuxiong Sun, Peng He, Rui Wang, Changwen Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.02888">Revisiting Communication Efficiency in Multi-Agent Reinforcement Learning from the Dimensional Analysis Perspective</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this work, we introduce a novel perspective, i.e., dimensional analysis, to address the challenge of communication efficiency in Multi-Agent Reinforcement Learning (MARL). Our findings reveal that simply optimizing the content and timing of communication at sending end is insufficient to fully resolve communication efficiency issues. Even after applying optimized and gated messages, dimensional redundancy and confounders still persist in the integrated message embeddings at receiving end, which negatively impact communication quality and decision-making. To address these challenges, we propose Dimensional Rational Multi-Agent Communication (DRMAC), designed to mitigate both dimensional redundancy and confounders in MARL. DRMAC incorporates a redundancy-reduction regularization term to encourage the decoupling of information across dimensions within the learned representations of integrated messages. Additionally, we introduce a dimensional mask that dynamically adjusts gradient weights during training to eliminate the influence of decision-irrelevant dimensions. We evaluate DRMAC across a diverse set of multi-agent tasks, demonstrating its superior performance over existing state-of-the-art methods in complex scenarios. Furthermore, the plug-and-play nature of DRMAC's key modules highlights its generalizable performance, serving as a valuable complement rather than a replacement for existing multi-agent communication strategies.
<div id='section'>Paperid: <span id='pid'>742, <a href='https://arxiv.org/pdf/2412.17838.pdf' target='_blank'>https://arxiv.org/pdf/2412.17838.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuyi Wang, Huan Zhao, Yuji Cao, Zibin Pan, Guolong Liu, Gaoqi Liang, Junhua Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.17838">Coordinated Power Smoothing Control for Wind Storage Integrated System with Physics-informed Deep Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The Wind Storage Integrated System with Power Smoothing Control (PSC) has emerged as a promising solution to ensure both efficient and reliable wind energy generation. However, existing PSC strategies overlook the intricate interplay and distinct control frequencies between batteries and wind turbines, and lack consideration of wake effect and battery degradation cost. In this paper, a novel coordinated control framework with hierarchical levels is devised to address these challenges effectively, which integrates the wake model and battery degradation model. In addition, after reformulating the problem as a Markov decision process, the multi-agent reinforcement learning method is introduced to overcome the bi-level characteristic of the problem. Moreover, a Physics-informed Neural Network-assisted Multi-agent Deep Deterministic Policy Gradient (PAMA-DDPG) algorithm is proposed to incorporate the power fluctuation differential equation and expedite the learning process. The effectiveness of the proposed methodology is evaluated through simulations conducted in four distinct scenarios using WindFarmSimulator (WFSim). The results demonstrate that the proposed algorithm facilitates approximately an 11% increase in total profit and a 19% decrease in power fluctuation compared to the traditional methods, thereby addressing the dual objectives of economic efficiency and grid-connected energy reliability.
<div id='section'>Paperid: <span id='pid'>743, <a href='https://arxiv.org/pdf/2410.17466.pdf' target='_blank'>https://arxiv.org/pdf/2410.17466.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yann Bouteiller, Karthik Soma, Giovanni Beltrame
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.17466">Evolution of Societies via Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The universe involves many independent co-learning agents as an ever-evolving part of our observed environment. Yet, in practice, Multi-Agent Reinforcement Learning (MARL) applications are typically constrained to small, homogeneous populations and remain computationally intensive. We propose a methodology that enables simulating populations of Reinforcement Learning agents at evolutionary scale. More specifically, we derive a fast, parallelizable implementation of Policy Gradient (PG) and Opponent-Learning Awareness (LOLA), tailored for evolutionary simulations where agents undergo random pairwise interactions in stateless normal-form games. We demonstrate our approach by simulating the evolution of very large populations made of heterogeneous co-learning agents, under both naive and advanced learning strategies. In our experiments, 200,000 PG or LOLA agents evolve in the classic games of Hawk-Dove, Stag-Hunt, and Rock-Paper-Scissors. Each game provides distinct insights into how populations evolve under both naive and advanced MARL rules, including compelling ways in which Opponent-Learning Awareness affects social evolution.
<div id='section'>Paperid: <span id='pid'>744, <a href='https://arxiv.org/pdf/2410.14383.pdf' target='_blank'>https://arxiv.org/pdf/2410.14383.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Toby Godfrey, William Hunt, Mohammad D. Soorati
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.14383">MARLIN: Multi-Agent Reinforcement Learning Guided by Language-Based Inter-Robot Negotiation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent reinforcement learning is a key method for training multi-robot systems over a series of episodes in which robots are rewarded or punished according to their performance; only once the system is trained to a suitable standard is it deployed in the real world. If the system is not trained enough, the task will likely not be completed and could pose a risk to the surrounding environment. We introduce Multi-Agent Reinforcement Learning guided by Language-based Inter-Robot Negotiation (MARLIN), in which the training process requires fewer training episodes to reach peak performance. Robots are equipped with large language models that negotiate and debate a task, producing plans used to guide the policy during training. The approach dynamically switches between using reinforcement learning and large language model-based action negotiation throughout training. This reduces the number of training episodes required, compared to standard multi-agent reinforcement learning, and hence allows the system to be deployed to physical hardware earlier. The performance of this approach is evaluated against multi-agent reinforcement learning, showing that our hybrid method achieves comparable results with significantly reduced training time.
<div id='section'>Paperid: <span id='pid'>745, <a href='https://arxiv.org/pdf/2410.06101.pdf' target='_blank'>https://arxiv.org/pdf/2410.06101.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao Ma, Tianyi Hu, Zhiqiang Pu, Boyin Liu, Xiaolin Ai, Yanyan Liang, Min Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.06101">Coevolving with the Other You: Fine-Tuning LLM with Sequential Cooperative Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reinforcement learning (RL) has emerged as a pivotal technique for fine-tuning large language models (LLMs) on specific tasks. However, prevailing RL fine-tuning methods predominantly rely on PPO and its variants. Though these algorithms are effective in general RL settings, they often exhibit suboptimal performance and vulnerability to distribution collapse when applied to the fine-tuning of LLMs. In this paper, we propose CORY, extending the RL fine-tuning of LLMs to a sequential cooperative multi-agent reinforcement learning framework, to leverage the inherent coevolution and emergent capabilities of multi-agent systems. In CORY, the LLM to be fine-tuned is initially duplicated into two autonomous agents: a pioneer and an observer. The pioneer generates responses based on queries, while the observer generates responses using both the queries and the pioneer's responses. The two agents are trained together. During training, the agents exchange roles periodically, fostering cooperation and coevolution between them. Experiments evaluate CORY's performance by fine-tuning GPT-2 and Llama-2 under subjective and objective reward functions on the IMDB Review and GSM8K datasets, respectively. Results show that CORY outperforms PPO in terms of policy optimality, resistance to distribution collapse, and training robustness, thereby underscoring its potential as a superior methodology for refining LLMs in real-world applications.
<div id='section'>Paperid: <span id='pid'>746, <a href='https://arxiv.org/pdf/2410.01954.pdf' target='_blank'>https://arxiv.org/pdf/2410.01954.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>The Viet Bui, Thanh Hong Nguyen, Tien Mai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.01954">ComaDICE: Offline Cooperative Multi-Agent Reinforcement Learning with Stationary Distribution Shift Regularization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Offline reinforcement learning (RL) has garnered significant attention for its ability to learn effective policies from pre-collected datasets without the need for further environmental interactions. While promising results have been demonstrated in single-agent settings, offline multi-agent reinforcement learning (MARL) presents additional challenges due to the large joint state-action space and the complexity of multi-agent behaviors. A key issue in offline RL is the distributional shift, which arises when the target policy being optimized deviates from the behavior policy that generated the data. This problem is exacerbated in MARL due to the interdependence between agents' local policies and the expansive joint state-action space. Prior approaches have primarily addressed this challenge by incorporating regularization in the space of either Q-functions or policies. In this work, we introduce a regularizer in the space of stationary distributions to better handle distributional shift. Our algorithm, ComaDICE, offers a principled framework for offline cooperative MARL by incorporating stationary distribution regularization for the global learning policy, complemented by a carefully structured multi-agent value decomposition strategy to facilitate multi-agent training. Through extensive experiments on the multi-agent MuJoCo and StarCraft II benchmarks, we demonstrate that ComaDICE achieves superior performance compared to state-of-the-art offline MARL methods across nearly all tasks.
<div id='section'>Paperid: <span id='pid'>747, <a href='https://arxiv.org/pdf/2409.18435.pdf' target='_blank'>https://arxiv.org/pdf/2409.18435.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xian Yeow Lee, Haiyan Wang, Daisuke Katsumata, Takaharu Matsui, Chetan Gupta
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.18435">Multi-agent Reinforcement Learning for Dynamic Dispatching in Material Handling Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper proposes a multi-agent reinforcement learning (MARL) approach to learn dynamic dispatching strategies, which is crucial for optimizing throughput in material handling systems across diverse industries. To benchmark our method, we developed a material handling environment that reflects the complexities of an actual system, such as various activities at different locations, physical constraints, and inherent uncertainties. To enhance exploration during learning, we propose a method to integrate domain knowledge in the form of existing dynamic dispatching heuristics. Our experimental results show that our method can outperform heuristics by up to 7.4 percent in terms of median throughput. Additionally, we analyze the effect of different architectures on MARL performance when training multiple agents with different functions. We also demonstrate that the MARL agents performance can be further improved by using the first iteration of MARL agents as heuristics to train a second iteration of MARL agents. This work demonstrates the potential of applying MARL to learn effective dynamic dispatching strategies that may be deployed in real-world systems to improve business outcomes.
<div id='section'>Paperid: <span id='pid'>748, <a href='https://arxiv.org/pdf/2409.05480.pdf' target='_blank'>https://arxiv.org/pdf/2409.05480.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yihong Tao, Bo Lei, Haoyang Shi, Jingkai Chen, Xing Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.05480">Adaptive Multi-Layer Deployment for A Digital Twin Empowered Satellite-Terrestrial Integrated Network</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the development of satellite communication technology, satellite-terrestrial integrated networks (STIN), which integrate satellite networks and ground networks, can realize seamless global coverage of communication services. Confronting the intricacies of network dynamics, the diversity of resource heterogeneity, and the unpredictability of user mobility, dynamic resource allocation within networks faces formidable challenges. Digital twin (DT), as a new technique, can reflect a physical network to a virtual network to monitor, analyze, and optimize the physical network. Nevertheless, in the process of constructing the DT model, the deployment location and resource allocation of DTs may adversely affect its performance. Therefore, we propose a STIN model, which alleviates the problem of insufficient single-layer deployment flexibility of the traditional edge network by deploying DTs in multi-layer nodes in a STIN. To address the challenge of deploying DTs in the network, we propose multi-layer DT deployment in a STIN to reduce system delay. Then we adopt a multi-agent reinforcement learning (MARL) scheme to explore the optimal strategy of the DT multi-layer deployment problem. The implemented scheme demonstrates a notable reduction in system delay, as evidenced by simulation outcomes.
<div id='section'>Paperid: <span id='pid'>749, <a href='https://arxiv.org/pdf/2409.04224.pdf' target='_blank'>https://arxiv.org/pdf/2409.04224.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Daniel J. Tan, Qianyi Xu, Kay Choong See, Dilruk Perera, Mengling Feng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.04224">Advancing Multi-Organ Disease Care: A Hierarchical Multi-Agent Reinforcement Learning Framework</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In healthcare, multi-organ system diseases pose unique and significant challenges as they impact multiple physiological systems concurrently, demanding complex and coordinated treatment strategies. Despite recent advancements in the AI based clinical decision support systems, these solutions only focus on individual organ systems, failing to account for complex interdependencies between them. This narrow focus greatly hinders their effectiveness in recommending holistic and clinically actionable treatments in the real world setting. To address this critical gap, we propose a novel Hierarchical Multi-Agent Reinforcement Learning (HMARL) framework. Our architecture deploys specialized and dedicated agents for each organ system and facilitates inter-agent communication to enable synergistic decision-making across organ systems. Furthermore, we introduce a dual-layer state representation technique that contextualizes patient conditions at both global and organ-specific levels, improving the accuracy and relevance of treatment decisions. We evaluate our HMARL solution on the task of sepsis management, a common and critical multi-organ disease, using both qualitative and quantitative metrics. Our method learns effective, clinically aligned treatment policies that considerably improve patient survival. We believe this framework represents a significant advancement in clinical decision support systems, introducing the first RL solution explicitly designed for multi-organ treatment recommendations. Our solution moves beyond prevailing simplified, single-organ models that fall short in addressing the complexity of multi-organ diseases.
<div id='section'>Paperid: <span id='pid'>750, <a href='https://arxiv.org/pdf/2408.08442.pdf' target='_blank'>https://arxiv.org/pdf/2408.08442.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bernard T. Agyeman, Benjamin Decard-Nelson, Jinfeng Liu, Sirish L. Shah
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.08442">A semi-centralized multi-agent RL framework for efficient irrigation scheduling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper proposes a Semi-Centralized Multi-Agent Reinforcement Learning (SCMARL) approach for irrigation scheduling in spatially variable agricultural fields, where management zones address spatial variability. The SCMARL framework is hierarchical in nature, with a centralized coordinator agent at the top level and decentralized local agents at the second level. The coordinator agent makes daily binary irrigation decisions based on field-wide conditions, which are communicated to the local agents. Local agents determine appropriate irrigation amounts for specific management zones using local conditions. The framework employs state augmentation approach to handle non-stationarity in the local agents' environments. An extensive evaluation on a large-scale field in Lethbridge, Canada, compares the SCMARL approach with a learning-based multi-agent model predictive control scheduling approach, highlighting its enhanced performance, resulting in water conservation and improved Irrigation Water Use Efficiency (IWUE). Notably, the proposed approach achieved a 4.0% savings in irrigation water while enhancing the IWUE by 6.3%.
<div id='section'>Paperid: <span id='pid'>751, <a href='https://arxiv.org/pdf/2407.21565.pdf' target='_blank'>https://arxiv.org/pdf/2407.21565.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Joel Vasanth, Jean Rabault, Francisco AlcÃ¡ntara-Ãvila, Mikael Mortensen, Ricardo Vinuesa
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.21565">Multi-agent reinforcement learning for the control of three-dimensional Rayleigh-BÃ©nard convection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep reinforcement learning (DRL) has found application in numerous use-cases pertaining to flow control. Multi-agent RL (MARL), a variant of DRL, has shown to be more effective than single-agent RL in controlling flows exhibiting locality and translational invariance. We present, for the first time, an implementation of MARL-based control of three-dimensional Rayleigh-BÃ©nard convection (RBC). Control is executed by modifying the temperature distribution along the bottom wall divided into multiple control segments, each of which acts as an independent agent. Two regimes of RBC are considered at Rayleigh numbers $\mathrm{Ra}=500$ and $750$. Evaluation of the learned control policy reveals a reduction in convection intensity by $23.5\%$ and $8.7\%$ at $\mathrm{Ra}=500$ and $750$, respectively. The MARL controller converts irregularly shaped convective patterns to regular straight rolls with lower convection that resemble flow in a relatively more stable regime. We draw comparisons with proportional control at both $\mathrm{Ra}$ and show that MARL is able to outperform the proportional controller. The learned control strategy is complex, featuring different non-linear segment-wise actuator delays and actuation magnitudes. We also perform successful evaluations on a larger domain than used for training, demonstrating that the invariant property of MARL allows direct transfer of the learnt policy.
<div id='section'>Paperid: <span id='pid'>752, <a href='https://arxiv.org/pdf/2407.17822.pdf' target='_blank'>https://arxiv.org/pdf/2407.17822.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Joongoo Jeon, Jean Rabault, Joel Vasanth, Francisco AlcÃ¡ntara-Ãvila, Shilaj Baral, Ricardo Vinuesa
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.17822">Advanced deep-reinforcement-learning methods for flow control: group-invariant and positional-encoding networks improve learning speed and quality</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Flow control is key to maximize energy efficiency in a wide range of applications. However, traditional flow-control methods face significant challenges in addressing non-linear systems and high-dimensional data, limiting their application in realistic energy systems. This study advances deep-reinforcement-learning (DRL) methods for flow control, particularly focusing on integrating group-invariant networks and positional encoding into DRL architectures. Our methods leverage multi-agent reinforcement learning (MARL) to exploit policy invariance in space, in combination with group-invariant networks to ensure local symmetry invariance. Additionally, a positional encoding inspired by the transformer architecture is incorporated to provide location information to the agents, mitigating action constraints from strict invariance. The proposed methods are verified using a case study of Rayleigh-BÃ©nard convection, where the goal is to minimize the Nusselt number Nu. The group-invariant neural networks (GI-NNs) show faster convergence compared to the base MARL, achieving better average policy performance. The GI-NNs not only cut DRL training time in half but also notably enhance learning reproducibility. Positional encoding further enhances these results, effectively reducing the minimum Nu and stabilizing convergence. Interestingly, group invariant networks specialize in improving learning speed and positional encoding specializes in improving learning quality. These results demonstrate that choosing a suitable feature-representation method according to the purpose as well as the characteristics of each control problem is essential. We believe that the results of this study will not only inspire novel DRL methods with invariant and unique representations, but also provide useful insights for industrial applications.
<div id='section'>Paperid: <span id='pid'>753, <a href='https://arxiv.org/pdf/2407.08964.pdf' target='_blank'>https://arxiv.org/pdf/2407.08964.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sicong Jiang, Seongjin Choi, Lijun Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.08964">Communication-Aware Reinforcement Learning for Cooperative Adaptive Cruise Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cooperative Adaptive Cruise Control (CACC) plays a pivotal role in enhancing traffic efficiency and safety in Connected and Autonomous Vehicles (CAVs). Reinforcement Learning (RL) has proven effective in optimizing complex decision-making processes in CACC, leading to improved system performance and adaptability. Among RL approaches, Multi-Agent Reinforcement Learning (MARL) has shown remarkable potential by enabling coordinated actions among multiple CAVs through Centralized Training with Decentralized Execution (CTDE). However, MARL often faces scalability issues, particularly when CACC vehicles suddenly join or leave the platoon, resulting in performance degradation. To address these challenges, we propose Communication-Aware Reinforcement Learning (CA-RL). CA-RL includes a communication-aware module that extracts and compresses vehicle communication information through forward and backward information transmission modules. This enables efficient cyclic information propagation within the CACC traffic flow, ensuring policy consistency and mitigating the scalability problems of MARL in CACC. Experimental results demonstrate that CA-RL significantly outperforms baseline methods in various traffic scenarios, achieving superior scalability, robustness, and overall system performance while maintaining reliable performance despite changes in the number of participating vehicles.
<div id='section'>Paperid: <span id='pid'>754, <a href='https://arxiv.org/pdf/2407.07086.pdf' target='_blank'>https://arxiv.org/pdf/2407.07086.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Logan Cross, Violet Xiang, Agam Bhatia, Daniel LK Yamins, Nick Haber
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.07086">Hypothetical Minds: Scaffolding Theory of Mind for Multi-Agent Tasks with Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent reinforcement learning (MARL) methods struggle with the non-stationarity of multi-agent systems and fail to adaptively learn online when tested with novel agents. Here, we leverage large language models (LLMs) to create an autonomous agent that can handle these challenges. Our agent, Hypothetical Minds, consists of a cognitively-inspired architecture, featuring modular components for perception, memory, and hierarchical planning over two levels of abstraction. We introduce the Theory of Mind module that scaffolds the high-level planning process by generating hypotheses about other agents' strategies in natural language. It then evaluates and iteratively refines these hypotheses by reinforcing hypotheses that make correct predictions about the other agents' behavior. Hypothetical Minds significantly improves performance over previous LLM-agent and RL baselines on a range of competitive, mixed motive, and collaborative domains in the Melting Pot benchmark, including both dyadic and population-based environments. Additionally, comparisons against LLM-agent baselines and ablations reveal the importance of hypothesis evaluation and refinement for succeeding on complex scenarios.
<div id='section'>Paperid: <span id='pid'>755, <a href='https://arxiv.org/pdf/2407.02759.pdf' target='_blank'>https://arxiv.org/pdf/2407.02759.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yang Zhao, Chang Zhou, Jin Cao, Yi Zhao, Shaobo Liu, Chiyu Cheng, Xingchen Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.02759">Multi-Scenario Combination Based on Multi-Agent Reinforcement Learning to Optimize the Advertising Recommendation System</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper explores multi-scenario optimization on large platforms using multi-agent reinforcement learning (MARL). We address this by treating scenarios like search, recommendation, and advertising as a cooperative, partially observable multi-agent decision problem. We introduce the Multi-Agent Recurrent Deterministic Policy Gradient (MARDPG) algorithm, which aligns different scenarios under a shared objective and allows for strategy communication to boost overall performance. Our results show marked improvements in metrics such as click-through rate (CTR), conversion rate, and total sales, confirming our method's efficacy in practical settings.
<div id='section'>Paperid: <span id='pid'>756, <a href='https://arxiv.org/pdf/2405.17794.pdf' target='_blank'>https://arxiv.org/pdf/2405.17794.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yutong Wang, Tanishq Duhan, Jiaoyang Li, Guillaume Sartoretti
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.17794">LNS2+RL: Combining Multi-Agent Reinforcement Learning with Large Neighborhood Search in Multi-Agent Path Finding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-Agent Path Finding (MAPF) is a critical component of logistics and warehouse management, which focuses on planning collision-free paths for a team of robots in a known environment. Recent work introduced a novel MAPF approach, LNS2, which proposed to repair a quickly obtained set of infeasible paths via iterative replanning, by relying on a fast, yet lower-quality, prioritized planning (PP) algorithm. At the same time, there has been a recent push for Multi-Agent Reinforcement Learning (MARL) based MAPF algorithms, which exhibit improved cooperation over such PP algorithms, although inevitably remaining slower. In this paper, we introduce a new MAPF algorithm, LNS2+RL, which combines the distinct yet complementary characteristics of LNS2 and MARL to effectively balance their individual limitations and get the best from both worlds. During early iterations, LNS2+RL relies on MARL for low-level replanning, which we show eliminates collisions much more than a PP algorithm. There, our MARL-based planner allows agents to reason about past and future information to gradually learn cooperative decision-making through a finely designed curriculum learning. At later stages of planning, LNS2+RL adaptively switches to PP algorithm to quickly resolve the remaining collisions, naturally trading off solution quality (number of collisions in the solution) and computational efficiency. Our comprehensive experiments on high-agent-density tasks across various team sizes, world sizes, and map structures consistently demonstrate the superior performance of LNS2+RL compared to many MAPF algorithms, including LNS2, LaCAM, EECBS, and SCRIMP. In maps with complex structures, the advantages of LNS2+RL are particularly pronounced, with LNS2+RL achieving a success rate of over 50% in nearly half of the tested tasks, while that of LaCAM, EECBS and SCRIMP falls to 0%.
<div id='section'>Paperid: <span id='pid'>757, <a href='https://arxiv.org/pdf/2405.08655.pdf' target='_blank'>https://arxiv.org/pdf/2405.08655.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Matteo Cederle, Marco Fabris, Gian Antonio Susto
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.08655">A Distributed Approach to Autonomous Intersection Management via Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autonomous intersection management (AIM) poses significant challenges due to the intricate nature of real-world traffic scenarios and the need for a highly expensive centralised server in charge of simultaneously controlling all the vehicles. This study addresses such issues by proposing a novel distributed approach to AIM utilizing multi-agent reinforcement learning (MARL). We show that by leveraging the 3D surround view technology for advanced assistance systems, autonomous vehicles can accurately navigate intersection scenarios without needing any centralised controller. The contributions of this paper thus include a MARL-based algorithm for the autonomous management of a 4-way intersection and also the introduction of a new strategy called prioritised scenario replay for improved training efficacy. We validate our approach as an innovative alternative to conventional centralised AIM techniques, ensuring the full reproducibility of our results. Specifically, experiments conducted in virtual environments using the SMARTS platform highlight its superiority over benchmarks across various metrics.
<div id='section'>Paperid: <span id='pid'>758, <a href='https://arxiv.org/pdf/2405.02849.pdf' target='_blank'>https://arxiv.org/pdf/2405.02849.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alicia Vidler, Toby Walsh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.02849">Modelling Opaque Bilateral Market Dynamics in Financial Trading: Insights from a Multi-Agent Simulation Study</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Exploring complex adaptive financial trading environments through multi-agent based simulation methods presents an innovative approach within the realm of quantitative finance. Despite the dominance of multi-agent reinforcement learning approaches in financial markets with observable data, there exists a set of systematically significant financial markets that pose challenges due to their partial or obscured data availability. We, therefore, devise a multi-agent simulation approach employing small-scale meta-heuristic methods. This approach aims to represent the opaque bilateral market for Australian government bond trading, capturing the bilateral nature of bank-to-bank trading, also referred to as "over-the-counter" (OTC) trading, and commonly occurring between "market makers". The uniqueness of the bilateral market, characterized by negotiated transactions and a limited number of agents, yields valuable insights for agent-based modelling and quantitative finance. The inherent rigidity of this market structure, which is at odds with the global proliferation of multilateral platforms and the decentralization of finance, underscores the unique insights offered by our agent-based model. We explore the implications of market rigidity on market structure and consider the element of stability, in market design. This extends the ongoing discourse on complex financial trading environments, providing an enhanced understanding of their dynamics and implications.
<div id='section'>Paperid: <span id='pid'>759, <a href='https://arxiv.org/pdf/2405.02456.pdf' target='_blank'>https://arxiv.org/pdf/2405.02456.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sihan Zeng, Thinh T. Doan, Justin Romberg
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.02456">Natural Policy Gradient and Actor Critic Methods for Constrained Multi-Task Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-task reinforcement learning (RL) aims to find a single policy that effectively solves multiple tasks at the same time. This paper presents a constrained formulation for multi-task RL where the goal is to maximize the average performance of the policy across tasks subject to bounds on the performance in each task. We consider solving this problem both in the centralized setting, where information for all tasks is accessible to a single server, and in the decentralized setting, where a network of agents, each given one task and observing local information, cooperate to find the solution of the globally constrained objective using local communication.
  We first propose a primal-dual algorithm that provably converges to the globally optimal solution of this constrained formulation under exact gradient evaluations. When the gradient is unknown, we further develop a sampled-based actor-critic algorithm that finds the optimal policy using online samples of state, action, and reward. Finally, we study the extension of the algorithm to the linear function approximation setting.
<div id='section'>Paperid: <span id='pid'>760, <a href='https://arxiv.org/pdf/2403.16301.pdf' target='_blank'>https://arxiv.org/pdf/2403.16301.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yao Kang, Xin Wang, Zhiling Lan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.16301">Q-adaptive: A Multi-Agent Reinforcement Learning Based Routing on Dragonfly Network</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>High-radix interconnects such as Dragonfly and its variants rely on adaptive routing to balance network traffic for optimum performance. Ideally, adaptive routing attempts to forward packets between minimal and non-minimal paths with the least congestion. In practice, current adaptive routing algorithms estimate routing path congestion based on local information such as output queue occupancy. Using local information to estimate global path congestion is inevitably inaccurate because a router has no precise knowledge of link states a few hops away. This inaccuracy could lead to interconnect congestion. In this study, we present Q-adaptive routing, a multi-agent reinforcement learning routing scheme for Dragonfly systems. Q-adaptive routing enables routers to learn to route autonomously by leveraging advanced reinforcement learning technology. The proposed Q-adaptive routing is highly scalable thanks to its fully distributed nature without using any shared information between routers. Furthermore, a new two-level Q-table is designed for Q-adaptive to make it computational lightly and saves 50% of router memory usage compared with the previous Q-routing. We implement the proposed Q-adaptive routing in SST/Merlin simulator. Our evaluation results show that Q-adaptive routing achieves up to 10.5% system throughput improvement and 5.2x average packet latency reduction compared with adaptive routing algorithms. Remarkably, Q-adaptive can even outperform the optimal VALn non-minimal routing under the ADV+1 adversarial traffic pattern with up to 3% system throughput improvement and 75% average packet latency reduction.
<div id='section'>Paperid: <span id='pid'>761, <a href='https://arxiv.org/pdf/2402.07082.pdf' target='_blank'>https://arxiv.org/pdf/2402.07082.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yan Dai, Qiwen Cui, Simon S. Du
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.07082">Refined Sample Complexity for Markov Games with Independent Linear Function Approximation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Markov Games (MG) is an important model for Multi-Agent Reinforcement Learning (MARL). It was long believed that the "curse of multi-agents" (i.e., the algorithmic performance drops exponentially with the number of agents) is unavoidable until several recent works (Daskalakis et al., 2023; Cui et al., 2023; Wang et al., 2023). While these works resolved the curse of multi-agents, when the state spaces are prohibitively large and (linear) function approximations are deployed, they either had a slower convergence rate of $O(T^{-1/4})$ or brought a polynomial dependency on the number of actions $A_{\max}$ -- which is avoidable in single-agent cases even when the loss functions can arbitrarily vary with time. This paper first refines the AVLPR framework by Wang et al. (2023), with an insight of designing *data-dependent* (i.e., stochastic) pessimistic estimation of the sub-optimality gap, allowing a broader choice of plug-in algorithms. When specialized to MGs with independent linear function approximations, we propose novel *action-dependent bonuses* to cover occasionally extreme estimation errors. With the help of state-of-the-art techniques from the single-agent RL literature, we give the first algorithm that tackles the curse of multi-agents, attains the optimal $O(T^{-1/2})$ convergence rate, and avoids $\text{poly}(A_{\max})$ dependency simultaneously.
<div id='section'>Paperid: <span id='pid'>762, <a href='https://arxiv.org/pdf/2401.04934.pdf' target='_blank'>https://arxiv.org/pdf/2401.04934.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiechuan Jiang, Kefan Su, Zongqing Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.04934">Fully Decentralized Cooperative Multi-Agent Reinforcement Learning: A Survey</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cooperative multi-agent reinforcement learning is a powerful tool to solve many real-world cooperative tasks, but restrictions of real-world applications may require training the agents in a fully decentralized manner. Due to the lack of information about other agents, it is challenging to derive algorithms that can converge to the optimal joint policy in a fully decentralized setting. Thus, this research area has not been thoroughly studied. In this paper, we seek to systematically review the fully decentralized methods in two settings: maximizing a shared reward of all agents and maximizing the sum of individual rewards of all agents, and discuss open questions and future research directions.
<div id='section'>Paperid: <span id='pid'>763, <a href='https://arxiv.org/pdf/2312.11084.pdf' target='_blank'>https://arxiv.org/pdf/2312.11084.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Min Hua, Dong Chen, Xinda Qi, Kun Jiang, Zemin Eitan Liu, Quan Zhou, Hongming Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.11084">Multi-Agent Reinforcement Learning for Connected and Automated Vehicles Control: Recent Advancements and Future Prospects</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Connected and automated vehicles (CAVs) are considered a potential solution for future transportation challenges, aiming to develop systems that are efficient, safe, and environmentally friendly. However, CAV control presents significant challenges due to the complexity of interconnectivity and coordination required among vehicles. Multi-agent reinforcement learning (MARL), which has shown notable advancements in addressing complex problems in autonomous driving, robotics, and human-vehicle interaction, emerges as a promising tool to enhance CAV capabilities. Despite its potential, there is a notable absence of current reviews on mainstream MARL algorithms for CAVs. To fill this gap, this paper offers a comprehensive review of MARL's application in CAV control. The paper begins with an introduction to MARL, explaining its unique advantages in handling complex and multi-agent scenarios. It then presents a detailed survey of MARL applications across various control dimensions for CAVs, including critical scenarios such as platooning control, lane-changing, and unsignalized intersections. Additionally, the paper reviews prominent simulation platforms essential for developing and testing MARL algorithms. Lastly, it examines the current challenges in deploying MARL for CAV control, including macro-micro optimization, communication, mixed traffic, and sim-to-real challenges. Potential solutions discussed include hierarchical MARL, decentralized MARL, adaptive interactions, and offline MARL.
<div id='section'>Paperid: <span id='pid'>764, <a href='https://arxiv.org/pdf/2312.08662.pdf' target='_blank'>https://arxiv.org/pdf/2312.08662.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Violet Xiang, Logan Cross, Jan-Philipp FrÃ¤nken, Nick Haber
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.08662">From Centralized to Self-Supervised: Pursuing Realistic Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In real-world environments, autonomous agents rely on their egocentric observations. They must learn adaptive strategies to interact with others who possess mixed motivations, discernible only through visible cues. Several Multi-Agent Reinforcement Learning (MARL) methods adopt centralized approaches that involve either centralized training or reward-sharing, often violating the realistic ways in which living organisms, like animals or humans, process information and interact. MARL strategies deploying decentralized training with intrinsic motivation offer a self-supervised approach, enable agents to develop flexible social strategies through the interaction of autonomous agents. However, by contrasting the self-supervised and centralized methods, we reveal that populations trained with reward-sharing methods surpass those using self-supervised methods in a mixed-motive environment. We link this superiority to specialized role emergence and an agent's expertise in its role. Interestingly, this gap shrinks in pure-motive settings, emphasizing the need for evaluations in more complex, realistic environments (mixed-motive). Our preliminary results suggest a gap in population performance that can be closed by improving self-supervised methods and thereby pushing MARL closer to real-world readiness.
<div id='section'>Paperid: <span id='pid'>765, <a href='https://arxiv.org/pdf/2312.01747.pdf' target='_blank'>https://arxiv.org/pdf/2312.01747.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lina Zhu, Jiyu Cheng, Hao Zhang, Zhichao Cui, Wei Zhang, Yuehu Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.01747">Autonomous and Adaptive Role Selection for Multi-robot Collaborative Area Search Based on Deep Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the tasks of multi-robot collaborative area search, we propose the unified approach for simultaneous mapping for sensing more targets (exploration) while searching and locating the targets (coverage). Specifically, we implement a hierarchical multi-agent reinforcement learning algorithm to decouple task planning from task execution. The role concept is integrated into the upper-level task planning for role selection, which enables robots to learn the role based on the state status from the upper-view. Besides, an intelligent role switching mechanism enables the role selection module to function between two timesteps, promoting both exploration and coverage interchangeably. Then the primitive policy learns how to plan based on their assigned roles and local observation for sub-task execution. The well-designed experiments show the scalability and generalization of our method compared with state-of-the-art approaches in the scenes with varying complexity and number of robots.
<div id='section'>Paperid: <span id='pid'>766, <a href='https://arxiv.org/pdf/2309.06021.pdf' target='_blank'>https://arxiv.org/pdf/2309.06021.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Marwa Chafii, Salmane Naoumi, Reda Alami, Ebtesam Almazrouei, Mehdi Bennis, Merouane Debbah
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.06021">Emergent Communication in Multi-Agent Reinforcement Learning for Future Wireless Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In different wireless network scenarios, multiple network entities need to cooperate in order to achieve a common task with minimum delay and energy consumption. Future wireless networks mandate exchanging high dimensional data in dynamic and uncertain environments, therefore implementing communication control tasks becomes challenging and highly complex. Multi-agent reinforcement learning with emergent communication (EC-MARL) is a promising solution to address high dimensional continuous control problems with partially observable states in a cooperative fashion where agents build an emergent communication protocol to solve complex tasks. This paper articulates the importance of EC-MARL within the context of future 6G wireless networks, which imbues autonomous decision-making capabilities into network entities to solve complex tasks such as autonomous driving, robot navigation, flying base stations network planning, and smart city applications. An overview of EC-MARL algorithms and their design criteria are provided while presenting use cases and research opportunities on this emerging topic.
<div id='section'>Paperid: <span id='pid'>767, <a href='https://arxiv.org/pdf/2309.04272.pdf' target='_blank'>https://arxiv.org/pdf/2309.04272.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiduan Wu, Anas Barakat, Ilyas Fatkhullin, Niao He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.04272">Learning Zero-Sum Linear Quadratic Games with Improved Sample Complexity and Last-Iterate Convergence</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Zero-sum Linear Quadratic (LQ) games are fundamental in optimal control and can be used (i)~as a dynamic game formulation for risk-sensitive or robust control and (ii)~as a benchmark setting for multi-agent reinforcement learning with two competing agents in continuous state-control spaces. In contrast to the well-studied single-agent linear quadratic regulator problem, zero-sum LQ games entail solving a challenging nonconvex-nonconcave min-max problem with an objective function that lacks coercivity. Recently, Zhang et al. showed that an~$Îµ$-Nash equilibrium (NE) of finite horizon zero-sum LQ games can be learned via nested model-free Natural Policy Gradient (NPG) algorithms with poly$(1/Îµ)$ sample complexity. In this work, we propose a simpler nested Zeroth-Order (ZO) algorithm improving sample complexity by several orders of magnitude and guaranteeing convergence of the last iterate. Our main results are two-fold: (i) in the deterministic setting, we establish the first global last-iterate linear convergence result for the nested algorithm that seeks NE of zero-sum LQ games; (ii) in the model-free setting, we establish a~$\widetilde{\mathcal{O}}(Îµ^{-2})$ sample complexity using a single-point ZO estimator. For our last-iterate convergence results, our analysis leverages the Implicit Regularization (IR) property and a new gradient domination condition for the primal function. Our key improvements in the sample complexity rely on a more sample-efficient nested algorithm design and a finer control of the ZO natural gradient estimation error utilizing the structure endowed by the finite-horizon setting.
<div id='section'>Paperid: <span id='pid'>768, <a href='https://arxiv.org/pdf/2308.10188.pdf' target='_blank'>https://arxiv.org/pdf/2308.10188.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>The Viet Bui, Tien Mai, Thanh Hong Nguyen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.10188">Mimicking To Dominate: Imitation Learning Strategies for Success in Multiagent Competitive Games</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Training agents in multi-agent competitive games presents significant challenges due to their intricate nature. These challenges are exacerbated by dynamics influenced not only by the environment but also by opponents' strategies. Existing methods often struggle with slow convergence and instability. To address this, we harness the potential of imitation learning to comprehend and anticipate opponents' behavior, aiming to mitigate uncertainties with respect to the game dynamics. Our key contributions include: (i) a new multi-agent imitation learning model for predicting next moves of the opponents -- our model works with hidden opponents' actions and local observations; (ii) a new multi-agent reinforcement learning algorithm that combines our imitation learning model and policy training into one single training process; and (iii) extensive experiments in three challenging game environments, including an advanced version of the Star-Craft multi-agent challenge (i.e., SMACv2). Experimental results show that our approach achieves superior performance compared to existing state-of-the-art multi-agent RL algorithms.
<div id='section'>Paperid: <span id='pid'>769, <a href='https://arxiv.org/pdf/2306.02029.pdf' target='_blank'>https://arxiv.org/pdf/2306.02029.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jichao Chen, Omid Esrafilian, Harald Bayerlein, David Gesbert, Marco Caccamo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.02029">Model-aided Federated Reinforcement Learning for Multi-UAV Trajectory Planning in IoT Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deploying teams of unmanned aerial vehicles (UAVs) to harvest data from distributed Internet of Things (IoT) devices requires efficient trajectory planning and coordination algorithms. Multi-agent reinforcement learning (MARL) has emerged as a solution, but requires extensive and costly real-world training data. To tackle this challenge, we propose a novel model-aided federated MARL algorithm to coordinate multiple UAVs on a data harvesting mission with only limited knowledge about the environment. The proposed algorithm alternates between building an environment simulation model from real-world measurements, specifically learning the radio channel characteristics and estimating unknown IoT device positions, and federated QMIX training in the simulated environment. Each UAV agent trains a local QMIX model in its simulated environment and continuously consolidates it through federated learning with other agents, accelerating the learning process. A performance comparison with standard MARL algorithms demonstrates that our proposed model-aided FedQMIX algorithm reduces the need for real-world training experiences by around three magnitudes while attaining similar data collection performance.
<div id='section'>Paperid: <span id='pid'>770, <a href='https://arxiv.org/pdf/2305.17372.pdf' target='_blank'>https://arxiv.org/pdf/2305.17372.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jueming Hu, Jean-Raphael Gaglione, Yanze Wang, Zhe Xu, Ufuk Topcu, Yongming Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.17372">Reinforcement Learning With Reward Machines in Stochastic Games</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We investigate multi-agent reinforcement learning for stochastic games with complex tasks, where the reward functions are non-Markovian. We utilize reward machines to incorporate high-level knowledge of complex tasks. We develop an algorithm called Q-learning with reward machines for stochastic games (QRM-SG), to learn the best-response strategy at Nash equilibrium for each agent. In QRM-SG, we define the Q-function at a Nash equilibrium in augmented state space. The augmented state space integrates the state of the stochastic game and the state of reward machines. Each agent learns the Q-functions of all agents in the system. We prove that Q-functions learned in QRM-SG converge to the Q-functions at a Nash equilibrium if the stage game at each time step during learning has a global optimum point or a saddle point, and the agents update Q-functions based on the best-response strategy at this point. We use the Lemke-Howson method to derive the best-response strategy given current Q-functions. The three case studies show that QRM-SG can learn the best-response strategies effectively. QRM-SG learns the best-response strategies after around 7500 episodes in Case Study I, 1000 episodes in Case Study II, and 1500 episodes in Case Study III, while baseline methods such as Nash Q-learning and MADDPG fail to converge to the Nash equilibrium in all three case studies.
<div id='section'>Paperid: <span id='pid'>771, <a href='https://arxiv.org/pdf/2305.13411.pdf' target='_blank'>https://arxiv.org/pdf/2305.13411.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kailash Gogineni, Peng Wei, Tian Lan, Guru Venkataramani
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.13411">Towards Efficient Multi-Agent Learning Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-Agent Reinforcement Learning (MARL) is an increasingly important research field that can model and control multiple large-scale autonomous systems. Despite its achievements, existing multi-agent learning methods typically involve expensive computations in terms of training time and power arising from large observation-action space and a huge number of training steps. Therefore, a key challenge is understanding and characterizing the computationally intensive functions in several popular classes of MARL algorithms during their training phases. Our preliminary experiments reveal new insights into the key modules of MARL algorithms that limit the adoption of MARL in real-world systems. We explore neighbor sampling strategy to improve cache locality and observe performance improvement ranging from 26.66% (3 agents) to 27.39% (12 agents) during the computationally intensive mini-batch sampling phase. Additionally, we demonstrate that improving the locality leads to an end-to-end training time reduction of 10.2% (for 12 agents) compared to existing multi-agent algorithms without significant degradation in the mean reward.
<div id='section'>Paperid: <span id='pid'>772, <a href='https://arxiv.org/pdf/2305.10167.pdf' target='_blank'>https://arxiv.org/pdf/2305.10167.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Emil Carlsson, Devdatt Dubhashi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.10167">Pragmatic Reasoning in Structured Signaling Games</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this work we introduce a structured signaling game, an extension of the classical signaling game with a similarity structure between meanings in the context, along with a variant of the Rational Speech Act (RSA) framework which we call structured-RSA (sRSA) for pragmatic reasoning in structured domains. We explore the behavior of the sRSA in the domain of color and show that pragmatic agents using sRSA on top of semantic representations, derived from the World Color Survey, attain efficiency very close to the information theoretic limit after only 1 or 2 levels of recursion. We also explore the interaction between pragmatic reasoning and learning in multi-agent reinforcement learning framework. Our results illustrate that artificial agents using sRSA develop communication closer to the information theoretic frontier compared to agents using RSA and just reinforcement learning. We also find that the ambiguity of the semantic representation increases as the pragmatic agents are allowed to perform deeper reasoning about each other during learning.
<div id='section'>Paperid: <span id='pid'>773, <a href='https://arxiv.org/pdf/2305.07069.pdf' target='_blank'>https://arxiv.org/pdf/2305.07069.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mojtaba Vaezi, Xingqin Lin, Hongliang Zhang, Walid Saad, H. Vincent Poor
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.07069">Deep Reinforcement Learning for Interference Management in UAV-based 3D Networks: Potentials and Challenges</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Modern cellular networks are multi-cell and use universal frequency reuse to maximize spectral efficiency. This results in high inter-cell interference. This problem is growing as cellular networks become three-dimensional with the adoption of unmanned aerial vehicles (UAVs). This is because the strength and number of interference links rapidly increase due to the line-of-sight channels in UAV communications. Existing interference management solutions need each transmitter to know the channel information of interfering signals, rendering them impractical due to excessive signaling overhead. In this paper, we propose leveraging deep reinforcement learning for interference management to tackle this shortcoming. In particular, we show that interference can still be effectively mitigated even without knowing its channel information. We then discuss novel approaches to scale the algorithms with linear/sublinear complexity and decentralize them using multi-agent reinforcement learning. By harnessing interference, the proposed solutions enable the continued growth of civilian UAVs.
<div id='section'>Paperid: <span id='pid'>774, <a href='https://arxiv.org/pdf/2304.13383.pdf' target='_blank'>https://arxiv.org/pdf/2304.13383.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zichuan Liu, Yuanyang Zhu, Chunlin Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.13383">N$\text{A}^\text{2}$Q: Neural Attention Additive Model for Interpretable Multi-Agent Q-Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Value decomposition is widely used in cooperative multi-agent reinforcement learning, however, its implicit credit assignment mechanism is not yet fully understood due to black-box networks. In this work, we study an interpretable value decomposition framework via the family of generalized additive models. We present a novel method, named Neural Attention Additive Q-learning (N$\text{A}^\text{2}$Q), providing inherent intelligibility of collaboration behavior. N$\text{A}^\text{2}$Q can explicitly factorize the optimal joint policy induced by enriching shape functions to model all possible coalitions of agents into individual policies. Moreover, we construct identity semantics to promote estimating credits together with the global state and individual value functions, where local semantic masks help us diagnose whether each agent captures relevant-task information. Extensive experiments show that N$\text{A}^\text{2}$Q consistently achieves superior performance compared to different state-of-the-art methods on all challenging tasks, while yielding human-like interpretability.
<div id='section'>Paperid: <span id='pid'>775, <a href='https://arxiv.org/pdf/2304.07297.pdf' target='_blank'>https://arxiv.org/pdf/2304.07297.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hengyuan Hu, Dorsa Sadigh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.07297">Language Instructed Reinforcement Learning for Human-AI Coordination</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>One of the fundamental quests of AI is to produce agents that coordinate well with humans. This problem is challenging, especially in domains that lack high quality human behavioral data, because multi-agent reinforcement learning (RL) often converges to different equilibria from the ones that humans prefer. We propose a novel framework, instructRL, that enables humans to specify what kind of strategies they expect from their AI partners through natural language instructions. We use pretrained large language models to generate a prior policy conditioned on the human instruction and use the prior to regularize the RL objective. This leads to the RL agent converging to equilibria that are aligned with human preferences. We show that instructRL converges to human-like policies that satisfy the given instructions in a proof-of-concept environment as well as the challenging Hanabi benchmark. Finally, we show that knowing the language instruction significantly boosts human-AI coordination performance in human evaluations in Hanabi.
<div id='section'>Paperid: <span id='pid'>776, <a href='https://arxiv.org/pdf/2304.02370.pdf' target='_blank'>https://arxiv.org/pdf/2304.02370.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Colin Vignon, Jean Rabault, Joel Vasanth, Francisco AlcÃ¡ntara-Ãvila, Mikael Mortensen, Ricardo Vinuesa
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.02370">Effective control of two-dimensional Rayleigh--BÃ©nard convection: invariant multi-agent reinforcement learning is all you need</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Rayleigh-BÃ©nard convection (RBC) is a recurrent phenomenon in several industrial and geoscience flows and a well-studied system from a fundamental fluid-mechanics viewpoint. However, controlling RBC, for example by modulating the spatial distribution of the bottom-plate heating in the canonical RBC configuration, remains a challenging topic for classical control-theory methods. In the present work, we apply deep reinforcement learning (DRL) for controlling RBC. We show that effective RBC control can be obtained by leveraging invariant multi-agent reinforcement learning (MARL), which takes advantage of the locality and translational invariance inherent to RBC flows inside wide channels. The MARL framework applied to RBC allows for an increase in the number of control segments without encountering the curse of dimensionality that would result from a naive increase in the DRL action-size dimension. This is made possible by the MARL ability for re-using the knowledge generated in different parts of the RBC domain. We show in a case study that MARL DRL is able to discover an advanced control strategy that destabilizes the spontaneous RBC double-cell pattern, changes the topology of RBC by coalescing adjacent convection cells, and actively controls the resulting coalesced cell to bring it to a new stable configuration. This modified flow configuration results in reduced convective heat transfer, which is beneficial in several industrial processes. Therefore, our work both shows the potential of MARL DRL for controlling large RBC systems, as well as demonstrates the possibility for DRL to discover strategies that move the RBC configuration between different topological configurations, yielding desirable heat-transfer characteristics. These results are useful for both gaining further understanding of the intrinsic properties of RBC, as well as for developing industrial applications.
<div id='section'>Paperid: <span id='pid'>777, <a href='https://arxiv.org/pdf/2303.09058.pdf' target='_blank'>https://arxiv.org/pdf/2303.09058.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuhan Qi, Shuhao Zhang, Qiang Wang, Jiajia Zhang, Jing Xiao, Xuan Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.09058">SVDE: Scalable Value-Decomposition Exploration for Cooperative Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Value-decomposition methods, which reduce the difficulty of a multi-agent system by decomposing the joint state-action space into local observation-action spaces, have become popular in cooperative multi-agent reinforcement learning (MARL). However, value-decomposition methods still have the problems of tremendous sample consumption for training and lack of active exploration. In this paper, we propose a scalable value-decomposition exploration (SVDE) method, which includes a scalable training mechanism, intrinsic reward design, and explorative experience replay. The scalable training mechanism asynchronously decouples strategy learning with environmental interaction, so as to accelerate sample generation in a MapReduce manner. For the problem of lack of exploration, an intrinsic reward design and explorative experience replay are proposed, so as to enhance exploration to produce diverse samples and filter non-novel samples, respectively. Empirically, our method achieves the best performance on almost all maps compared to other popular algorithms in a set of StarCraft II micromanagement games. A data-efficiency experiment also shows the acceleration of SVDE for sample collection and policy convergence, and we demonstrate the effectiveness of factors in SVDE through a set of ablation experiments.
<div id='section'>Paperid: <span id='pid'>778, <a href='https://arxiv.org/pdf/2303.00912.pdf' target='_blank'>https://arxiv.org/pdf/2303.00912.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Woojun Kim, Youngchul Sung
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.00912">Parameter Sharing with Network Pruning for Scalable Multi-Agent Deep Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Handling the problem of scalability is one of the essential issues for multi-agent reinforcement learning (MARL) algorithms to be applied to real-world problems typically involving massively many agents. For this, parameter sharing across multiple agents has widely been used since it reduces the training time by decreasing the number of parameters and increasing the sample efficiency. However, using the same parameters across agents limits the representational capacity of the joint policy and consequently, the performance can be degraded in multi-agent tasks that require different behaviors for different agents. In this paper, we propose a simple method that adopts structured pruning for a deep neural network to increase the representational capacity of the joint policy without introducing additional parameters. We evaluate the proposed method on several benchmark tasks, and numerical results show that the proposed method significantly outperforms other parameter-sharing methods.
<div id='section'>Paperid: <span id='pid'>779, <a href='https://arxiv.org/pdf/2303.00605.pdf' target='_blank'>https://arxiv.org/pdf/2303.00605.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yutong Wang, Bairan Xiang, Shinan Huang, Guillaume Sartoretti
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.00605">SCRIMP: Scalable Communication for Reinforcement- and Imitation-Learning-Based Multi-Agent Pathfinding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Trading off performance guarantees in favor of scalability, the Multi-Agent Path Finding (MAPF) community has recently started to embrace Multi-Agent Reinforcement Learning (MARL), where agents learn to collaboratively generate individual, collision-free (but often suboptimal) paths. Scalability is usually achieved by assuming a local field of view (FOV) around the agents, helping scale to arbitrary world sizes. However, this assumption significantly limits the amount of information available to the agents, making it difficult for them to enact the type of joint maneuvers needed in denser MAPF tasks. In this paper, we propose SCRIMP, where agents learn individual policies from even very small (down to 3x3) FOVs, by relying on a highly-scalable global/local communication mechanism based on a modified transformer. We further equip agents with a state-value-based tie-breaking strategy to further improve performance in symmetric situations, and introduce intrinsic rewards to encourage exploration while mitigating the long-term credit assignment problem. Empirical evaluations on a set of experiments indicate that SCRIMP can achieve higher performance with improved scalability compared to other state-of-the-art learning-based MAPF planners with larger FOVs, and even yields similar performance as a classical centralized planner in many cases. Ablation studies further validate the effectiveness of our proposed techniques. Finally, we show that our trained model can be directly implemented on real robots for online MAPF through high-fidelity simulations in gazebo.
<div id='section'>Paperid: <span id='pid'>780, <a href='https://arxiv.org/pdf/2303.00451.pdf' target='_blank'>https://arxiv.org/pdf/2303.00451.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Woojun Kim, Whiyoung Jung, Myungsik Cho, Youngchul Sung
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.00451">A Variational Approach to Mutual Information-Based Coordination for Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we propose a new mutual information framework for multi-agent reinforcement learning to enable multiple agents to learn coordinated behaviors by regularizing the accumulated return with the simultaneous mutual information between multi-agent actions. By introducing a latent variable to induce nonzero mutual information between multi-agent actions and applying a variational bound, we derive a tractable lower bound on the considered MMI-regularized objective function. The derived tractable objective can be interpreted as maximum entropy reinforcement learning combined with uncertainty reduction of other agents actions. Applying policy iteration to maximize the derived lower bound, we propose a practical algorithm named variational maximum mutual information multi-agent actor-critic, which follows centralized learning with decentralized execution. We evaluated VM3-AC for several games requiring coordination, and numerical results show that VM3-AC outperforms other MARL algorithms in multi-agent tasks requiring high-quality coordination.
<div id='section'>Paperid: <span id='pid'>781, <a href='https://arxiv.org/pdf/2302.11070.pdf' target='_blank'>https://arxiv.org/pdf/2302.11070.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zheng Xiong, Jacob Beck, Shimon Whiteson
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.11070">Universal Morphology Control via Contextual Modulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Learning a universal policy across different robot morphologies can significantly improve learning efficiency and generalization in continuous control. However, it poses a challenging multi-task reinforcement learning problem, as the optimal policy may be quite different across robots and critically depend on the morphology. Existing methods utilize graph neural networks or transformers to handle heterogeneous state and action spaces across different morphologies, but pay little attention to the dependency of a robot's control policy on its morphology context. In this paper, we propose a hierarchical architecture to better model this dependency via contextual modulation, which includes two key submodules: (1) Instead of enforcing hard parameter sharing across robots, we use hypernetworks to generate morphology-dependent control parameters; (2) We propose a fixed attention mechanism that solely depends on the morphology to modulate the interactions between different limbs in a robot. Experimental results show that our method not only improves learning performance on a diverse set of training robots, but also generalizes better to unseen morphologies in a zero-shot fashion.
<div id='section'>Paperid: <span id='pid'>782, <a href='https://arxiv.org/pdf/2302.08328.pdf' target='_blank'>https://arxiv.org/pdf/2302.08328.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruohong Liu, Yize Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.08328">Learning a Multi-Agent Controller for Shared Energy Storage System</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deployment of shared energy storage systems (SESS) allows users to use the stored energy to meet their own energy demands while saving energy costs without installing private energy storage equipment. In this paper, we consider a group of building users in the community with SESS, and each user can schedule power injection from the grid as well as SESS according to their demand and real-time electricity price to minimize energy cost and meet energy demand simultaneously. SESS is encouraged to charge when the price is low, thus providing as much energy as possible for users while achieving energy savings. However, due to the complex dynamics of buildings and real-time external signals, it is a challenging task to find high-performance power dispatch decisions in real-time. By designing a multi-agent reinforcement learning framework with state-aware reward functions, SESS and users can realize power scheduling to meet the users' energy demand and SESS's charging/discharging balance without additional communication, so as to achieve energy optimization. Compared with the baseline approach without the participation of the SESS, the energy cost is saved by around 2.37% to 21.58%.
<div id='section'>Paperid: <span id='pid'>783, <a href='https://arxiv.org/pdf/2302.05007.pdf' target='_blank'>https://arxiv.org/pdf/2302.05007.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kailash Gogineni, Peng Wei, Tian Lan, Guru Venkataramani
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.05007">Scalability Bottlenecks in Multi-Agent Reinforcement Learning Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-Agent Reinforcement Learning (MARL) is a promising area of research that can model and control multiple, autonomous decision-making agents. During online training, MARL algorithms involve performance-intensive computations such as exploration and exploitation phases originating from large observation-action space belonging to multiple agents. In this article, we seek to characterize the scalability bottlenecks in several popular classes of MARL algorithms during their training phases. Our experimental results reveal new insights into the key modules of MARL algorithms that limit the scalability, and outline potential strategies that may help address these performance issues.
<div id='section'>Paperid: <span id='pid'>784, <a href='https://arxiv.org/pdf/2302.03673.pdf' target='_blank'>https://arxiv.org/pdf/2302.03673.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qiwen Cui, Kaiqing Zhang, Simon S. Du
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.03673">Breaking the Curse of Multiagents in a Large State Space: RL in Markov Games with Independent Linear Function Approximation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a new model, independent linear Markov game, for multi-agent reinforcement learning with a large state space and a large number of agents. This is a class of Markov games with independent linear function approximation, where each agent has its own function approximation for the state-action value functions that are marginalized by other players' policies. We design new algorithms for learning the Markov coarse correlated equilibria (CCE) and Markov correlated equilibria (CE) with sample complexity bounds that only scale polynomially with each agent's own function class complexity, thus breaking the curse of multiagents. In contrast, existing works for Markov games with function approximation have sample complexity bounds scale with the size of the \emph{joint action space} when specialized to the canonical tabular Markov game setting, which is exponentially large in the number of agents. Our algorithms rely on two key technical innovations: (1) utilizing policy replay to tackle non-stationarity incurred by multiple agents and the use of function approximation; (2) separating learning Markov equilibria and exploration in the Markov games, which allows us to use the full-information no-regret learning oracle instead of the stronger bandit-feedback no-regret learning oracle used in the tabular setting. Furthermore, we propose an iterative-best-response type algorithm that can learn pure Markov Nash equilibria in independent linear Markov potential games. In the tabular case, by adapting the policy replay mechanism for independent linear Markov games, we propose an algorithm with $\widetilde{O}(Îµ^{-2})$ sample complexity to learn Markov CCE, which improves the state-of-the-art result $\widetilde{O}(Îµ^{-3})$ in Daskalakis et al. 2022, where $Îµ$ is the desired accuracy, and also significantly improves other problem parameters.
<div id='section'>Paperid: <span id='pid'>785, <a href='https://arxiv.org/pdf/2302.01188.pdf' target='_blank'>https://arxiv.org/pdf/2302.01188.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiechuan Jiang, Zongqing Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.01188">Best Possible Q-Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Fully decentralized learning, where the global information, i.e., the actions of other agents, is inaccessible, is a fundamental challenge in cooperative multi-agent reinforcement learning. However, the convergence and optimality of most decentralized algorithms are not theoretically guaranteed, since the transition probabilities are non-stationary as all agents are updating policies simultaneously. To tackle this challenge, we propose best possible operator, a novel decentralized operator, and prove that the policies of agents will converge to the optimal joint policy if each agent independently updates its individual state-action value by the operator. Further, to make the update more efficient and practical, we simplify the operator and prove that the convergence and optimality still hold with the simplified one. By instantiating the simplified operator, the derived fully decentralized algorithm, best possible Q-learning (BQL), does not suffer from non-stationarity. Empirically, we show that BQL achieves remarkable improvement over baselines in a variety of cooperative multi-agent tasks.
<div id='section'>Paperid: <span id='pid'>786, <a href='https://arxiv.org/pdf/2301.05294.pdf' target='_blank'>https://arxiv.org/pdf/2301.05294.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dawei Wang, Weizi Li, Lei Zhu, Jia Pan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.05294">Learning to Control and Coordinate Mixed Traffic Through Robot Vehicles at Complex and Unsignalized Intersections</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Intersections are essential road infrastructures for traffic in modern metropolises. However, they can also be the bottleneck of traffic flows as a result of traffic incidents or the absence of traffic coordination mechanisms such as traffic lights. Recently, various control and coordination mechanisms that are beyond traditional control methods have been proposed to improve the efficiency of intersection traffic by leveraging the ability of autonomous vehicles. Amongst these methods, the control of foreseeable mixed traffic that consists of human-driven vehicles (HVs) and robot vehicles (RVs) has emerged. We propose a decentralized multi-agent reinforcement learning approach for the control and coordination of mixed traffic by RVs at real-world, complex intersections -- an open challenge to date. We design comprehensive experiments to evaluate the effectiveness, robustness, generalizablility, and adaptability of our approach. In particular, our method can prevent congestion formation via merely 5% RVs under a real-world traffic demand of 700 vehicles per hour. In contrast, without RVs, congestion will form when the traffic demand reaches as low as 200 vehicles per hour. Moreover, when the RV penetration rate exceeds 60%, our method starts to outperform traffic signal control in terms of the average waiting time of all vehicles. Our method is not only robust against blackout events, sudden RV percentage drops, and V2V communication error, but also enjoys excellent generalizablility, evidenced by its successful deployment in five unseen intersections. Lastly, our method performs well under various traffic rules, demonstrating its adaptability to diverse scenarios. Videos and code of our work are available at https://sites.google.com/view/mixedtrafficcontrol
<div id='section'>Paperid: <span id='pid'>787, <a href='https://arxiv.org/pdf/2211.03936.pdf' target='_blank'>https://arxiv.org/pdf/2211.03936.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ou Deng, Qun Jin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2211.03936">Policy-Based Reinforcement Learning for Assortative Matching in Human Behavior Modeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper explores human behavior in virtual networked communities, specifically individuals or groups' potential and expressive capacity to respond to internal and external stimuli, with assortative matching as a typical example. A modeling approach based on Multi-Agent Reinforcement Learning (MARL) is proposed, adding a multi-head attention function to the A3C algorithm to enhance learning effectiveness. This approach simulates human behavior in certain scenarios through various environmental parameter settings and agent action strategies. In our experiment, reinforcement learning is employed to serve specific agents that learn from environment status and competitor behaviors, optimizing strategies to achieve better results. The simulation includes individual and group levels, displaying possible paths to forming competitive advantages. This modeling approach provides a means for further analysis of the evolutionary dynamics of human behavior, communities, and organizations in various socioeconomic issues.
<div id='section'>Paperid: <span id='pid'>788, <a href='https://arxiv.org/pdf/2209.12660.pdf' target='_blank'>https://arxiv.org/pdf/2209.12660.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Thomas Langerak, Sammy Christen, Mert Albaba, Christoph Gebhardt, Otmar Hilliges
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2209.12660">MARLUI: Multi-Agent Reinforcement Learning for Adaptive UIs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Adaptive user interfaces (UIs) automatically change an interface to better support users' tasks. Recently, machine learning techniques have enabled the transition to more powerful and complex adaptive UIs. However, a core challenge for adaptive user interfaces is the reliance on high-quality user data that has to be collected offline for each task. We formulate UI adaptation as a multi-agent reinforcement learning problem to overcome this challenge. In our formulation, a user agent mimics a real user and learns to interact with a UI. Simultaneously, an interface agent learns UI adaptations to maximize the user agent's performance. The interface agent learns the task structure from the user agent's behavior and, based on that, can support the user agent in completing its task. Our method produces adaptation policies that are learned in simulation only and, therefore, does not need real user data. Our experiments show that learned policies generalize to real users and achieve on par performance with data-driven supervised learning baselines.
<div id='section'>Paperid: <span id='pid'>789, <a href='https://arxiv.org/pdf/2206.15378.pdf' target='_blank'>https://arxiv.org/pdf/2206.15378.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Julien Perolat, Bart de Vylder, Daniel Hennes, Eugene Tarassov, Florian Strub, Vincent de Boer, Paul Muller, Jerome T. Connor, Neil Burch, Thomas Anthony, Stephen McAleer, Romuald Elie, Sarah H. Cen, Zhe Wang, Audrunas Gruslys, Aleksandra Malysheva, Mina Khan, Sherjil Ozair, Finbarr Timbers, Toby Pohlen, Tom Eccles, Mark Rowland, Marc Lanctot, Jean-Baptiste Lespiau, Bilal Piot, Shayegan Omidshafiei, Edward Lockhart, Laurent Sifre, Nathalie Beauguerlange, Remi Munos, David Silver, Satinder Singh, Demis Hassabis, Karl Tuyls
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2206.15378">Mastering the Game of Stratego with Model-Free Multiagent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce DeepNash, an autonomous agent capable of learning to play the imperfect information game Stratego from scratch, up to a human expert level. Stratego is one of the few iconic board games that Artificial Intelligence (AI) has not yet mastered. This popular game has an enormous game tree on the order of $10^{535}$ nodes, i.e., $10^{175}$ times larger than that of Go. It has the additional complexity of requiring decision-making under imperfect information, similar to Texas hold'em poker, which has a significantly smaller game tree (on the order of $10^{164}$ nodes). Decisions in Stratego are made over a large number of discrete actions with no obvious link between action and outcome. Episodes are long, with often hundreds of moves before a player wins, and situations in Stratego can not easily be broken down into manageably-sized sub-problems as in poker. For these reasons, Stratego has been a grand challenge for the field of AI for decades, and existing AI methods barely reach an amateur level of play. DeepNash uses a game-theoretic, model-free deep reinforcement learning method, without search, that learns to master Stratego via self-play. The Regularised Nash Dynamics (R-NaD) algorithm, a key component of DeepNash, converges to an approximate Nash equilibrium, instead of 'cycling' around it, by directly modifying the underlying multi-agent learning dynamics. DeepNash beats existing state-of-the-art AI methods in Stratego and achieved a yearly (2022) and all-time top-3 rank on the Gravon games platform, competing with human expert players.
<div id='section'>Paperid: <span id='pid'>790, <a href='https://arxiv.org/pdf/2202.00082.pdf' target='_blank'>https://arxiv.org/pdf/2202.00082.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mingfei Sun, Sam Devlin, Jacob Beck, Katja Hofmann, Shimon Whiteson
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2202.00082">Trust Region Bounds for Decentralized PPO Under Non-stationarity</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present trust region bounds for optimizing decentralized policies in cooperative Multi-Agent Reinforcement Learning (MARL), which holds even when the transition dynamics are non-stationary. This new analysis provides a theoretical understanding of the strong performance of two recent actor-critic methods for MARL, which both rely on independent ratios, i.e., computing probability ratios separately for each agent's policy. We show that, despite the non-stationarity that independent ratios cause, a monotonic improvement guarantee still arises as a result of enforcing the trust region constraint over all decentralized policies. We also show this trust region constraint can be effectively enforced in a principled way by bounding independent ratios based on the number of agents in training, providing a theoretical foundation for proximal ratio clipping. Finally, our empirical results support the hypothesis that the strong performance of IPPO and MAPPO is a direct result of enforcing such a trust region constraint via clipping in centralized training, and tuning the hyperparameters with regards to the number of agents, as predicted by our theoretical analysis.
<div id='section'>Paperid: <span id='pid'>791, <a href='https://arxiv.org/pdf/2110.04857.pdf' target='_blank'>https://arxiv.org/pdf/2110.04857.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Paul Maria Scheikl, BalÃ¡zs Gyenes, Tornike Davitashvili, Rayan Younis, AndrÃ© Schulze, Beat P. MÃ¼ller-Stich, Gerhard Neumann, Martin Wagner, Franziska Mathis-Ullrich
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2110.04857">Cooperative Assistance in Robotic Surgery through Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cognitive cooperative assistance in robot-assisted surgery holds the potential to increase quality of care in minimally invasive interventions. Automation of surgical tasks promises to reduce the mental exertion and fatigue of surgeons. In this work, multi-agent reinforcement learning is demonstrated to be robust to the distribution shift introduced by pairing a learned policy with a human team member. Multi-agent policies are trained directly from images in simulation to control multiple instruments in a sub task of the minimally invasive removal of the gallbladder. These agents are evaluated individually and in cooperation with humans to demonstrate their suitability as autonomous assistants. Compared to human teams, the hybrid teams with artificial agents perform better considering completion time (44.4% to 71.2% shorter) as well as number of collisions (44.7% to 98.0% fewer). Path lengths, however, increase under control of an artificial agent (11.4% to 33.5% longer). A multi-agent formulation of the learning problem was favored over a single-agent formulation on this surgical sub task, due to the sequential learning of the two instruments. This approach may be extended to other tasks that are difficult to formulate within the standard reinforcement learning framework. Multi-agent reinforcement learning may shift the paradigm of cognitive robotic surgery towards seamless cooperation between surgeons and assistive technologies.
<div id='section'>Paperid: <span id='pid'>792, <a href='https://arxiv.org/pdf/2108.01832.pdf' target='_blank'>https://arxiv.org/pdf/2108.01832.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiechuan Jiang, Zongqing Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2108.01832">Offline Decentralized Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In many real-world multi-agent cooperative tasks, due to high cost and risk, agents cannot continuously interact with the environment and collect experiences during learning, but have to learn from offline datasets. However, the transition dynamics in the dataset of each agent can be much different from the ones induced by the learned policies of other agents in execution, creating large errors in value estimates. Consequently, agents learn uncoordinated low-performing policies. In this paper, we propose a framework for offline decentralized multi-agent reinforcement learning, which exploits value deviation and transition normalization to deliberately modify the transition probabilities. Value deviation optimistically increases the transition probabilities of high-value next states, and transition normalization normalizes the transition probabilities of next states. They together enable agents to learn high-performing and coordinated policies. Theoretically, we prove the convergence of Q-learning under the altered non-stationary transition dynamics. Empirically, we show that the framework can be easily built on many existing offline reinforcement learning algorithms and achieve substantial improvement in a variety of multi-agent tasks.
<div id='section'>Paperid: <span id='pid'>793, <a href='https://arxiv.org/pdf/1901.08129.pdf' target='_blank'>https://arxiv.org/pdf/1901.08129.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Diego Perez-Liebana, Katja Hofmann, Sharada Prasanna Mohanty, Noboru Kuno, Andre Kramer, Sam Devlin, Raluca D. Gaina, Daniel Ionita
</span></div><div id="title">Title: <a href="https://arxiv.org/html/1901.08129">The Multi-Agent Reinforcement Learning in MalmÃ (MARLÃ) Competition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Learning in multi-agent scenarios is a fruitful research direction, but current approaches still show scalability problems in multiple games with general reward settings and different opponent types. The Multi-Agent Reinforcement Learning in MalmÃ (MARLÃ) competition is a new challenge that proposes research in this domain using multiple 3D games. The goal of this contest is to foster research in general agents that can learn across different games and opponent types, proposing a challenge as a milestone in the direction of Artificial General Intelligence.
<div id='section'>Paperid: <span id='pid'>794, <a href='https://arxiv.org/pdf/2509.20766.pdf' target='_blank'>https://arxiv.org/pdf/2509.20766.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gawon Lee, Daesol Cho, H. Jin Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.20766">Leveraging Temporally Extended Behavior Sharing for Multi-task Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-task reinforcement learning (MTRL) offers a promising approach to improve sample efficiency and generalization by training agents across multiple tasks, enabling knowledge sharing between them. However, applying MTRL to robotics remains challenging due to the high cost of collecting diverse task data. To address this, we propose MT-LÃ©vy, a novel exploration strategy that enhances sample efficiency in MTRL environments by combining behavior sharing across tasks with temporally extended exploration inspired by LÃ©vy flight. MT-LÃ©vy leverages policies trained on related tasks to guide exploration towards key states, while dynamically adjusting exploration levels based on task success ratios. This approach enables more efficient state-space coverage, even in complex robotics environments. Empirical results demonstrate that MT-LÃ©vy significantly improves exploration and sample efficiency, supported by quantitative and qualitative analyses. Ablation studies further highlight the contribution of each component, showing that combining behavior sharing with adaptive exploration strategies can significantly improve the practicality of MTRL in robotics applications.
<div id='section'>Paperid: <span id='pid'>795, <a href='https://arxiv.org/pdf/2509.20766.pdf' target='_blank'>https://arxiv.org/pdf/2509.20766.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gawon Lee, Daesol Cho, H. Jin Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.20766">Leveraging Temporally Extended Behavior Sharing for Multi-task Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-task reinforcement learning (MTRL) offers a promising approach to improve sample efficiency and generalization by training agents across multiple tasks, enabling knowledge sharing between them. However, applying MTRL to robotics remains challenging due to the high cost of collecting diverse task data. To address this, we propose MT-LÃ©vy, a novel exploration strategy that enhances sample efficiency in MTRL environments by combining behavior sharing across tasks with temporally extended exploration inspired by LÃ©vy flight. MT-LÃ©vy leverages policies trained on related tasks to guide exploration towards key states, while dynamically adjusting exploration levels based on task success ratios. This approach enables more efficient state-space coverage, even in complex robotics environments. Empirical results demonstrate that MT-LÃ©vy significantly improves exploration and sample efficiency, supported by quantitative and qualitative analyses. Ablation studies further highlight the contribution of each component, showing that combining behavior sharing with adaptive exploration strategies can significantly improve the practicality of MTRL in robotics applications.
<div id='section'>Paperid: <span id='pid'>796, <a href='https://arxiv.org/pdf/2509.17350.pdf' target='_blank'>https://arxiv.org/pdf/2509.17350.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoran Zhou, Yangwei You, Shuaijun Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.17350">DyDexHandover: Human-like Bimanual Dynamic Dexterous Handover using RGB-only Perception</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Dynamic in air handover is a fundamental challenge for dual-arm robots, requiring accurate perception, precise coordination, and natural motion. Prior methods often rely on dynamics models, strong priors, or depth sensing, limiting generalization and naturalness. We present DyDexHandover, a novel framework that employs multi-agent reinforcement learning to train an end to end RGB based policy for bimanual object throwing and catching. To achieve more human-like behavior, the throwing policy is guided by a human policy regularization scheme, encouraging fluid and natural motion, and enhancing the generalization capability of the policy. A dual arm simulation environment was built in Isaac Sim for experimental evaluation. DyDexHandover achieves nearly 99 percent success on training objects and 75 percent on unseen objects, while generating human-like throwing and catching behaviors. To our knowledge, it is the first method to realize dual-arm in-air handover using only raw RGB perception.
<div id='section'>Paperid: <span id='pid'>797, <a href='https://arxiv.org/pdf/2509.17350.pdf' target='_blank'>https://arxiv.org/pdf/2509.17350.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoran Zhou, Yangwei You, Shuaijun Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.17350">DyDexHandover: Human-like Bimanual Dynamic Dexterous Handover using RGB-only Perception</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Dynamic in air handover is a fundamental challenge for dual-arm robots, requiring accurate perception, precise coordination, and natural motion. Prior methods often rely on dynamics models, strong priors, or depth sensing, limiting generalization and naturalness. We present DyDexHandover, a novel framework that employs multi-agent reinforcement learning to train an end to end RGB based policy for bimanual object throwing and catching. To achieve more human-like behavior, the throwing policy is guided by a human policy regularization scheme, encouraging fluid and natural motion, and enhancing the generalization capability of the policy. A dual arm simulation environment was built in Isaac Sim for experimental evaluation. DyDexHandover achieves nearly 99 percent success on training objects and 75 percent on unseen objects, while generating human-like throwing and catching behaviors. To our knowledge, it is the first method to realize dual-arm in-air handover using only raw RGB perception.
<div id='section'>Paperid: <span id='pid'>798, <a href='https://arxiv.org/pdf/2508.12524.pdf' target='_blank'>https://arxiv.org/pdf/2508.12524.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Joseph SuÃ¡rez, Kyoung Whan Choe, David Bloomin, Jianming Gao, Yunkun Li, Yao Feng, Saidinesh Pola, Kun Zhang, Yonghui Zhu, Nikhil Pinnaparaju, Hao Xiang Li, Nishaanth Kanna, Daniel Scott, Ryan Sullivan, Rose S. Shuman, Lucas de AlcÃ¢ntara, Herbie Bradley, Kirsty You, Bo Wu, Yuhao Jiang, Qimai Li, Jiaxin Chen, Louis Castricato, Xiaolong Zhu, Phillip Isola
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.12524">Results of the NeurIPS 2023 Neural MMO Competition on Multi-task Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present the results of the NeurIPS 2023 Neural MMO Competition, which attracted over 200 participants and submissions. Participants trained goal-conditional policies that generalize to tasks, maps, and opponents never seen during training. The top solution achieved a score 4x higher than our baseline within 8 hours of training on a single 4090 GPU. We open-source everything relating to Neural MMO and the competition under the MIT license, including the policy weights and training code for our baseline and for the top submissions.
<div id='section'>Paperid: <span id='pid'>799, <a href='https://arxiv.org/pdf/2508.11706.pdf' target='_blank'>https://arxiv.org/pdf/2508.11706.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhuofan Xu, Benedikt Bollig, Matthias FÃ¼gger, Thomas Nowak, Vincent Le DrÃ©au
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.11706">Centralized Permutation Equivariant Policy for Cooperative Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The Centralized Training with Decentralized Execution (CTDE) paradigm has gained significant attention in multi-agent reinforcement learning (MARL) and is the foundation of many recent algorithms. However, decentralized policies operate under partial observability and often yield suboptimal performance compared to centralized policies, while fully centralized approaches typically face scalability challenges as the number of agents increases.
  We propose Centralized Permutation Equivariant (CPE) learning, a centralized training and execution framework that employs a fully centralized policy to overcome these limitations. Our approach leverages a novel permutation equivariant architecture, Global-Local Permutation Equivariant (GLPE) networks, that is lightweight, scalable, and easy to implement. Experiments show that CPE integrates seamlessly with both value decomposition and actor-critic methods, substantially improving the performance of standard CTDE algorithms across cooperative benchmarks including MPE, SMAC, and RWARE, and matching the performance of state-of-the-art RWARE implementations.
<div id='section'>Paperid: <span id='pid'>800, <a href='https://arxiv.org/pdf/2508.10253.pdf' target='_blank'>https://arxiv.org/pdf/2508.10253.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Guanzi Yao, Heyao Liu, Linyan Dai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.10253">Multi-Agent Reinforcement Learning for Adaptive Resource Orchestration in Cloud-Native Clusters</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper addresses the challenges of high resource dynamism and scheduling complexity in cloud-native database systems. It proposes an adaptive resource orchestration method based on multi-agent reinforcement learning. The method introduces a heterogeneous role-based agent modeling mechanism. This allows different resource entities, such as compute nodes, storage nodes, and schedulers, to adopt distinct policy representations. These agents are better able to reflect diverse functional responsibilities and local environmental characteristics within the system. A reward-shaping mechanism is designed to integrate local observations with global feedback. This helps mitigate policy learning bias caused by incomplete state observations. By combining real-time local performance signals with global system value estimation, the mechanism improves coordination among agents and enhances policy convergence stability. A unified multi-agent training framework is developed and evaluated on a representative production scheduling dataset. Experimental results show that the proposed method outperforms traditional approaches across multiple key metrics. These include resource utilization, scheduling latency, policy convergence speed, system stability, and fairness. The results demonstrate strong generalization and practical utility. Across various experimental scenarios, the method proves effective in handling orchestration tasks with high concurrency, high-dimensional state spaces, and complex dependency relationships. This confirms its advantages in real-world, large-scale scheduling environments.
<div id='section'>Paperid: <span id='pid'>801, <a href='https://arxiv.org/pdf/2508.07001.pdf' target='_blank'>https://arxiv.org/pdf/2508.07001.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Myeung Suk Oh, Zhiyao Zhang, FNU Hairi, Alvaro Velasquez, Jia Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.07001">Consensus-based Decentralized Multi-agent Reinforcement Learning for Random Access Network Optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With wireless devices increasingly forming a unified smart network for seamless, user-friendly operations, random access (RA) medium access control (MAC) design is considered a key solution for handling unpredictable data traffic from multiple terminals. However, it remains challenging to design an effective RA-based MAC protocol to minimize collisions and ensure transmission fairness across the devices. While existing multi-agent reinforcement learning (MARL) approaches with centralized training and decentralized execution (CTDE) have been proposed to optimize RA performance, their reliance on centralized training and the significant overhead required for information collection can make real-world applications unrealistic. In this work, we adopt a fully decentralized MARL architecture, where policy learning does not rely on centralized tasks but leverages consensus-based information exchanges across devices. We design our MARL algorithm over an actor-critic (AC) network and propose exchanging only local rewards to minimize communication overhead. Furthermore, we provide a theoretical proof of global convergence for our approach. Numerical experiments show that our proposed MARL algorithm can significantly improve RA network performance compared to other baselines.
<div id='section'>Paperid: <span id='pid'>802, <a href='https://arxiv.org/pdf/2508.04811.pdf' target='_blank'>https://arxiv.org/pdf/2508.04811.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lin Jiang, Yu Yang, Guang Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.04811">HCRide: Harmonizing Passenger Fairness and Driver Preference for Human-Centered Ride-Hailing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Order dispatch systems play a vital role in ride-hailing services, which directly influence operator revenue, driver profit, and passenger experience. Most existing work focuses on improving system efficiency in terms of operator revenue, which may cause a bad experience for both passengers and drivers. Hence, in this work, we aim to design a human-centered ride-hailing system by considering both passenger fairness and driver preference without compromising the overall system efficiency. However, it is nontrivial to achieve this target due to the potential conflicts between passenger fairness and driver preference since optimizing one may sacrifice the other. To address this challenge, we design HCRide, a Human-Centered Ride-hailing system based on a novel multi-agent reinforcement learning algorithm called Harmonization-oriented Actor-Bi-Critic (Habic), which includes three major components (i.e., a multi-agent competition mechanism, a dynamic Actor network, and a Bi-Critic network) to optimize system efficiency and passenger fairness with driver preference consideration. We extensively evaluate our HCRide using two real-world ride-hailing datasets from Shenzhen and New York City. Experimental results show our HCRide effectively improves system efficiency by 2.02%, fairness by 5.39%, and driver preference by 10.21% compared to state-of-the-art baselines.
<div id='section'>Paperid: <span id='pid'>803, <a href='https://arxiv.org/pdf/2508.01561.pdf' target='_blank'>https://arxiv.org/pdf/2508.01561.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zijian Guo, Ä°lker IÅÄ±k, H. M. Sabbir Ahmad, Wenchao Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.01561">One Subgoal at a Time: Zero-Shot Generalization to Arbitrary Linear Temporal Logic Requirements in Multi-Task Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generalizing to complex and temporally extended task objectives and safety constraints remains a critical challenge in reinforcement learning (RL). Linear temporal logic (LTL) offers a unified formalism to specify such requirements, yet existing methods are limited in their abilities to handle nested long-horizon tasks and safety constraints, and cannot identify situations when a subgoal is not satisfiable and an alternative should be sought. In this paper, we introduce GenZ-LTL, a method that enables zero-shot generalization to arbitrary LTL specifications. GenZ-LTL leverages the structure of BÃ¼chi automata to decompose an LTL task specification into sequences of reach-avoid subgoals. Contrary to the current state-of-the-art method that conditions on subgoal sequences, we show that it is more effective to achieve zero-shot generalization by solving these reach-avoid problems \textit{one subgoal at a time} through proper safe RL formulations. In addition, we introduce a novel subgoal-induced observation reduction technique that can mitigate the exponential complexity of subgoal-state combinations under realistic assumptions. Empirical results show that GenZ-LTL substantially outperforms existing methods in zero-shot generalization to unseen LTL specifications.
<div id='section'>Paperid: <span id='pid'>804, <a href='https://arxiv.org/pdf/2508.01561.pdf' target='_blank'>https://arxiv.org/pdf/2508.01561.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zijian Guo, Ä°lker IÅÄ±k, H. M. Sabbir Ahmad, Wenchao Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.01561">One Subgoal at a Time: Zero-Shot Generalization to Arbitrary Linear Temporal Logic Requirements in Multi-Task Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generalizing to complex and temporally extended task objectives and safety constraints remains a critical challenge in reinforcement learning (RL). Linear temporal logic (LTL) offers a unified formalism to specify such requirements, yet existing methods are limited in their abilities to handle nested long-horizon tasks and safety constraints, and cannot identify situations when a subgoal is not satisfiable and an alternative should be sought. In this paper, we introduce GenZ-LTL, a method that enables zero-shot generalization to arbitrary LTL specifications. GenZ-LTL leverages the structure of BÃ¼chi automata to decompose an LTL task specification into sequences of reach-avoid subgoals. Contrary to the current state-of-the-art method that conditions on subgoal sequences, we show that it is more effective to achieve zero-shot generalization by solving these reach-avoid problems \textit{one subgoal at a time} through proper safe RL formulations. In addition, we introduce a novel subgoal-induced observation reduction technique that can mitigate the exponential complexity of subgoal-state combinations under realistic assumptions. Empirical results show that GenZ-LTL substantially outperforms existing methods in zero-shot generalization to unseen LTL specifications.
<div id='section'>Paperid: <span id='pid'>805, <a href='https://arxiv.org/pdf/2507.19151.pdf' target='_blank'>https://arxiv.org/pdf/2507.19151.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Michael Amir, Guang Yang, Zhan Gao, Keisuke Okumura, Heedo Woo, Amanda Prorok
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.19151">ReCoDe: Reinforcement Learning-based Dynamic Constraint Design for Multi-Agent Coordination</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Constraint-based optimization is a cornerstone of robotics, enabling the design of controllers that reliably encode task and safety requirements such as collision avoidance or formation adherence. However, handcrafted constraints can fail in multi-agent settings that demand complex coordination. We introduce ReCoDe--Reinforcement-based Constraint Design--a decentralized, hybrid framework that merges the reliability of optimization-based controllers with the adaptability of multi-agent reinforcement learning. Rather than discarding expert controllers, ReCoDe improves them by learning additional, dynamic constraints that capture subtler behaviors, for example, by constraining agent movements to prevent congestion in cluttered scenarios. Through local communication, agents collectively constrain their allowed actions to coordinate more effectively under changing conditions. In this work, we focus on applications of ReCoDe to multi-agent navigation tasks requiring intricate, context-based movements and consensus, where we show that it outperforms purely handcrafted controllers, other hybrid approaches, and standard MARL baselines. We give empirical (real robot) and theoretical evidence that retaining a user-defined controller, even when it is imperfect, is more efficient than learning from scratch, especially because ReCoDe can dynamically change the degree to which it relies on this controller.
<div id='section'>Paperid: <span id='pid'>806, <a href='https://arxiv.org/pdf/2506.05527.pdf' target='_blank'>https://arxiv.org/pdf/2506.05527.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Caroline Wang, Di Yang Shi, Elad Liebman, Ishan Durugkar, Arrasy Rahman, Peter Stone
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.05527">Sequence Modeling for N-Agent Ad Hoc Teamwork</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>N-agent ad hoc teamwork (NAHT) is a newly introduced challenge in multi-agent reinforcement learning, where controlled subteams of varying sizes must dynamically collaborate with varying numbers and types of unknown teammates without pre-coordination. The existing learning algorithm (POAM) considers only independent learning for its flexibility in dealing with a changing number of agents. However, independent learning fails to fully capture the inter-agent dynamics essential for effective collaboration. Based on our observation that transformers deal effectively with sequences with varying lengths and have been shown to be highly effective for a variety of machine learning problems, this work introduces a centralized, transformer-based method for N-agent ad hoc teamwork. Our proposed approach incorporates historical observations and actions of all controlled agents, enabling optimal responses to diverse and unseen teammates in partially observable environments. Empirical evaluation on a StarCraft II task demonstrates that MAT-NAHT outperforms POAM, achieving superior sample efficiency and generalization, without auxiliary agent-modeling objectives.
<div id='section'>Paperid: <span id='pid'>807, <a href='https://arxiv.org/pdf/2505.18433.pdf' target='_blank'>https://arxiv.org/pdf/2505.18433.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiyao Zhang, Myeung Suk Oh, FNU Hairi, Ziyue Luo, Alvaro Velasquez, Jia Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.18433">Finite-Time Global Optimality Convergence in Deep Neural Actor-Critic Methods for Decentralized Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Actor-critic methods for decentralized multi-agent reinforcement learning (MARL) facilitate collaborative optimal decision making without centralized coordination, thus enabling a wide range of applications in practice. To date, however, most theoretical convergence studies for existing actor-critic decentralized MARL methods are limited to the guarantee of a stationary solution under the linear function approximation. This leaves a significant gap between the highly successful use of deep neural actor-critic for decentralized MARL in practice and the current theoretical understanding. To bridge this gap, in this paper, we make the first attempt to develop a deep neural actor-critic method for decentralized MARL, where both the actor and critic components are inherently non-linear. We show that our proposed method enjoys a global optimality guarantee with a finite-time convergence rate of O(1/T), where T is the total iteration times. This marks the first global convergence result for deep neural actor-critic methods in the MARL literature. We also conduct extensive numerical experiments, which verify our theoretical results.
<div id='section'>Paperid: <span id='pid'>808, <a href='https://arxiv.org/pdf/2505.12811.pdf' target='_blank'>https://arxiv.org/pdf/2505.12811.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wei-Chen Liao, Ti-Rong Wu, I-Chen Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.12811">Dynamic Sight Range Selection in Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent reinforcement Learning (MARL) is often challenged by the sight range dilemma, where agents either receive insufficient or excessive information from their environment. In this paper, we propose a novel method, called Dynamic Sight Range Selection (DSR), to address this issue. DSR utilizes an Upper Confidence Bound (UCB) algorithm and dynamically adjusts the sight range during training. Experiment results show several advantages of using DSR. First, we demonstrate using DSR achieves better performance in three common MARL environments, including Level-Based Foraging (LBF), Multi-Robot Warehouse (RWARE), and StarCraft Multi-Agent Challenge (SMAC). Second, our results show that DSR consistently improves performance across multiple MARL algorithms, including QMIX and MAPPO. Third, DSR offers suitable sight ranges for different training steps, thereby accelerating the training process. Finally, DSR provides additional interpretability by indicating the optimal sight range used during training. Unlike existing methods that rely on global information or communication mechanisms, our approach operates solely based on the individual sight ranges of agents. This approach offers a practical and efficient solution to the sight range dilemma, making it broadly applicable to real-world complex environments.
<div id='section'>Paperid: <span id='pid'>809, <a href='https://arxiv.org/pdf/2505.08448.pdf' target='_blank'>https://arxiv.org/pdf/2505.08448.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yanggang Xu, Weijie Hong, Jirong Zha, Geng Chen, Jianfeng Zheng, Chen-Chun Hsia, Xinlei Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.08448">Scalable UAV Multi-Hop Networking via Multi-Agent Reinforcement Learning with Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In disaster scenarios, establishing robust emergency communication networks is critical, and unmanned aerial vehicles (UAVs) offer a promising solution to rapidly restore connectivity. However, organizing UAVs to form multi-hop networks in large-scale dynamic environments presents significant challenges, including limitations in algorithmic scalability and the vast exploration space required for coordinated decision-making. To address these issues, we propose MRLMN, a novel framework that integrates multi-agent reinforcement learning (MARL) and large language models (LLMs) to jointly optimize UAV agents toward achieving optimal networking performance. The framework incorporates a grouping strategy with reward decomposition to enhance algorithmic scalability and balance decision-making across UAVs. In addition, behavioral constraints are applied to selected key UAVs to improve the robustness of the network. Furthermore, the framework integrates LLM agents, leveraging knowledge distillation to transfer their high-level decision-making capabilities to MARL agents. This enhances both the efficiency of exploration and the overall training process. In the distillation module, a Hungarian algorithm-based matching scheme is applied to align the decision outputs of the LLM and MARL agents and define the distillation loss. Extensive simulation results validate the effectiveness of our approach, demonstrating significant improvements in network performance, including enhanced coverage and communication quality.
<div id='section'>Paperid: <span id='pid'>810, <a href='https://arxiv.org/pdf/2503.22958.pdf' target='_blank'>https://arxiv.org/pdf/2503.22958.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Supriyo Maji, Linran Zhao, Souradip Poddar, David Z. Pan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.22958">Late Breaking Results: Breaking Symmetry- Unconventional Placement of Analog Circuits using Multi-Level Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Layout-dependent effects (LDEs) significantly impact analog circuit performance. Traditionally, designers have relied on symmetric placement of circuit components to mitigate variations caused by LDEs. However, due to non-linear nature of these effects, conventional methods often fall short. We propose an objective-driven, multi-level, multi-agent Q-learning framework to explore unconventional design space of analog layout, opening new avenues for optimizing analog circuit performance. Our approach achieves better variation performance than the state-of-the-art layout techniques. Notably, this is the first application of multi-agent RL in analog layout automation. The proposed approach is compared with non-ML approach based on simulated annealing.
<div id='section'>Paperid: <span id='pid'>811, <a href='https://arxiv.org/pdf/2503.07129.pdf' target='_blank'>https://arxiv.org/pdf/2503.07129.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Deuksin Kwon, Jiwon Hae, Emma Clift, Daniel Shamsoddini, Jonathan Gratch, Gale M. Lucas
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.07129">ASTRA: A Negotiation Agent with Adaptive and Strategic Reasoning via Tool-integrated Action for Dynamic Offer Optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Negotiation requires dynamically balancing self-interest and cooperation within the flow of conversation to maximize one's own utility. Yet, existing agents struggle due to bounded rationality in human data, low adaptability to counterpart behavior, and limited strategic reasoning. To address this, we introduce principle-driven negotiation agents, powered by ASTRA, a novel framework for turn-level offer optimization grounded in two core principles: opponent modeling and Tit-for-Tat reciprocity. ASTRA operates in three stages: (1) interpreting counterpart behavior, (2) optimizing counteroffers via a tool-integrated action with a linear programming (LP) solver, and (3) selecting offers based on strategy assessment and the partner's acceptance probability. Through simulations and human evaluations, our agent effectively adapts to an opponent's shifting stance and achieves favorable outcomes through enhanced adaptability and strategic reasoning. Beyond enhancing negotiation performance, it also serves as a powerful coaching tool, offering interpretable strategic feedback and optimal offer recommendations beyond human bounded rationality, with its potential further validated through human evaluation.
<div id='section'>Paperid: <span id='pid'>812, <a href='https://arxiv.org/pdf/2503.07129.pdf' target='_blank'>https://arxiv.org/pdf/2503.07129.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Deuksin Kwon, Jiwon Hae, Emma Clift, Daniel Shamsoddini, Jonathan Gratch, Gale M. Lucas
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.07129">ASTRA: A Negotiation Agent with Adaptive and Strategic Reasoning via Tool-integrated Action for Dynamic Offer Optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Negotiation requires dynamically balancing self-interest and cooperation within the flow of conversation to maximize one's own utility. Yet, existing agents struggle due to bounded rationality in human data, low adaptability to counterpart behavior, and limited strategic reasoning. To address this, we introduce principle-driven negotiation agents, powered by ASTRA, a novel framework for turn-level offer optimization grounded in two core principles: opponent modeling and Tit-for-Tat reciprocity. ASTRA operates in three stages: (1) interpreting counterpart behavior, (2) optimizing counteroffers via a tool-integrated action with a linear programming (LP) solver, and (3) selecting offers based on strategy assessment and the partner's acceptance probability. Through simulations and human evaluations, our agent effectively adapts to an opponent's shifting stance and achieves favorable outcomes through enhanced adaptability and strategic reasoning. Beyond enhancing negotiation performance, it also serves as a powerful coaching tool, offering interpretable strategic feedback and optimal offer recommendations beyond human bounded rationality, with its potential further validated through human evaluation.
<div id='section'>Paperid: <span id='pid'>813, <a href='https://arxiv.org/pdf/2501.08234.pdf' target='_blank'>https://arxiv.org/pdf/2501.08234.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Enrique Adrian Villarrubia-Martin, Luis Rodriguez-Benitez, David MuÃ±oz-Valero, Giovanni Montana, Luis Jimenez-Linares
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.08234">Dynamic Pricing in High-Speed Railways Using Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper addresses a critical challenge in the high-speed passenger railway industry: designing effective dynamic pricing strategies in the context of competing and cooperating operators. To address this, a multi-agent reinforcement learning (MARL) framework based on a non-zero-sum Markov game is proposed, incorporating random utility models to capture passenger decision making. Unlike prior studies in areas such as energy, airlines, and mobile networks, dynamic pricing for railway systems using deep reinforcement learning has received limited attention. A key contribution of this paper is a parametrisable and versatile reinforcement learning simulator designed to model a variety of railway network configurations and demand patterns while enabling realistic, microscopic modelling of user behaviour, called RailPricing-RL. This environment supports the proposed MARL framework, which models heterogeneous agents competing to maximise individual profits while fostering cooperative behaviour to synchronise connecting services. Experimental results validate the framework, demonstrating how user preferences affect MARL performance and how pricing policies influence passenger choices, utility, and overall system dynamics. This study provides a foundation for advancing dynamic pricing strategies in railway systems, aligning profitability with system-wide efficiency, and supporting future research on optimising pricing policies.
<div id='section'>Paperid: <span id='pid'>814, <a href='https://arxiv.org/pdf/2501.08234.pdf' target='_blank'>https://arxiv.org/pdf/2501.08234.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Enrique Adrian Villarrubia-Martin, Luis Rodriguez-Benitez, David MuÃ±oz-Valero, Giovanni Montana, Luis Jimenez-Linares
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.08234">Dynamic Pricing in High-Speed Railways Using Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper addresses a critical challenge in the high-speed passenger railway industry: designing effective dynamic pricing strategies in the context of competing and cooperating operators. To address this, a multi-agent reinforcement learning (MARL) framework based on a non-zero-sum Markov game is proposed, incorporating random utility models to capture passenger decision making. Unlike prior studies in areas such as energy, airlines, and mobile networks, dynamic pricing for railway systems using deep reinforcement learning has received limited attention. A key contribution of this paper is a parametrisable and versatile reinforcement learning simulator designed to model a variety of railway network configurations and demand patterns while enabling realistic, microscopic modelling of user behaviour, called RailPricing-RL. This environment supports the proposed MARL framework, which models heterogeneous agents competing to maximise individual profits while fostering cooperative behaviour to synchronise connecting services. Experimental results validate the framework, demonstrating how user preferences affect MARL performance and how pricing policies influence passenger choices, utility, and overall system dynamics. This study provides a foundation for advancing dynamic pricing strategies in railway systems, aligning profitability with system-wide efficiency, and supporting future research on optimising pricing policies.
<div id='section'>Paperid: <span id='pid'>815, <a href='https://arxiv.org/pdf/2501.08234.pdf' target='_blank'>https://arxiv.org/pdf/2501.08234.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Enrique Adrian Villarrubia-Martin, Luis Rodriguez-Benitez, David Muñoz-Valero, Giovanni Montana, Luis Jimenez-Linares
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.08234">Dynamic Pricing in High-Speed Railways Using Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper addresses a critical challenge in the high-speed passenger railway industry: designing effective dynamic pricing strategies in the context of competing and cooperating operators. To address this, a multi-agent reinforcement learning (MARL) framework based on a non-zero-sum Markov game is proposed, incorporating random utility models to capture passenger decision making. Unlike prior studies in areas such as energy, airlines, and mobile networks, dynamic pricing for railway systems using deep reinforcement learning has received limited attention. A key contribution of this paper is a parametrisable and versatile reinforcement learning simulator designed to model a variety of railway network configurations and demand patterns while enabling realistic, microscopic modelling of user behaviour, called RailPricing-RL. This environment supports the proposed MARL framework, which models heterogeneous agents competing to maximise individual profits while fostering cooperative behaviour to synchronise connecting services. Experimental results validate the framework, demonstrating how user preferences affect MARL performance and how pricing policies influence passenger choices, utility, and overall system dynamics. This study provides a foundation for advancing dynamic pricing strategies in railway systems, aligning profitability with system-wide efficiency, and supporting future research on optimising pricing policies.
<div id='section'>Paperid: <span id='pid'>816, <a href='https://arxiv.org/pdf/2501.08234.pdf' target='_blank'>https://arxiv.org/pdf/2501.08234.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Enrique Adrian Villarrubia-Martin, Luis Rodriguez-Benitez, David MuÃ±oz-Valero, Giovanni Montana, Luis Jimenez-Linares
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.08234">Dynamic Pricing in High-Speed Railways Using Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper addresses a critical challenge in the high-speed passenger railway industry: designing effective dynamic pricing strategies in the context of competing and cooperating operators. To address this, a multi-agent reinforcement learning (MARL) framework based on a non-zero-sum Markov game is proposed, incorporating random utility models to capture passenger decision making. Unlike prior studies in areas such as energy, airlines, and mobile networks, dynamic pricing for railway systems using deep reinforcement learning has received limited attention. A key contribution of this paper is a parametrisable and versatile reinforcement learning simulator designed to model a variety of railway network configurations and demand patterns while enabling realistic, microscopic modelling of user behaviour, called RailPricing-RL. This environment supports the proposed MARL framework, which models heterogeneous agents competing to maximise individual profits while fostering cooperative behaviour to synchronise connecting services. Experimental results validate the framework, demonstrating how user preferences affect MARL performance and how pricing policies influence passenger choices, utility, and overall system dynamics. This study provides a foundation for advancing dynamic pricing strategies in railway systems, aligning profitability with system-wide efficiency, and supporting future research on optimising pricing policies.
<div id='section'>Paperid: <span id='pid'>817, <a href='https://arxiv.org/pdf/2501.08234.pdf' target='_blank'>https://arxiv.org/pdf/2501.08234.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Enrique Adrian Villarrubia-Martin, Luis Rodriguez-Benitez, David Muñoz-Valero, Giovanni Montana, Luis Jimenez-Linares
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.08234">Dynamic Pricing in High-Speed Railways Using Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper addresses a critical challenge in the high-speed passenger railway industry: designing effective dynamic pricing strategies in the context of competing and cooperating operators. To address this, a multi-agent reinforcement learning (MARL) framework based on a non-zero-sum Markov game is proposed, incorporating random utility models to capture passenger decision making. Unlike prior studies in areas such as energy, airlines, and mobile networks, dynamic pricing for railway systems using deep reinforcement learning has received limited attention. A key contribution of this paper is a parametrisable and versatile reinforcement learning simulator designed to model a variety of railway network configurations and demand patterns while enabling realistic, microscopic modelling of user behaviour, called RailPricing-RL. This environment supports the proposed MARL framework, which models heterogeneous agents competing to maximise individual profits while fostering cooperative behaviour to synchronise connecting services. Experimental results validate the framework, demonstrating how user preferences affect MARL performance and how pricing policies influence passenger choices, utility, and overall system dynamics. This study provides a foundation for advancing dynamic pricing strategies in railway systems, aligning profitability with system-wide efficiency, and supporting future research on optimising pricing policies.
<div id='section'>Paperid: <span id='pid'>818, <a href='https://arxiv.org/pdf/2501.02174.pdf' target='_blank'>https://arxiv.org/pdf/2501.02174.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Peihong Yu, Manav Mishra, Syed Zaidi, Pratap Tokekar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.02174">TACTIC: Task-Agnostic Contrastive pre-Training for Inter-Agent Communication</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The "sight range dilemma" in cooperative Multi-Agent Reinforcement Learning (MARL) presents a significant challenge: limited observability hinders team coordination, while extensive sight ranges lead to distracted attention and reduced performance. While communication can potentially address this issue, existing methods often struggle to generalize across different sight ranges, limiting their effectiveness. We propose TACTIC, Task-Agnostic Contrastive pre-Training strategy Inter-Agent Communication. TACTIC is an adaptive communication mechanism that enhances agent coordination even when the sight range during execution is vastly different from that during training. The communication mechanism encodes messages and integrates them with local observations, generating representations grounded in the global state using contrastive learning. By learning to generate and interpret messages that capture important information about the whole environment, TACTIC enables agents to effectively "see" more through communication, regardless of their sight ranges. We comprehensively evaluate TACTIC on the SMACv2 benchmark across various scenarios with broad sight ranges. The results demonstrate that TACTIC consistently outperforms traditional state-of-the-art MARL techniques with and without communication, in terms of generalizing to sight ranges different from those seen in training, particularly in cases of extremely limited or extensive observability.
<div id='section'>Paperid: <span id='pid'>819, <a href='https://arxiv.org/pdf/2411.13934.pdf' target='_blank'>https://arxiv.org/pdf/2411.13934.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yancheng Liang, Daphne Chen, Abhishek Gupta, Simon S. Du, Natasha Jaques
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.13934">Learning to Cooperate with Humans using Generative Agents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Training agents that can coordinate zero-shot with humans is a key mission in multi-agent reinforcement learning (MARL). Current algorithms focus on training simulated human partner policies which are then used to train a Cooperator agent. The simulated human is produced either through behavior cloning over a dataset of human cooperation behavior, or by using MARL to create a population of simulated agents. However, these approaches often struggle to produce a Cooperator that can coordinate well with real humans, since the simulated humans fail to cover the diverse strategies and styles employed by people in the real world. We show \emph{learning a generative model of human partners} can effectively address this issue. Our model learns a latent variable representation of the human that can be regarded as encoding the human's unique strategy, intention, experience, or style. This generative model can be flexibly trained from any (human or neural policy) agent interaction data. By sampling from the latent space, we can use the generative model to produce different partners to train Cooperator agents. We evaluate our method -- \textbf{G}enerative \textbf{A}gent \textbf{M}odeling for \textbf{M}ulti-agent \textbf{A}daptation (GAMMA) -- on Overcooked, a challenging cooperative cooking game that has become a standard benchmark for zero-shot coordination. We conduct an evaluation with real human teammates, and the results show that GAMMA consistently improves performance, whether the generative model is trained on simulated populations or human datasets. Further, we propose a method for posterior sampling from the generative model that is biased towards the human data, enabling us to efficiently improve performance with only a small amount of expensive human interaction data.
<div id='section'>Paperid: <span id='pid'>820, <a href='https://arxiv.org/pdf/2410.16024.pdf' target='_blank'>https://arxiv.org/pdf/2410.16024.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yue Deng, Weiyu Ma, Yuxin Fan, Ruyi Song, Yin Zhang, Haifeng Zhang, Jian Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.16024">SMAC-R1: The Emergence of Intelligence in Decision-Making Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>StarCraft Multi-Agent Challenge (SMAC) has been one of the most commonly used experimental environments in multi-agent reinforcement learning (MARL), where the specific task is to control a set number of allied units to defeat enemy forces. Traditional MARL algorithms often require interacting with the environment for millions of steps to train a parametric model, of which the resulting policies are typically non-interpretable with weak transferability. In this paper, we introduce SMAC-R1 which is based on the Qwen2.5-7B-Base LLM distilled from DeepSeek-Coder-v2.5-236B. Similar to online reinforcement learning after behavior cloning in offline learning process, in our pipeline, agents leverage the DeepSeek LLM to generate decision tree code by providing task descriptions, and the agents are further self-reflected using feedback from the rewards provided by the environment. Based on that, we augment the generated scripts to fine-tune a small LLM, Qwen2.5-7B-Base, to distill the decision-making ability via Supervised Fine-Tuning (SFT) and enhance the script generation ability by the Group Relative Policy Optimization (GRPO) algorithm. We conduct experiments in the original 23 SMAC tasks and 10 newly-designed tasks to demonstrate that our method can produce high-quality, interpretable decision trees with minimal environmental exploration. Moreover, these scripts exhibit strong transferability, successfully applying to homogeneous SMAC environments without modification. We believe this approach offers a new direction for solving decision-making tasks and domain-specific LLM training pipelines in the future.
<div id='section'>Paperid: <span id='pid'>821, <a href='https://arxiv.org/pdf/2410.05673.pdf' target='_blank'>https://arxiv.org/pdf/2410.05673.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fivos Kalogiannis, Jingming Yan, Ioannis Panageas
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.05673">Learning Equilibria in Adversarial Team Markov Games: A Nonconvex-Hidden-Concave Min-Max Optimization Problem</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We study the problem of learning a Nash equilibrium (NE) in Markov games which is a cornerstone in multi-agent reinforcement learning (MARL). In particular, we focus on infinite-horizon adversarial team Markov games (ATMGs) in which agents that share a common reward function compete against a single opponent, the adversary. These games unify two-player zero-sum Markov games and Markov potential games, resulting in a setting that encompasses both collaboration and competition. Kalogiannis et al. (2023a) provided an efficient equilibrium computation algorithm for ATMGs which presumes knowledge of the reward and transition functions and has no sample complexity guarantees. We contribute a learning algorithm that utilizes MARL policy gradient methods with iteration and sample complexity that is polynomial in the approximation error $Îµ$ and the natural parameters of the ATMG, resolving the main caveats of the solution by (Kalogiannis et al., 2023a). It is worth noting that previously, the existence of learning algorithms for NE was known for Markov two-player zero-sum and potential games but not for ATMGs.
  Seen through the lens of min-max optimization, computing a NE in these games consists a nonconvex-nonconcave saddle-point problem. Min-max optimization has received extensive study. Nevertheless, the case of nonconvex-nonconcave landscapes remains elusive: in full generality, finding saddle-points is computationally intractable (Daskalakis et al., 2021). We circumvent the aforementioned intractability by developing techniques that exploit the hidden structure of the objective function via a nonconvex-concave reformulation. However, this introduces the challenge of a feasibility set with coupled constraints. We tackle these challenges by establishing novel techniques for optimizing weakly-smooth nonconvex functions, extending the framework of (Devolder et al., 2014).
<div id='section'>Paperid: <span id='pid'>822, <a href='https://arxiv.org/pdf/2409.19831.pdf' target='_blank'>https://arxiv.org/pdf/2409.19831.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhengran Ji, Lingyu Zhang, Paul Sajda, Boyuan Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.19831">Enabling Multi-Robot Collaboration from Single-Human Guidance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Learning collaborative behaviors is essential for multi-agent systems. Traditionally, multi-agent reinforcement learning solves this implicitly through a joint reward and centralized observations, assuming collaborative behavior will emerge. Other studies propose to learn from demonstrations of a group of collaborative experts. Instead, we propose an efficient and explicit way of learning collaborative behaviors in multi-agent systems by leveraging expertise from only a single human. Our insight is that humans can naturally take on various roles in a team. We show that agents can effectively learn to collaborate by allowing a human operator to dynamically switch between controlling agents for a short period and incorporating a human-like theory-of-mind model of teammates. Our experiments showed that our method improves the success rate of a challenging collaborative hide-and-seek task by up to 58% with only 40 minutes of human guidance. We further demonstrate our findings transfer to the real world by conducting multi-robot experiments.
<div id='section'>Paperid: <span id='pid'>823, <a href='https://arxiv.org/pdf/2408.13567.pdf' target='_blank'>https://arxiv.org/pdf/2408.13567.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mingliang Zhang, Sichang Su, Chengyang He, Guillaume Sartoretti
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.13567">Hybrid Training for Enhanced Multi-task Generalization in Multi-agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In multi-agent reinforcement learning (MARL), achieving multi-task generalization to diverse agents and objectives presents significant challenges. Existing online MARL algorithms primarily focus on single-task performance, but their lack of multi-task generalization capabilities typically results in substantial computational waste and limited real-life applicability. Meanwhile, existing offline multi-task MARL approaches are heavily dependent on data quality, often resulting in poor performance on unseen tasks. In this paper, we introduce HyGen, a novel hybrid MARL framework, Hybrid Training for Enhanced Multi-Task Generalization, which integrates online and offline learning to ensure both multi-task generalization and training efficiency. Specifically, our framework extracts potential general skills from offline multi-task datasets. We then train policies to select the optimal skills under the centralized training and decentralized execution paradigm (CTDE). During this stage, we utilize a replay buffer that integrates both offline data and online interactions. We empirically demonstrate that our framework effectively extracts and refines general skills, yielding impressive generalization to unseen tasks. Comparative analyses on the StarCraft multi-agent challenge show that HyGen outperforms a wide range of existing solely online and offline methods.
<div id='section'>Paperid: <span id='pid'>824, <a href='https://arxiv.org/pdf/2408.10858.pdf' target='_blank'>https://arxiv.org/pdf/2408.10858.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haozhe Ma, Zhengding Luo, Thanh Vinh Vo, Kuankuan Sima, Tze-Yun Leong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.10858">Centralized Reward Agent for Knowledge Sharing and Transfer in Multi-Task Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reward shaping is effective in addressing the sparse-reward challenge in reinforcement learning by providing immediate feedback through auxiliary informative rewards. Based on the reward shaping strategy, we propose a novel multi-task reinforcement learning framework that integrates a centralized reward agent (CRA) and multiple distributed policy agents. The CRA functions as a knowledge pool, which aims to distill knowledge from various tasks and distribute it to individual policy agents to improve learning efficiency. Specifically, the shaped rewards serve as a straightforward metric to encode knowledge. This framework not only enhances knowledge sharing across established tasks but also adapts to new tasks by transferring meaningful reward signals. We validate the proposed method on both discrete and continuous domains, including the representative meta world benchmark, demonstrating its robustness in multi-task sparse-reward settings and its effective transferability to unseen tasks.
<div id='section'>Paperid: <span id='pid'>825, <a href='https://arxiv.org/pdf/2408.07397.pdf' target='_blank'>https://arxiv.org/pdf/2408.07397.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhuohui Zhang, Bin He, Bin Cheng, Gang Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.07397">Bridging Training and Execution via Dynamic Directed Graph-Based Communication in Cooperative Multi-Agent Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent systems must learn to communicate and understand interactions between agents to achieve cooperative goals in partially observed tasks. However, existing approaches lack a dynamic directed communication mechanism and rely on global states, thus diminishing the role of communication in centralized training. Thus, we propose the Transformer-based graph coarsening network (TGCNet), a novel multi-agent reinforcement learning (MARL) algorithm. TGCNet learns the topological structure of a dynamic directed graph to represent the communication policy and integrates graph coarsening networks to approximate the representation of global state during training. It also utilizes the Transformer decoder for feature extraction during execution. Experiments on multiple cooperative MARL benchmarks demonstrate state-of-the-art performance compared to popular MARL algorithms. Further ablation studies validate the effectiveness of our dynamic directed graph communication mechanism and graph coarsening networks.
<div id='section'>Paperid: <span id='pid'>826, <a href='https://arxiv.org/pdf/2408.02380.pdf' target='_blank'>https://arxiv.org/pdf/2408.02380.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>JÃ©rÃ´me Arjonilla, Abdallah Saffidine, Tristan Cazenave
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.02380">Perfect Information Monte Carlo with Postponing Reasoning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Imperfect information games, such as Bridge and Skat, present challenges due to state-space explosion and hidden information, posing formidable obstacles for search algorithms. Determinization-based algorithms offer a resolution by sampling hidden information and solving the game in a perfect information setting, facilitating rapid and effective action estimation. However, transitioning to perfect information introduces challenges, notably one called strategy fusion.This research introduces `Extended Perfect Information Monte Carlo' (EPIMC), an online algorithm inspired by the state-of-the-art determinization-based approach Perfect Information Monte Carlo (PIMC). EPIMC enhances the capabilities of PIMC by postponing the perfect information resolution, reducing alleviating issues related to strategy fusion. However, the decision to postpone the leaf evaluator introduces novel considerations, such as the interplay between prior levels of reasoning and the newly deferred resolution. In our empirical analysis, we investigate the performance of EPIMC across a range of games, with a particular focus on those characterized by varying degrees of strategy fusion. Our results demonstrate notable performance enhancements, particularly in games where strategy fusion significantly impacts gameplay. Furthermore, our research contributes to the theoretical foundation of determinization-based algorithms addressing challenges associated with strategy fusion.%, thereby enhancing our understanding of these algorithms within the context of imperfect information game scenarios.
<div id='section'>Paperid: <span id='pid'>827, <a href='https://arxiv.org/pdf/2407.20441.pdf' target='_blank'>https://arxiv.org/pdf/2407.20441.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>NicolÃ² Dal Fabbro, Arman Adibi, Aritra Mitra, George J. Pappas
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.20441">Finite-Time Analysis of Asynchronous Multi-Agent TD Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent research endeavours have theoretically shown the beneficial effect of cooperation in multi-agent reinforcement learning (MARL). In a setting involving $N$ agents, this beneficial effect usually comes in the form of an $N$-fold linear convergence speedup, i.e., a reduction - proportional to $N$ - in the number of iterations required to reach a certain convergence precision. In this paper, we show for the first time that this speedup property also holds for a MARL framework subject to asynchronous delays in the local agents' updates. In particular, we consider a policy evaluation problem in which multiple agents cooperate to evaluate a common policy by communicating with a central aggregator. In this setting, we study the finite-time convergence of \texttt{AsyncMATD}, an asynchronous multi-agent temporal difference (TD) learning algorithm in which agents' local TD update directions are subject to asynchronous bounded delays. Our main contribution is providing a finite-time analysis of \texttt{AsyncMATD}, for which we establish a linear convergence speedup while highlighting the effect of time-varying asynchronous delays on the resulting convergence rate.
<div id='section'>Paperid: <span id='pid'>828, <a href='https://arxiv.org/pdf/2407.20351.pdf' target='_blank'>https://arxiv.org/pdf/2407.20351.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mingyang Liu, Gabriele Farina, Asuman Ozdaglar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.20351">LiteEFG: An Efficient Python Library for Solving Extensive-form Games</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>LiteEFG is an efficient library with easy-to-use Python bindings, which can solve multiplayer extensive-form games (EFGs). LiteEFG enables the user to express computation graphs in Python to define updates on the game tree structure. The graph is then executed by the C++ backend, leading to significant speedups compared to running the algorithm in Python. Moreover, in LiteEFG, the user needs to only specify the computation graph of the update rule in a decision node of the game, and LiteEFG will automatically distribute the update rule to each decision node and handle the structure of the imperfect-information game.
<div id='section'>Paperid: <span id='pid'>829, <a href='https://arxiv.org/pdf/2407.13466.pdf' target='_blank'>https://arxiv.org/pdf/2407.13466.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Elie Aljalbout, Nikolaos Sotirakis, Patrick van der Smagt, Maximilian Karl, Nutan Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.13466">LIMT: Language-Informed Multi-Task Visual World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Most recent successes in robot reinforcement learning involve learning a specialized single-task agent.
  However, robots capable of performing multiple tasks can be much more valuable in real-world applications.
  Multi-task reinforcement learning can be very challenging due to the increased sample complexity and the potentially conflicting task objectives.
  Previous work on this topic is dominated by model-free approaches.
  The latter can be very sample inefficient even when learning specialized single-task agents.
  In this work, we focus on model-based multi-task reinforcement learning.
  We propose a method for learning multi-task visual world models, leveraging pre-trained language models to extract semantically meaningful task representations.
  These representations are used by the world model and policy to reason about task similarity in dynamics and behavior.
  Our results highlight the benefits of using language-driven task representations for world models and a clear advantage of model-based multi-task learning over the more common model-free paradigm.
<div id='section'>Paperid: <span id='pid'>830, <a href='https://arxiv.org/pdf/2406.11653.pdf' target='_blank'>https://arxiv.org/pdf/2406.11653.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Min Hua, Dong Chen, Kun Jiang, Fanggang Zhang, Jinhai Wang, Bo Wang, Quan Zhou, Hongming Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.11653">Communication-Efficient MARL for Platoon Stability and Energy-efficiency Co-optimization in Cooperative Adaptive Cruise Control of CAVs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cooperative adaptive cruise control (CACC) has been recognized as a fundamental function of autonomous driving, in which platoon stability and energy efficiency are outstanding challenges that are difficult to accommodate in real-world operations. This paper studied the CACC of connected and autonomous vehicles (CAVs) based on the multi-agent reinforcement learning algorithm (MARL) to optimize platoon stability and energy efficiency simultaneously. The optimal use of communication bandwidth is the key to guaranteeing learning performance in real-world driving, and thus this paper proposes a communication-efficient MARL by incorporating the quantified stochastic gradient descent (QSGD) and a binary differential consensus (BDC) method into a fully-decentralized MARL framework. We benchmarked the performance of our proposed BDC-MARL algorithm against several several non-communicative andcommunicative MARL algorithms, e.g., IA2C, FPrint, and DIAL, through the evaluation of platoon stability, fuel economy, and driving comfort. Our results show that BDC-MARL achieved the highest energy savings, improving by up to 5.8%, with an average velocity of 15.26 m/s and an inter-vehicle spacing of 20.76 m. In addition, we conducted different information-sharing analyses to assess communication efficacy, along with sensitivity analyses and scalability tests with varying platoon sizes. The practical effectiveness of our approach is further demonstrated using real-world scenarios sourced from open-sourced OpenACC.
<div id='section'>Paperid: <span id='pid'>831, <a href='https://arxiv.org/pdf/2406.02890.pdf' target='_blank'>https://arxiv.org/pdf/2406.02890.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dom Huh, Prasant Mohapatra
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.02890">Representation Learning For Efficient Deep Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Sample efficiency remains a key challenge in multi-agent reinforcement learning (MARL). A promising approach is to learn a meaningful latent representation space through auxiliary learning objectives alongside the MARL objective to aid in learning a successful control policy. In our work, we present MAPO-LSO (Multi-Agent Policy Optimization with Latent Space Optimization) which applies a form of comprehensive representation learning devised to supplement MARL training. Specifically, MAPO-LSO proposes a multi-agent extension of transition dynamics reconstruction and self-predictive learning that constructs a latent state optimization scheme that can be trivially extended to current state-of-the-art MARL algorithms. Empirical results demonstrate MAPO-LSO to show notable improvements in sample efficiency and learning performance compared to its vanilla MARL counterpart without any additional MARL hyperparameter tuning on a diverse suite of MARL tasks.
<div id='section'>Paperid: <span id='pid'>832, <a href='https://arxiv.org/pdf/2405.16420.pdf' target='_blank'>https://arxiv.org/pdf/2405.16420.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zheng Wang, Shu Xian Teo, Jieer Ouyang, Yongjun Xu, Wei Shi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.16420">M-RAG: Reinforcing Large Language Model Performance through Retrieval-Augmented Generation with Multiple Partitions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Retrieval-Augmented Generation (RAG) enhances Large Language Models (LLMs) by retrieving relevant memories from an external database. However, existing RAG methods typically organize all memories in a whole database, potentially limiting focus on crucial memories and introducing noise. In this paper, we introduce a multiple partition paradigm for RAG (called M-RAG), where each database partition serves as a basic unit for RAG execution. Based on this paradigm, we propose a novel framework that leverages LLMs with Multi-Agent Reinforcement Learning to optimize different language generation tasks explicitly. Through comprehensive experiments conducted on seven datasets, spanning three language generation tasks and involving three distinct language model architectures, we confirm that M-RAG consistently outperforms various baseline methods, achieving improvements of 11%, 8%, and 12% for text summarization, machine translation, and dialogue generation, respectively.
<div id='section'>Paperid: <span id='pid'>833, <a href='https://arxiv.org/pdf/2405.00282.pdf' target='_blank'>https://arxiv.org/pdf/2405.00282.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Anran Hu, Junzi Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.00282">MF-OML: Online Mean-Field Reinforcement Learning with Occupation Measures for Large Population Games</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reinforcement learning for multi-agent games has attracted lots of attention recently. However, given the challenge of solving Nash equilibria for large population games, existing works with guaranteed polynomial complexities either focus on variants of zero-sum and potential games, or aim at solving (coarse) correlated equilibria, or require access to simulators, or rely on certain assumptions that are hard to verify. This work proposes MF-OML (Mean-Field Occupation-Measure Learning), an online mean-field reinforcement learning algorithm for computing approximate Nash equilibria of large population sequential symmetric games. MF-OML is the first fully polynomial multi-agent reinforcement learning algorithm for provably solving Nash equilibria (up to mean-field approximation gaps that vanish as the number of players $N$ goes to infinity) beyond variants of zero-sum and potential games. When evaluated by the cumulative deviation from Nash equilibria, the algorithm is shown to achieve a high probability regret bound of $\tilde{O}(M^{3/4}+N^{-1/2}M)$ for games with the strong Lasry-Lions monotonicity condition, and a regret bound of $\tilde{O}(M^{11/12}+N^{- 1/6}M)$ for games with only the Lasry-Lions monotonicity condition, where $M$ is the total number of episodes and $N$ is the number of agents of the game. As a byproduct, we also obtain the first tractable globally convergent computational algorithm for computing approximate Nash equilibria of monotone mean-field games.
<div id='section'>Paperid: <span id='pid'>834, <a href='https://arxiv.org/pdf/2405.00282.pdf' target='_blank'>https://arxiv.org/pdf/2405.00282.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Anran Hu, Junzi Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.00282">MF-OML: Online Mean-Field Reinforcement Learning with Occupation Measures for Large Population Games</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reinforcement learning for multi-agent games has attracted lots of attention recently. However, given the challenge of solving Nash equilibria for large population games, existing works with guaranteed polynomial complexities either focus on variants of zero-sum and potential games, or aim at solving (coarse) correlated equilibria, or require access to simulators, or rely on certain assumptions that are hard to verify. This work proposes MF-OML (Mean-Field Occupation-Measure Learning), an online mean-field reinforcement learning algorithm for computing approximate Nash equilibria of large population sequential symmetric games. MF-OML is the first fully polynomial multi-agent reinforcement learning algorithm for provably solving Nash equilibria (up to mean-field approximation gaps that vanish as the number of players $N$ goes to infinity) beyond variants of zero-sum and potential games. When evaluated by the cumulative deviation from Nash equilibria, the algorithm is shown to achieve a high probability regret bound of $\tilde{O}(M^{3/4}+N^{-1/2}M)$ for games with the strong Lasry-Lions monotonicity condition, and a regret bound of $\tilde{O}(M^{11/12}+N^{- 1/6}M)$ for games with only the Lasry-Lions monotonicity condition, where $M$ is the total number of episodes and $N$ is the number of agents of the game. As a byproduct, we also obtain the first tractable globally convergent computational algorithm for computing approximate Nash equilibria of monotone mean-field games.
<div id='section'>Paperid: <span id='pid'>835, <a href='https://arxiv.org/pdf/2404.10740.pdf' target='_blank'>https://arxiv.org/pdf/2404.10740.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Caroline Wang, Arrasy Rahman, Ishan Durugkar, Elad Liebman, Peter Stone
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.10740">N-Agent Ad Hoc Teamwork</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current approaches to learning cooperative multi-agent behaviors assume relatively restrictive settings. In standard fully cooperative multi-agent reinforcement learning, the learning algorithm controls $\textit{all}$ agents in the scenario, while in ad hoc teamwork, the learning algorithm usually assumes control over only a $\textit{single}$ agent in the scenario. However, many cooperative settings in the real world are much less restrictive. For example, in an autonomous driving scenario, a company might train its cars with the same learning algorithm, yet once on the road, these cars must cooperate with cars from another company. Towards expanding the class of scenarios that cooperative learning methods may optimally address, we introduce $N$-agent ad hoc teamwork (NAHT), where a set of autonomous agents must interact and cooperate with dynamically varying numbers and types of teammates. This paper formalizes the problem, and proposes the Policy Optimization with Agent Modelling (POAM) algorithm. POAM is a policy gradient, multi-agent reinforcement learning approach to the NAHT problem, that enables adaptation to diverse teammate behaviors by learning representations of teammate behaviors. Empirical evaluation on tasks from the multi-agent particle environment and StarCraft II shows that POAM improves cooperative task returns compared to baseline approaches, and enables out-of-distribution generalization to unseen teammates.
<div id='section'>Paperid: <span id='pid'>836, <a href='https://arxiv.org/pdf/2403.18079.pdf' target='_blank'>https://arxiv.org/pdf/2403.18079.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bora Yongacoglu, GÃ¼rdal Arslan, Lacra Pavel, Serdar YÃ¼ksel
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.18079">Paths to Equilibrium in Games</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In multi-agent reinforcement learning (MARL) and game theory, agents repeatedly interact and revise their strategies as new data arrives, producing a sequence of strategy profiles. This paper studies sequences of strategies satisfying a pairwise constraint inspired by policy updating in reinforcement learning, where an agent who is best responding in one period does not switch its strategy in the next period. This constraint merely requires that optimizing agents do not switch strategies, but does not constrain the non-optimizing agents in any way, and thus allows for exploration. Sequences with this property are called satisficing paths, and arise naturally in many MARL algorithms. A fundamental question about strategic dynamics is such: for a given game and initial strategy profile, is it always possible to construct a satisficing path that terminates at an equilibrium? The resolution of this question has implications about the capabilities or limitations of a class of MARL algorithms. We answer this question in the affirmative for normal-form games. Our analysis reveals a counterintuitive insight that reward deteriorating strategic updates are key to driving play to equilibrium along a satisficing path.
<div id='section'>Paperid: <span id='pid'>837, <a href='https://arxiv.org/pdf/2403.13093.pdf' target='_blank'>https://arxiv.org/pdf/2403.13093.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Anthony Goeckner, Yueyuan Sui, Nicolas Martinet, Xinliang Li, Qi Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.13093">Graph Neural Network-based Multi-agent Reinforcement Learning for Resilient Distributed Coordination of Multi-Robot Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing multi-agent coordination techniques are often fragile and vulnerable to anomalies such as agent attrition and communication disturbances, which are quite common in the real-world deployment of systems like field robotics. To better prepare these systems for the real world, we present a graph neural network (GNN)-based multi-agent reinforcement learning (MARL) method for resilient distributed coordination of a multi-robot system. Our method, Multi-Agent Graph Embedding-based Coordination (MAGEC), is trained using multi-agent proximal policy optimization (PPO) and enables distributed coordination around global objectives under agent attrition, partial observability, and limited or disturbed communications. We use a multi-robot patrolling scenario to demonstrate our MAGEC method in a ROS 2-based simulator and then compare its performance with prior coordination approaches. Results demonstrate that MAGEC outperforms existing methods in several experiments involving agent attrition and communication disturbance, and provides competitive results in scenarios without such anomalies.
<div id='section'>Paperid: <span id='pid'>838, <a href='https://arxiv.org/pdf/2402.04867.pdf' target='_blank'>https://arxiv.org/pdf/2402.04867.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zheng Wang, Bingzheng Gan, Wei Shi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.04867">Multimodal Query Suggestion with Multi-Agent Reinforcement Learning from Human Feedback</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the rapidly evolving landscape of information retrieval, search engines strive to provide more personalized and relevant results to users. Query suggestion systems play a crucial role in achieving this goal by assisting users in formulating effective queries. However, existing query suggestion systems mainly rely on textual inputs, potentially limiting user search experiences for querying images. In this paper, we introduce a novel Multimodal Query Suggestion (MMQS) task, which aims to generate query suggestions based on user query images to improve the intentionality and diversity of search results. We present the RL4Sugg framework, leveraging the power of Large Language Models (LLMs) with Multi-Agent Reinforcement Learning from Human Feedback to optimize the generation process. Through comprehensive experiments, we validate the effectiveness of RL4Sugg, demonstrating a 18% improvement compared to the best existing approach. Moreover, the MMQS has been transferred into real-world search engine products, which yield enhanced user engagement. Our research advances query suggestion systems and provides a new perspective on multimodal information retrieval.
<div id='section'>Paperid: <span id='pid'>839, <a href='https://arxiv.org/pdf/2401.15059.pdf' target='_blank'>https://arxiv.org/pdf/2401.15059.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rafael Pina, Varuna De Silva, Corentin Artaud, Xiaolan Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.15059">Fully Independent Communication in Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-Agent Reinforcement Learning (MARL) comprises a broad area of research within the field of multi-agent systems. Several recent works have focused specifically on the study of communication approaches in MARL. While multiple communication methods have been proposed, these might still be too complex and not easily transferable to more practical contexts. One of the reasons for that is due to the use of the famous parameter sharing trick. In this paper, we investigate how independent learners in MARL that do not share parameters can communicate. We demonstrate that this setting might incur into some problems, to which we propose a new learning scheme as a solution. Our results show that, despite the challenges, independent agents can still learn communication strategies following our method. Additionally, we use this method to investigate how communication in MARL is affected by different network capacities, both for sharing and not sharing parameters. We observe that communication may not always be needed and that the chosen agent network sizes need to be considered when used together with communication in order to achieve efficient learning.
<div id='section'>Paperid: <span id='pid'>840, <a href='https://arxiv.org/pdf/2401.05860.pdf' target='_blank'>https://arxiv.org/pdf/2401.05860.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Thomy Phan, Joseph Driscoll, Justin Romberg, Sven Koenig
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.05860">Confidence-Based Curriculum Learning for Multi-Agent Path Finding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A wide range of real-world applications can be formulated as Multi-Agent Path Finding (MAPF) problem, where the goal is to find collision-free paths for multiple agents with individual start and goal locations. State-of-the-art MAPF solvers are mainly centralized and depend on global information, which limits their scalability and flexibility regarding changes or new maps that would require expensive replanning. Multi-agent reinforcement learning (MARL) offers an alternative way by learning decentralized policies that can generalize over a variety of maps. While there exist some prior works that attempt to connect both areas, the proposed techniques are heavily engineered and very complex due to the integration of many mechanisms that limit generality and are expensive to use. We argue that much simpler and general approaches are needed to bring the areas of MARL and MAPF closer together with significantly lower costs. In this paper, we propose Confidence-based Auto-Curriculum for Team Update Stability (CACTUS) as a lightweight MARL approach to MAPF. CACTUS defines a simple reverse curriculum scheme, where the goal of each agent is randomly placed within an allocation radius around the agent's start location. The allocation radius increases gradually as all agents improve, which is assessed by a confidence-based measure. We evaluate CACTUS in various maps of different sizes, obstacle densities, and numbers of agents. Our experiments demonstrate better performance and generalization capabilities than state-of-the-art MARL approaches with less than 600,000 trainable parameters, which is less than 5% of the neural network size of current MARL approaches to MAPF.
<div id='section'>Paperid: <span id='pid'>841, <a href='https://arxiv.org/pdf/2312.10256.pdf' target='_blank'>https://arxiv.org/pdf/2312.10256.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dom Huh, Prasant Mohapatra
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.10256">Multi-agent Reinforcement Learning: A Comprehensive Survey</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent systems (MAS) are widely prevalent and crucially important in numerous real-world applications, where multiple agents must make decisions to achieve their objectives in a shared environment. Despite their ubiquity, the development of intelligent decision-making agents in MAS poses several open challenges to their effective implementation. This survey examines these challenges, placing an emphasis on studying seminal concepts from game theory (GT) and machine learning (ML) and connecting them to recent advancements in multi-agent reinforcement learning (MARL), i.e. the research of data-driven decision-making within MAS. Therefore, the objective of this survey is to provide a comprehensive perspective along the various dimensions of MARL, shedding light on the unique opportunities that are presented in MARL applications while highlighting the inherent challenges that accompany this potential. Therefore, we hope that our work will not only contribute to the field by analyzing the current landscape of MARL but also motivate future directions with insights for deeper integration of concepts from related domains of GT and ML. With this in mind, this work delves into a detailed exploration of recent and past efforts of MARL and its related fields and describes prior solutions that were proposed and their limitations, as well as their applications.
<div id='section'>Paperid: <span id='pid'>842, <a href='https://arxiv.org/pdf/2312.04371.pdf' target='_blank'>https://arxiv.org/pdf/2312.04371.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Han Xu, Jialin Zheng, Guannan Qu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.04371">A Scalable Network-Aware Multi-Agent Reinforcement Learning Framework for Decentralized Inverter-based Voltage Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper addresses the challenges associated with decentralized voltage control in power grids due to an increase in distributed generations (DGs). Traditional model-based voltage control methods struggle with the rapid energy fluctuations and uncertainties of these DGs. While multi-agent reinforcement learning (MARL) has shown potential for decentralized secondary control, scalability issues arise when dealing with a large number of DGs. This problem lies in the dominant centralized training and decentralized execution (CTDE) framework, where the critics take global observations and actions. To overcome these challenges, we propose a scalable network-aware (SNA) framework that leverages network structure to truncate the input to the critic's Q-function, thereby improving scalability and reducing communication costs during training. Further, the SNA framework is theoretically grounded with provable approximation guarantee, and it can seamlessly integrate with multiple multi-agent actor-critic algorithms. The proposed SNA framework is successfully demonstrated in a system with 114 DGs, providing a promising solution for decentralized voltage control in increasingly complex power grid systems.
<div id='section'>Paperid: <span id='pid'>843, <a href='https://arxiv.org/pdf/2311.04740.pdf' target='_blank'>https://arxiv.org/pdf/2311.04740.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Peihong Yu, Bhoram Lee, Aswin Raghavan, Supun Samarasekara, Pratap Tokekar, James Zachary Hare
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.04740">Enhancing Multi-Agent Coordination through Common Operating Picture Integration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In multi-agent systems, agents possess only local observations of the environment. Communication between teammates becomes crucial for enhancing coordination. Past research has primarily focused on encoding local information into embedding messages which are unintelligible to humans. We find that using these messages in agent's policy learning leads to brittle policies when tested on out-of-distribution initial states. We present an approach to multi-agent coordination, where each agent is equipped with the capability to integrate its (history of) observations, actions and messages received into a Common Operating Picture (COP) and disseminate the COP. This process takes into account the dynamic nature of the environment and the shared mission. We conducted experiments in the StarCraft2 environment to validate our approach. Our results demonstrate the efficacy of COP integration, and show that COP-based training leads to robust policies compared to state-of-the-art Multi-Agent Reinforcement Learning (MARL) methods when faced with out-of-distribution initial states.
<div id='section'>Paperid: <span id='pid'>844, <a href='https://arxiv.org/pdf/2311.02741.pdf' target='_blank'>https://arxiv.org/pdf/2311.02741.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rafael Pina, Varuna De Silva, Corentin Artaud
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.02741">Learning Independently from Causality in Multi-Agent Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-Agent Reinforcement Learning (MARL) comprises an area of growing interest in the field of machine learning. Despite notable advances, there are still problems that require investigation. The lazy agent pathology is a famous problem in MARL that denotes the event when some of the agents in a MARL team do not contribute to the common goal, letting the teammates do all the work. In this work, we aim to investigate this problem from a causality-based perspective. We intend to create the bridge between the fields of MARL and causality and argue about the usefulness of this link. We study a fully decentralised MARL setup where agents need to learn cooperation strategies and show that there is a causal relation between individual observations and the team reward. The experiments carried show how this relation can be used to improve independent agents in MARL, resulting not only on better performances as a team but also on the rise of more intelligent behaviours on individual agents.
<div id='section'>Paperid: <span id='pid'>845, <a href='https://arxiv.org/pdf/2308.14602.pdf' target='_blank'>https://arxiv.org/pdf/2308.14602.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Min Hua, Bin Shuai, Quan Zhou, Jinhai Wang, Yinglong He, Hongming Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.14602">Recent Progress in Energy Management of Connected Hybrid Electric Vehicles Using Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The growing adoption of hybrid electric vehicles (HEVs) presents a transformative opportunity for revolutionizing transportation energy systems. The shift towards electrifying transportation aims to curb environmental concerns related to fossil fuel consumption. This necessitates efficient energy management systems (EMS) to optimize energy efficiency. The evolution of EMS from HEVs to connected hybrid electric vehicles (CHEVs) represent a pivotal shift. For HEVs, EMS now confronts the intricate energy cooperation requirements of CHEVs, necessitating advanced algorithms for route optimization, charging coordination, and load distribution. Challenges persist in both domains, including optimal energy utilization for HEVs, and cooperative eco-driving control (CED) for CHEVs across diverse vehicle types. Reinforcement learning (RL) stands out as a promising tool for addressing these challenges. Specifically, within the realm of CHEVs, the application of multi-agent reinforcement learning (MARL) emerges as a powerful approach for effectively tackling the intricacies of CED control. Despite extensive research, few reviews span from individual vehicles to multi-vehicle scenarios. This review bridges the gap, highlighting challenges, advancements, and potential contributions of RL-based solutions for future sustainable transportation systems.
<div id='section'>Paperid: <span id='pid'>846, <a href='https://arxiv.org/pdf/2308.10124.pdf' target='_blank'>https://arxiv.org/pdf/2308.10124.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yi Hu, Jinhang Zuo, Bob Iannucci, Carlee Joe-Wong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.10124">Intelligent Communication Planning for Constrained Environmental IoT Sensing with Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Internet of Things (IoT) technologies have enabled numerous data-driven mobile applications and have the potential to significantly improve environmental monitoring and hazard warnings through the deployment of a network of IoT sensors. However, these IoT devices are often power-constrained and utilize wireless communication schemes with limited bandwidth. Such power constraints limit the amount of information each device can share across the network, while bandwidth limitations hinder sensors' coordination of their transmissions. In this work, we formulate the communication planning problem of IoT sensors that track the state of the environment. We seek to optimize sensors' decisions in collecting environmental data under stringent resource constraints. We propose a multi-agent reinforcement learning (MARL) method to find the optimal communication policies for each sensor that maximize the tracking accuracy subject to the power and bandwidth limitations. MARL learns and exploits the spatial-temporal correlation of the environmental data at each sensor's location to reduce the redundant reports from the sensors. Experiments on wildfire spread with LoRA wireless network simulators show that our MARL method can learn to balance the need to collect enough data to predict wildfire spread with unknown bandwidth limitations.
<div id='section'>Paperid: <span id='pid'>847, <a href='https://arxiv.org/pdf/2308.04938.pdf' target='_blank'>https://arxiv.org/pdf/2308.04938.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Astrid Vanneste, Simon Vanneste, Kevin Mets, Tom De Schepper, Siegfried Mercelis, Peter Hellinckx
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.04938">An In-Depth Analysis of Discretization Methods for Communication Learning using Backpropagation with Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Communication is crucial in multi-agent reinforcement learning when agents are not able to observe the full state of the environment. The most common approach to allow learned communication between agents is the use of a differentiable communication channel that allows gradients to flow between agents as a form of feedback. However, this is challenging when we want to use discrete messages to reduce the message size, since gradients cannot flow through a discrete communication channel. Previous work proposed methods to deal with this problem. However, these methods are tested in different communication learning architectures and environments, making it hard to compare them. In this paper, we compare several state-of-the-art discretization methods as well as a novel approach. We do this comparison in the context of communication learning using gradients from other agents and perform tests on several environments. In addition, we present COMA-DIAL, a communication learning approach based on DIAL and COMA extended with learning rate scaling and adapted exploration. Using COMA-DIAL allows us to perform experiments on more complex environments. Our results show that the novel ST-DRU method, proposed in this paper, achieves the best results out of all discretization methods across the different environments. It achieves the best or close to the best performance in each of the experiments and is the only method that does not fail on any of the tested environments.
<div id='section'>Paperid: <span id='pid'>848, <a href='https://arxiv.org/pdf/2308.04844.pdf' target='_blank'>https://arxiv.org/pdf/2308.04844.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Astrid Vanneste, Thomas Somers, Simon Vanneste, Kevin Mets, Tom De Schepper, Siegfried Mercelis, Peter Hellinckx
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.04844">Scalability of Message Encoding Techniques for Continuous Communication Learned with Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Many multi-agent systems require inter-agent communication to properly achieve their goal. By learning the communication protocol alongside the action protocol using multi-agent reinforcement learning techniques, the agents gain the flexibility to determine which information should be shared. However, when the number of agents increases we need to create an encoding of the information contained in these messages. In this paper, we investigate the effect of increasing the amount of information that should be contained in a message and increasing the number of agents. We evaluate these effects on two different message encoding methods, the mean message encoder and the attention message encoder. We perform our experiments on a matrix environment. Surprisingly, our results show that the mean message encoder consistently outperforms the attention message encoder. Therefore, we analyse the communication protocol used by the agents that use the mean message encoder and can conclude that the agents use a combination of an exponential and a logarithmic function in their communication policy to avoid the loss of important information after applying the mean message encoder.
<div id='section'>Paperid: <span id='pid'>849, <a href='https://arxiv.org/pdf/2308.03358.pdf' target='_blank'>https://arxiv.org/pdf/2308.03358.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jingdi Chen, Tian Lan, Carlee Joe-Wong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.03358">RGMComm: Return Gap Minimization via Discrete Communications in Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Communication is crucial for solving cooperative Multi-Agent Reinforcement Learning tasks in partially observable Markov Decision Processes. Existing works often rely on black-box methods to encode local information/features into messages shared with other agents, leading to the generation of continuous messages with high communication overhead and poor interpretability. Prior attempts at discrete communication methods generate one-hot vectors trained as part of agents' actions and use the Gumbel softmax operation for calculating message gradients, which are all heuristic designs that do not provide any quantitative guarantees on the expected return. This paper establishes an upper bound on the return gap between an ideal policy with full observability and an optimal partially observable policy with discrete communication. This result enables us to recast multi-agent communication into a novel online clustering problem over the local observations at each agent, with messages as cluster labels and the upper bound on the return gap as clustering loss. To minimize the return gap, we propose the Return-Gap-Minimization Communication (RGMComm) algorithm, which is a surprisingly simple design of discrete message generation functions and is integrated with reinforcement learning through the utilization of a novel Regularized Information Maximization loss function, which incorporates cosine-distance as the clustering metric. Evaluations show that RGMComm significantly outperforms state-of-the-art multi-agent communication baselines and can achieve nearly optimal returns with few-bit messages that are naturally interpretable.
<div id='section'>Paperid: <span id='pid'>850, <a href='https://arxiv.org/pdf/2308.03239.pdf' target='_blank'>https://arxiv.org/pdf/2308.03239.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bora Yongacoglu, GÃ¼rdal Arslan, Serdar YÃ¼ksel
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.03239">Unsynchronized Decentralized Q-Learning: Two Timescale Analysis By Persistence</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Non-stationarity is a fundamental challenge in multi-agent reinforcement learning (MARL), where agents update their behaviour as they learn. Many theoretical advances in MARL avoid the challenge of non-stationarity by coordinating the policy updates of agents in various ways, including synchronizing times at which agents are allowed to revise their policies. Synchronization enables analysis of many MARL algorithms via multi-timescale methods, but such synchronization is infeasible in many decentralized applications. In this paper, we study an unsynchronized variant of the decentralized Q-learning algorithm, a recent MARL algorithm for stochastic games. We provide sufficient conditions under which the unsynchronized algorithm drives play to equilibrium with high probability. Our solution utilizes constant learning rates in the Q-factor update, which we show to be critical for relaxing the synchronization assumptions of earlier work. Our analysis also applies to unsynchronized generalizations of a number of other algorithms from the regret testing tradition, whose performance is analyzed by multi-timescale methods that study Markov chains obtained via policy update dynamics. This work extends the applicability of the decentralized Q-learning algorithm and its relatives to settings in which parameters are selected in an independent manner, and tames non-stationarity without imposing the coordination assumptions of prior work.
<div id='section'>Paperid: <span id='pid'>851, <a href='https://arxiv.org/pdf/2307.00907.pdf' target='_blank'>https://arxiv.org/pdf/2307.00907.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Weiran Guo, Guanjun Liu, Ziyuan Zhou, Ling Wang, Jiacun Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.00907">Enhancing the Robustness of QMIX against State-adversarial Attacks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep reinforcement learning (DRL) performance is generally impacted by state-adversarial attacks, a perturbation applied to an agent's observation. Most recent research has concentrated on robust single-agent reinforcement learning (SARL) algorithms against state-adversarial attacks. Still, there has yet to be much work on robust multi-agent reinforcement learning. Using QMIX, one of the popular cooperative multi-agent reinforcement algorithms, as an example, we discuss four techniques to improve the robustness of SARL algorithms and extend them to multi-agent scenarios. To increase the robustness of multi-agent reinforcement learning (MARL) algorithms, we train models using a variety of attacks in this research. We then test the models taught using the other attacks by subjecting them to the corresponding attacks throughout the training phase. In this way, we organize and summarize techniques for enhancing robustness when used with MARL.
<div id='section'>Paperid: <span id='pid'>852, <a href='https://arxiv.org/pdf/2306.11846.pdf' target='_blank'>https://arxiv.org/pdf/2306.11846.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rafael Pina, Varuna De Silva, Corentin Artaud
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.11846">Discovering Causality for Efficient Cooperation in Multi-Agent Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In cooperative Multi-Agent Reinforcement Learning (MARL) agents are required to learn behaviours as a team to achieve a common goal. However, while learning a task, some agents may end up learning sub-optimal policies, not contributing to the objective of the team. Such agents are called lazy agents due to their non-cooperative behaviours that may arise from failing to understand whether they caused the rewards. As a consequence, we observe that the emergence of cooperative behaviours is not necessarily a byproduct of being able to solve a task as a team. In this paper, we investigate the applications of causality in MARL and how it can be applied in MARL to penalise these lazy agents. We observe that causality estimations can be used to improve the credit assignment to the agents and show how it can be leveraged to improve independent learning in MARL. Furthermore, we investigate how Amortized Causal Discovery can be used to automate causality detection within MARL environments. The results demonstrate that causality relations between individual observations and the team reward can be used to detect and punish lazy agents, making them develop more intelligent behaviours. This results in improvements not only in the overall performances of the team but also in their individual capabilities. In addition, results show that Amortized Causal Discovery can be used efficiently to find causal relations in MARL.
<div id='section'>Paperid: <span id='pid'>853, <a href='https://arxiv.org/pdf/2306.11128.pdf' target='_blank'>https://arxiv.org/pdf/2306.11128.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nikunj Gupta, Somjit Nath, Samira Ebrahimi Kahou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.11128">CAMMARL: Conformal Action Modeling in Multi Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Before taking actions in an environment with more than one intelligent agent, an autonomous agent may benefit from reasoning about the other agents and utilizing a notion of a guarantee or confidence about the behavior of the system. In this article, we propose a novel multi-agent reinforcement learning (MARL) algorithm CAMMARL, which involves modeling the actions of other agents in different situations in the form of confident sets, i.e., sets containing their true actions with a high probability. We then use these estimates to inform an agent's decision-making. For estimating such sets, we use the concept of conformal predictions, by means of which, we not only obtain an estimate of the most probable outcome but get to quantify the operable uncertainty as well. For instance, we can predict a set that provably covers the true predictions with high probabilities (e.g., 95%). Through several experiments in two fully cooperative multi-agent tasks, we show that CAMMARL elevates the capabilities of an autonomous agent in MARL by modeling conformal prediction sets over the behavior of other agents in the environment and utilizing such estimates to enhance its policy learning.
<div id='section'>Paperid: <span id='pid'>854, <a href='https://arxiv.org/pdf/2306.09742.pdf' target='_blank'>https://arxiv.org/pdf/2306.09742.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinyuan Ji, Xu Zhang, Wei Xi, Haozhi Wang, Olga Gadyatskaya, Yinchuan Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.09742">Meta Generative Flow Networks with Personalization for Task-Specific Adaptation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-task reinforcement learning and meta-reinforcement learning have been developed to quickly adapt to new tasks, but they tend to focus on tasks with higher rewards and more frequent occurrences, leading to poor performance on tasks with sparse rewards. To address this issue, GFlowNets can be integrated into meta-learning algorithms (GFlowMeta) by leveraging the advantages of GFlowNets on tasks with sparse rewards. However, GFlowMeta suffers from performance degradation when encountering heterogeneous transitions from distinct tasks. To overcome this challenge, this paper proposes a personalized approach named pGFlowMeta, which combines task-specific personalized policies with a meta policy. Each personalized policy balances the loss on its personalized task and the difference from the meta policy, while the meta policy aims to minimize the average loss of all tasks. The theoretical analysis shows that the algorithm converges at a sublinear rate. Extensive experiments demonstrate that the proposed algorithm outperforms state-of-the-art reinforcement learning algorithms in discrete environments.
<div id='section'>Paperid: <span id='pid'>855, <a href='https://arxiv.org/pdf/2306.08815.pdf' target='_blank'>https://arxiv.org/pdf/2306.08815.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rohan Chandra, Rahul Menon, Zayne Sprague, Arya Anantula, Joydeep Biswas
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.08815">Decentralized Social Navigation with Non-Cooperative Robots via Bi-Level Optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a fully decentralized approach for realtime non-cooperative multi-robot navigation in social mini-games, such as navigating through a narrow doorway or negotiating right of way at a corridor intersection. Our contribution is a new realtime bi-level optimization algorithm, in which the top-level optimization consists of computing a fair and collision-free ordering followed by the bottom-level optimization which plans optimal trajectories conditioned on the ordering. We show that, given such a priority order, we can impose simple kinodynamic constraints on each robot that are sufficient for it to plan collision-free trajectories with minimal deviation from their preferred velocities, similar to how humans navigate in these scenarios.
  We successfully deploy the proposed algorithm in the real world using F$1/10$ robots, a Clearpath Jackal, and a Boston Dynamics Spot as well as in simulation using the SocialGym 2.0 multi-agent social navigation simulator, in the doorway and corridor intersection scenarios. We compare with state-of-the-art social navigation methods using multi-agent reinforcement learning, collision avoidance algorithms, and crowd simulation models. We show that $(i)$ classical navigation performs $44\%$ better than the state-of-the-art learning-based social navigation algorithms, $(ii)$ without a scheduling protocol, our approach results in collisions in social mini-games $(iii)$ our approach yields $2\times$ and $5\times$ fewer velocity changes than CADRL in doorways and intersections, and finally $(iv)$ bi-level navigation in doorways at a flow rate of $2.8 - 3.3$ (ms)$^{-1}$ is comparable to flow rate in human navigation at a flow rate of $4$ (ms)$^{-1}$.
<div id='section'>Paperid: <span id='pid'>856, <a href='https://arxiv.org/pdf/2306.06136.pdf' target='_blank'>https://arxiv.org/pdf/2306.06136.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziyuan Zhou, Guanjun Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.06136">Robustness Testing for Multi-Agent Reinforcement Learning: State Perturbations on Critical Agents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-Agent Reinforcement Learning (MARL) has been widely applied in many fields such as smart traffic and unmanned aerial vehicles. However, most MARL algorithms are vulnerable to adversarial perturbations on agent states. Robustness testing for a trained model is an essential step for confirming the trustworthiness of the model against unexpected perturbations. This work proposes a novel Robustness Testing framework for MARL that attacks states of Critical Agents (RTCA). The RTCA has two innovations: 1) a Differential Evolution (DE) based method to select critical agents as victims and to advise the worst-case joint actions on them; and 2) a team cooperation policy evaluation method employed as the objective function for the optimization of DE. Then, adversarial state perturbations of the critical agents are generated based on the worst-case joint actions. This is the first robustness testing framework with varying victim agents. RTCA demonstrates outstanding performance in terms of the number of victim agents and destroying cooperation policies.
<div id='section'>Paperid: <span id='pid'>857, <a href='https://arxiv.org/pdf/2305.10091.pdf' target='_blank'>https://arxiv.org/pdf/2305.10091.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziyuan Zhou, Guanjun Liu, Ying Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.10091">Multi-Agent Reinforcement Learning: Methods, Applications, Visionary Prospects, and Challenges</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent reinforcement learning (MARL) is a widely used Artificial Intelligence (AI) technique. However, current studies and applications need to address its scalability, non-stationarity, and trustworthiness. This paper aims to review methods and applications and point out research trends and visionary prospects for the next decade. First, this paper summarizes the basic methods and application scenarios of MARL. Second, this paper outlines the corresponding research methods and their limitations on safety, robustness, generalization, and ethical constraints that need to be addressed in the practical applications of MARL. In particular, we believe that trustworthy MARL will become a hot research topic in the next decade. In addition, we suggest that considering human interaction is essential for the practical application of MARL in various societies. Therefore, this paper also analyzes the challenges while MARL is applied to human-machine interaction.
<div id='section'>Paperid: <span id='pid'>858, <a href='https://arxiv.org/pdf/2303.17984.pdf' target='_blank'>https://arxiv.org/pdf/2303.17984.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zifan Wu, Chao Yu, Chen Chen, Jianye Hao, Hankz Hankui Zhuo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.17984">Models as Agents: Optimizing Multi-Step Predictions of Interactive Local Models in Model-Based Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Research in model-based reinforcement learning has made significant progress in recent years. Compared to single-agent settings, the exponential dimension growth of the joint state-action space in multi-agent systems dramatically increases the complexity of the environment dynamics, which makes it infeasible to learn an accurate global model and thus necessitates the use of agent-wise local models. However, during multi-step model rollouts, the prediction of one local model can affect the predictions of other local models in the next step. As a result, local prediction errors can be propagated to other localities and eventually give rise to considerably large global errors. Furthermore, since the models are generally used to predict for multiple steps, simply minimizing one-step prediction errors regardless of their long-term effect on other models may further aggravate the propagation of local errors. To this end, we propose Models as AGents (MAG), a multi-agent model optimization framework that reversely treats the local models as multi-step decision making agents and the current policies as the dynamics during the model rollout process. In this way, the local models are able to consider the multi-step mutual affect between each other before making predictions. Theoretically, we show that the objective of MAG is approximately equivalent to maximizing a lower bound of the true environment return. Experiments on the challenging StarCraft II benchmark demonstrate the effectiveness of MAG.
<div id='section'>Paperid: <span id='pid'>859, <a href='https://arxiv.org/pdf/2303.15471.pdf' target='_blank'>https://arxiv.org/pdf/2303.15471.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chaoyi Gu, Varuna De Silva, Corentin Artaud, Rafael Pina
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.15471">Embedding Contextual Information through Reward Shaping in Multi-Agent Learning: A Case Study from Google Football</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Artificial Intelligence has been used to help human complete difficult tasks in complicated environments by providing optimized strategies for decision-making or replacing the manual labour. In environments including multiple agents, such as football, the most common methods to train agents are Imitation Learning and Multi-Agent Reinforcement Learning (MARL). However, the agents trained by Imitation Learning cannot outperform the expert demonstrator, which makes humans hardly get new insights from the learnt policy. Besides, MARL is prone to the credit assignment problem. In environments with sparse reward signal, this method can be inefficient. The objective of our research is to create a novel reward shaping method by embedding contextual information in reward function to solve the aforementioned challenges. We demonstrate this in the Google Research Football (GRF) environment. We quantify the contextual information extracted from game state observation and use this quantification together with original sparse reward to create the shaped reward. The experiment results in the GRF environment prove that our reward shaping method is a useful addition to state-of-the-art MARL algorithms for training agents in environments with sparse reward signal.
<div id='section'>Paperid: <span id='pid'>860, <a href='https://arxiv.org/pdf/2303.14227.pdf' target='_blank'>https://arxiv.org/pdf/2303.14227.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rafael Pina, Varuna De Silva, Corentin Artaud
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.14227">Causality Detection for Efficient Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>When learning a task as a team, some agents in Multi-Agent Reinforcement Learning (MARL) may fail to understand their true impact in the performance of the team. Such agents end up learning sub-optimal policies, demonstrating undesired lazy behaviours. To investigate this problem, we start by formalising the use of temporal causality applied to MARL problems. We then show how causality can be used to penalise such lazy agents and improve their behaviours. By understanding how their local observations are causally related to the team reward, each agent in the team can adjust their individual credit based on whether they helped to cause the reward or not. We show empirically that using causality estimations in MARL improves not only the holistic performance of the team, but also the individual capabilities of each agent. We observe that the improvements are consistent in a set of different environments.
<div id='section'>Paperid: <span id='pid'>861, <a href='https://arxiv.org/pdf/2303.13539.pdf' target='_blank'>https://arxiv.org/pdf/2303.13539.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Awni Altabaa, Bora Yongacoglu, Serdar YÃ¼ksel
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.13539">Decentralized Multi-Agent Reinforcement Learning for Continuous-Space Stochastic Games</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Stochastic games are a popular framework for studying multi-agent reinforcement learning (MARL). Recent advances in MARL have focused primarily on games with finitely many states. In this work, we study multi-agent learning in stochastic games with general state spaces and an information structure in which agents do not observe each other's actions. In this context, we propose a decentralized MARL algorithm and we prove the near-optimality of its policy updates. Furthermore, we study the global policy-updating dynamics for a general class of best-reply based algorithms and derive a closed-form characterization of convergence probabilities over the joint policy space.
<div id='section'>Paperid: <span id='pid'>862, <a href='https://arxiv.org/pdf/2303.12802.pdf' target='_blank'>https://arxiv.org/pdf/2303.12802.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shashank Jere, Yifei Song, Yang Yi, Lingjia Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.12802">Distributed Learning Meets 6G: A Communication and Computing Perspective</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the ever-improving computing capabilities and storage capacities of mobile devices in line with evolving telecommunication network paradigms, there has been an explosion of research interest towards exploring Distributed Learning (DL) frameworks to realize stringent key performance indicators (KPIs) that are expected in next-generation/6G cellular networks. In conjunction with Edge Computing, Federated Learning (FL) has emerged as the DL architecture of choice in prominent wireless applications. This article lays an outline of how DL in general and FL-based strategies specifically can contribute towards realizing a part of the 6G vision and strike a balance between communication and computing constraints. As a practical use case, we apply Multi-Agent Reinforcement Learning (MARL) within the FL framework to the Dynamic Spectrum Access (DSA) problem and present preliminary evaluation results. Top contemporary challenges in applying DL approaches to 6G networks are also highlighted.
<div id='section'>Paperid: <span id='pid'>863, <a href='https://arxiv.org/pdf/2303.05584.pdf' target='_blank'>https://arxiv.org/pdf/2303.05584.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zayne Sprague, Rohan Chandra, Jarrett Holtz, Joydeep Biswas
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.05584">SOCIALGYM 2.0: Simulator for Multi-Agent Social Robot Navigation in Shared Human Spaces</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present SocialGym 2, a multi-agent navigation simulator for social robot research. Our simulator models multiple autonomous agents, replicating real-world dynamics in complex environments, including doorways, hallways, intersections, and roundabouts. Unlike traditional simulators that concentrate on single robots with basic kinematic constraints in open spaces, SocialGym 2 employs multi-agent reinforcement learning (MARL) to develop optimal navigation policies for multiple robots with diverse, dynamic constraints in complex environments. Built on the PettingZoo MARL library and Stable Baselines3 API, SocialGym 2 offers an accessible python interface that integrates with a navigation stack through ROS messaging. SocialGym 2 can be easily installed and is packaged in a docker container, and it provides the capability to swap and evaluate different MARL algorithms, as well as customize observation and reward functions. We also provide scripts to allow users to create their own environments and have conducted benchmarks using various social navigation algorithms, reporting a broad range of social navigation metrics. Projected hosted at: https://amrl.cs.utexas.edu/social_gym/index.html
<div id='section'>Paperid: <span id='pid'>864, <a href='https://arxiv.org/pdf/2302.14276.pdf' target='_blank'>https://arxiv.org/pdf/2302.14276.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Seth Karten, Siva Kailas, Huao Li, Katia Sycara
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.14276">On the Role of Emergent Communication for Social Learning in Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Explicit communication among humans is key to coordinating and learning. Social learning, which uses cues from experts, can greatly benefit from the usage of explicit communication to align heterogeneous policies, reduce sample complexity, and solve partially observable tasks. Emergent communication, a type of explicit communication, studies the creation of an artificial language to encode a high task-utility message directly from data. However, in most cases, emergent communication sends insufficiently compressed messages with little or null information, which also may not be understandable to a third-party listener. This paper proposes an unsupervised method based on the information bottleneck to capture both referential complexity and task-specific utility to adequately explore sparse social communication scenarios in multi-agent reinforcement learning (MARL). We show that our model is able to i) develop a natural-language-inspired lexicon of messages that is independently composed of a set of emergent concepts, which span the observations and intents with minimal bits, ii) develop communication to align the action policies of heterogeneous agents with dissimilar feature models, and iii) learn a communication policy from watching an expert's action policy, which we term `social shadowing'.
<div id='section'>Paperid: <span id='pid'>865, <a href='https://arxiv.org/pdf/2302.06606.pdf' target='_blank'>https://arxiv.org/pdf/2302.06606.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuanhao Wang, Qinghua Liu, Yu Bai, Chi Jin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.06606">Breaking the Curse of Multiagency: Provably Efficient Decentralized Multi-Agent RL with Function Approximation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A unique challenge in Multi-Agent Reinforcement Learning (MARL) is the curse of multiagency, where the description length of the game as well as the complexity of many existing learning algorithms scale exponentially with the number of agents. While recent works successfully address this challenge under the model of tabular Markov Games, their mechanisms critically rely on the number of states being finite and small, and do not extend to practical scenarios with enormous state spaces where function approximation must be used to approximate value functions or policies.
  This paper presents the first line of MARL algorithms that provably resolve the curse of multiagency under function approximation. We design a new decentralized algorithm -- V-Learning with Policy Replay, which gives the first polynomial sample complexity results for learning approximate Coarse Correlated Equilibria (CCEs) of Markov Games under decentralized linear function approximation. Our algorithm always outputs Markov CCEs, and achieves an optimal rate of $\widetilde{\mathcal{O}}(Îµ^{-2})$ for finding $Îµ$-optimal solutions. Also, when restricted to the tabular case, our result improves over the current best decentralized result $\widetilde{\mathcal{O}}(Îµ^{-3})$ for finding Markov CCEs. We further present an alternative algorithm -- Decentralized Optimistic Policy Mirror Descent, which finds policy-class-restricted CCEs using a polynomial number of samples. In exchange for learning a weaker version of CCEs, this algorithm applies to a wider range of problems under generic function approximation, such as linear quadratic games and MARL problems with low ''marginal'' Eluder dimension.
<div id='section'>Paperid: <span id='pid'>866, <a href='https://arxiv.org/pdf/2302.02571.pdf' target='_blank'>https://arxiv.org/pdf/2302.02571.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuheng Zhang, Yu Bai, Nan Jiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.02571">Offline Learning in Markov Games with General Function Approximation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We study offline multi-agent reinforcement learning (RL) in Markov games, where the goal is to learn an approximate equilibrium -- such as Nash equilibrium and (Coarse) Correlated Equilibrium -- from an offline dataset pre-collected from the game. Existing works consider relatively restricted tabular or linear models and handle each equilibria separately. In this work, we provide the first framework for sample-efficient offline learning in Markov games under general function approximation, handling all 3 equilibria in a unified manner. By using Bellman-consistent pessimism, we obtain interval estimation for policies' returns, and use both the upper and the lower bounds to obtain a relaxation on the gap of a candidate policy, which becomes our optimization objective. Our results generalize prior works and provide several additional insights. Importantly, we require a data coverage condition that improves over the recently proposed "unilateral concentrability". Our condition allows selective coverage of deviation policies that optimally trade-off between their greediness (as approximate best responses) and coverage, and we show scenarios where this leads to significantly better guarantees. As a new connection, we also show how our algorithmic framework can subsume seemingly different solution concepts designed for the special case of two-player zero-sum games.
<div id='section'>Paperid: <span id='pid'>867, <a href='https://arxiv.org/pdf/2212.06357.pdf' target='_blank'>https://arxiv.org/pdf/2212.06357.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xin Liu, Honghao Wei, Lei Ying
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2212.06357">Scalable and Sample Efficient Distributed Policy Gradient Algorithms in Multi-Agent Networked Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper studies a class of multi-agent reinforcement learning (MARL) problems where the reward that an agent receives depends on the states of other agents, but the next state only depends on the agent's own current state and action. We name it REC-MARL standing for REward-Coupled Multi-Agent Reinforcement Learning. REC-MARL has a range of important applications such as real-time access control and distributed power control in wireless networks. This paper presents a distributed policy gradient algorithm for REC-MARL. The proposed algorithm is distributed in two aspects: (i) the learned policy is a distributed policy that maps a local state of an agent to its local action and (ii) the learning/training is distributed, during which each agent updates its policy based on its own and neighbors' information. The learned algorithm achieves a stationary policy and its iterative complexity bounds depend on the dimension of local states and actions. The experimental results of our algorithm for the real-time access control and power control in wireless networks show that our policy significantly outperforms the state-of-the-art algorithms and well-known benchmarks.
<div id='section'>Paperid: <span id='pid'>868, <a href='https://arxiv.org/pdf/2212.01351.pdf' target='_blank'>https://arxiv.org/pdf/2212.01351.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Clement Ruah, Osvaldo Simeone, Bashir Al-Hashimi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2212.01351">A Bayesian Framework for Digital Twin-Based Control, Monitoring, and Data Collection in Wireless Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Commonly adopted in the manufacturing and aerospace sectors, digital twin (DT) platforms are increasingly seen as a promising paradigm to control, monitor, and analyze software-based, "open", communication systems. Notably, DT platforms provide a sandbox in which to test artificial intelligence (AI) solutions for communication systems, potentially reducing the need to collect data and test algorithms in the field, i.e., on the physical twin (PT). A key challenge in the deployment of DT systems is to ensure that virtual control optimization, monitoring, and analysis at the DT are safe and reliable, avoiding incorrect decisions caused by "model exploitation". To address this challenge, this paper presents a general Bayesian framework with the aim of quantifying and accounting for model uncertainty at the DT that is caused by limitations in the amount and quality of data available at the DT from the PT. In the proposed framework, the DT builds a Bayesian model of the communication system, which is leveraged to enable core DT functionalities such as control via multi-agent reinforcement learning (MARL), monitoring of the PT for anomaly detection, prediction, data-collection optimization, and counterfactual analysis. To exemplify the application of the proposed framework, we specifically investigate a case-study system encompassing multiple sensing devices that report to a common receiver. Experimental results validate the effectiveness of the proposed Bayesian framework as compared to standard frequentist model-based solutions.
<div id='section'>Paperid: <span id='pid'>869, <a href='https://arxiv.org/pdf/2210.13763.pdf' target='_blank'>https://arxiv.org/pdf/2210.13763.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiying Xu, Francis Y. Yan, Rachee Singh, Justin T. Chiu, Alexander M. Rush, Minlan Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2210.13763">Teal: Learning-Accelerated Optimization of WAN Traffic Engineering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid expansion of global cloud wide-area networks (WANs) has posed a challenge for commercial optimization engines to efficiently solve network traffic engineering (TE) problems at scale. Existing acceleration strategies decompose TE optimization into concurrent subproblems but realize limited parallelism due to an inherent tradeoff between run time and allocation performance.
  We present Teal, a learning-based TE algorithm that leverages the parallel processing power of GPUs to accelerate TE control. First, Teal designs a flow-centric graph neural network (GNN) to capture WAN connectivity and network flows, learning flow features as inputs to downstream allocation. Second, to reduce the problem scale and make learning tractable, Teal employs a multi-agent reinforcement learning (RL) algorithm to independently allocate each traffic demand while optimizing a central TE objective. Finally, Teal fine-tunes allocations with ADMM (Alternating Direction Method of Multipliers), a highly parallelizable optimization algorithm for reducing constraint violations such as overutilized links.
  We evaluate Teal using traffic matrices from Microsoft's WAN. On a large WAN topology with >1,700 nodes, Teal generates near-optimal flow allocations while running several orders of magnitude faster than the production optimization engine. Compared with other TE acceleration schemes, Teal satisfies 6--32% more traffic demand and yields 197--625x speedups.
<div id='section'>Paperid: <span id='pid'>870, <a href='https://arxiv.org/pdf/2210.05582.pdf' target='_blank'>https://arxiv.org/pdf/2210.05582.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Clement Ruah, Osvaldo Simeone, Bashir Al-Hashimi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2210.05582">Digital Twin-Based Multiple Access Optimization and Monitoring via Model-Driven Bayesian Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Commonly adopted in the manufacturing and aerospace sectors, digital twin (DT) platforms are increasingly seen as a promising paradigm to control and monitor software-based, "open", communication systems, which play the role of the physical twin (PT). In the general framework presented in this work, the DT builds a Bayesian model of the communication system, which is leveraged to enable core DT functionalities such as control via multi-agent reinforcement learning (MARL) and monitoring of the PT for anomaly detection. We specifically investigate the application of the proposed framework to a simple case-study system encompassing multiple sensing devices that report to a common receiver. The Bayesian model trained at the DT has the key advantage of capturing epistemic uncertainty regarding the communication system, e.g., regarding current traffic conditions, which arise from limited PT-to-DT data transfer. Experimental results validate the effectiveness of the proposed Bayesian framework as compared to standard frequentist model-based solutions.
<div id='section'>Paperid: <span id='pid'>871, <a href='https://arxiv.org/pdf/2206.00233.pdf' target='_blank'>https://arxiv.org/pdf/2206.00233.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Caroline Wang, Ishan Durugkar, Elad Liebman, Peter Stone
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2206.00233">DM$^2$: Decentralized Multi-Agent Reinforcement Learning for Distribution Matching</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current approaches to multi-agent cooperation rely heavily on centralized mechanisms or explicit communication protocols to ensure convergence. This paper studies the problem of distributed multi-agent learning without resorting to centralized components or explicit communication. It examines the use of distribution matching to facilitate the coordination of independent agents. In the proposed scheme, each agent independently minimizes the distribution mismatch to the corresponding component of a target visitation distribution. The theoretical analysis shows that under certain conditions, each agent minimizing its individual distribution mismatch allows the convergence to the joint policy that generated the target distribution. Further, if the target distribution is from a joint policy that optimizes a cooperative task, the optimal policy for a combination of this task reward and the distribution matching reward is the same joint policy. This insight is used to formulate a practical algorithm (DM$^2$), in which each individual agent matches a target distribution derived from concurrently sampled trajectories from a joint expert policy. Experimental validation on the StarCraft domain shows that combining (1) a task reward, and (2) a distribution matching reward for expert demonstrations for the same task, allows agents to outperform a naive distributed baseline. Additional experiments probe the conditions under which expert demonstrations need to be sampled to obtain the learning benefits.
<div id='section'>Paperid: <span id='pid'>872, <a href='https://arxiv.org/pdf/2201.11580.pdf' target='_blank'>https://arxiv.org/pdf/2201.11580.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qibin Zhou, Dongdong Bai, Junge Zhang, Fuqing Duan, Kaiqi Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2201.11580">DecisionHoldem: Safe Depth-Limited Solving With Diverse Opponents for Imperfect-Information Games</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>An imperfect-information game is a type of game with asymmetric information. It is more common in life than perfect-information game. Artificial intelligence (AI) in imperfect-information games, such like poker, has made considerable progress and success in recent years. The great success of superhuman poker AI, such as Libratus and Deepstack, attracts researchers to pay attention to poker research. However, the lack of open-source code limits the development of Texas hold'em AI to some extent. This article introduces DecisionHoldem, a high-level AI for heads-up no-limit Texas hold'em with safe depth-limited subgame solving by considering possible ranges of opponent's private hands to reduce the exploitability of the strategy. Experimental results show that DecisionHoldem defeats the strongest openly available agent in heads-up no-limit Texas hold'em poker, namely Slumbot, and a high-level reproduction of Deepstack, viz, Openstack, by more than 730 mbb/h (one-thousandth big blind per round) and 700 mbb/h. Moreover, we release the source codes and tools of DecisionHoldem to promote AI development in imperfect-information games.
<div id='section'>Paperid: <span id='pid'>873, <a href='https://arxiv.org/pdf/2201.07452.pdf' target='_blank'>https://arxiv.org/pdf/2201.07452.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Seth Karten, Mycal Tucker, Huao Li, Siva Kailas, Michael Lewis, Katia Sycara
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2201.07452">Interpretable Learned Emergent Communication for Human-Agent Teams</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Learning interpretable communication is essential for multi-agent and human-agent teams (HATs). In multi-agent reinforcement learning for partially-observable environments, agents may convey information to others via learned communication, allowing the team to complete its task. Inspired by human languages, recent works study discrete (using only a finite set of tokens) and sparse (communicating only at some time-steps) communication. However, the utility of such communication in human-agent team experiments has not yet been investigated. In this work, we analyze the efficacy of sparse-discrete methods for producing emergent communication that enables high agent-only and human-agent team performance. We develop agent-only teams that communicate sparsely via our scheme of Enforcers that sufficiently constrain communication to any budget. Our results show no loss or minimal loss of performance in benchmark environments and tasks. In human-agent teams tested in benchmark environments, where agents have been modeled using the Enforcers, we find that a prototype-based method produces meaningful discrete tokens that enable human partners to learn agent communication faster and better than a one-hot baseline. Additional HAT experiments show that an appropriate sparsity level lowers the cognitive load of humans when communicating with teams of agents and leads to superior team performance.
<div id='section'>Paperid: <span id='pid'>874, <a href='https://arxiv.org/pdf/2110.04638.pdf' target='_blank'>https://arxiv.org/pdf/2110.04638.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bora Yongacoglu, GÃ¼rdal Arslan, Serdar YÃ¼ksel
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2110.04638">Satisficing Paths and Independent Multi-Agent Reinforcement Learning in Stochastic Games</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In multi-agent reinforcement learning (MARL), independent learners are those that do not observe the actions of other agents in the system. Due to the decentralization of information, it is challenging to design independent learners that drive play to equilibrium. This paper investigates the feasibility of using satisficing dynamics to guide independent learners to approximate equilibrium in stochastic games. For $Îµ\geq 0$, an $Îµ$-satisficing policy update rule is any rule that instructs the agent to not change its policy when it is $Îµ$-best-responding to the policies of the remaining players; $Îµ$-satisficing paths are defined to be sequences of joint policies obtained when each agent uses some $Îµ$-satisficing policy update rule to select its next policy. We establish structural results on the existence of $Îµ$-satisficing paths into $Îµ$-equilibrium in both symmetric $N$-player games and general stochastic games with two players. We then present an independent learning algorithm for $N$-player symmetric games and give high probability guarantees of convergence to $Îµ$-equilibrium under self-play. This guarantee is made using symmetry alone, leveraging the previously unexploited structure of $Îµ$-satisficing paths.
<div id='section'>Paperid: <span id='pid'>875, <a href='https://arxiv.org/pdf/2003.06069.pdf' target='_blank'>https://arxiv.org/pdf/2003.06069.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xin Guo, Anran Hu, Renyuan Xu, Junzi Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2003.06069">A General Framework for Learning Mean-Field Games</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a general mean-field game (GMFG) framework for simultaneous learning and decision-making in stochastic games with a large population. It first establishes the existence of a unique Nash Equilibrium to this GMFG, and demonstrates that naively combining reinforcement learning with the fixed-point approach in classical MFGs yields unstable algorithms. It then proposes value-based and policy-based reinforcement learning algorithms (GMF-V and GMF-P, respectively) with smoothed policies, with analysis of their convergence properties and computational complexities. Experiments on an equilibrium product pricing problem demonstrate that GMF-V-Q and GMF-P-TRPO, two specific instantiations of GMF-V and GMF-P, respectively, with Q-learning and TRPO, are both efficient and robust in the GMFG setting. Moreover, their performance is superior in convergence speed, accuracy, and stability when compared with existing algorithms for multi-agent reinforcement learning in the $N$-player setting.
<div id='section'>Paperid: <span id='pid'>876, <a href='https://arxiv.org/pdf/2510.10633.pdf' target='_blank'>https://arxiv.org/pdf/2510.10633.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiabao Shi, Minfeng Qi, Lefeng Zhang, Di Wang, Yingjie Zhao, Ziying Li, Yalong Xing, Ningran Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.10633">Collaborative Text-to-Image Generation via Multi-Agent Reinforcement Learning and Semantic Fusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multimodal text-to-image generation remains constrained by the difficulty of maintaining semantic alignment and professional-level detail across diverse visual domains. We propose a multi-agent reinforcement learning framework that coordinates domain-specialized agents (e.g., focused on architecture, portraiture, and landscape imagery) within two coupled subsystems: a text enhancement module and an image generation module, each augmented with multimodal integration components. Agents are trained using Proximal Policy Optimization (PPO) under a composite reward function that balances semantic similarity, linguistic visual quality, and content diversity. Cross-modal alignment is enforced through contrastive learning, bidirectional attention, and iterative feedback between text and image. Across six experimental settings, our system significantly enriches generated content (word count increased by 1614%) while reducing ROUGE-1 scores by 69.7%. Among fusion methods, Transformer-based strategies achieve the highest composite score (0.521), despite occasional stability issues. Multimodal ensembles yield moderate consistency (ranging from 0.444 to 0.481), reflecting the persistent challenges of cross-modal semantic grounding. These findings underscore the promise of collaborative, specialization-driven architectures for advancing reliable multimodal generative systems.
<div id='section'>Paperid: <span id='pid'>877, <a href='https://arxiv.org/pdf/2510.07813.pdf' target='_blank'>https://arxiv.org/pdf/2510.07813.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Valerio La Gatta, Dolev Mutzari, Sarit Kraus, VS Subrahmanian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.07813">Strategic Communication under Threat: Learning Information Trade-offs in Pursuit-Evasion Games</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Adversarial environments require agents to navigate a key strategic trade-off: acquiring information enhances situational awareness, but may simultaneously expose them to threats. To investigate this tension, we formulate a PursuitEvasion-Exposure-Concealment Game (PEEC) in which a pursuer agent must decide when to communicate in order to obtain the evader's position. Each communication reveals the pursuer's location, increasing the risk of being targeted. Both agents learn their movement policies via reinforcement learning, while the pursuer additionally learns a communication policy that balances observability and risk. We propose SHADOW (Strategic-communication Hybrid Action Decision-making under partial Observation for Warfare), a multi-headed sequential reinforcement learning framework that integrates continuous navigation control, discrete communication actions, and opponent modeling for behavior prediction. Empirical evaluations show that SHADOW pursuers achieve higher success rates than six competitive baselines. Our ablation study confirms that temporal sequence modeling and opponent modeling are critical for effective decision-making. Finally, our sensitivity analysis reveals that the learned policies generalize well across varying communication risks and physical asymmetries between agents.
<div id='section'>Paperid: <span id='pid'>878, <a href='https://arxiv.org/pdf/2510.07813.pdf' target='_blank'>https://arxiv.org/pdf/2510.07813.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Valerio La Gatta, Dolev Mutzari, Sarit Kraus, VS Subrahmanian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.07813">Strategic Communication under Threat: Learning Information Trade-offs in Pursuit-Evasion Games</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Adversarial environments require agents to navigate a key strategic trade-off: acquiring information enhances situational awareness, but may simultaneously expose them to threats. To investigate this tension, we formulate a PursuitEvasion-Exposure-Concealment Game (PEEC) in which a pursuer agent must decide when to communicate in order to obtain the evader's position. Each communication reveals the pursuer's location, increasing the risk of being targeted. Both agents learn their movement policies via reinforcement learning, while the pursuer additionally learns a communication policy that balances observability and risk. We propose SHADOW (Strategic-communication Hybrid Action Decision-making under partial Observation for Warfare), a multi-headed sequential reinforcement learning framework that integrates continuous navigation control, discrete communication actions, and opponent modeling for behavior prediction. Empirical evaluations show that SHADOW pursuers achieve higher success rates than six competitive baselines. Our ablation study confirms that temporal sequence modeling and opponent modeling are critical for effective decision-making. Finally, our sensitivity analysis reveals that the learned policies generalize well across varying communication risks and physical asymmetries between agents.
<div id='section'>Paperid: <span id='pid'>879, <a href='https://arxiv.org/pdf/2510.07363.pdf' target='_blank'>https://arxiv.org/pdf/2510.07363.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianxiang Xu, Zhichao Wen, Xinyu Zhao, Jun Wang, Yan Li, Chang Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.07363">L2M-AID: Autonomous Cyber-Physical Defense by Fusing Semantic Reasoning of Large Language Models with Multi-Agent Reinforcement Learning (Preprint)</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The increasing integration of Industrial IoT (IIoT) exposes critical cyber-physical systems to sophisticated, multi-stage attacks that elude traditional defenses lacking contextual awareness. This paper introduces L2M-AID, a novel framework for Autonomous Industrial Defense using LLM-empowered, Multi-agent reinforcement learning. L2M-AID orchestrates a team of collaborative agents, each driven by a Large Language Model (LLM), to achieve adaptive and resilient security. The core innovation lies in the deep fusion of two AI paradigms: we leverage an LLM as a semantic bridge to translate vast, unstructured telemetry into a rich, contextual state representation, enabling agents to reason about adversary intent rather than merely matching patterns. This semantically-aware state empowers a Multi-Agent Reinforcement Learning (MARL) algorithm, MAPPO, to learn complex cooperative strategies. The MARL reward function is uniquely engineered to balance security objectives (threat neutralization) with operational imperatives, explicitly penalizing actions that disrupt physical process stability. To validate our approach, we conduct extensive experiments on the benchmark SWaT dataset and a novel synthetic dataset generated based on the MITRE ATT&CK for ICS framework. Results demonstrate that L2M-AID significantly outperforms traditional IDS, deep learning anomaly detectors, and single-agent RL baselines across key metrics, achieving a 97.2% detection rate while reducing false positives by over 80% and improving response times by a factor of four. Crucially, it demonstrates superior performance in maintaining physical process stability, presenting a robust new paradigm for securing critical national infrastructure.
<div id='section'>Paperid: <span id='pid'>880, <a href='https://arxiv.org/pdf/2510.07363.pdf' target='_blank'>https://arxiv.org/pdf/2510.07363.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianxiang Xu, Zhichao Wen, Xinyu Zhao, Jun Wang, Yan Li, Chang Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.07363">L2M-AID: Autonomous Cyber-Physical Defense by Fusing Semantic Reasoning of Large Language Models with Multi-Agent Reinforcement Learning (Preprint)</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The increasing integration of Industrial IoT (IIoT) exposes critical cyber-physical systems to sophisticated, multi-stage attacks that elude traditional defenses lacking contextual awareness. This paper introduces L2M-AID, a novel framework for Autonomous Industrial Defense using LLM-empowered, Multi-agent reinforcement learning. L2M-AID orchestrates a team of collaborative agents, each driven by a Large Language Model (LLM), to achieve adaptive and resilient security. The core innovation lies in the deep fusion of two AI paradigms: we leverage an LLM as a semantic bridge to translate vast, unstructured telemetry into a rich, contextual state representation, enabling agents to reason about adversary intent rather than merely matching patterns. This semantically-aware state empowers a Multi-Agent Reinforcement Learning (MARL) algorithm, MAPPO, to learn complex cooperative strategies. The MARL reward function is uniquely engineered to balance security objectives (threat neutralization) with operational imperatives, explicitly penalizing actions that disrupt physical process stability. To validate our approach, we conduct extensive experiments on the benchmark SWaT dataset and a novel synthetic dataset generated based on the MITRE ATT&CK for ICS framework. Results demonstrate that L2M-AID significantly outperforms traditional IDS, deep learning anomaly detectors, and single-agent RL baselines across key metrics, achieving a 97.2% detection rate while reducing false positives by over 80% and improving response times by a factor of four. Crucially, it demonstrates superior performance in maintaining physical process stability, presenting a robust new paradigm for securing critical national infrastructure.
<div id='section'>Paperid: <span id='pid'>881, <a href='https://arxiv.org/pdf/2509.16709.pdf' target='_blank'>https://arxiv.org/pdf/2509.16709.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>NicolÃ² Botteghi, Matteo Tomasetto, Urban Fasel, Francesco Braghin, Andrea Manzoni
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.16709">HypeMARL: Multi-Agent Reinforcement Learning For High-Dimensional, Parametric, and Distributed Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep reinforcement learning has recently emerged as a promising feedback control strategy for complex dynamical systems governed by partial differential equations (PDEs). When dealing with distributed, high-dimensional problems in state and control variables, multi-agent reinforcement learning (MARL) has been proposed as a scalable approach for breaking the curse of dimensionality. In particular, through decentralized training and execution, multiple agents cooperate to steer the system towards a target configuration, relying solely on local state and reward information. However, the principle of locality may become a limiting factor whenever a collective, nonlocal behavior of the agents is crucial to maximize the reward function, as typically happens in PDE-constrained optimal control problems. In this work, we propose HypeMARL: a decentralized MARL algorithm tailored to the control of high-dimensional, parametric, and distributed systems. HypeMARL employs hypernetworks to effectively parametrize the agents' policies and value functions with respect to the system parameters and the agents' relative positions, encoded by sinusoidal positional encoding. Through the application on challenging control problems, such as density and flow control, we show that HypeMARL (i) can effectively control systems through a collective behavior of the agents, outperforming state-of-the-art decentralized MARL, (ii) can efficiently deal with parametric dependencies, (iii) requires minimal hyperparameter tuning and (iv) can reduce the amount of expensive environment interactions by a factor of ~10 thanks to its model-based extension, MB-HypeMARL, which relies on computationally efficient deep learning-based surrogate models approximating the dynamics locally, with minimal deterioration of the policy performance.
<div id='section'>Paperid: <span id='pid'>882, <a href='https://arxiv.org/pdf/2509.16709.pdf' target='_blank'>https://arxiv.org/pdf/2509.16709.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>NicolÃ² Botteghi, Matteo Tomasetto, Urban Fasel, Francesco Braghin, Andrea Manzoni
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.16709">HypeMARL: Multi-Agent Reinforcement Learning For High-Dimensional, Parametric, and Distributed Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep reinforcement learning has recently emerged as a promising feedback control strategy for complex dynamical systems governed by partial differential equations (PDEs). When dealing with distributed, high-dimensional problems in state and control variables, multi-agent reinforcement learning (MARL) has been proposed as a scalable approach for breaking the curse of dimensionality. In particular, through decentralized training and execution, multiple agents cooperate to steer the system towards a target configuration, relying solely on local state and reward information. However, the principle of locality may become a limiting factor whenever a collective, nonlocal behavior of the agents is crucial to maximize the reward function, as typically happens in PDE-constrained optimal control problems. In this work, we propose HypeMARL: a decentralized MARL algorithm tailored to the control of high-dimensional, parametric, and distributed systems. HypeMARL employs hypernetworks to effectively parametrize the agents' policies and value functions with respect to the system parameters and the agents' relative positions, encoded by sinusoidal positional encoding. Through the application on challenging control problems, such as density and flow control, we show that HypeMARL (i) can effectively control systems through a collective behavior of the agents, outperforming state-of-the-art decentralized MARL, (ii) can efficiently deal with parametric dependencies, (iii) requires minimal hyperparameter tuning and (iv) can reduce the amount of expensive environment interactions by a factor of ~10 thanks to its model-based extension, MB-HypeMARL, which relies on computationally efficient deep learning-based surrogate models approximating the dynamics locally, with minimal deterioration of the policy performance.
<div id='section'>Paperid: <span id='pid'>883, <a href='https://arxiv.org/pdf/2509.14276.pdf' target='_blank'>https://arxiv.org/pdf/2509.14276.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuxiang Mai, Qiyue Yin, Wancheng Ni, Pei Xu, Kaiqi Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.14276">Constructive Conflict-Driven Multi-Agent Reinforcement Learning for Strategic Diversity</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, diversity has emerged as a useful mechanism to enhance the efficiency of multi-agent reinforcement learning (MARL). However, existing methods predominantly focus on designing policies based on individual agent characteristics, often neglecting the interplay and mutual influence among agents during policy formation. To address this gap, we propose Competitive Diversity through Constructive Conflict (CoDiCon), a novel approach that incorporates competitive incentives into cooperative scenarios to encourage policy exchange and foster strategic diversity among agents. Drawing inspiration from sociological research, which highlights the benefits of moderate competition and constructive conflict in group decision-making, we design an intrinsic reward mechanism using ranking features to introduce competitive motivations. A centralized intrinsic reward module generates and distributes varying reward values to agents, ensuring an effective balance between competition and cooperation. By optimizing the parameterized centralized reward module to maximize environmental rewards, we reformulate the constrained bilevel optimization problem to align with the original task objectives. We evaluate our algorithm against state-of-the-art methods in the SMAC and GRF environments. Experimental results demonstrate that CoDiCon achieves superior performance, with competitive intrinsic rewards effectively promoting diverse and adaptive strategies among cooperative agents.
<div id='section'>Paperid: <span id='pid'>884, <a href='https://arxiv.org/pdf/2509.14276.pdf' target='_blank'>https://arxiv.org/pdf/2509.14276.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuxiang Mai, Qiyue Yin, Wancheng Ni, Pei Xu, Kaiqi Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.14276">Constructive Conflict-Driven Multi-Agent Reinforcement Learning for Strategic Diversity</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, diversity has emerged as a useful mechanism to enhance the efficiency of multi-agent reinforcement learning (MARL). However, existing methods predominantly focus on designing policies based on individual agent characteristics, often neglecting the interplay and mutual influence among agents during policy formation. To address this gap, we propose Competitive Diversity through Constructive Conflict (CoDiCon), a novel approach that incorporates competitive incentives into cooperative scenarios to encourage policy exchange and foster strategic diversity among agents. Drawing inspiration from sociological research, which highlights the benefits of moderate competition and constructive conflict in group decision-making, we design an intrinsic reward mechanism using ranking features to introduce competitive motivations. A centralized intrinsic reward module generates and distributes varying reward values to agents, ensuring an effective balance between competition and cooperation. By optimizing the parameterized centralized reward module to maximize environmental rewards, we reformulate the constrained bilevel optimization problem to align with the original task objectives. We evaluate our algorithm against state-of-the-art methods in the SMAC and GRF environments. Experimental results demonstrate that CoDiCon achieves superior performance, with competitive intrinsic rewards effectively promoting diverse and adaptive strategies among cooperative agents.
<div id='section'>Paperid: <span id='pid'>885, <a href='https://arxiv.org/pdf/2509.00678.pdf' target='_blank'>https://arxiv.org/pdf/2509.00678.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qintong Xie, Edward Koh, Xavier Cadet, Peter Chin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.00678">Nash Q-Network for Multi-Agent Cybersecurity Simulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cybersecurity defense involves interactions between adversarial parties (namely defenders and hackers), making multi-agent reinforcement learning (MARL) an ideal approach for modeling and learning strategies for these scenarios. This paper addresses one of the key challenges to MARL, the complexity of simultaneous training of agents in nontrivial environments, and presents a novel policy-based Nash Q-learning to directly converge onto a steady equilibrium. We demonstrate the successful implementation of this algorithm in a notable complex cyber defense simulation treated as a two-player zero-sum Markov game setting. We propose the Nash Q-Network, which aims to learn Nash-optimal strategies that translate to robust defenses in cybersecurity settings. Our approach incorporates aspects of proximal policy optimization (PPO), deep Q-network (DQN), and the Nash-Q algorithm, addressing common challenges like non-stationarity and instability in multi-agent learning. The training process employs distributed data collection and carefully designed neural architectures for both agents and critics.
<div id='section'>Paperid: <span id='pid'>886, <a href='https://arxiv.org/pdf/2509.00678.pdf' target='_blank'>https://arxiv.org/pdf/2509.00678.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qintong Xie, Edward Koh, Xavier Cadet, Peter Chin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.00678">Nash Q-Network for Multi-Agent Cybersecurity Simulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cybersecurity defense involves interactions between adversarial parties (namely defenders and hackers), making multi-agent reinforcement learning (MARL) an ideal approach for modeling and learning strategies for these scenarios. This paper addresses one of the key challenges to MARL, the complexity of simultaneous training of agents in nontrivial environments, and presents a novel policy-based Nash Q-learning to directly converge onto a steady equilibrium. We demonstrate the successful implementation of this algorithm in a notable complex cyber defense simulation treated as a two-player zero-sum Markov game setting. We propose the Nash Q-Network, which aims to learn Nash-optimal strategies that translate to robust defenses in cybersecurity settings. Our approach incorporates aspects of proximal policy optimization (PPO), deep Q-network (DQN), and the Nash-Q algorithm, addressing common challenges like non-stationarity and instability in multi-agent learning. The training process employs distributed data collection and carefully designed neural architectures for both agents and critics.
<div id='section'>Paperid: <span id='pid'>887, <a href='https://arxiv.org/pdf/2508.08800.pdf' target='_blank'>https://arxiv.org/pdf/2508.08800.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>David Mguni, Yaqi Sun, Haojun Chen, Amir Darabi, Larry Olanrewaju Orimoloye, Yaodong Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.08800">Fault Tolerant Multi-Agent Learning with Adversarial Budget Constraints</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In multi-agent systems, the safe and reliable execution of tasks often depends on agents correctly coordinating their actions. However, in real-world deployments, failures of computational components are inevitable, presenting a critical challenge: ensuring that multi-agent reinforcement learning (MARL) policies remain effective even when some agents malfunction. We propose the Multi-Agent Robust Training Algorithm (MARTA), a plug-and-play framework for training MARL agents to be resilient to potentially severe faults. MARTA operates in cooperative multi-agent settings where agents may lose the ability to execute their intended actions. It learns to identify failure scenarios that are especially detrimental to system performance and equips agents with strategies to mitigate their impact. At the heart of MARTA is a novel adversarial Markov game in which an adversary -- modelled via \emph{Markov switching controls} -- learns to disable agents in high-risk state regions, while the remaining agents are trained to \emph{jointly} best-respond to such targeted malfunctions. To ensure practicality, MARTA enforces a malfunction budget, constraining the adversary to a fixed number of failures and learning robust policies accordingly. We provide theoretical guarantees that MARTA converges to a Markov perfect equilibrium, ensuring agents optimally counteract worst-case faults. Empirically, we show that MARTA achieves state-of-the-art fault-tolerant performance across benchmark environments, including Multi-Agent Particle World and Level-Based Foraging.
<div id='section'>Paperid: <span id='pid'>888, <a href='https://arxiv.org/pdf/2508.01049.pdf' target='_blank'>https://arxiv.org/pdf/2508.01049.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nicholas E. Corrado, Josiah P. Hanna
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.01049">Centralized Adaptive Sampling for Reliable Co-Training of Independent Multi-Agent Policies</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Independent on-policy policy gradient algorithms are widely used for multi-agent reinforcement learning (MARL) in cooperative and no-conflict games, but they are known to converge suboptimally when each agent's policy gradient points toward a suboptimal equilibrium. In this work, we identify a subtler failure mode that arises \textit{even when the expected policy gradients of all agents point toward an optimal solution.} After collecting a finite set of trajectories, stochasticity in independent action sampling can cause the joint data distribution to deviate from the expected joint on-policy distribution. This \textit{sampling error} w.r.t. the joint on-policy distribution produces inaccurate gradient estimates that can lead agents to converge suboptimally. In this paper, we investigate if joint sampling error can be reduced through coordinated action selection and whether doing so improves the reliability of policy gradient learning in MARL. Toward this end, we introduce an adaptive action sampling approach to reduce joint sampling error. Our method, Multi-Agent Proximal Robust On-Policy Sampling (MA-PROPS), uses a centralized behavior policy that we continually adapt to place larger probability on joint actions that are currently under-sampled w.r.t. the current joint policy. We empirically evaluate MA-PROPS in a diverse range of multi-agent games and demonstrate that (1) MA-PROPS reduces joint sampling error more efficiently than standard on-policy sampling and (2) improves the reliability of independent policy gradient algorithms, increasing the fraction of training runs that converge to an optimal joint policy.
<div id='section'>Paperid: <span id='pid'>889, <a href='https://arxiv.org/pdf/2507.18095.pdf' target='_blank'>https://arxiv.org/pdf/2507.18095.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yi Wang, Dawei Qiu, Fei Teng, Goran Strbac
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.18095">Towards Microgrid Resilience Enhancement via Mobile Power Sources and Repair Crews: A Multi-Agent Reinforcement Learning Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Mobile power sources (MPSs) have been gradually deployed in microgrids as critical resources to coordinate with repair crews (RCs) towards resilience enhancement owing to their flexibility and mobility in handling the complex coupled power-transport systems. However, previous work solves the coordinated dispatch problem of MPSs and RCs in a centralized manner with the assumption that the communication network is still fully functioning after the event. However, there is growing evidence that certain extreme events will damage or degrade communication infrastructure, which makes centralized decision making impractical. To fill this gap, this paper formulates the resilience-driven dispatch problem of MPSs and RCs in a decentralized framework. To solve this problem, a hierarchical multi-agent reinforcement learning method featuring a two-level framework is proposed, where the high-level action is used to switch decision-making between power and transport networks, and the low-level action constructed via a hybrid policy is used to compute continuous scheduling and discrete routing decisions in power and transport networks, respectively. The proposed method also uses an embedded function encapsulating system dynamics to enhance learning stability and scalability. Case studies based on IEEE 33-bus and 69-bus power networks are conducted to validate the effectiveness of the proposed method in load restoration.
<div id='section'>Paperid: <span id='pid'>890, <a href='https://arxiv.org/pdf/2506.19417.pdf' target='_blank'>https://arxiv.org/pdf/2506.19417.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yisak Park, Sunwoo Lee, Seungyul Han
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.19417">Center of Gravity-Guided Focusing Influence Mechanism for Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cooperative multi-agent reinforcement learning (MARL) under sparse rewards presents a fundamental challenge due to limited exploration and insufficient coordinated attention among agents. In this work, we propose the Focusing Influence Mechanism (FIM), a novel framework that enhances cooperation by directing agent influence toward task-critical elements, referred to as Center of Gravity (CoG) state dimensions, inspired by Clausewitz's military theory. FIM consists of three core components: (1) identifying CoG state dimensions based on their stability under agent behavior, (2) designing counterfactual intrinsic rewards to promote meaningful influence on these dimensions, and (3) encouraging persistent and synchronized focus through eligibility-trace-based credit accumulation. These mechanisms enable agents to induce more targeted and effective state transitions, facilitating robust cooperation even in extremely sparse reward settings. Empirical evaluations across diverse MARL benchmarks demonstrate that the proposed FIM significantly improves cooperative performance compared to baselines.
<div id='section'>Paperid: <span id='pid'>891, <a href='https://arxiv.org/pdf/2506.18627.pdf' target='_blank'>https://arxiv.org/pdf/2506.18627.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yannik Mahlau, Maximilian Schier, Christoph Reinders, Frederik Schubert, Marco BÃ¼gling, Bodo Rosenhahn
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.18627">Multi-Agent Reinforcement Learning for Inverse Design in Photonic Integrated Circuits</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Inverse design of photonic integrated circuits (PICs) has traditionally relied on gradientbased optimization. However, this approach is prone to end up in local minima, which results in suboptimal design functionality. As interest in PICs increases due to their potential for addressing modern hardware demands through optical computing, more adaptive optimization algorithms are needed. We present a reinforcement learning (RL) environment as well as multi-agent RL algorithms for the design of PICs. By discretizing the design space into a grid, we formulate the design task as an optimization problem with thousands of binary variables. We consider multiple two- and three-dimensional design tasks that represent PIC components for an optical computing system. By decomposing the design space into thousands of individual agents, our algorithms are able to optimize designs with only a few thousand environment samples. They outperform previous state-of-the-art gradient-based optimization in both twoand three-dimensional design tasks. Our work may also serve as a benchmark for further exploration of sample-efficient RL for inverse design in photonics.
<div id='section'>Paperid: <span id='pid'>892, <a href='https://arxiv.org/pdf/2506.13113.pdf' target='_blank'>https://arxiv.org/pdf/2506.13113.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Stella C. Dong, James R. Finlay
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.13113">Dynamic Reinsurance Treaty Bidding via Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper develops a novel multi-agent reinforcement learning (MARL) framework for reinsurance treaty bidding, addressing long-standing inefficiencies in traditional broker-mediated placement processes. We pose the core research question: Can autonomous, learning-based bidding systems improve risk transfer efficiency and outperform conventional pricing approaches in reinsurance markets?
  In our model, each reinsurer is represented by an adaptive agent that iteratively refines its bidding strategy within a competitive, partially observable environment. The simulation explicitly incorporates institutional frictions including broker intermediation, incumbent advantages, last-look privileges, and asymmetric access to underwriting information.
  Empirical analysis demonstrates that MARL agents achieve up to 15% higher underwriting profit, 20% lower tail risk (CVaR), and over 25% improvement in Sharpe ratios relative to actuarial and heuristic baselines. Sensitivity tests confirm robustness across hyperparameter settings, and stress testing reveals strong resilience under simulated catastrophe shocks and capital constraints.
  These findings suggest that MARL offers a viable path toward more transparent, adaptive, and risk-sensitive reinsurance markets. The proposed framework contributes to emerging literature at the intersection of algorithmic market design, strategic bidding, and AI-enabled financial decision-making.
<div id='section'>Paperid: <span id='pid'>893, <a href='https://arxiv.org/pdf/2506.11445.pdf' target='_blank'>https://arxiv.org/pdf/2506.11445.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuan Duy Ta, Bang Giang Le, Thanh Ha Le, Viet Cuong Ta
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.11445">Resolve Highway Conflict in Multi-Autonomous Vehicle Controls with Local State Attention</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In mixed-traffic environments, autonomous vehicles must adapt to human-controlled vehicles and other unusual driving situations. This setting can be framed as a multi-agent reinforcement learning (MARL) environment with full cooperative reward among the autonomous vehicles. While methods such as Multi-agent Proximal Policy Optimization can be effective in training MARL tasks, they often fail to resolve local conflict between agents and are unable to generalize to stochastic events. In this paper, we propose a Local State Attention module to assist the input state representation. By relying on the self-attention operator, the module is expected to compress the essential information of nearby agents to resolve the conflict in traffic situations. Utilizing a simulated highway merging scenario with the priority vehicle as the unexpected event, our approach is able to prioritize other vehicles' information to manage the merging process. The results demonstrate significant improvements in merging efficiency compared to popular baselines, especially in high-density traffic settings.
<div id='section'>Paperid: <span id='pid'>894, <a href='https://arxiv.org/pdf/2506.11445.pdf' target='_blank'>https://arxiv.org/pdf/2506.11445.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuan Duy Ta, Bang Giang Le, Thanh Ha Le, Viet Cuong Ta
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.11445">Resolve Highway Conflict in Multi-Autonomous Vehicle Controls with Local State Attention</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In mixed-traffic environments, autonomous vehicles must adapt to human-controlled vehicles and other unusual driving situations. This setting can be framed as a multi-agent reinforcement learning (MARL) environment with full cooperative reward among the autonomous vehicles. While methods such as Multi-agent Proximal Policy Optimization can be effective in training MARL tasks, they often fail to resolve local conflict between agents and are unable to generalize to stochastic events. In this paper, we propose a Local State Attention module to assist the input state representation. By relying on the self-attention operator, the module is expected to compress the essential information of nearby agents to resolve the conflict in traffic situations. Utilizing a simulated highway merging scenario with the priority vehicle as the unexpected event, our approach is able to prioritize other vehicles' information to manage the merging process. The results demonstrate significant improvements in merging efficiency compared to popular baselines, especially in high-density traffic settings.
<div id='section'>Paperid: <span id='pid'>895, <a href='https://arxiv.org/pdf/2506.06179.pdf' target='_blank'>https://arxiv.org/pdf/2506.06179.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Muhammed Ustaomeroglu, Guannan Qu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.06179">A Theoretical Study of (Hyper) Self-Attention through the Lens of Interactions: Representation, Training, Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Self-attention has emerged as a core component of modern neural architectures, yet its theoretical underpinnings remain elusive. In this paper, we study self-attention through the lens of interacting entities, ranging from agents in multi-agent reinforcement learning to alleles in genetic sequences, and show that a single layer linear self-attention can efficiently represent, learn, and generalize functions capturing pairwise interactions, including out-of-distribution scenarios. Our analysis reveals that self-attention acts as a mutual interaction learner under minimal assumptions on the diversity of interaction patterns observed during training, thereby encompassing a wide variety of real-world domains. In addition, we validate our theoretical insights through experiments demonstrating that self-attention learns interaction functions and generalizes across both population distributions and out-of-distribution scenarios. Building on our theories, we introduce HyperFeatureAttention, a novel neural network module designed to learn couplings of different feature-level interactions between entities. Furthermore, we propose HyperAttention, a new module that extends beyond pairwise interactions to capture multi-entity dependencies, such as three-way, four-way, or general n-way interactions.
<div id='section'>Paperid: <span id='pid'>896, <a href='https://arxiv.org/pdf/2506.06032.pdf' target='_blank'>https://arxiv.org/pdf/2506.06032.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Edward Hughes, Tina O. Zhu, Martin J. Chadwick, Raphael Koster, Antonio GarcÃ­a CastaÃ±eda, Charles Beattie, Thore Graepel, Matthew M. Botvinick, Joel Z. Leibo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.06032">Modeling human reputation-seeking behavior in a spatio-temporally complex public good provision game</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent reinforcement learning algorithms are useful for simulating social behavior in settings that are too complex for other theoretical approaches like game theory. However, they have not yet been empirically supported by laboratory experiments with real human participants. In this work we demonstrate how multi-agent reinforcement learning can model group behavior in a spatially and temporally complex public good provision game called Clean Up. We show that human groups succeed in Clean Up when they can see who is who and track reputations over time but fail under conditions of anonymity. A new multi-agent reinforcement learning model of reputation-based cooperation demonstrates the same difference between identifiable and anonymous conditions. Furthermore, both human groups and artificial agent groups solve the problem via turn-taking despite other options being available. Our results highlight the benefits of using multi-agent reinforcement learning to model human social behavior in complex environments.
<div id='section'>Paperid: <span id='pid'>897, <a href='https://arxiv.org/pdf/2505.22979.pdf' target='_blank'>https://arxiv.org/pdf/2505.22979.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bengisu Guresti, Chongjie Zhang, Yevgeniy Vorobeychik
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.22979">Learning Recommender Mechanisms for Bayesian Stochastic Games</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>An important challenge in non-cooperative game theory is coordinating on a single (approximate) equilibrium from many possibilities - a challenge that becomes even more complex when players hold private information. Recommender mechanisms tackle this problem by recommending strategies to players based on their reported type profiles. A key consideration in such mechanisms is to ensure that players are incentivized to participate, report their private information truthfully, and follow the recommendations. While previous work has focused on designing recommender mechanisms for one-shot and extensive-form games, these approaches cannot be effectively applied to stochastic games, particularly if we constrain recommendations to be Markov stationary policies. To bridge this gap, we introduce a novel bi-level reinforcement learning approach for automatically designing recommender mechanisms in Bayesian stochastic games. Our method produces a mechanism represented by a parametric function (such as a neural network), and is therefore highly efficient at execution time. Experimental results on two repeated and two stochastic games demonstrate that our approach achieves social welfare levels competitive with cooperative multi-agent reinforcement learning baselines, while also providing significantly improved incentive properties.
<div id='section'>Paperid: <span id='pid'>898, <a href='https://arxiv.org/pdf/2505.19316.pdf' target='_blank'>https://arxiv.org/pdf/2505.19316.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rex Chen, Stephanie Milani, Zhicheng Zhang, Norman Sadeh, Fei Fang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.19316">Making Teams and Influencing Agents: Efficiently Coordinating Decision Trees for Interpretable Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Poor interpretability hinders the practical applicability of multi-agent reinforcement learning (MARL) policies. Deploying interpretable surrogates of uninterpretable policies enhances the safety and verifiability of MARL for real-world applications. However, if these surrogates are to interact directly with the environment within human supervisory frameworks, they must be both performant and computationally efficient. Prior work on interpretable MARL has either sacrificed performance for computational efficiency or computational efficiency for performance. To address this issue, we propose HYDRAVIPER, a decision tree-based interpretable MARL algorithm. HYDRAVIPER coordinates training between agents based on expected team performance, and adaptively allocates budgets for environment interaction to improve computational efficiency. Experiments on standard benchmark environments for multi-agent coordination and traffic signal control show that HYDRAVIPER matches the performance of state-of-the-art methods using a fraction of the runtime, and that it maintains a Pareto frontier of performance for different interaction budgets.
<div id='section'>Paperid: <span id='pid'>899, <a href='https://arxiv.org/pdf/2505.13543.pdf' target='_blank'>https://arxiv.org/pdf/2505.13543.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Muyang Fan, Songyang Liu, Shuai Li, Weizi Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.13543">Origin-Destination Pattern Effects on Large-Scale Mixed Traffic Control via Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Traffic congestion remains a major challenge for modern urban transportation, diminishing both efficiency and quality of life. While autonomous driving technologies and reinforcement learning (RL) have shown promise for improving traffic control, most prior work has focused on small-scale networks or isolated intersections. Large-scale mixed traffic control, involving both human-driven and robotic vehicles, remains underexplored. In this study, we propose a decentralized multi-agent reinforcement learning framework for managing large-scale mixed traffic networks, where intersections are controlled either by traditional traffic signals or by robotic vehicles. We evaluate our approach on a real-world network of 14 intersections in Colorado Springs, Colorado, USA, using average vehicle waiting time as the primary measure of traffic efficiency. We are exploring a problem that has not been sufficiently addressed: Is large-scale Multi-Agent Traffic Control (MTC) still feasible when facing time-varying Origin-Destination (OD) patterns?
<div id='section'>Paperid: <span id='pid'>900, <a href='https://arxiv.org/pdf/2505.08825.pdf' target='_blank'>https://arxiv.org/pdf/2505.08825.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pedro Antonio Alarcon Granadeno, Theodore Chambers, Jane Cleland-Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.08825">Multi-source Plume Tracing via Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Industrial catastrophes like the Bhopal disaster (1984) and the Aliso Canyon gas leak (2015) demonstrate the urgent need for rapid and reliable plume tracing algorithms to protect public health and the environment. Traditional methods, such as gradient-based or biologically inspired approaches, often fail in realistic, turbulent conditions. To address these challenges, we present a Multi-Agent Reinforcement Learning (MARL) algorithm designed for localizing multiple airborne pollution sources using a swarm of small uncrewed aerial systems (sUAS). Our method models the problem as a Partially Observable Markov Game (POMG), employing a Long Short-Term Memory (LSTM)-based Action-specific Double Deep Recurrent Q-Network (ADDRQN) that uses full sequences of historical action-observation pairs, effectively approximating latent states. Unlike prior work, we use a general-purpose simulation environment based on the Gaussian Plume Model (GPM), incorporating realistic elements such as a three-dimensional environment, sensor noise, multiple interacting agents, and multiple plume sources. The incorporation of action histories as part of the inputs further enhances the adaptability of our model in complex, partially observable environments. Extensive simulations show that our algorithm significantly outperforms conventional approaches. Specifically, our model allows agents to explore only 1.29\% of the environment to successfully locate pollution sources.
<div id='section'>Paperid: <span id='pid'>901, <a href='https://arxiv.org/pdf/2505.08630.pdf' target='_blank'>https://arxiv.org/pdf/2505.08630.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuai Han, Mehdi Dastani, Shihan Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.08630">Credit Assignment and Efficient Exploration based on Influence Scope in Multi-agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Training cooperative agents in sparse-reward scenarios poses significant challenges for multi-agent reinforcement learning (MARL). Without clear feedback on actions at each step in sparse-reward setting, previous methods struggle with precise credit assignment among agents and effective exploration. In this paper, we introduce a novel method to deal with both credit assignment and exploration problems in reward-sparse domains. Accordingly, we propose an algorithm that calculates the Influence Scope of Agents (ISA) on states by taking specific value of the dimensions/attributes of states that can be influenced by individual agents. The mutual dependence between agents' actions and state attributes are then used to calculate the credit assignment and to delimit the exploration space for each individual agent. We then evaluate ISA in a variety of sparse-reward multi-agent scenarios. The results show that our method significantly outperforms the state-of-art baselines.
<div id='section'>Paperid: <span id='pid'>902, <a href='https://arxiv.org/pdf/2505.07854.pdf' target='_blank'>https://arxiv.org/pdf/2505.07854.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yufei Lin, Chengwei Ye, Huanzhen Zhang, Kangsheng Wang, Linuo Xu, Shuyan Liu, Zeyu Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.07854">CCL: Collaborative Curriculum Learning for Sparse-Reward Multi-Agent Reinforcement Learning via Co-evolutionary Task Evolution</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Sparse reward environments pose significant challenges in reinforcement learning, especially within multi-agent systems (MAS) where feedback is delayed and shared across agents, leading to suboptimal learning. We propose Collaborative Multi-dimensional Course Learning (CCL), a novel curriculum learning framework that addresses this by (1) refining intermediate tasks for individual agents, (2) using a variational evolutionary algorithm to generate informative subtasks, and (3) co-evolving agents with their environment to enhance training stability. Experiments on five cooperative tasks in the MPE and Hide-and-Seek environments show that CCL outperforms existing methods in sparse reward settings.
<div id='section'>Paperid: <span id='pid'>903, <a href='https://arxiv.org/pdf/2504.04691.pdf' target='_blank'>https://arxiv.org/pdf/2504.04691.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Songyang Liu, Muyang Fan, Weizi Li, Jing Du, Shuai Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.04691">Large-Scale Mixed-Traffic and Intersection Control using Multi-agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Traffic congestion remains a significant challenge in modern urban networks. Autonomous driving technologies have emerged as a potential solution. Among traffic control methods, reinforcement learning has shown superior performance over traffic signals in various scenarios. However, prior research has largely focused on small-scale networks or isolated intersections, leaving large-scale mixed traffic control largely unexplored. This study presents the first attempt to use decentralized multi-agent reinforcement learning for large-scale mixed traffic control in which some intersections are managed by traffic signals and others by robot vehicles. Evaluating a real-world network in Colorado Springs, CO, USA with 14 intersections, we measure traffic efficiency via average waiting time of vehicles at intersections and the number of vehicles reaching their destinations within a time window (i.e., throughput). At 80% RV penetration rate, our method reduces waiting time from 6.17s to 5.09s and increases throughput from 454 vehicles per 500 seconds to 493 vehicles per 500 seconds, outperforming the baseline of fully signalized intersections. These findings suggest that integrating reinforcement learning-based control large-scale traffic can improve overall efficiency and may inform future urban planning strategies.
<div id='section'>Paperid: <span id='pid'>904, <a href='https://arxiv.org/pdf/2504.04675.pdf' target='_blank'>https://arxiv.org/pdf/2504.04675.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tzu-Han Hsu, Arshia Rafieioskouei, Borzoo Bonakdarpour
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.04675">HypRL: Reinforcement Learning of Control Policies for Hyperproperties</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reward shaping in multi-agent reinforcement learning (MARL) for complex tasks remains a significant challenge. Existing approaches often fail to find optimal solutions or cannot efficiently handle such tasks. We propose HYPRL, a specification-guided reinforcement learning framework that learns control policies w.r.t. hyperproperties expressed in HyperLTL. Hyperproperties constitute a powerful formalism for specifying objectives and constraints over sets of execution traces across agents. To learn policies that maximize the satisfaction of a HyperLTL formula $Ï$, we apply Skolemization to manage quantifier alternations and define quantitative robustness functions to shape rewards over execution traces of a Markov decision process with unknown transitions. A suitable RL algorithm is then used to learn policies that collectively maximize the expected reward and, consequently, increase the probability of satisfying $Ï$. We evaluate HYPRL on a diverse set of benchmarks, including safety-aware planning, Deep Sea Treasure, and the Post Correspondence Problem. We also compare with specification-driven baselines to demonstrate the effectiveness and efficiency of HYPRL.
<div id='section'>Paperid: <span id='pid'>905, <a href='https://arxiv.org/pdf/2504.04160.pdf' target='_blank'>https://arxiv.org/pdf/2504.04160.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alexandre Oliveira, Katarina Dyreby, Francisco Caldas, ClÃ¡udia Soares
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.04160">OrbitZoo: Multi-Agent Reinforcement Learning Environment for Orbital Dynamics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The increasing number of satellites and orbital debris has made space congestion a critical issue, threatening satellite safety and sustainability. Challenges such as collision avoidance, station-keeping, and orbital maneuvering require advanced techniques to handle dynamic uncertainties and multi-agent interactions. Reinforcement learning (RL) has shown promise in this domain, enabling adaptive, autonomous policies for space operations; however, many existing RL frameworks rely on custom-built environments developed from scratch, which often use simplified models and require significant time to implement and validate the orbital dynamics, limiting their ability to fully capture real-world complexities. To address this, we introduce OrbitZoo, a versatile multi-agent RL environment built on a high-fidelity industry standard library, that enables realistic data generation, supports scenarios like collision avoidance and cooperative maneuvers, and ensures robust and accurate orbital dynamics. The environment is validated against a real satellite constellation, Starlink, achieving a Mean Absolute Percentage Error (MAPE) of 0.16% compared to real-world data. This validation ensures reliability for generating high-fidelity simulations and enabling autonomous and independent satellite operations.
<div id='section'>Paperid: <span id='pid'>906, <a href='https://arxiv.org/pdf/2503.20078.pdf' target='_blank'>https://arxiv.org/pdf/2503.20078.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Volkan Ustun, Soham Hans, Rajay Kumar, Yunzhe Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.20078">Abstracting Geo-specific Terrains to Scale Up Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent reinforcement learning (MARL) is increasingly ubiquitous in training dynamic and adaptive synthetic characters for interactive simulations on geo-specific terrains. Frameworks such as Unity's ML-Agents help to make such reinforcement learning experiments more accessible to the simulation community. Military training simulations also benefit from advances in MARL, but they have immense computational requirements due to their complex, continuous, stochastic, partially observable, non-stationary, and doctrine-based nature. Furthermore, these simulations require geo-specific terrains, further exacerbating the computational resources problem. In our research, we leverage Unity's waypoints to automatically generate multi-layered representation abstractions of the geo-specific terrains to scale up reinforcement learning while still allowing the transfer of learned policies between different representations. Our early exploratory results on a novel MARL scenario, where each side has differing objectives, indicate that waypoint-based navigation enables faster and more efficient learning while producing trajectories similar to those taken by expert human players in CSGO gaming environments. This research points out the potential of waypoint-based navigation for reducing the computational costs of developing and training MARL models for military training simulations, where geo-specific terrains and differing objectives are crucial.
<div id='section'>Paperid: <span id='pid'>907, <a href='https://arxiv.org/pdf/2503.14576.pdf' target='_blank'>https://arxiv.org/pdf/2503.14576.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zihao Guo, Shuqing Shi, Richard Willis, Tristan Tomilin, Joel Z. Leibo, Yali Du
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.14576">SocialJax: An Evaluation Suite for Multi-agent Reinforcement Learning in Sequential Social Dilemmas</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Sequential social dilemmas pose a significant challenge in the field of multi-agent reinforcement learning (MARL), requiring environments that accurately reflect the tension between individual and collective interests. Previous benchmarks and environments, such as Melting Pot, provide an evaluation protocol that measures generalization to new social partners in various test scenarios. However, running reinforcement learning algorithms in traditional environments requires substantial computational resources. In this paper, we introduce SocialJax, a suite of sequential social dilemma environments and algorithms implemented in JAX. JAX is a high-performance numerical computing library for Python that enables significant improvements in operational efficiency. Our experiments demonstrate that the SocialJax training pipeline achieves at least 50\texttimes{} speed-up in real-time performance compared to Melting Pot RLlib baselines. Additionally, we validate the effectiveness of baseline algorithms within SocialJax environments. Finally, we use Schelling diagrams to verify the social dilemma properties of these environments, ensuring that they accurately capture the dynamics of social dilemmas.
<div id='section'>Paperid: <span id='pid'>908, <a href='https://arxiv.org/pdf/2503.11829.pdf' target='_blank'>https://arxiv.org/pdf/2503.11829.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jushan Chen, Santiago Paternain
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.11829">Learning Closed-Loop Parametric Nash Equilibria of Multi-Agent Collaborative Field Coverage</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent reinforcement learning is a challenging and active field of research due to the inherent nonstationary property and coupling between agents. A popular approach to modeling the multi-agent interactions underlying the multi-agent RL problem is the Markov Game. There is a special type of Markov Game, termed Markov Potential Game, which allows us to reduce the Markov Game to a single-objective optimal control problem where the objective function is a potential function. In this work, we prove that a multi-agent collaborative field coverage problem, which is found in many engineering applications, can be formulated as a Markov Potential Game, and we can learn a parameterized closed-loop Nash Equilibrium by solving an equivalent single-objective optimal control problem. As a result, our algorithm is 10x faster during training compared to a game-theoretic baseline and converges faster during policy execution.
<div id='section'>Paperid: <span id='pid'>909, <a href='https://arxiv.org/pdf/2503.11829.pdf' target='_blank'>https://arxiv.org/pdf/2503.11829.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jushan Chen, Santiago Paternain
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.11829">Learning Closed-Loop Parametric Nash Equilibria of Multi-Agent Collaborative Field Coverage</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent reinforcement learning is a challenging and active field of research due to the inherent nonstationary property and coupling between agents. A popular approach to modeling the multi-agent interactions underlying the multi-agent RL problem is the Markov Game. There is a special type of Markov Game, termed Markov Potential Game, which allows us to reduce the Markov Game to a single-objective optimal control problem where the objective function is a potential function. In this work, we prove that a multi-agent collaborative field coverage problem, which is found in many engineering applications, can be formulated as a Markov Potential Game, and we can learn a parameterized closed-loop Nash Equilibrium by solving an equivalent single-objective optimal control problem. As a result, our algorithm is 10x faster during training compared to a game-theoretic baseline and converges faster during policy execution.
<div id='section'>Paperid: <span id='pid'>910, <a href='https://arxiv.org/pdf/2503.11829.pdf' target='_blank'>https://arxiv.org/pdf/2503.11829.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jushan Chen, Santiago Paternain
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.11829">Learning Closed-Loop Parametric Nash Equilibria of Multi-Agent Collaborative Field Coverage</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent reinforcement learning is a challenging and active field of research due to the inherent nonstationary property and coupling between agents. A popular approach to modeling the multi-agent interactions underlying the multi-agent RL problem is the Markov Game. There is a special type of Markov Game, termed Markov Potential Game, which allows us to reduce the Markov Game to a single-objective optimal control problem where the objective function is a potential function. In this work, we prove that a multi-agent collaborative field coverage problem, which is found in many engineering applications, can be formulated as a Markov Potential Game, and we can learn a parameterized closed-loop Nash Equilibrium by solving an equivalent single-objective optimal control problem. As a result, our algorithm is 10x faster during training compared to a game-theoretic baseline and converges faster during policy execution.
<div id='section'>Paperid: <span id='pid'>911, <a href='https://arxiv.org/pdf/2503.02437.pdf' target='_blank'>https://arxiv.org/pdf/2503.02437.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Antonio Marino, Esteban Restrepo, Claudio Pacchierotti, Paolo Robuffo Giordano
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.02437">Decentralized Reinforcement Learning for Multi-Agent Multi-Resource Allocation via Dynamic Cluster Agreements</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper addresses the challenge of allocating heterogeneous resources among multiple agents in a decentralized manner. Our proposed method, LGTC-IPPO, builds upon Independent Proximal Policy Optimization (IPPO) by integrating dynamic cluster consensus, a mechanism that allows agents to form and adapt local sub-teams based on resource demands. This decentralized coordination strategy reduces reliance on global information and enhances scalability. We evaluate LGTC-IPPO against standard multi-agent reinforcement learning baselines and a centralized expert solution across a range of team sizes and resource distributions. Experimental results demonstrate that LGTC-IPPO achieves more stable rewards, better coordination, and robust performance even as the number of agents or resource types increases. Additionally, we illustrate how dynamic clustering enables agents to reallocate resources efficiently also for scenarios with discharging resources.
<div id='section'>Paperid: <span id='pid'>912, <a href='https://arxiv.org/pdf/2502.16863.pdf' target='_blank'>https://arxiv.org/pdf/2502.16863.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kartik Nagpal, Dayi Dong, Jean-Baptiste Bouvier, Negar Mehr
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.16863">Leveraging Large Language Models for Effective and Explainable Multi-Agent Credit Assignment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent work, spanning from autonomous vehicle coordination to in-space assembly, has shown the importance of learning collaborative behavior for enabling robots to achieve shared goals. A common approach for learning this cooperative behavior is to utilize the centralized-training decentralized-execution paradigm. However, this approach also introduces a new challenge: how do we evaluate the contributions of each agent's actions to the overall success or failure of the team. This credit assignment problem has remained open, and has been extensively studied in the Multi-Agent Reinforcement Learning literature. In fact, humans manually inspecting agent behavior often generate better credit evaluations than existing methods. We combine this observation with recent works which show Large Language Models demonstrate human-level performance at many pattern recognition tasks. Our key idea is to reformulate credit assignment to the two pattern recognition problems of sequence improvement and attribution, which motivates our novel LLM-MCA method. Our approach utilizes a centralized LLM reward-critic which numerically decomposes the environment reward based on the individualized contribution of each agent in the scenario. We then update the agents' policy networks based on this feedback. We also propose an extension LLM-TACA where our LLM critic performs explicit task assignment by passing an intermediary goal directly to each agent policy in the scenario. Both our methods far outperform the state-of-the-art on a variety of benchmarks, including Level-Based Foraging, Robotic Warehouse, and our new Spaceworld benchmark which incorporates collision-related safety constraints. As an artifact of our methods, we generate large trajectory datasets with each timestep annotated with per-agent reward information, as sampled from our LLM critics.
<div id='section'>Paperid: <span id='pid'>913, <a href='https://arxiv.org/pdf/2502.04038.pdf' target='_blank'>https://arxiv.org/pdf/2502.04038.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuchen Lian, Arianna Bisazza, Tessa Verhoef
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.04038">Simulating the Emergence of Differential Case Marking with Communicating Neural-Network Agents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Differential Case Marking (DCM) refers to the phenomenon where grammatical case marking is applied selectively based on semantic, pragmatic, or other factors. The emergence of DCM has been studied in artificial language learning experiments with human participants, which were specifically aimed at disentangling the effects of learning from those of communication (Smith & Culbertson, 2020). Multi-agent reinforcement learning frameworks based on neural networks have gained significant interest to simulate the emergence of human-like linguistic phenomena. In this study, we employ such a framework in which agents first acquire an artificial language before engaging in communicative interactions, enabling direct comparisons to human result. Using a very generic communication optimization algorithm and neural-network learners that have no prior experience with language or semantic preferences, our results demonstrate that learning alone does not lead to DCM, but when agents communicate, differential use of markers arises. This supports Smith and Culbertson (2020)'s findings that highlight the critical role of communication in shaping DCM and showcases the potential of neural-agent models to complement experimental research on language evolution.
<div id='section'>Paperid: <span id='pid'>914, <a href='https://arxiv.org/pdf/2502.00558.pdf' target='_blank'>https://arxiv.org/pdf/2502.00558.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sydney Dolan, Siddharth Nayak, Jasmine Jerry Aloor, Hamsa Balakrishnan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.00558">Asynchronous Cooperative Multi-Agent Reinforcement Learning with Limited Communication</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We consider the problem setting in which multiple autonomous agents must cooperatively navigate and perform tasks in an unknown, communication-constrained environment. Traditional multi-agent reinforcement learning (MARL) approaches assume synchronous communications and perform poorly in such environments. We propose AsynCoMARL, an asynchronous MARL approach that uses graph transformers to learn communication protocols from dynamic graphs. AsynCoMARL can accommodate infrequent and asynchronous communications between agents, with edges of the graph only forming when agents communicate with each other. We show that AsynCoMARL achieves similar success and collision rates as leading baselines, despite 26\% fewer messages being passed between agents.
<div id='section'>Paperid: <span id='pid'>915, <a href='https://arxiv.org/pdf/2501.16138.pdf' target='_blank'>https://arxiv.org/pdf/2501.16138.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Richard Willis, Yali Du, Joel Z Leibo, Michael Luck
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.16138">Quantifying the Self-Interest Level of Markov Social Dilemmas</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces a novel method for estimating the self-interest level of Markov social dilemmas. We extend the concept of self-interest level from normal-form games to Markov games, providing a quantitative measure of the minimum reward exchange required to align individual and collective interests. We demonstrate our method on three environments from the Melting Pot suite, representing either common-pool resources or public goods. Our results illustrate how reward exchange can enable agents to transition from selfish to collective equilibria in a Markov social dilemma. This work contributes to multi-agent reinforcement learning by providing a practical tool for analysing complex, multistep social dilemmas. Our findings offer insights into how reward structures can promote or hinder cooperation, with potential applications in areas such as mechanism design.
<div id='section'>Paperid: <span id='pid'>916, <a href='https://arxiv.org/pdf/2501.02221.pdf' target='_blank'>https://arxiv.org/pdf/2501.02221.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kanefumi Matsuyama, Kefan Su, Jiangxing Wang, Deheng Ye, Zongqing Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.02221">CORD: Generalizable Cooperation via Role Diversity</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cooperative multi-agent reinforcement learning (MARL) aims to develop agents that can collaborate effectively. However, most cooperative MARL methods overfit training agents, making learned policies not generalize well to unseen collaborators, which is a critical issue for real-world deployment. Some methods attempt to address the generalization problem but require prior knowledge or predefined policies of new teammates, limiting real-world applications. To this end, we propose a hierarchical MARL approach to enable generalizable cooperation via role diversity, namely CORD. CORD's high-level controller assigns roles to low-level agents by maximizing the role entropy with constraints. We show this constrained objective can be decomposed into causal influence in role that enables reasonable role assignment, and role heterogeneity that yields coherent, non-redundant role clusters. Evaluated on a variety of cooperative multi-agent tasks, CORD achieves better performance than baselines, especially in generalization tests. Ablation studies further demonstrate the efficacy of the constrained objective in generalizable cooperation.
<div id='section'>Paperid: <span id='pid'>917, <a href='https://arxiv.org/pdf/2501.01136.pdf' target='_blank'>https://arxiv.org/pdf/2501.01136.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nikolaos Bousias, Stefanos Pertigkiozoglou, Kostas Daniilidis, George Pappas
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.01136">Symmetries-enhanced Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent reinforcement learning has emerged as a powerful framework for enabling agents to learn complex, coordinated behaviors but faces persistent challenges regarding its generalization, scalability and sample efficiency. Recent advancements have sought to alleviate those issues by embedding intrinsic symmetries of the systems in the policy. Yet, most dynamical systems exhibit little to no symmetries to exploit. This paper presents a novel framework for embedding extrinsic symmetries in multi-agent system dynamics that enables the use of symmetry-enhanced methods to address systems with insufficient intrinsic symmetries, expanding the scope of equivariant learning to a wide variety of MARL problems. Central to our framework is the Group Equivariant Graphormer, a group-modular architecture specifically designed for distributed swarming tasks. Extensive experiments on a swarm of symmetry-breaking quadrotors validate the effectiveness of our approach, showcasing its potential for improved generalization and zero-shot scalability. Our method achieves significant reductions in collision rates and enhances task success rates across a diverse range of scenarios and varying swarm sizes.
<div id='section'>Paperid: <span id='pid'>918, <a href='https://arxiv.org/pdf/2412.15517.pdf' target='_blank'>https://arxiv.org/pdf/2412.15517.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yangkun Chen, Kai Yang, Jian Tao, Jiafei Lyu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.15517">Novelty-Guided Data Reuse for Efficient and Diversified Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, deep Multi-Agent Reinforcement Learning (MARL) has demonstrated its potential to tackle complex cooperative tasks, pushing the boundaries of AI in collaborative environments. However, the efficiency of these systems is often compromised by inadequate sample utilization and a lack of diversity in learning strategies. To enhance MARL performance, we introduce a novel sample reuse approach that dynamically adjusts policy updates based on observation novelty. Specifically, we employ a Random Network Distillation (RND) network to gauge the novelty of each agent's current state, assigning additional sample update opportunities based on the uniqueness of the data. We name our method Multi-Agent Novelty-GuidEd sample Reuse (MANGER). This method increases sample efficiency and promotes exploration and diverse agent behaviors. Our evaluations confirm substantial improvements in MARL effectiveness in complex cooperative scenarios such as Google Research Football and super-hard StarCraft II micromanagement tasks.
<div id='section'>Paperid: <span id='pid'>919, <a href='https://arxiv.org/pdf/2412.12326.pdf' target='_blank'>https://arxiv.org/pdf/2412.12326.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yue Jin, Shuangqing Wei, Giovanni Montana
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.12326">Achieving Collective Welfare in Multi-Agent Reinforcement Learning via Suggestion Sharing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In human society, the conflict between self-interest and collective well-being often obstructs efforts to achieve shared welfare. Related concepts like the Tragedy of the Commons and Social Dilemmas frequently manifest in our daily lives. As artificial agents increasingly serve as autonomous proxies for humans, we propose a novel multi-agent reinforcement learning (MARL) method to address this issue - learning policies to maximise collective returns even when individual agents' interests conflict with the collective one. Unlike traditional cooperative MARL solutions that involve sharing rewards, values, and policies or designing intrinsic rewards to encourage agents to learn collectively optimal policies, we propose a novel MARL approach where agents exchange action suggestions. Our method reveals less private information compared to sharing rewards, values, or policies, while enabling effective cooperation without the need to design intrinsic rewards. Our algorithm is supported by our theoretical analysis that establishes a bound on the discrepancy between collective and individual objectives, demonstrating how sharing suggestions can align agents' behaviours with the collective objective. Experimental results demonstrate that our algorithm performs competitively with baselines that rely on value or policy sharing or intrinsic rewards.
<div id='section'>Paperid: <span id='pid'>920, <a href='https://arxiv.org/pdf/2412.07639.pdf' target='_blank'>https://arxiv.org/pdf/2412.07639.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zongkai Liu, Qian Lin, Chao Yu, Xiawei Wu, Yile Liang, Donghui Li, Xuetao Ding
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.07639">Offline Multi-Agent Reinforcement Learning via In-Sample Sequential Policy Optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Offline Multi-Agent Reinforcement Learning (MARL) is an emerging field that aims to learn optimal multi-agent policies from pre-collected datasets. Compared to single-agent case, multi-agent setting involves a large joint state-action space and coupled behaviors of multiple agents, which bring extra complexity to offline policy optimization. In this work, we revisit the existing offline MARL methods and show that in certain scenarios they can be problematic, leading to uncoordinated behaviors and out-of-distribution (OOD) joint actions. To address these issues, we propose a new offline MARL algorithm, named In-Sample Sequential Policy Optimization (InSPO). InSPO sequentially updates each agent's policy in an in-sample manner, which not only avoids selecting OOD joint actions but also carefully considers teammates' updated policies to enhance coordination. Additionally, by thoroughly exploring low-probability actions in the behavior policy, InSPO can well address the issue of premature convergence to sub-optimal solutions. Theoretically, we prove InSPO guarantees monotonic policy improvement and converges to quantal response equilibrium (QRE). Experimental results demonstrate the effectiveness of our method compared to current state-of-the-art offline MARL methods.
<div id='section'>Paperid: <span id='pid'>921, <a href='https://arxiv.org/pdf/2411.19526.pdf' target='_blank'>https://arxiv.org/pdf/2411.19526.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yang Lv, Jinlong Lei, Peng Yi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.19526">A Local Information Aggregation based Multi-Agent Reinforcement Learning for Robot Swarm Dynamic Task Allocation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we explore how to optimize task allocation for robot swarms in dynamic environments, emphasizing the necessity of formulating robust, flexible, and scalable strategies for robot cooperation. We introduce a novel framework using a decentralized partially observable Markov decision process (Dec_POMDP), specifically designed for distributed robot swarm networks. At the core of our methodology is the Local Information Aggregation Multi-Agent Deep Deterministic Policy Gradient (LIA_MADDPG) algorithm, which merges centralized training with distributed execution (CTDE). During the centralized training phase, a local information aggregation (LIA) module is meticulously designed to gather critical data from neighboring robots, enhancing decision-making efficiency. In the distributed execution phase, a strategy improvement method is proposed to dynamically adjust task allocation based on changing and partially observable environmental conditions. Our empirical evaluations show that the LIA module can be seamlessly integrated into various CTDE-based MARL methods, significantly enhancing their performance. Additionally, by comparing LIA_MADDPG with six conventional reinforcement learning algorithms and a heuristic algorithm, we demonstrate its superior scalability, rapid adaptation to environmental changes, and ability to maintain both stability and convergence speed. These results underscore LIA_MADDPG's outstanding performance and its potential to significantly improve dynamic task allocation in robot swarms through enhanced local collaboration and adaptive strategy execution.
<div id='section'>Paperid: <span id='pid'>922, <a href='https://arxiv.org/pdf/2411.11099.pdf' target='_blank'>https://arxiv.org/pdf/2411.11099.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ting Zhu, Yue Jin, Jeremie Houssineau, Giovanni Montana
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.11099">Mitigating Relative Over-Generalization in Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In decentralized multi-agent reinforcement learning, agents learning in isolation can lead to relative over-generalization (RO), where optimal joint actions are undervalued in favor of suboptimal ones. This hinders effective coordination in cooperative tasks, as agents tend to choose actions that are individually rational but collectively suboptimal. To address this issue, we introduce MaxMax Q-Learning (MMQ), which employs an iterative process of sampling and evaluating potential next states, selecting those with maximal Q-values for learning. This approach refines approximations of ideal state transitions, aligning more closely with the optimal joint policy of collaborating agents. We provide theoretical analysis supporting MMQ's potential and present empirical evaluations across various environments susceptible to RO. Our results demonstrate that MMQ frequently outperforms existing baselines, exhibiting enhanced convergence and sample efficiency.
<div id='section'>Paperid: <span id='pid'>923, <a href='https://arxiv.org/pdf/2411.08299.pdf' target='_blank'>https://arxiv.org/pdf/2411.08299.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xin Tang, Qian Chen, Wenjie Weng, Binhan Liao, Jiacheng Wang, Xianbin Cao, Xiaohuan Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.08299">DNN Task Assignment in UAV Networks: A Generative AI Enhanced Multi-Agent Reinforcement Learning Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Unmanned Aerial Vehicles (UAVs) possess high mobility and flexible deployment capabilities, prompting the development of UAVs for various application scenarios within the Internet of Things (IoT). The unique capabilities of UAVs give rise to increasingly critical and complex tasks in uncertain and potentially harsh environments. The substantial amount of data generated from these applications necessitates processing and analysis through deep neural networks (DNNs). However, UAVs encounter challenges due to their limited computing resources when managing DNN models. This paper presents a joint approach that combines multiple-agent reinforcement learning (MARL) and generative diffusion models (GDM) for assigning DNN tasks to a UAV swarm, aimed at reducing latency from task capture to result output. To address these challenges, we first consider the task size of the target area to be inspected and the shortest flying path as optimization constraints, employing a greedy algorithm to resolve the subproblem with a focus on minimizing the UAV's flying path and the overall system cost. In the second stage, we introduce a novel DNN task assignment algorithm, termed GDM-MADDPG, which utilizes the reverse denoising process of GDM to replace the actor network in multi-agent deep deterministic policy gradient (MADDPG). This approach generates specific DNN task assignment actions based on agents' observations in a dynamic environment. Simulation results indicate that our algorithm performs favorably compared to benchmarks in terms of path planning, Age of Information (AoI), energy consumption, and task load balancing.
<div id='section'>Paperid: <span id='pid'>924, <a href='https://arxiv.org/pdf/2410.14916.pdf' target='_blank'>https://arxiv.org/pdf/2410.14916.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jasmine Jerry Aloor, Siddharth Nayak, Sydney Dolan, Hamsa Balakrishnan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.14916">Cooperation and Fairness in Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent systems are trained to maximize shared cost objectives, which typically reflect system-level efficiency. However, in the resource-constrained environments of mobility and transportation systems, efficiency may be achieved at the expense of fairness -- certain agents may incur significantly greater costs or lower rewards compared to others. Tasks could be distributed inequitably, leading to some agents receiving an unfair advantage while others incur disproportionately high costs. It is important to consider the tradeoffs between efficiency and fairness. We consider the problem of fair multi-agent navigation for a group of decentralized agents using multi-agent reinforcement learning (MARL). We consider the reciprocal of the coefficient of variation of the distances traveled by different agents as a measure of fairness and investigate whether agents can learn to be fair without significantly sacrificing efficiency (i.e., increasing the total distance traveled). We find that by training agents using min-max fair distance goal assignments along with a reward term that incentivizes fairness as they move towards their goals, the agents (1) learn a fair assignment of goals and (2) achieve almost perfect goal coverage in navigation scenarios using only local observations. For goal coverage scenarios, we find that, on average, our model yields a 14% improvement in efficiency and a 5% improvement in fairness over a baseline trained using random assignments. Furthermore, an average of 21% improvement in fairness can be achieved compared to a model trained on optimally efficient assignments; this increase in fairness comes at the expense of only a 7% decrease in efficiency. Finally, we extend our method to environments in which agents must complete coverage tasks in prescribed formations and show that it is possible to do so without tailoring the models to specific formation shapes.
<div id='section'>Paperid: <span id='pid'>925, <a href='https://arxiv.org/pdf/2410.02516.pdf' target='_blank'>https://arxiv.org/pdf/2410.02516.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Vasanth Reddy Baddam, Suat Gumussoy, Almuatazbellah Boker, Hoda Eldardiry
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.02516">Learning Emergence of Interaction Patterns across Independent RL Agents in Multi-Agent Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Many real-world problems, such as controlling swarms of drones and urban traffic, naturally lend themselves to modeling as multi-agent reinforcement learning (RL) problems. However, existing multi-agent RL methods often suffer from scalability challenges, primarily due to the introduction of communication among agents. Consequently, a key challenge lies in adapting the success of deep learning in single-agent RL to the multi-agent setting. In response to this challenge, we propose an approach that fundamentally reimagines multi-agent environments. Unlike conventional methods that model each agent individually with separate networks, our approach, the Bottom Up Network (BUN), adopts a unique perspective. BUN treats the collective of multi-agents as a unified entity while employing a specialized weight initialization strategy that promotes independent learning. Furthermore, we dynamically establish connections among agents using gradient information, enabling coordination when necessary while maintaining these connections as limited and sparse to effectively manage the computational budget. Our extensive empirical evaluations across a variety of cooperative multi-agent scenarios, including tasks such as cooperative navigation and traffic control, consistently demonstrate BUN's superiority over baseline methods with substantially reduced computational costs.
<div id='section'>Paperid: <span id='pid'>926, <a href='https://arxiv.org/pdf/2409.16720.pdf' target='_blank'>https://arxiv.org/pdf/2409.16720.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xian Wang, Jin Zhou, Yuanli Feng, Jiahao Mei, Jiming Chen, Shuo Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.16720">Dashing for the Golden Snitch: Multi-Drone Time-Optimal Motion Planning with Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent innovations in autonomous drones have facilitated time-optimal flight in single-drone configurations, and enhanced maneuverability in multi-drone systems by applying optimal control and learning-based methods. However, few studies have achieved time-optimal motion planning for multi-drone systems, particularly during highly agile maneuvers or in dynamic scenarios. This paper presents a decentralized policy network using multi-agent reinforcement learning for time-optimal multi-drone flight. To strike a balance between flight efficiency and collision avoidance, we introduce a soft collision-free mechanism inspired by optimization-based methods. By customizing PPO in a centralized training, decentralized execution (CTDE) fashion, we unlock higher efficiency and stability in training while ensuring lightweight implementation. Extensive simulations show that, despite slight performance trade-offs compared to single-drone systems, our multi-drone approach maintains near-time-optimal performance with a low collision rate. Real-world experiments validate our method, with two quadrotors using the same network as in simulation achieving a maximum speed of 13.65 m/s and a maximum body rate of 13.4 rad/s in a 5.5 m * 5.5 m * 2.0 m space across various tracks, relying entirely on onboard computation.
<div id='section'>Paperid: <span id='pid'>927, <a href='https://arxiv.org/pdf/2408.15173.pdf' target='_blank'>https://arxiv.org/pdf/2408.15173.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Batuhan Yardim, Niao He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.15173">Exploiting Approximate Symmetry for Efficient Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Mean-field games (MFG) have become significant tools for solving large-scale multi-agent reinforcement learning problems under symmetry. However, the assumption of exact symmetry limits the applicability of MFGs, as real-world scenarios often feature inherent heterogeneity. Furthermore, most works on MFG assume access to a known MFG model, which might not be readily available for real-world finite-agent games. In this work, we broaden the applicability of MFGs by providing a methodology to extend any finite-player, possibly asymmetric, game to an "induced MFG". First, we prove that $N$-player dynamic games can be symmetrized and smoothly extended to the infinite-player continuum via explicit Kirszbraun extensions. Next, we propose the notion of $Î±,Î²$-symmetric games, a new class of dynamic population games that incorporate approximate permutation invariance. For $Î±,Î²$-symmetric games, we establish explicit approximation bounds, demonstrating that a Nash policy of the induced MFG is an approximate Nash of the $N$-player dynamic game. We show that TD learning converges up to a small bias using trajectories of the $N$-player game with finite-sample guarantees, permitting symmetrized learning without building an explicit MFG model. Finally, for certain games satisfying monotonicity, we prove a sample complexity of $\widetilde{\mathcal{O}}(\varepsilon^{-6})$ for the $N$-agent game to learn an $\varepsilon$-Nash up to symmetrization bias. Our theory is supported by evaluations on MARL benchmarks with thousands of agents.
<div id='section'>Paperid: <span id='pid'>928, <a href='https://arxiv.org/pdf/2408.08075.pdf' target='_blank'>https://arxiv.org/pdf/2408.08075.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pragnya Alatur, Anas Barakat, Niao He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.08075">Independent Policy Mirror Descent for Markov Potential Games: Scaling to Large Number of Players</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Markov Potential Games (MPGs) form an important sub-class of Markov games, which are a common framework to model multi-agent reinforcement learning problems. In particular, MPGs include as a special case the identical-interest setting where all the agents share the same reward function. Scaling the performance of Nash equilibrium learning algorithms to a large number of agents is crucial for multi-agent systems. To address this important challenge, we focus on the independent learning setting where agents can only have access to their local information to update their own policy. In prior work on MPGs, the iteration complexity for obtaining $Îµ$-Nash regret scales linearly with the number of agents $N$. In this work, we investigate the iteration complexity of an independent policy mirror descent (PMD) algorithm for MPGs. We show that PMD with KL regularization, also known as natural policy gradient, enjoys a better $\sqrt{N}$ dependence on the number of agents, improving over PMD with Euclidean regularization and prior work. Furthermore, the iteration complexity is also independent of the sizes of the agents' action spaces.
<div id='section'>Paperid: <span id='pid'>929, <a href='https://arxiv.org/pdf/2408.03692.pdf' target='_blank'>https://arxiv.org/pdf/2408.03692.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yongheng Liang, Hejun Wu, Haitao Wang, Hao Cai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.03692">Asynchronous Credit Assignment for Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Credit assignment is a critical problem in multi-agent reinforcement learning (MARL), aiming to identify agents' marginal contributions for optimizing cooperative policies. Current credit assignment methods typically assume synchronous decision-making among agents. However, many real-world scenarios require agents to act asynchronously without waiting for others. This asynchrony introduces conditional dependencies between actions, which pose great challenges to current methods. To address this issue, we propose an asynchronous credit assignment framework, incorporating a Virtual Synchrony Proxy (VSP) mechanism and a Multiplicative Value Decomposition (MVD) algorithm. VSP enables physically asynchronous actions to be virtually synchronized during credit assignment. We theoretically prove that VSP preserves both task equilibrium and algorithm convergence. Furthermore, MVD leverages multiplicative interactions to effectively model dependencies among asynchronous actions, offering theoretical advantages in handling asynchronous tasks. Extensive experiments show that our framework consistently outperforms state-of-the-art MARL methods on challenging tasks while providing improved interpretability for asynchronous cooperation.
<div id='section'>Paperid: <span id='pid'>930, <a href='https://arxiv.org/pdf/2407.15077.pdf' target='_blank'>https://arxiv.org/pdf/2407.15077.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenjing Zhang, Wei Zhang, Wenqing Hu, Yifan Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.15077">B2MAPO: A Batch-by-Batch Multi-Agent Policy Optimization to Balance Performance and Efficiency</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Most multi-agent reinforcement learning approaches adopt two types of policy optimization methods that either update policy simultaneously or sequentially. Simultaneously updating policies of all agents introduces non-stationarity problem. Although sequentially updating policies agent-by-agent in an appropriate order improves policy performance, it is prone to low efficiency due to sequential execution, resulting in longer model training and execution time. Intuitively, partitioning policies of all agents according to their interdependence and updating joint policy batch-by-batch can effectively balance performance and efficiency. However, how to determine the optimal batch partition of policies and batch updating order are challenging problems. Firstly, a sequential batched policy updating scheme, B2MAPO (Batch by Batch Multi-Agent Policy Optimization), is proposed with a theoretical guarantee of the monotonic incrementally tightened bound. Secondly, a universal modulized plug-and-play B2MAPO hierarchical framework, which satisfies CTDE principle, is designed to conveniently integrate any MARL models to fully exploit and merge their merits, including policy optimality and inference efficiency. Next, a DAG-based B2MAPO algorithm is devised, which is a carefully designed implementation of B2MAPO framework. Comprehensive experimental results conducted on StarCraftII Multi-agent Challenge and Google Football Research demonstrate the performance of DAG-based B2MAPO algorithm outperforms baseline methods. Meanwhile, compared with A2PO, our algorithm reduces the model training and execution time by 60.4% and 78.7%, respectively.
<div id='section'>Paperid: <span id='pid'>931, <a href='https://arxiv.org/pdf/2407.08021.pdf' target='_blank'>https://arxiv.org/pdf/2407.08021.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuhang Zhang, Zhiyao Zhang, Marcos QuiÃ±ones-Grueiro, William Barbour, Clay Weston, Gautam Biswas, Daniel Work
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.08021">Field Deployment of Multi-Agent Reinforcement Learning Based Variable Speed Limit Controllers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This article presents the first field deployment of a multi-agent reinforcement-learning (MARL) based variable speed limit (VSL) control system on the I-24 freeway near Nashville, Tennessee. We describe how we train MARL agents in a traffic simulator and directly deploy the simulation-based policy on a 17-mile stretch of Interstate 24 with 67 VSL controllers. We use invalid action masking and several safety guards to ensure the posted speed limits satisfy the real-world constraints from the traffic management center and the Tennessee Department of Transportation. Since the time of launch of the system through April, 2024, the system has made approximately 10,000,000 decisions on 8,000,000 trips. The analysis of the controller shows that the MARL policy takes control for up to 98% of the time without intervention from safety guards. The time-space diagrams of traffic speed and control commands illustrate how the algorithm behaves during rush hour. Finally, we quantify the domain mismatch between the simulation and real-world data and demonstrate the robustness of the MARL policy to this mismatch.
<div id='section'>Paperid: <span id='pid'>932, <a href='https://arxiv.org/pdf/2406.18152.pdf' target='_blank'>https://arxiv.org/pdf/2406.18152.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junkai Zhang, Yifan Zhang, Xi Sheryl Zhang, Yifan Zang, Jian Cheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.18152">Intrinsic Action Tendency Consistency for Cooperative Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Efficient collaboration in the centralized training with decentralized execution (CTDE) paradigm remains a challenge in cooperative multi-agent systems. We identify divergent action tendencies among agents as a significant obstacle to CTDE's training efficiency, requiring a large number of training samples to achieve a unified consensus on agents' policies. This divergence stems from the lack of adequate team consensus-related guidance signals during credit assignments in CTDE. To address this, we propose Intrinsic Action Tendency Consistency, a novel approach for cooperative multi-agent reinforcement learning. It integrates intrinsic rewards, obtained through an action model, into a reward-additive CTDE (RA-CTDE) framework. We formulate an action model that enables surrounding agents to predict the central agent's action tendency. Leveraging these predictions, we compute a cooperative intrinsic reward that encourages agents to match their actions with their neighbors' predictions. We establish the equivalence between RA-CTDE and CTDE through theoretical analyses, demonstrating that CTDE's training process can be achieved using agents' individual targets. Building on this insight, we introduce a novel method to combine intrinsic rewards and CTDE. Extensive experiments on challenging tasks in SMAC and GRF benchmarks showcase the improved performance of our method.
<div id='section'>Paperid: <span id='pid'>933, <a href='https://arxiv.org/pdf/2406.17425.pdf' target='_blank'>https://arxiv.org/pdf/2406.17425.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhen Chen, Yong Liao, Youpeng Zhao, Zipeng Dai, Jian Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.17425">CuDA2: An approach for Incorporating Traitor Agents into Cooperative Multi-Agent Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cooperative Multi-Agent Reinforcement Learning (CMARL) strategies are well known to be vulnerable to adversarial perturbations. Previous works on adversarial attacks have primarily focused on white-box attacks that directly perturb the states or actions of victim agents, often in scenarios with a limited number of attacks. However, gaining complete access to victim agents in real-world environments is exceedingly difficult. To create more realistic adversarial attacks, we introduce a novel method that involves injecting traitor agents into the CMARL system. We model this problem as a Traitor Markov Decision Process (TMDP), where traitors cannot directly attack the victim agents but can influence their formation or positioning through collisions. In TMDP, traitors are trained using the same MARL algorithm as the victim agents, with their reward function set as the negative of the victim agents' reward. Despite this, the training efficiency for traitors remains low because it is challenging for them to directly associate their actions with the victim agents' rewards. To address this issue, we propose the Curiosity-Driven Adversarial Attack (CuDA2) framework. CuDA2 enhances the efficiency and aggressiveness of attacks on the specified victim agents' policies while maintaining the optimal policy invariance of the traitors. Specifically, we employ a pre-trained Random Network Distillation (RND) module, where the extra reward generated by the RND module encourages traitors to explore states unencountered by the victim agents. Extensive experiments on various scenarios from SMAC demonstrate that our CuDA2 framework offers comparable or superior adversarial attack capabilities compared to other baselines.
<div id='section'>Paperid: <span id='pid'>934, <a href='https://arxiv.org/pdf/2406.01853.pdf' target='_blank'>https://arxiv.org/pdf/2406.01853.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Riqiang Gao, Florin C. Ghesu, Simon Arberet, Shahab Basiri, Esa Kuusela, Martin Kraus, Dorin Comaniciu, Ali Kamen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.01853">Multi-Agent Reinforcement Learning Meets Leaf Sequencing in Radiotherapy</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In contemporary radiotherapy planning (RTP), a key module leaf sequencing is predominantly addressed by optimization-based approaches. In this paper, we propose a novel deep reinforcement learning (DRL) model termed as Reinforced Leaf Sequencer (RLS) in a multi-agent framework for leaf sequencing. The RLS model offers improvements to time-consuming iterative optimization steps via large-scale training and can control movement patterns through the design of reward mechanisms. We have conducted experiments on four datasets with four metrics and compared our model with a leading optimization sequencer. Our findings reveal that the proposed RLS model can achieve reduced fluence reconstruction errors, and potential faster convergence when integrated in an optimization planner. Additionally, RLS has shown promising results in a full artificial intelligence RTP pipeline. We hope this pioneer multi-agent RL leaf sequencer can foster future research on machine learning for RTP.
<div id='section'>Paperid: <span id='pid'>935, <a href='https://arxiv.org/pdf/2405.02769.pdf' target='_blank'>https://arxiv.org/pdf/2405.02769.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Youbang Sun, Tao Liu, P. R. Kumar, Shahin Shahrampour
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.02769">Linear Convergence of Independent Natural Policy Gradient in Games with Entropy Regularization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work focuses on the entropy-regularized independent natural policy gradient (NPG) algorithm in multi-agent reinforcement learning. In this work, agents are assumed to have access to an oracle with exact policy evaluation and seek to maximize their respective independent rewards. Each individual's reward is assumed to depend on the actions of all the agents in the multi-agent system, leading to a game between agents. We assume all agents make decisions under a policy with bounded rationality, which is enforced by the introduction of entropy regularization. In practice, a smaller regularization implies the agents are more rational and behave closer to Nash policies. On the other hand, agents with larger regularization acts more randomly, which ensures more exploration. We show that, under sufficient entropy regularization, the dynamics of this system converge at a linear rate to the quantal response equilibrium (QRE). Although regularization assumptions prevent the QRE from approximating a Nash equilibrium, our findings apply to a wide range of games, including cooperative, potential, and two-player matrix games. We also provide extensive empirical results on multiple games (including Markov games) as a verification of our theoretical analysis.
<div id='section'>Paperid: <span id='pid'>936, <a href='https://arxiv.org/pdf/2404.10728.pdf' target='_blank'>https://arxiv.org/pdf/2404.10728.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao-Lun Hsu, Weixin Wang, Miroslav Pajic, Pan Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.10728">Randomized Exploration in Cooperative Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present the first study on provably efficient randomized exploration in cooperative multi-agent reinforcement learning (MARL). We propose a unified algorithm framework for randomized exploration in parallel Markov Decision Processes (MDPs), and two Thompson Sampling (TS)-type algorithms, CoopTS-PHE and CoopTS-LMC, incorporating the perturbed-history exploration (PHE) strategy and the Langevin Monte Carlo exploration (LMC) strategy, respectively, which are flexible in design and easy to implement in practice. For a special class of parallel MDPs where the transition is (approximately) linear, we theoretically prove that both CoopTS-PHE and CoopTS-LMC achieve a $\widetilde{\mathcal{O}}(d^{3/2}H^2\sqrt{MK})$ regret bound with communication complexity $\widetilde{\mathcal{O}}(dHM^2)$, where $d$ is the feature dimension, $H$ is the horizon length, $M$ is the number of agents, and $K$ is the number of episodes. This is the first theoretical result for randomized exploration in cooperative MARL. We evaluate our proposed method on multiple parallel RL environments, including a deep exploration problem (i.e., $N$-chain), a video game, and a real-world problem in energy systems. Our experimental results support that our framework can achieve better performance, even under conditions of misspecified transition models. Additionally, we establish a connection between our unified framework and the practical application of federated learning.
<div id='section'>Paperid: <span id='pid'>937, <a href='https://arxiv.org/pdf/2404.02361.pdf' target='_blank'>https://arxiv.org/pdf/2404.02361.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tiago Fonseca, Luis Ferreira, Bernardo Cabral, Ricardo Severino, Isabel Praca
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.02361">EnergAIze: Multi Agent Deep Deterministic Policy Gradient for Vehicle to Grid Energy Management</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper investigates the increasing roles of Renewable Energy Sources (RES) and Electric Vehicles (EVs). While indicating a new era of sustainable energy, these also introduce complex challenges, including the need to balance supply and demand and smooth peak consumptions amidst rising EV adoption rates. Addressing these challenges requires innovative solutions such as Demand Response (DR), energy flexibility management, Renewable Energy Communities (RECs), and more specifically for EVs, Vehicle-to-Grid (V2G). However, existing V2G approaches often fall short in real-world adaptability, global REC optimization with other flexible assets, scalability, and user engagement. To bridge this gap, this paper introduces EnergAIze, a Multi-Agent Reinforcement Learning (MARL) energy management framework, leveraging the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm. EnergAIze enables user-centric and multi-objective energy management by allowing each prosumer to select from a range of personal management objectives, thus encouraging engagement. Additionally, it architects' data protection and ownership through decentralized computing, where each prosumer can situate an energy management optimization node directly at their own dwelling. The local node not only manages local energy assets but also fosters REC wide optimization. The efficacy of EnergAIze was evaluated through case studies employing the CityLearn simulation framework. These simulations were instrumental in demonstrating EnergAIze's adeptness at implementing V2G technology within a REC and other energy assets. The results show reduction in peak loads, ramping, carbon emissions, and electricity costs at the REC level while optimizing for individual prosumers objectives.
<div id='section'>Paperid: <span id='pid'>938, <a href='https://arxiv.org/pdf/2404.01557.pdf' target='_blank'>https://arxiv.org/pdf/2404.01557.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Raffaele Galliera, Thies MÃ¶hlenhof, Alessandro Amato, Daniel Duran, Kristen Brent Venable, Niranjan Suri
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.01557">Distributed Autonomous Swarm Formation for Dynamic Network Bridging</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Effective operation and seamless cooperation of robotic systems are a fundamental component of next-generation technologies and applications. In contexts such as disaster response, swarm operations require coordinated behavior and mobility control to be handled in a distributed manner, with the quality of the agents' actions heavily relying on the communication between them and the underlying network. In this paper, we formulate the problem of dynamic network bridging in a novel Decentralized Partially Observable Markov Decision Process (Dec-POMDP), where a swarm of agents cooperates to form a link between two distant moving targets. Furthermore, we propose a Multi-Agent Reinforcement Learning (MARL) approach for the problem based on Graph Convolutional Reinforcement Learning (DGN) which naturally applies to the networked, distributed nature of the task. The proposed method is evaluated in a simulated environment and compared to a centralized heuristic baseline showing promising results. Moreover, a further step in the direction of sim-to-real transfer is presented, by additionally evaluating the proposed approach in a near Live Virtual Constructive (LVC) UAV framework.
<div id='section'>Paperid: <span id='pid'>939, <a href='https://arxiv.org/pdf/2404.01551.pdf' target='_blank'>https://arxiv.org/pdf/2404.01551.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Raffaele Galliera, Konstantinos Mitsopoulos, Niranjan Suri, Raffaele Romagnoli
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.01551">Safety-Aware Multi-Agent Learning for Dynamic Network Bridging</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Addressing complex cooperative tasks in safety-critical environments poses significant challenges for multi-agent systems, especially under conditions of partial observability. We focus on a dynamic network bridging task, where agents must learn to maintain a communication path between two moving targets. To ensure safety during training and deployment, we integrate a control-theoretic safety filter that enforces collision avoidance through local setpoint updates. We develop and evaluate multi-agent reinforcement learning safety-informed message passing, showing that encoding safety filter activations as edge-level features improves coordination. The results suggest that local safety enforcement and decentralized learning can be effectively combined in distributed multi-agent tasks.
<div id='section'>Paperid: <span id='pid'>940, <a href='https://arxiv.org/pdf/2402.17885.pdf' target='_blank'>https://arxiv.org/pdf/2402.17885.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Philip Jordan, Anas Barakat, Niao He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.17885">Independent Learning in Constrained Markov Potential Games</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Constrained Markov games offer a formal mathematical framework for modeling multi-agent reinforcement learning problems where the behavior of the agents is subject to constraints. In this work, we focus on the recently introduced class of constrained Markov Potential Games. While centralized algorithms have been proposed for solving such constrained games, the design of converging independent learning algorithms tailored for the constrained setting remains an open question. We propose an independent policy gradient algorithm for learning approximate constrained Nash equilibria: Each agent observes their own actions and rewards, along with a shared state. Inspired by the optimization literature, our algorithm performs proximal-point-like updates augmented with a regularized constraint set. Each proximal step is solved inexactly using a stochastic switching gradient algorithm. Notably, our algorithm can be implemented independently without a centralized coordination mechanism requiring turn-based agent updates. Under some technical constraint qualification conditions, we establish convergence guarantees towards constrained approximate Nash equilibria. We perform simulations to illustrate our results.
<div id='section'>Paperid: <span id='pid'>941, <a href='https://arxiv.org/pdf/2402.05757.pdf' target='_blank'>https://arxiv.org/pdf/2402.05757.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Batuhan Yardim, Artur Goldman, Niao He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.05757">When is Mean-Field Reinforcement Learning Tractable and Relevant?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Mean-field reinforcement learning has become a popular theoretical framework for efficiently approximating large-scale multi-agent reinforcement learning (MARL) problems exhibiting symmetry. However, questions remain regarding the applicability of mean-field approximations: in particular, their approximation accuracy of real-world systems and conditions under which they become computationally tractable. We establish explicit finite-agent bounds for how well the MFG solution approximates the true $N$-player game for two popular mean-field solution concepts. Furthermore, for the first time, we establish explicit lower bounds indicating that MFGs are poor or uninformative at approximating $N$-player games assuming only Lipschitz dynamics and rewards. Finally, we analyze the computational complexity of solving MFGs with only Lipschitz properties and prove that they are in the class of \textsc{PPAD}-complete problems conjectured to be intractable, similar to general sum $N$ player games. Our theoretical results underscore the limitations of MFGs and complement and justify existing work by proving difficulty in the absence of common theoretical assumptions.
<div id='section'>Paperid: <span id='pid'>942, <a href='https://arxiv.org/pdf/2401.13127.pdf' target='_blank'>https://arxiv.org/pdf/2401.13127.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pierce Howell, Max Rudolph, Reza Torbati, Kevin Fu, Harish Ravichandar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.13127">Generalization of Heterogeneous Multi-Robot Policies via Awareness and Communication of Capabilities</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in multi-agent reinforcement learning (MARL) are enabling impressive coordination in heterogeneous multi-robot teams. However, existing approaches often overlook the challenge of generalizing learned policies to teams of new compositions, sizes, and robots. While such generalization might not be important in teams of virtual agents that can retrain policies on-demand, it is pivotal in multi-robot systems that are deployed in the real-world and must readily adapt to inevitable changes. As such, multi-robot policies must remain robust to team changes -- an ability we call adaptive teaming. In this work, we investigate if awareness and communication of robot capabilities can provide such generalization by conducting detailed experiments involving an established multi-robot test bed. We demonstrate that shared decentralized policies, that enable robots to be both aware of and communicate their capabilities, can achieve adaptive teaming by implicitly capturing the fundamental relationship between collective capabilities and effective coordination. Videos of trained policies can be viewed at: https://sites.google.com/view/cap-comm
<div id='section'>Paperid: <span id='pid'>943, <a href='https://arxiv.org/pdf/2312.11768.pdf' target='_blank'>https://arxiv.org/pdf/2312.11768.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rupali Bhati, Sai Krishna Gottipati, ClodÃ©ric Mars, Matthew E. Taylor
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.11768">Curriculum Learning for Cooperation in Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While there has been significant progress in curriculum learning and continuous learning for training agents to generalize across a wide variety of environments in the context of single-agent reinforcement learning, it is unclear if these algorithms would still be valid in a multi-agent setting. In a competitive setting, a learning agent can be trained by making it compete with a curriculum of increasingly skilled opponents. However, a general intelligent agent should also be able to learn to act around other agents and cooperate with them to achieve common goals. When cooperating with other agents, the learning agent must (a) learn how to perform the task (or subtask), and (b) increase the overall team reward. In this paper, we aim to answer the question of what kind of cooperative teammate, and a curriculum of teammates should a learning agent be trained with to achieve these two objectives. Our results on the game Overcooked show that a pre-trained teammate who is less skilled is the best teammate for overall team reward but the worst for the learning of the agent. Moreover, somewhat surprisingly, a curriculum of teammates with decreasing skill levels performs better than other types of curricula.
<div id='section'>Paperid: <span id='pid'>944, <a href='https://arxiv.org/pdf/2312.05686.pdf' target='_blank'>https://arxiv.org/pdf/2312.05686.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ananta Mukherjee, Peeyush Kumar, Boling Yang, Nishanth Chandran, Divya Gupta
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.05686">Privacy Preserving Multi-Agent Reinforcement Learning in Supply Chains</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper addresses privacy concerns in multi-agent reinforcement learning (MARL), specifically within the context of supply chains where individual strategic data must remain confidential. Organizations within the supply chain are modeled as agents, each seeking to optimize their own objectives while interacting with others. As each organization's strategy is contingent on neighboring strategies, maintaining privacy of state and action-related information is crucial. To tackle this challenge, we propose a game-theoretic, privacy-preserving mechanism, utilizing a secure multi-party computation (MPC) framework in MARL settings. Our major contribution is the successful implementation of a secure MPC framework, SecFloat on EzPC, to solve this problem. However, simply implementing policy gradient methods such as MADDPG operations using SecFloat, while conceptually feasible, would be programmatically intractable. To overcome this hurdle, we devise a novel approach that breaks down the forward and backward pass of the neural network into elementary operations compatible with SecFloat , creating efficient and secure versions of the MADDPG algorithm. Furthermore, we present a learning mechanism that carries out floating point operations in a privacy-preserving manner, an important feature for successful learning in MARL framework. Experiments reveal that there is on average 68.19% less supply chain wastage in 2 PC compared to no data share, while also giving on average 42.27% better average cumulative revenue for each player. This work paves the way for practical, privacy-preserving MARL, promising significant improvements in secure computation within supply chain contexts and broadly.
<div id='section'>Paperid: <span id='pid'>945, <a href='https://arxiv.org/pdf/2312.00907.pdf' target='_blank'>https://arxiv.org/pdf/2312.00907.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rambod Mojgani, Daniel Waelchli, Yifei Guan, Petros Koumoutsakos, Pedram Hassanzadeh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.00907">Extreme Event Prediction with Multi-agent Reinforcement Learning-based Parametrization of Atmospheric and Oceanic Turbulence</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Global climate models (GCMs) are the main tools for understanding and predicting climate change. However, due to limited numerical resolutions, these models suffer from major structural uncertainties; e.g., they cannot resolve critical processes such as small-scale eddies in atmospheric and oceanic turbulence. Thus, such small-scale processes have to be represented as a function of the resolved scales via closures (parametrization). The accuracy of these closures is particularly important for capturing climate extremes. Traditionally, such closures are based on heuristics and simplifying assumptions about the unresolved physics. Recently, supervised-learned closures, trained offline on high-fidelity data, have been shown to outperform the classical physics-based closures. However, this approach requires a significant amount of high-fidelity training data and can also lead to instabilities. Reinforcement learning is emerging as a potent alternative for developing such closures as it requires only low-order statistics and leads to stable closures. In Scientific Multi-Agent Reinforcement Learning (SMARL) computational elements serve a dual role of discretization points and learning agents. We leverage SMARL and fundamentals of turbulence physics to learn closures for prototypes of atmospheric and oceanic turbulence. The policy is trained using only the enstrophy spectrum, which is nearly invariant and can be estimated from a few high-fidelity samples (these few samples are far from enough for supervised/offline learning). We show that these closures lead to stable low-resolution simulations that, at a fraction of the cost, can reproduce the high-fidelity simulations' statistics, including the tails of the probability density functions. The results demonstrate the high potential of SMARL for closure modeling for GCMs, especially in the regime of scarce data and indirect observations.
<div id='section'>Paperid: <span id='pid'>946, <a href='https://arxiv.org/pdf/2311.16576.pdf' target='_blank'>https://arxiv.org/pdf/2311.16576.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaojie Wang, Jiameng Li, Zhaolong Ning, Qingyang Song, Lei Guo, Abbas Jamalipour
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.16576">Wireless Powered Metaverse: Joint Task Scheduling and Trajectory Design for Multi-Devices and Multi-UAVs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To support the running of human-centric metaverse applications on mobile devices, Unmanned Aerial Vehicle (UAV)-assisted Wireless Powered Mobile Edge Computing (WPMEC) is promising to compensate for limited computational capabilities and energy supplies of mobile devices. The high-speed computational processing demands and significant energy consumption of metaverse applications require joint resource scheduling of multiple devices and UAVs, but existing WPMEC solutions address either device or UAV scheduling due to the complexity of combinatorial optimization. To solve the above challenge, we propose a two-stage alternating optimization algorithm based on multi-task Deep Reinforcement Learning (DRL) to jointly allocate charging time, schedule computation tasks, and optimize trajectory of UAVs and mobile devices in a wireless powered metaverse scenario. First, considering energy constraints of both UAVs and mobile devices, we formulate an optimization problem to maximize the computation efficiency of the system. Second, we propose a heuristic algorithm to efficiently perform time allocation and charging scheduling for mobile devices. Following this, we design a multi-task DRL scheme to make charging scheduling and trajectory design decisions for UAVs. Finally, theoretical analysis and performance results demonstrate that our algorithm exhibits significant advantages over representative methods in terms of convergence speed and average computation efficiency.
<div id='section'>Paperid: <span id='pid'>947, <a href='https://arxiv.org/pdf/2310.16659.pdf' target='_blank'>https://arxiv.org/pdf/2310.16659.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qizhen Wu, Kexin Liu, Lei Chen, Jinhu LÃ¼
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.16659">Multi-Agent Reinforcement Learning-Based UAV Pathfinding for Obstacle Avoidance in Stochastic Environment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Traditional methods plan feasible paths for multiple agents in the stochastic environment. However, the methods' iterations with the changes in the environment result in computation complexities, especially for the decentralized agents without a centralized planner. Although reinforcement learning provides a plausible solution because of the generalization for different environments, it struggles with enormous agent-environment interactions in training. Here, we propose a novel centralized training with decentralized execution method based on multi-agent reinforcement learning, which is improved based on the idea of model predictive control. In our approach, agents communicate only with the centralized planner to make decentralized decisions online in the stochastic environment. Furthermore, considering the communication constraint with the centralized planner, each agent plans feasible paths through the extended observation, which combines information on neighboring agents based on the distance-weighted mean field approach. Inspired by the rolling optimization approach of model predictive control, we conduct multi-step value convergence in multi-agent reinforcement learning to enhance the training efficiency, which reduces the expensive interactions in convergence. Experiment results in both comparison, ablation, and real-robot studies validate the effectiveness and generalization performance of our method.
<div id='section'>Paperid: <span id='pid'>948, <a href='https://arxiv.org/pdf/2310.09727.pdf' target='_blank'>https://arxiv.org/pdf/2310.09727.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Youbang Sun, Tao Liu, Ruida Zhou, P. R. Kumar, Shahin Shahrampour
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.09727">Provably Fast Convergence of Independent Natural Policy Gradient for Markov Potential Games</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work studies an independent natural policy gradient (NPG) algorithm for the multi-agent reinforcement learning problem in Markov potential games. It is shown that, under mild technical assumptions and the introduction of the \textit{suboptimality gap}, the independent NPG method with an oracle providing exact policy evaluation asymptotically reaches an $Îµ$-Nash Equilibrium (NE) within $\mathcal{O}(1/Îµ)$ iterations. This improves upon the previous best result of $\mathcal{O}(1/Îµ^2)$ iterations and is of the same order, $\mathcal{O}(1/Îµ)$, that is achievable for the single-agent case. Empirical results for a synthetic potential game and a congestion game are presented to verify the theoretical bounds.
<div id='section'>Paperid: <span id='pid'>949, <a href='https://arxiv.org/pdf/2310.08549.pdf' target='_blank'>https://arxiv.org/pdf/2310.08549.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lucy Xiaoyang Shi, Yunfan Jiang, Jake Grigsby, Linxi "Jim" Fan, Yuke Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.08549">Cross-Episodic Curriculum for Transformer Agents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a new algorithm, Cross-Episodic Curriculum (CEC), to boost the learning efficiency and generalization of Transformer agents. Central to CEC is the placement of cross-episodic experiences into a Transformer's context, which forms the basis of a curriculum. By sequentially structuring online learning trials and mixed-quality demonstrations, CEC constructs curricula that encapsulate learning progression and proficiency increase across episodes. Such synergy combined with the potent pattern recognition capabilities of Transformer models delivers a powerful cross-episodic attention mechanism. The effectiveness of CEC is demonstrated under two representative scenarios: one involving multi-task reinforcement learning with discrete control, such as in DeepMind Lab, where the curriculum captures the learning progression in both individual and progressively complex settings; and the other involving imitation learning with mixed-quality data for continuous control, as seen in RoboMimic, where the curriculum captures the improvement in demonstrators' expertise. In all instances, policies resulting from CEC exhibit superior performance and strong generalization. Code is open-sourced at https://cec-agent.github.io/ to facilitate research on Transformer agent learning.
<div id='section'>Paperid: <span id='pid'>950, <a href='https://arxiv.org/pdf/2310.01695.pdf' target='_blank'>https://arxiv.org/pdf/2310.01695.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tarik Dzanic, Ketan Mittal, Dohyun Kim, Jiachen Yang, Socratis Petrides, Brendan Keith, Robert Anderson
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.01695">DynAMO: Multi-agent reinforcement learning for dynamic anticipatory mesh optimization with applications to hyperbolic conservation laws</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce DynAMO, a reinforcement learning paradigm for Dynamic Anticipatory Mesh Optimization. Adaptive mesh refinement is an effective tool for optimizing computational cost and solution accuracy in numerical methods for partial differential equations. However, traditional adaptive mesh refinement approaches for time-dependent problems typically rely only on instantaneous error indicators to guide adaptivity. As a result, standard strategies often require frequent remeshing to maintain accuracy. In the DynAMO approach, multi-agent reinforcement learning is used to discover new local refinement policies that can anticipate and respond to future solution states by producing meshes that deliver more accurate solutions for longer time intervals. By applying DynAMO to discontinuous Galerkin methods for the linear advection and compressible Euler equations in two dimensions, we demonstrate that this new mesh refinement paradigm can outperform conventional threshold-based strategies while also generalizing to different mesh sizes, remeshing and simulation times, and initial conditions.
<div id='section'>Paperid: <span id='pid'>951, <a href='https://arxiv.org/pdf/2309.15603.pdf' target='_blank'>https://arxiv.org/pdf/2309.15603.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bang Giang Le, Viet Cuong Ta
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.15603">Distill Knowledge in Multi-task Reinforcement Learning with Optimal-Transport Regularization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In multi-task reinforcement learning, it is possible to improve the data efficiency of training agents by transferring knowledge from other different but related tasks. Because the experiences from different tasks are usually biased toward the specific task goals. Traditional methods rely on Kullback-Leibler regularization to stabilize the transfer of knowledge from one task to the others. In this work, we explore the direction of replacing the Kullback-Leibler divergence with a novel Optimal transport-based regularization. By using the Sinkhorn mapping, we can approximate the Optimal transport distance between the state distribution of tasks. The distance is then used as an amortized reward to regularize the amount of sharing information. We experiment our frameworks on several grid-based navigation multi-goal to validate the effectiveness of the approach. The results show that our added Optimal transport-based rewards are able to speed up the learning process of agents and outperforms several baselines on multi-task learning.
<div id='section'>Paperid: <span id='pid'>952, <a href='https://arxiv.org/pdf/2308.16198.pdf' target='_blank'>https://arxiv.org/pdf/2308.16198.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Raffaele Galliera, Kristen Brent Venable, Matteo Bassani, Niranjan Suri
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.16198">Collaborative Information Dissemination with Graph-based Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Efficient information dissemination is crucial for supporting critical operations across domains like disaster response, autonomous vehicles, and sensor networks. This paper introduces a Multi-Agent Reinforcement Learning (MARL) approach as a significant step forward in achieving more decentralized, efficient, and collaborative information dissemination. We propose a Partially Observable Stochastic Game (POSG) formulation for information dissemination empowering each agent to decide on message forwarding independently, based on the observation of their one-hop neighborhood. This constitutes a significant paradigm shift from heuristics currently employed in real-world broadcast protocols. Our novel approach harnesses Graph Convolutional Reinforcement Learning and Graph Attention Networks (GATs) with dynamic attention to capture essential network features. We propose two approaches, L-DyAN and HL-DyAN, which differ in terms of the information exchanged among agents. Our experimental results show that our trained policies outperform existing methods, including the state-of-the-art heuristic, in terms of network coverage as well as communication overhead on dynamic networks of varying density and behavior.
<div id='section'>Paperid: <span id='pid'>953, <a href='https://arxiv.org/pdf/2308.14111.pdf' target='_blank'>https://arxiv.org/pdf/2308.14111.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiarong Fan, Hao Wang, Ariel Liebman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.14111">MARL for Decentralized Electric Vehicle Charging Coordination with V2V Energy Exchange</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Effective energy management of electric vehicle (EV) charging stations is critical to supporting the transport sector's sustainable energy transition. This paper addresses the EV charging coordination by considering vehicle-to-vehicle (V2V) energy exchange as the flexibility to harness in EV charging stations. Moreover, this paper takes into account EV user experiences, such as charging satisfaction and fairness. We propose a Multi-Agent Reinforcement Learning (MARL) approach to coordinate EV charging with V2V energy exchange while considering uncertainties in the EV arrival time, energy price, and solar energy generation. The exploration capability of MARL is enhanced by introducing parameter noise into MARL's neural network models. Experimental results demonstrate the superior performance and scalability of our proposed method compared to traditional optimization baselines. The decentralized execution of the algorithm enables it to effectively deal with partial system faults in the charging station.
<div id='section'>Paperid: <span id='pid'>954, <a href='https://arxiv.org/pdf/2308.09909.pdf' target='_blank'>https://arxiv.org/pdf/2308.09909.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chenghao Li, Tonghan Wang, Chongjie Zhang, Qianchuan Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.09909">Never Explore Repeatedly in Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the realm of multi-agent reinforcement learning, intrinsic motivations have emerged as a pivotal tool for exploration. While the computation of many intrinsic rewards relies on estimating variational posteriors using neural network approximators, a notable challenge has surfaced due to the limited expressive capability of these neural statistics approximators. We pinpoint this challenge as the "revisitation" issue, where agents recurrently explore confined areas of the task space. To combat this, we propose a dynamic reward scaling approach. This method is crafted to stabilize the significant fluctuations in intrinsic rewards in previously explored areas and promote broader exploration, effectively curbing the revisitation phenomenon. Our experimental findings underscore the efficacy of our approach, showcasing enhanced performance in demanding environments like Google Research Football and StarCraft II micromanagement tasks, especially in sparse reward settings.
<div id='section'>Paperid: <span id='pid'>955, <a href='https://arxiv.org/pdf/2307.11046.pdf' target='_blank'>https://arxiv.org/pdf/2307.11046.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>David Abel, AndrÃ© Barreto, Benjamin Van Roy, Doina Precup, Hado van Hasselt, Satinder Singh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.11046">A Definition of Continual Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In a standard view of the reinforcement learning problem, an agent's goal is to efficiently identify a policy that maximizes long-term reward. However, this perspective is based on a restricted view of learning as finding a solution, rather than treating learning as endless adaptation. In contrast, continual reinforcement learning refers to the setting in which the best agents never stop learning. Despite the importance of continual reinforcement learning, the community lacks a simple definition of the problem that highlights its commitments and makes its primary concepts precise and clear. To this end, this paper is dedicated to carefully defining the continual reinforcement learning problem. We formalize the notion of agents that "never stop learning" through a new mathematical language for analyzing and cataloging agents. Using this new language, we define a continual learning agent as one that can be understood as carrying out an implicit search process indefinitely, and continual reinforcement learning as the setting in which the best agents are all continual learning agents. We provide two motivating examples, illustrating that traditional views of multi-task reinforcement learning and continual supervised learning are special cases of our definition. Collectively, these definitions and perspectives formalize many intuitive concepts at the heart of learning, and open new research pathways surrounding continual learning agents.
<div id='section'>Paperid: <span id='pid'>956, <a href='https://arxiv.org/pdf/2307.02200.pdf' target='_blank'>https://arxiv.org/pdf/2307.02200.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shanqi Liu, Weiwei Liu, Wenzhou Chen, Guanzhong Tian, Yong Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.02200">Multi-Agent Cooperation via Unsupervised Learning of Joint Intentions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The field of cooperative multi-agent reinforcement learning (MARL) has seen widespread use in addressing complex coordination tasks. While value decomposition methods in MARL have been popular, they have limitations in solving tasks with non-monotonic returns, restricting their general application. Our work highlights the significance of joint intentions in cooperation, which can overcome non-monotonic problems and increase the interpretability of the learning process. To this end, we present a novel MARL method that leverages learnable joint intentions. Our method employs a hierarchical framework consisting of a joint intention policy and a behavior policy to formulate the optimal cooperative policy. The joint intentions are autonomously learned in a latent space through unsupervised learning and enable the method adaptable to different agent configurations. Our results demonstrate significant performance improvements in both the StarCraft micromanagement benchmark and challenging MAgent domains, showcasing the effectiveness of our method in learning meaningful joint intentions.
<div id='section'>Paperid: <span id='pid'>957, <a href='https://arxiv.org/pdf/2306.08359.pdf' target='_blank'>https://arxiv.org/pdf/2306.08359.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuechen Mu, Hankz Hankui Zhuo, Chen Chen, Kai Zhang, Chao Yu, Jianye Hao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.08359">Hierarchical Task Network Planning for Facilitating Cooperative Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Exploring sparse reward multi-agent reinforcement learning (MARL) environments with traps in a collaborative manner is a complex task. Agents typically fail to reach the goal state and fall into traps, which affects the overall performance of the system. To overcome this issue, we present SOMARL, a framework that uses prior knowledge to reduce the exploration space and assist learning. In SOMARL, agents are treated as part of the MARL environment, and symbolic knowledge is embedded using a tree structure to build a knowledge hierarchy. The framework has a two-layer hierarchical structure, comprising a hybrid module with a Hierarchical Task Network (HTN) planning and meta-controller at the higher level, and a MARL-based interactive module at the lower level. The HTN module and meta-controller use Hierarchical Domain Definition Language (HDDL) and the option framework to formalize symbolic knowledge and obtain domain knowledge and a symbolic option set, respectively. Moreover, the HTN module leverages domain knowledge to guide low-level agent exploration by assisting the meta-controller in selecting symbolic options. The meta-controller further computes intrinsic rewards of symbolic options to limit exploration behavior and adjust HTN planning solutions as needed. We evaluate SOMARL on two benchmarks, FindTreasure and MoveBox, and report superior performance over state-of-the-art MARL and subgoal-based baselines for MARL environments significantly.
<div id='section'>Paperid: <span id='pid'>958, <a href='https://arxiv.org/pdf/2305.17375.pdf' target='_blank'>https://arxiv.org/pdf/2305.17375.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dianbo Liu, Samuele Bolotta, He Zhu, Yoshua Bengio, Guillaume Dumas
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.17375">Attention Schema in Neural Agents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Attention has become a common ingredient in deep learning architectures. It adds a dynamical selection of information on top of the static selection of information supported by weights. In the same way, we can imagine a higher-order informational filter built on top of attention: an Attention Schema (AS), namely, a descriptive and predictive model of attention. In cognitive neuroscience, Attention Schema Theory (AST) supports this idea of distinguishing attention from AS. A strong prediction of this theory is that an agent can use its own AS to also infer the states of other agents' attention and consequently enhance coordination with other agents. As such, multi-agent reinforcement learning would be an ideal setting to experimentally test the validity of AST. We explore different ways in which attention and AS interact with each other. Our preliminary results indicate that agents that implement the AS as a recurrent internal control achieve the best performance. In general, these exploratory experiments suggest that equipping artificial agents with a model of attention can enhance their social intelligence.
<div id='section'>Paperid: <span id='pid'>959, <a href='https://arxiv.org/pdf/2305.08723.pdf' target='_blank'>https://arxiv.org/pdf/2305.08723.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lukas Kesper, Sebastian Trimpe, Dominik Baumann
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.08723">Toward Multi-Agent Reinforcement Learning for Distributed Event-Triggered Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Event-triggered communication and control provide high control performance in networked control systems without overloading the communication network. However, most approaches require precise mathematical models of the system dynamics, which may not always be available. Model-free learning of communication and control policies provides an alternative. Nevertheless, existing methods typically consider single-agent settings. This paper proposes a model-free reinforcement learning algorithm that jointly learns resource-aware communication and control policies for distributed multi-agent systems from data. We evaluate the algorithm in a high-dimensional and nonlinear simulation example and discuss promising avenues for further research.
<div id='section'>Paperid: <span id='pid'>960, <a href='https://arxiv.org/pdf/2304.11652.pdf' target='_blank'>https://arxiv.org/pdf/2304.11652.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dylan Bellier, Massimo Benerecetti, Dario Della Monica, Fabio Mogavero
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.11652">Alternating (In)Dependence-Friendly Logic</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Hintikka and Sandu originally proposed Independence Friendly Logic (IF) as a first-order logic of imperfect information to describe game-theoretic phenomena underlying the semantics of natural language. The logic allows for expressing independence constraints among quantified variables, in a similar vein to Henkin quantifiers, and has a nice game-theoretic semantics in terms of imperfect information games. However, the IF semantics exhibits some limitations. It treats the players asymmetrically, considering only one of the two players as having imperfect information when evaluating truth, resp., falsity, of a sentence. In addition, the truth and falsity of sentences coincide with the existence of a uniform winning strategy for one of the two players in the semantic imperfect information game. As a consequence, IF does admit undetermined sentences, which are neither true nor false, thus failing the law of excluded middle. In this paper, we investigate an extension of IF, called Alternating Dependence/Independence Friendly Logic (ADIF), tailored to overcome these limitations. To this end, we introduce a novel compositional semantics, generalising the one based on trumps proposed by Hodges for IF. The new semantics (i) allows for meaningfully restricting both players at the same time, (ii) enjoys the property of game-theoretic determinacy, (iii) recovers the law of excluded middle for sentences, and (iv) grants ADIF the full descriptive power of Second Order Logic. We also provide an equivalent Herbrand-Skolem semantics and a game-theoretic semantics for the prenex fragment of ADIF, the latter being defined in terms of a determined infinite-duration game that precisely captures the other two semantics on finite structures.
<div id='section'>Paperid: <span id='pid'>961, <a href='https://arxiv.org/pdf/2304.06011.pdf' target='_blank'>https://arxiv.org/pdf/2304.06011.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aravind Venugopal, Stephanie Milani, Fei Fang, Balaraman Ravindran
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.06011">MABL: Bi-Level Latent-Variable World Model for Sample-Efficient Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent reinforcement learning (MARL) methods often suffer from high sample complexity, limiting their use in real-world problems where data is sparse or expensive to collect. Although latent-variable world models have been employed to address this issue by generating abundant synthetic data for MARL training, most of these models cannot encode vital global information available during training into their latent states, which hampers learning efficiency. The few exceptions that incorporate global information assume centralized execution of their learned policies, which is impractical in many applications with partial observability.
  We propose a novel model-based MARL algorithm, MABL (Multi-Agent Bi-Level world model), that learns a bi-level latent-variable world model from high-dimensional inputs. Unlike existing models, MABL is capable of encoding essential global information into the latent states during training while guaranteeing the decentralized execution of learned policies. For each agent, MABL learns a global latent state at the upper level, which is used to inform the learning of an agent latent state at the lower level. During execution, agents exclusively use lower-level latent states and act independently. Crucially, MABL can be combined with any model-free MARL algorithm for policy learning. In our empirical evaluation with complex discrete and continuous multi-agent tasks including SMAC, Flatland, and MAMuJoCo, MABL surpasses SOTA multi-agent latent-variable world models in both sample efficiency and overall performance.
<div id='section'>Paperid: <span id='pid'>962, <a href='https://arxiv.org/pdf/2304.01547.pdf' target='_blank'>https://arxiv.org/pdf/2304.01547.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Talal Algumaei, Ruben Solozabal, Reda Alami, Hakim Hacid, Merouane Debbah, Martin Takac
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.01547">Regularization of the policy updates for stabilizing Mean Field Games</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work studies non-cooperative Multi-Agent Reinforcement Learning (MARL) where multiple agents interact in the same environment and whose goal is to maximize the individual returns. Challenges arise when scaling up the number of agents due to the resultant non-stationarity that the many agents introduce. In order to address this issue, Mean Field Games (MFG) rely on the symmetry and homogeneity assumptions to approximate games with very large populations. Recently, deep Reinforcement Learning has been used to scale MFG to games with larger number of states. Current methods rely on smoothing techniques such as averaging the q-values or the updates on the mean-field distribution. This work presents a different approach to stabilize the learning based on proximal updates on the mean-field policy. We name our algorithm Mean Field Proximal Policy Optimization (MF-PPO), and we empirically show the effectiveness of our method in the OpenSpiel framework.
<div id='section'>Paperid: <span id='pid'>963, <a href='https://arxiv.org/pdf/2303.04150.pdf' target='_blank'>https://arxiv.org/pdf/2303.04150.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hui Bai, Ran Cheng, Yaochu Jin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.04150">Evolutionary Reinforcement Learning: A Survey</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reinforcement learning (RL) is a machine learning approach that trains agents to maximize cumulative rewards through interactions with environments. The integration of RL with deep learning has recently resulted in impressive achievements in a wide range of challenging tasks, including board games, arcade games, and robot control. Despite these successes, there remain several crucial challenges, including brittle convergence properties caused by sensitive hyperparameters, difficulties in temporal credit assignment with long time horizons and sparse rewards, a lack of diverse exploration, especially in continuous search space scenarios, difficulties in credit assignment in multi-agent reinforcement learning, and conflicting objectives for rewards. Evolutionary computation (EC), which maintains a population of learning agents, has demonstrated promising performance in addressing these limitations. This article presents a comprehensive survey of state-of-the-art methods for integrating EC into RL, referred to as evolutionary reinforcement learning (EvoRL). We categorize EvoRL methods according to key research fields in RL, including hyperparameter optimization, policy search, exploration, reward shaping, meta-RL, and multi-objective RL. We then discuss future research directions in terms of efficient methods, benchmarks, and scalable platforms. This survey serves as a resource for researchers and practitioners interested in the field of EvoRL, highlighting the important challenges and opportunities for future research. With the help of this survey, researchers and practitioners can develop more efficient methods and tailored benchmarks for EvoRL, further advancing this promising cross-disciplinary research field.
<div id='section'>Paperid: <span id='pid'>964, <a href='https://arxiv.org/pdf/2303.02532.pdf' target='_blank'>https://arxiv.org/pdf/2303.02532.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhuqing Liu, Xin Zhang, Songtao Lu, Jia Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.02532">PRECISION: Decentralized Constrained Min-Max Learning with Low Communication and Sample Complexities</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, min-max optimization problems have received increasing attention due to their wide range of applications in machine learning (ML). However, most existing min-max solution techniques are either single-machine or distributed algorithms coordinated by a central server. In this paper, we focus on the decentralized min-max optimization for learning with domain constraints, where multiple agents collectively solve a nonconvex-strongly-concave min-max saddle point problem without coordination from any server. Decentralized min-max optimization problems with domain constraints underpins many important ML applications, including multi-agent ML fairness assurance, and policy evaluations in multi-agent reinforcement learning. We propose an algorithm called PRECISION (proximal gradient-tracking and stochastic recursive variance reduction) that enjoys a convergence rate of $O(1/T)$, where $T$ is the maximum number of iterations. To further reduce sample complexity, we propose PRECISION$^+$ with an adaptive batch size technique. We show that the fast $O(1/T)$ convergence of PRECISION and PRECISION$^+$ to an $Îµ$-stationary point imply $O(Îµ^{-2})$ communication complexity and $O(m\sqrt{n}Îµ^{-2})$ sample complexity, where $m$ is the number of agents and $n$ is the size of dataset at each agent. To our knowledge, this is the first work that achieves $O(Îµ^{-2})$ in both sample and communication complexities in decentralized min-max learning with domain constraints. Our experiments also corroborate the theoretical results.
<div id='section'>Paperid: <span id='pid'>965, <a href='https://arxiv.org/pdf/2303.01772.pdf' target='_blank'>https://arxiv.org/pdf/2303.01772.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Thomas Wolgast, Astrid NieÃe
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.01772">Approximating Energy Market Clearing and Bidding With Model-Based Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Energy market rules should incentivize market participants to behave in a market and grid conform way. However, they can also provide incentives for undesired and unexpected strategies if the market design is flawed. Multi-agent Reinforcement learning (MARL) is a promising new approach to predicting the expected profit-maximizing behavior of energy market participants in simulation. However, reinforcement learning requires many interactions with the system to converge, and the power system environment often consists of extensive computations, e.g., optimal power flow (OPF) calculation for market clearing. To tackle this complexity, we provide a model of the energy market to a basic MARL algorithm in the form of a learned OPF approximation and explicit market rules. The learned OPF surrogate model makes an explicit solving of the OPF completely unnecessary. Our experiments demonstrate that the model additionally reduces training time by about one order of magnitude but at the cost of a slightly worse performance. Potential applications of our method are market design, more realistic modeling of market participants, and analysis of manipulative behavior.
<div id='section'>Paperid: <span id='pid'>966, <a href='https://arxiv.org/pdf/2302.03439.pdf' target='_blank'>https://arxiv.org/pdf/2302.03439.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lukas SchÃ¤fer, Oliver Slumbers, Stephen McAleer, Yali Du, Stefano V. Albrecht, David Mguni
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.03439">Ensemble Value Functions for Efficient Exploration in Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent reinforcement learning (MARL) requires agents to explore within a vast joint action space to find joint actions that lead to coordination. Existing value-based MARL algorithms commonly rely on random exploration, such as $Îµ$-greedy, to explore the environment which is not systematic and inefficient at identifying effective actions in multi-agent problems. Additionally, the concurrent training of the policies of multiple agents during training can render the optimisation non-stationary. This can lead to unstable value estimates, highly variant gradients, and ultimately hinder coordination between agents. To address these challenges, we propose ensemble value functions for multi-agent exploration (EMAX). EMAX is a framework to seamlessly extend value-based MARL algorithms. EMAX leverages an ensemble of value functions for each agent to guide their exploration, reduce the variance of their optimisation, and makes their policies more robust to miscoordination. EMAX achieves these benefits by (1) systematically guiding the exploration of agents with a UCB policy towards parts of the environment that require multiple agents to coordinate. (2) EMAX computes average value estimates across the ensemble as target values to reduce the variance of gradients and make optimisation more stable. (3) During evaluation, EMAX selects actions following a majority vote across the ensemble to reduce the likelihood of miscoordination. We first instantiate independent DQN with EMAX and evaluate it in 11 general-sum tasks with sparse rewards. We show that EMAX improves final evaluation returns by 185% across all tasks. We then evaluate EMAX on top of IDQN, VDN and QMIX in 21 common-reward tasks, and show that EMAX improves sample efficiency and final evaluation returns across all tasks over all three vanilla algorithms by 60%, 47%, and 538%, respectively.
<div id='section'>Paperid: <span id='pid'>967, <a href='https://arxiv.org/pdf/2302.03438.pdf' target='_blank'>https://arxiv.org/pdf/2302.03438.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Robert Loftin, Mustafa Mert Ãelikok, Herke van Hoof, Samuel Kaski, Frans A. Oliehoek
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.03438">Uncoupled Learning of Differential Stackelberg Equilibria with Commitments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In multi-agent problems requiring a high degree of cooperation, success often depends on the ability of the agents to adapt to each other's behavior. A natural solution concept in such settings is the Stackelberg equilibrium, in which the ``leader'' agent selects the strategy that maximizes its own payoff given that the ``follower'' agent will choose their best response to this strategy. Recent work has extended this solution concept to two-player differentiable games, such as those arising from multi-agent deep reinforcement learning, in the form of the \textit{differential} Stackelberg equilibrium. While this previous work has presented learning dynamics which converge to such equilibria, these dynamics are ``coupled'' in the sense that the learning updates for the leader's strategy require some information about the follower's payoff function. As such, these methods cannot be applied to truly decentralised multi-agent settings, particularly ad hoc cooperation, where each agent only has access to its own payoff function. In this work we present ``uncoupled'' learning dynamics based on zeroth-order gradient estimators, in which each agent's strategy update depends only on their observations of the other's behavior. We analyze the convergence of these dynamics in general-sum games, and prove that they converge to differential Stackelberg equilibria under the same conditions as previous coupled methods. Furthermore, we present an online mechanism by which symmetric learners can negotiate leader-follower roles. We conclude with a discussion of the implications of our work for multi-agent reinforcement learning and ad hoc collaboration more generally.
<div id='section'>Paperid: <span id='pid'>968, <a href='https://arxiv.org/pdf/2212.11498.pdf' target='_blank'>https://arxiv.org/pdf/2212.11498.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aleksandar Krnjaic, Raul D. Steleac, Jonathan D. Thomas, Georgios Papoudakis, Lukas SchÃ¤fer, Andrew Wing Keung To, Kuan-Ho Lao, Murat Cubuktepe, Matthew Haley, Peter BÃ¶rsting, Stefano V. Albrecht
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2212.11498">Scalable Multi-Agent Reinforcement Learning for Warehouse Logistics with Robotic and Human Co-Workers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We consider a warehouse in which dozens of mobile robots and human pickers work together to collect and deliver items within the warehouse. The fundamental problem we tackle, called the order-picking problem, is how these worker agents must coordinate their movement and actions in the warehouse to maximise performance in this task. Established industry methods using heuristic approaches require large engineering efforts to optimise for innately variable warehouse configurations. In contrast, multi-agent reinforcement learning (MARL) can be flexibly applied to diverse warehouse configurations (e.g. size, layout, number/types of workers, item replenishment frequency), and different types of order-picking paradigms (e.g. Goods-to-Person and Person-to-Goods), as the agents can learn how to cooperate optimally through experience. We develop hierarchical MARL algorithms in which a manager agent assigns goals to worker agents, and the policies of the manager and workers are co-trained toward maximising a global objective (e.g. pick rate). Our hierarchical algorithms achieve significant gains in sample efficiency over baseline MARL algorithms and overall pick rates over multiple established industry heuristics in a diverse set of warehouse configurations and different order-picking paradigms.
<div id='section'>Paperid: <span id='pid'>969, <a href='https://arxiv.org/pdf/2212.07489.pdf' target='_blank'>https://arxiv.org/pdf/2212.07489.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Benjamin Ellis, Jonathan Cook, Skander Moalla, Mikayel Samvelyan, Mingfei Sun, Anuj Mahajan, Jakob N. Foerster, Shimon Whiteson
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2212.07489">SMACv2: An Improved Benchmark for Cooperative Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The availability of challenging benchmarks has played a key role in the recent progress of machine learning. In cooperative multi-agent reinforcement learning, the StarCraft Multi-Agent Challenge (SMAC) has become a popular testbed for centralised training with decentralised execution. However, after years of sustained improvement on SMAC, algorithms now achieve near-perfect performance. In this work, we conduct new analysis demonstrating that SMAC lacks the stochasticity and partial observability to require complex *closed-loop* policies. In particular, we show that an *open-loop* policy conditioned only on the timestep can achieve non-trivial win rates for many SMAC scenarios. To address this limitation, we introduce SMACv2, a new version of the benchmark where scenarios are procedurally generated and require agents to generalise to previously unseen settings (from the same distribution) during evaluation. We also introduce the extended partial observability challenge (EPO), which augments SMACv2 to ensure meaningful partial observability. We show that these changes ensure the benchmark requires the use of *closed-loop* policies. We evaluate state-of-the-art algorithms on SMACv2 and show that it presents significant challenges not present in the original benchmark. Our analysis illustrates that SMACv2 addresses the discovered deficiencies of SMAC and can help benchmark the next generation of MARL methods. Videos of training are available at https://sites.google.com/view/smacv2.
<div id='section'>Paperid: <span id='pid'>970, <a href='https://arxiv.org/pdf/2212.02376.pdf' target='_blank'>https://arxiv.org/pdf/2212.02376.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Peiwen Qiu, Yining Li, Zhuqing Liu, Prashant Khanduri, Jia Liu, Ness B. Shroff, Elizabeth Serena Bentley, Kurt Turck
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2212.02376">DIAMOND: Taming Sample and Communication Complexities in Decentralized Bilevel Optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Decentralized bilevel optimization has received increasing attention recently due to its foundational role in many emerging multi-agent learning paradigms (e.g., multi-agent meta-learning and multi-agent reinforcement learning) over peer-to-peer edge networks. However, to work with the limited computation and communication capabilities of edge networks, a major challenge in developing decentralized bilevel optimization techniques is to lower sample and communication complexities. This motivates us to develop a new decentralized bilevel optimization called DIAMOND (decentralized single-timescale stochastic approximation with momentum and gradient-tracking). The contributions of this paper are as follows: i) our DIAMOND algorithm adopts a single-loop structure rather than following the natural double-loop structure of bilevel optimization, which offers low computation and implementation complexity; ii) compared to existing approaches, the DIAMOND algorithm does not require any full gradient evaluations, which further reduces both sample and computational complexities; iii) through a careful integration of momentum information and gradient tracking techniques, we show that the DIAMOND algorithm enjoys $\mathcal{O}(Îµ^{-3/2})$ in sample and communication complexities for achieving an $Îµ$-stationary solution, both of which are independent of the dataset sizes and significantly outperform existing works. Extensive experiments also verify our theoretical findings.
<div id='section'>Paperid: <span id='pid'>971, <a href='https://arxiv.org/pdf/2211.15612.pdf' target='_blank'>https://arxiv.org/pdf/2211.15612.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qi Tian, Kun Kuang, Furui Liu, Baoxiang Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2211.15612">Learning from Good Trajectories in Offline Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Offline multi-agent reinforcement learning (MARL) aims to learn effective multi-agent policies from pre-collected datasets, which is an important step toward the deployment of multi-agent systems in real-world applications. However, in practice, each individual behavior policy that generates multi-agent joint trajectories usually has a different level of how well it performs. e.g., an agent is a random policy while other agents are medium policies. In the cooperative game with global reward, one agent learned by existing offline MARL often inherits this random policy, jeopardizing the performance of the entire team. In this paper, we investigate offline MARL with explicit consideration on the diversity of agent-wise trajectories and propose a novel framework called Shared Individual Trajectories (SIT) to address this problem. Specifically, an attention-based reward decomposition network assigns the credit to each agent through a differentiable key-value memory mechanism in an offline manner. These decomposed credits are then used to reconstruct the joint offline datasets into prioritized experience replay with individual trajectories, thereafter agents can share their good trajectories and conservatively train their policies with a graph attention network (GAT) based critic. We evaluate our method in both discrete control (i.e., StarCraft II and multi-agent particle environment) and continuous control (i.e, multi-agent mujoco). The results indicate that our method achieves significantly better results in complex and mixed offline multi-agent datasets, especially when the difference of data quality between individual trajectories is large.
<div id='section'>Paperid: <span id='pid'>972, <a href='https://arxiv.org/pdf/2211.06136.pdf' target='_blank'>https://arxiv.org/pdf/2211.06136.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Man Luo, Bowen Du, Wenzhe Zhang, Tianyou Song, Kun Li, Hongming Zhu, Mark Birkin, Hongkai Wen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2211.06136">Fleet Rebalancing for Expanding Shared e-Mobility Systems: A Multi-agent Deep Reinforcement Learning Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The electrification of shared mobility has become popular across the globe. Many cities have their new shared e-mobility systems deployed, with continuously expanding coverage from central areas to the city edges. A key challenge in the operation of these systems is fleet rebalancing, i.e., how EVs should be repositioned to better satisfy future demand. This is particularly challenging in the context of expanding systems, because i) the range of the EVs is limited while charging time is typically long, which constrain the viable rebalancing operations; and ii) the EV stations in the system are dynamically changing, i.e., the legitimate targets for rebalancing operations can vary over time. We tackle these challenges by first investigating rich sets of data collected from a real-world shared e-mobility system for one year, analyzing the operation model, usage patterns and expansion dynamics of this new mobility mode. With the learned knowledge we design a high-fidelity simulator, which is able to abstract key operation details of EV sharing at fine granularity. Then we model the rebalancing task for shared e-mobility systems under continuous expansion as a Multi-Agent Reinforcement Learning (MARL) problem, which directly takes the range and charging properties of the EVs into account. We further propose a novel policy optimization approach with action cascading, which is able to cope with the expansion dynamics and solve the formulated MARL. We evaluate the proposed approach extensively, and experimental results show that our approach outperforms the state-of-the-art, offering significant performance gain in both satisfied demand and net revenue.
<div id='section'>Paperid: <span id='pid'>973, <a href='https://arxiv.org/pdf/2211.03658.pdf' target='_blank'>https://arxiv.org/pdf/2211.03658.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sydney Dolan, Siddharth Nayak, Hamsa Balakrishnan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2211.03658">Satellite Navigation and Coordination with Limited Information Sharing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We explore space traffic management as an application of collision-free navigation in multi-agent systems where vehicles have limited observation and communication ranges. We investigate the effectiveness of transferring a collision avoidance multi-agent reinforcement (MARL) model trained on a ground environment to a space one. We demonstrate that the transfer learning model outperforms a model that is trained directly on the space environment. Furthermore, we find that our approach works well even when we consider the perturbations to satellite dynamics caused by the Earth's oblateness. Finally, we show how our methods can be used to evaluate the benefits of information-sharing between satellite operators in order to improve coordination.
<div id='section'>Paperid: <span id='pid'>974, <a href='https://arxiv.org/pdf/2211.00801.pdf' target='_blank'>https://arxiv.org/pdf/2211.00801.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiachen Yang, Ketan Mittal, Tarik Dzanic, Socratis Petrides, Brendan Keith, Brenden Petersen, Daniel Faissol, Robert Anderson
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2211.00801">Multi-Agent Reinforcement Learning for Adaptive Mesh Refinement</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Adaptive mesh refinement (AMR) is necessary for efficient finite element simulations of complex physical phenomenon, as it allocates limited computational budget based on the need for higher or lower resolution, which varies over space and time. We present a novel formulation of AMR as a fully-cooperative Markov game, in which each element is an independent agent who makes refinement and de-refinement choices based on local information. We design a novel deep multi-agent reinforcement learning (MARL) algorithm called Value Decomposition Graph Network (VDGN), which solves the two core challenges that AMR poses for MARL: posthumous credit assignment due to agent creation and deletion, and unstructured observations due to the diversity of mesh geometries. For the first time, we show that MARL enables anticipatory refinement of regions that will encounter complex features at future times, thereby unlocking entirely new regions of the error-cost objective landscape that are inaccessible by traditional methods based on local error estimators. Comprehensive experiments show that VDGN policies significantly outperform error threshold-based policies in global error and cost metrics. We show that learned policies generalize to test problems with physical features, mesh geometries, and longer simulation times that were not seen in training. We also extend VDGN with multi-objective optimization capabilities to find the Pareto front of the tradeoff between cost and error.
<div id='section'>Paperid: <span id='pid'>975, <a href='https://arxiv.org/pdf/2208.02993.pdf' target='_blank'>https://arxiv.org/pdf/2208.02993.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jingtao Tang, Yuan Gao, Tin Lun Lam
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2208.02993">Learning to Coordinate for a Worker-Station Multi-robot System in Planar Coverage Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>For massive large-scale tasks, a multi-robot system (MRS) can effectively improve efficiency by utilizing each robot's different capabilities, mobility, and functionality. In this paper, we focus on the multi-robot coverage path planning (mCPP) problem in large-scale planar areas with random dynamic interferers in the environment, where the robots have limited resources. We introduce a worker-station MRS consisting of multiple workers with limited resources for actual work, and one station with enough resources for resource replenishment. We aim to solve the mCPP problem for the worker-station MRS by formulating it as a fully cooperative multi-agent reinforcement learning problem. Then we propose an end-to-end decentralized online planning method, which simultaneously solves coverage planning for workers and rendezvous planning for station. Our method manages to reduce the influence of random dynamic interferers on planning, while the robots can avoid collisions with them. We conduct simulation and real robot experiments, and the comparison results show that our method has competitive performance in solving the mCPP problem for worker-station MRS in metric of task finish time.
<div id='section'>Paperid: <span id='pid'>976, <a href='https://arxiv.org/pdf/2207.11143.pdf' target='_blank'>https://arxiv.org/pdf/2207.11143.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianing Ye, Chenghao Li, Jianhao Wang, Chongjie Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2207.11143">Towards Global Optimality in Cooperative MARL with the Transformation And Distillation Framework</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Decentralized execution is one core demand in cooperative multi-agent reinforcement learning (MARL). Recently, most popular MARL algorithms have adopted decentralized policies to enable decentralized execution and use gradient descent as their optimizer. However, there is hardly any theoretical analysis of these algorithms taking the optimization method into consideration, and we find that various popular MARL algorithms with decentralized policies are suboptimal in toy tasks when gradient descent is chosen as their optimization method. In this paper, we theoretically analyze two common classes of algorithms with decentralized policies -- multi-agent policy gradient methods and value-decomposition methods to prove their suboptimality when gradient descent is used. In addition, we propose the Transformation And Distillation (TAD) framework, which reformulates a multi-agent MDP as a special single-agent MDP with a sequential structure and enables decentralized execution by distilling the learned policy on the derived ``single-agent" MDP. This approach uses a two-stage learning paradigm to address the optimization problem in cooperative MARL, maintaining its performance guarantee. Empirically, we implement TAD-PPO based on PPO, which can theoretically perform optimal policy learning in the finite multi-agent MDPs and shows significant outperformance on a large set of cooperative multi-agent tasks.
<div id='section'>Paperid: <span id='pid'>977, <a href='https://arxiv.org/pdf/2207.02249.pdf' target='_blank'>https://arxiv.org/pdf/2207.02249.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lukas SchÃ¤fer, Filippos Christianos, Amos Storkey, Stefano V. Albrecht
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2207.02249">Learning Task Embeddings for Teamwork Adaptation in Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Successful deployment of multi-agent reinforcement learning often requires agents to adapt their behaviour. In this work, we discuss the problem of teamwork adaptation in which a team of agents needs to adapt their policies to solve novel tasks with limited fine-tuning. Motivated by the intuition that agents need to be able to identify and distinguish tasks in order to adapt their behaviour to the current task, we propose to learn multi-agent task embeddings (MATE). These task embeddings are trained using an encoder-decoder architecture optimised for reconstruction of the transition and reward functions which uniquely identify tasks. We show that a team of agents is able to adapt to novel tasks when provided with task embeddings. We propose three MATE training paradigms: independent MATE, centralised MATE, and mixed MATE which vary in the information used for the task encoding. We show that the embeddings learned by MATE identify tasks and provide useful information which agents leverage during adaptation to novel tasks.
<div id='section'>Paperid: <span id='pid'>978, <a href='https://arxiv.org/pdf/2204.12064.pdf' target='_blank'>https://arxiv.org/pdf/2204.12064.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tingting Yuan, Hwei-Ming Chung, Xiaoming Fu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2204.12064">PP-MARL: Efficient Privacy-Preserving Multi-Agent Reinforcement Learning for Cooperative Intelligence in Communications</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cooperative intelligence (CI) is expected to become an integral element in next-generation networks because it can aggregate the capabilities and intelligence of multiple devices. Multi-agent reinforcement learning (MARL) is a popular approach for achieving CI in communication problems by enabling effective collaboration among agents to address sequential problems. However, ensuring privacy protection for MARL is a challenging task because of the presence of heterogeneous agents that learn interdependently via sharing information. Implementing privacy protection techniques such as data encryption and federated learning to MARL introduces the notable overheads (e.g., computation and bandwidth). To overcome these challenges, we propose PP-MARL, an efficient privacy-preserving learning scheme for MARL. PP-MARL leverages homomorphic encryption (HE) and differential privacy (DP) to protect privacy, while introducing split learning to decrease overheads via reducing the volume of shared messages, and then improve efficiency. We apply and evaluate PP-MARL in two communication-related use cases. Simulation results reveal that PP-MARL can achieve efficient and reliable collaboration with 1.1-6 times better privacy protection and lower overheads (e.g., 84-91% reduction in bandwidth) than state-of-the-art approaches.
<div id='section'>Paperid: <span id='pid'>979, <a href='https://arxiv.org/pdf/2203.08975.pdf' target='_blank'>https://arxiv.org/pdf/2203.08975.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Changxi Zhu, Mehdi Dastani, Shihan Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2203.08975">A Survey of Multi-Agent Deep Reinforcement Learning with Communication</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Communication is an effective mechanism for coordinating the behaviors of multiple agents, broadening their views of the environment, and to support their collaborations. In the field of multi-agent deep reinforcement learning (MADRL), agents can improve the overall learning performance and achieve their objectives by communication. Agents can communicate various types of messages, either to all agents or to specific agent groups, or conditioned on specific constraints. With the growing body of research work in MADRL with communication (Comm-MADRL), there is a lack of a systematic and structural approach to distinguish and classify existing Comm-MADRL approaches. In this paper, we survey recent works in the Comm-MADRL field and consider various aspects of communication that can play a role in designing and developing multi-agent reinforcement learning systems. With these aspects in mind, we propose 9 dimensions along which Comm-MADRL approaches can be analyzed, developed, and compared. By projecting existing works into the multi-dimensional space, we discover interesting trends. We also propose some novel directions for designing future Comm-MADRL systems through exploring possible combinations of the dimensions.
<div id='section'>Paperid: <span id='pid'>980, <a href='https://arxiv.org/pdf/2112.03178.pdf' target='_blank'>https://arxiv.org/pdf/2112.03178.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Martin Schmid, Matej Moravcik, Neil Burch, Rudolf Kadlec, Josh Davidson, Kevin Waugh, Nolan Bard, Finbarr Timbers, Marc Lanctot, G. Zacharias Holland, Elnaz Davoodi, Alden Christianson, Michael Bowling
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2112.03178">Student of Games: A unified learning algorithm for both perfect and imperfect information games</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Games have a long history as benchmarks for progress in artificial intelligence. Approaches using search and learning produced strong performance across many perfect information games, and approaches using game-theoretic reasoning and learning demonstrated strong performance for specific imperfect information poker variants. We introduce Student of Games, a general-purpose algorithm that unifies previous approaches, combining guided search, self-play learning, and game-theoretic reasoning. Student of Games achieves strong empirical performance in large perfect and imperfect information games -- an important step towards truly general algorithms for arbitrary environments. We prove that Student of Games is sound, converging to perfect play as available computation and approximation capacity increases. Student of Games reaches strong performance in chess and Go, beats the strongest openly available agent in heads-up no-limit Texas hold'em poker, and defeats the state-of-the-art agent in Scotland Yard, an imperfect information game that illustrates the value of guided search, learning, and game-theoretic reasoning.
<div id='section'>Paperid: <span id='pid'>981, <a href='https://arxiv.org/pdf/2106.06613.pdf' target='_blank'>https://arxiv.org/pdf/2106.06613.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Johannes Treutlein, Michael Dennis, Caspar Oesterheld, Jakob Foerster
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2106.06613">A New Formalism, Method and Open Issues for Zero-Shot Coordination</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In many coordination problems, independently reasoning humans are able to discover mutually compatible policies. In contrast, independently trained self-play policies are often mutually incompatible. Zero-shot coordination (ZSC) has recently been proposed as a new frontier in multi-agent reinforcement learning to address this fundamental issue. Prior work approaches the ZSC problem by assuming players can agree on a shared learning algorithm but not on labels for actions and observations, and proposes other-play as an optimal solution. However, until now, this "label-free" problem has only been informally defined. We formalize this setting as the label-free coordination (LFC) problem by defining the label-free coordination game. We show that other-play is not an optimal solution to the LFC problem as it fails to consistently break ties between incompatible maximizers of the other-play objective. We introduce an extension of the algorithm, other-play with tie-breaking, and prove that it is optimal in the LFC problem and an equilibrium in the LFC game. Since arbitrary tie-breaking is precisely what the ZSC setting aims to prevent, we conclude that the LFC problem does not reflect the aims of ZSC. To address this, we introduce an alternative informal operationalization of ZSC as a starting point for future work.
<div id='section'>Paperid: <span id='pid'>982, <a href='https://arxiv.org/pdf/2103.04982.pdf' target='_blank'>https://arxiv.org/pdf/2103.04982.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kevin R. McKee, Edward Hughes, Tina O. Zhu, Martin J. Chadwick, Raphael Koster, Antonio Garcia Castaneda, Charlie Beattie, Thore Graepel, Matt Botvinick, Joel Z. Leibo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2103.04982">A multi-agent reinforcement learning model of reputation and cooperation in human groups</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Collective action demands that individuals efficiently coordinate how much, where, and when to cooperate. Laboratory experiments have extensively explored the first part of this process, demonstrating that a variety of social-cognitive mechanisms influence how much individuals choose to invest in group efforts. However, experimental research has been unable to shed light on how social cognitive mechanisms contribute to the where and when of collective action. We build and test a computational model of human behavior in Clean Up, a social dilemma task popular in multi-agent reinforcement learning research. We show that human groups effectively cooperate in Clean Up when they can identify group members and track reputations over time, but fail to organize under conditions of anonymity. A multi-agent reinforcement learning model of reputation demonstrates the same difference in cooperation under conditions of identifiability and anonymity. In addition, the model accurately predicts spatial and temporal patterns of group behavior: in this public goods dilemma, the intrinsic motivation for reputation catalyzes the development of a non-territorial, turn-taking strategy to coordinate collective action.
<div id='section'>Paperid: <span id='pid'>983, <a href='https://arxiv.org/pdf/2510.09937.pdf' target='_blank'>https://arxiv.org/pdf/2510.09937.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shahbaz P Qadri Syed, He Bai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.09937">Structured Cooperative Multi-Agent Reinforcement Learning: a Bayesian Network Perspective</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The empirical success of multi-agent reinforcement learning (MARL) has motivated the search for more efficient and scalable algorithms for large scale multi-agent systems. However, existing state-of-the-art algorithms do not fully exploit inter-agent coupling information to develop MARL algorithms. In this paper, we propose a systematic approach to leverage structures in the inter-agent couplings for efficient model-free reinforcement learning. We model the cooperative MARL problem via a Bayesian network and characterize the subset of agents, termed as the value dependency set, whose information is required by each agent to estimate its local action value function exactly. Moreover, we propose a partially decentralized training decentralized execution (P-DTDE) paradigm based on the value dependency set. We theoretically establish that the total variance of our P-DTDE policy gradient estimator is less than the centralized training decentralized execution (CTDE) policy gradient estimator. We derive a multi-agent policy gradient theorem based on the P-DTDE scheme and develop a scalable actor-critic algorithm. We demonstrate the efficiency and scalability of the proposed algorithm on multi-warehouse resource allocation and multi-zone temperature control examples. For dense value dependency sets, we propose an approximation scheme based on truncation of the Bayesian network and empirically show that it achieves a faster convergence than the exact value dependence set for applications with a large number of agents.
<div id='section'>Paperid: <span id='pid'>984, <a href='https://arxiv.org/pdf/2510.07971.pdf' target='_blank'>https://arxiv.org/pdf/2510.07971.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Oskar Bohn Lassen, Serio Angelo Maria Agriesti, Filipe Rodrigues, Francisco Camara Pereira
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.07971">Climate Surrogates for Scalable Multi-Agent Reinforcement Learning: A Case Study with CICERO-SCM</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Climate policy studies require models that capture the combined effects of multiple greenhouse gases on global temperature, but these models are computationally expensive and difficult to embed in reinforcement learning. We present a multi-agent reinforcement learning (MARL) framework that integrates a high-fidelity, highly efficient climate surrogate directly in the environment loop, enabling regional agents to learn climate policies under multi-gas dynamics. As a proof of concept, we introduce a recurrent neural network architecture pretrained on ($20{,}000$) multi-gas emission pathways to surrogate the climate model CICERO-SCM. The surrogate model attains near-simulator accuracy with global-mean temperature RMSE $\approx 0.0004 \mathrm{K}$ and approximately $1000\times$ faster one-step inference. When substituted for the original simulator in a climate-policy MARL setting, it accelerates end-to-end training by $>\!100\times$. We show that the surrogate and simulator converge to the same optimal policies and propose a methodology to assess this property in cases where using the simulator is intractable. Our work allows to bypass the core computational bottleneck without sacrificing policy fidelity, enabling large-scale multi-agent experiments across alternative climate-policy regimes with multi-gas dynamics and high-fidelity climate response.
<div id='section'>Paperid: <span id='pid'>985, <a href='https://arxiv.org/pdf/2510.07971.pdf' target='_blank'>https://arxiv.org/pdf/2510.07971.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Oskar Bohn Lassen, Serio Angelo Maria Agriesti, Filipe Rodrigues, Francisco Camara Pereira
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.07971">Climate Surrogates for Scalable Multi-Agent Reinforcement Learning: A Case Study with CICERO-SCM</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Climate policy studies require models that capture the combined effects of multiple greenhouse gases on global temperature, but these models are computationally expensive and difficult to embed in reinforcement learning. We present a multi-agent reinforcement learning (MARL) framework that integrates a high-fidelity, highly efficient climate surrogate directly in the environment loop, enabling regional agents to learn climate policies under multi-gas dynamics. As a proof of concept, we introduce a recurrent neural network architecture pretrained on ($20{,}000$) multi-gas emission pathways to surrogate the climate model CICERO-SCM. The surrogate model attains near-simulator accuracy with global-mean temperature RMSE $\approx 0.0004 \mathrm{K}$ and approximately $1000\times$ faster one-step inference. When substituted for the original simulator in a climate-policy MARL setting, it accelerates end-to-end training by $>\!100\times$. We show that the surrogate and simulator converge to the same optimal policies and propose a methodology to assess this property in cases where using the simulator is intractable. Our work allows to bypass the core computational bottleneck without sacrificing policy fidelity, enabling large-scale multi-agent experiments across alternative climate-policy regimes with multi-gas dynamics and high-fidelity climate response.
<div id='section'>Paperid: <span id='pid'>986, <a href='https://arxiv.org/pdf/2509.25034.pdf' target='_blank'>https://arxiv.org/pdf/2509.25034.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Heming Fu, Guojun Xiong, Shan Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.25034">MARLIN: Multi-Agent Reinforcement Learning with Murmuration Intelligence and LLM Guidance for Reservoir Management</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As climate change intensifies extreme weather events, water disasters pose growing threats to global communities, making adaptive reservoir management critical for protecting vulnerable populations and ensuring water security. Modern water resource management faces unprecedented challenges from cascading uncertainties propagating through interconnected reservoir networks. These uncertainties, rooted in physical water transfer losses and environmental variability, make precise control difficult. For example, sending 10 tons downstream may yield only 8-12 tons due to evaporation and seepage. Traditional centralized optimization approaches suffer from exponential computational complexity and cannot effectively handle such real-world uncertainties, while existing multi-agent reinforcement learning (MARL) methods fail to achieve effective coordination under uncertainty. To address these challenges, we present MARLIN, a decentralized reservoir management framework inspired by starling murmurations intelligence. Integrating bio-inspired alignment, separation, and cohesion rules with MARL, MARLIN enables individual reservoirs to make local decisions while achieving emergent global coordination. In addition, a LLM provides real-time reward shaping signals, guiding agents to adapt to environmental changes and human-defined preferences. Experiments on real-world USGS data show that MARLIN improves uncertainty handling by 23\%, cuts computation by 35\%, and accelerates flood response by 68\%, exhibiting super-linear coordination, with complexity scaling 5.4x from 400 to 10,000 nodes. These results demonstrate MARLIN's potential for disaster prevention and protecting communities through intelligent, scalable water resource management.
<div id='section'>Paperid: <span id='pid'>987, <a href='https://arxiv.org/pdf/2509.25034.pdf' target='_blank'>https://arxiv.org/pdf/2509.25034.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Heming Fu, Guojun Xiong, Shan Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.25034">MARLIN: Multi-Agent Reinforcement Learning with Murmuration Intelligence and LLM Guidance for Reservoir Management</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As climate change intensifies extreme weather events, water disasters pose growing threats to global communities, making adaptive reservoir management critical for protecting vulnerable populations and ensuring water security. Modern water resource management faces unprecedented challenges from cascading uncertainties propagating through interconnected reservoir networks. These uncertainties, rooted in physical water transfer losses and environmental variability, make precise control difficult. For example, sending 10 tons downstream may yield only 8-12 tons due to evaporation and seepage. Traditional centralized optimization approaches suffer from exponential computational complexity and cannot effectively handle such real-world uncertainties, while existing multi-agent reinforcement learning (MARL) methods fail to achieve effective coordination under uncertainty. To address these challenges, we present MARLIN, a decentralized reservoir management framework inspired by starling murmurations intelligence. Integrating bio-inspired alignment, separation, and cohesion rules with MARL, MARLIN enables individual reservoirs to make local decisions while achieving emergent global coordination. In addition, a LLM provides real-time reward shaping signals, guiding agents to adapt to environmental changes and human-defined preferences. Experiments on real-world USGS data show that MARLIN improves uncertainty handling by 23\%, cuts computation by 35\%, and accelerates flood response by 68\%, exhibiting super-linear coordination, with complexity scaling 5.4x from 400 to 10,000 nodes. These results demonstrate MARLIN's potential for disaster prevention and protecting communities through intelligent, scalable water resource management.
<div id='section'>Paperid: <span id='pid'>988, <a href='https://arxiv.org/pdf/2509.18526.pdf' target='_blank'>https://arxiv.org/pdf/2509.18526.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Han Zeng, Haibo Wang, Luhao Fan, Bingcheng Zhu, Xiaohu You, Zaichen Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.18526">AI Agent Access (A\^3) Network: An Embodied, Communication-Aware Multi-Agent Framework for 6G Coverage</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The vision of 6G communication demands autonomous and resilient networking in environments without fixed infrastructure. Yet most multi-agent reinforcement learning (MARL) approaches focus on isolated stages - exploration, relay formation, or access - under static deployments and centralized control, limiting adaptability. We propose the AI Agent Access (A\^3) Network, a unified, embodied intelligence-driven framework that transforms multi-agent networking into a dynamic, decentralized, and end-to-end system. Unlike prior schemes, the A\^3 Network integrates exploration, target user access, and backhaul maintenance within a single learning process, while supporting on-demand agent addition during runtime. Its decentralized policies ensure that even a single agent can operate independently with limited observations, while coordinated agents achieve scalable, communication-optimized coverage. By embedding link-level communication metrics into actor-critic learning, the A\^3 Network couples topology formation with robust decision-making. Numerical simulations demonstrate that the A\^3 Network not only balances exploration and communication efficiency but also delivers system-level adaptability absent in existing MARL frameworks, offering a new paradigm for 6G multi-agent networks.
<div id='section'>Paperid: <span id='pid'>989, <a href='https://arxiv.org/pdf/2509.18526.pdf' target='_blank'>https://arxiv.org/pdf/2509.18526.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Han Zeng, Haibo Wang, Luhao Fan, Bingcheng Zhu, Xiaohu You, Zaichen Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.18526">AI Agent Access (A\^3) Network: An Embodied, Communication-Aware Multi-Agent Framework for 6G Coverage</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The vision of 6G communication demands autonomous and resilient networking in environments without fixed infrastructure. Yet most multi-agent reinforcement learning (MARL) approaches focus on isolated stages - exploration, relay formation, or access - under static deployments and centralized control, limiting adaptability. We propose the AI Agent Access (A\^3) Network, a unified, embodied intelligence-driven framework that transforms multi-agent networking into a dynamic, decentralized, and end-to-end system. Unlike prior schemes, the A\^3 Network integrates exploration, target user access, and backhaul maintenance within a single learning process, while supporting on-demand agent addition during runtime. Its decentralized policies ensure that even a single agent can operate independently with limited observations, while coordinated agents achieve scalable, communication-optimized coverage. By embedding link-level communication metrics into actor-critic learning, the A\^3 Network couples topology formation with robust decision-making. Numerical simulations demonstrate that the A\^3 Network not only balances exploration and communication efficiency but also delivers system-level adaptability absent in existing MARL frameworks, offering a new paradigm for 6G multi-agent networks.
<div id='section'>Paperid: <span id='pid'>990, <a href='https://arxiv.org/pdf/2509.17676.pdf' target='_blank'>https://arxiv.org/pdf/2509.17676.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Abdullahi Isa Ahmed, Jamal Bentahar, El Mehdi Amhoud
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.17676">GLo-MAPPO: A Multi-Agent Proximal Policy Optimization for Energy Efficiency in UAV-Assisted LoRa Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Long Range (LoRa) based low-power wide area networks (LPWANs) are crucial for enabling next-generation IoT (NG-IoT) applications in 5G/6G ecosystems due to their long-range, low-power, and low-cost characteristics. However, achieving high energy efficiency in such networks remains a critical challenge, particularly in large-scale or dynamically changing environments. Traditional terrestrial LoRa deployments often suffer from coverage gaps and non-line-of-sight (NLoS) propagation losses, while satellite-based IoT solutions consume excessive energy and introduce high latency, limiting their suitability for energy-constrained and delay-sensitive applications. To address these limitations, we propose a novel architecture using multiple unmanned aerial vehicles (UAVs) as flying LoRa gateways to dynamically collect data from ground-based LoRa end devices. Our approach aims to maximize the system's weighted global energy efficiency by jointly optimizing spreading factors, transmission powers, UAV trajectories, and end-device associations. Additionally, we formulate this complex optimization problem as a partially observable Markov decision process (POMDP) and propose green LoRa multi-agent proximal policy optimization (GLo-MAPPO), a multi-agent reinforcement learning (MARL) framework based on centralized training with decentralized execution (CTDE). Simulation results show that GLo-MAPPO significantly outperforms benchmark algorithms, achieving energy efficiency improvements of 71.25%, 18.56%, 67.00%, 59.73%, and 49.95% for networks with 10, 20, 30, 40, and 50 LoRa end devices, respectively.
<div id='section'>Paperid: <span id='pid'>991, <a href='https://arxiv.org/pdf/2509.17676.pdf' target='_blank'>https://arxiv.org/pdf/2509.17676.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Abdullahi Isa Ahmed, Jamal Bentahar, El Mehdi Amhoud
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.17676">GLo-MAPPO: A Multi-Agent Proximal Policy Optimization for Energy Efficiency in UAV-Assisted LoRa Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Long Range (LoRa) based low-power wide area networks (LPWANs) are crucial for enabling next-generation IoT (NG-IoT) applications in 5G/6G ecosystems due to their long-range, low-power, and low-cost characteristics. However, achieving high energy efficiency in such networks remains a critical challenge, particularly in large-scale or dynamically changing environments. Traditional terrestrial LoRa deployments often suffer from coverage gaps and non-line-of-sight (NLoS) propagation losses, while satellite-based IoT solutions consume excessive energy and introduce high latency, limiting their suitability for energy-constrained and delay-sensitive applications. To address these limitations, we propose a novel architecture using multiple unmanned aerial vehicles (UAVs) as flying LoRa gateways to dynamically collect data from ground-based LoRa end devices. Our approach aims to maximize the system's weighted global energy efficiency by jointly optimizing spreading factors, transmission powers, UAV trajectories, and end-device associations. Additionally, we formulate this complex optimization problem as a partially observable Markov decision process (POMDP) and propose green LoRa multi-agent proximal policy optimization (GLo-MAPPO), a multi-agent reinforcement learning (MARL) framework based on centralized training with decentralized execution (CTDE). Simulation results show that GLo-MAPPO significantly outperforms benchmark algorithms, achieving energy efficiency improvements of 71.25%, 18.56%, 67.00%, 59.73%, and 49.95% for networks with 10, 20, 30, 40, and 50 LoRa end devices, respectively.
<div id='section'>Paperid: <span id='pid'>992, <a href='https://arxiv.org/pdf/2509.17353.pdf' target='_blank'>https://arxiv.org/pdf/2509.17353.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ahmed T. Elboardy, Ghada Khoriba, Essam A. Rashed
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.17353">Medical AI Consensus: A Multi-Agent Framework for Radiology Report Generation and Evaluation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Automating radiology report generation poses a dual challenge: building clinically reliable systems and designing rigorous evaluation protocols. We introduce a multi-agent reinforcement learning framework that serves as both a benchmark and evaluation environment for multimodal clinical reasoning in the radiology ecosystem. The proposed framework integrates large language models (LLMs) and large vision models (LVMs) within a modular architecture composed of ten specialized agents responsible for image analysis, feature extraction, report generation, review, and evaluation. This design enables fine-grained assessment at both the agent level (e.g., detection and segmentation accuracy) and the consensus level (e.g., report quality and clinical relevance). We demonstrate an implementation using chatGPT-4o on public radiology datasets, where LLMs act as evaluators alongside medical radiologist feedback. By aligning evaluation protocols with the LLM development lifecycle, including pretraining, finetuning, alignment, and deployment, the proposed benchmark establishes a path toward trustworthy deviance-based radiology report generation.
<div id='section'>Paperid: <span id='pid'>993, <a href='https://arxiv.org/pdf/2509.17353.pdf' target='_blank'>https://arxiv.org/pdf/2509.17353.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ahmed T. Elboardy, Ghada Khoriba, Essam A. Rashed
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.17353">Medical AI Consensus: A Multi-Agent Framework for Radiology Report Generation and Evaluation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Automating radiology report generation poses a dual challenge: building clinically reliable systems and designing rigorous evaluation protocols. We introduce a multi-agent reinforcement learning framework that serves as both a benchmark and evaluation environment for multimodal clinical reasoning in the radiology ecosystem. The proposed framework integrates large language models (LLMs) and large vision models (LVMs) within a modular architecture composed of ten specialized agents responsible for image analysis, feature extraction, report generation, review, and evaluation. This design enables fine-grained assessment at both the agent level (e.g., detection and segmentation accuracy) and the consensus level (e.g., report quality and clinical relevance). We demonstrate an implementation using chatGPT-4o on public radiology datasets, where LLMs act as evaluators alongside medical radiologist feedback. By aligning evaluation protocols with the LLM development lifecycle, including pretraining, finetuning, alignment, and deployment, the proposed benchmark establishes a path toward trustworthy deviance-based radiology report generation.
<div id='section'>Paperid: <span id='pid'>994, <a href='https://arxiv.org/pdf/2509.14431.pdf' target='_blank'>https://arxiv.org/pdf/2509.14431.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Keqin Wang, Tao Zhong, David Chang, Christine Allen-Blanchette
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.14431">Local-Canonicalization Equivariant Graph Neural Networks for Sample-Efficient and Generalizable Swarm Robot Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent reinforcement learning (MARL) has emerged as a powerful paradigm for coordinating swarms of agents in complex decision-making, yet major challenges remain. In competitive settings such as pursuer-evader tasks, simultaneous adaptation can destabilize training; non-kinetic countermeasures often fail under adverse conditions; and policies trained in one configuration rarely generalize to environments with a different number of agents. To address these issues, we propose the Local-Canonicalization Equivariant Graph Neural Networks (LEGO) framework, which integrates seamlessly with popular MARL algorithms such as MAPPO. LEGO employs graph neural networks to capture permutation equivariance and generalization to different agent numbers, canonicalization to enforce E(n)-equivariance, and heterogeneous representations to encode role-specific inductive biases. Experiments on cooperative and competitive swarm benchmarks show that LEGO outperforms strong baselines and improves generalization. In real-world experiments, LEGO demonstrates robustness to varying team sizes and agent failure.
<div id='section'>Paperid: <span id='pid'>995, <a href='https://arxiv.org/pdf/2509.14431.pdf' target='_blank'>https://arxiv.org/pdf/2509.14431.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Keqin Wang, Tao Zhong, David Chang, Christine Allen-Blanchette
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.14431">Local-Canonicalization Equivariant Graph Neural Networks for Sample-Efficient and Generalizable Swarm Robot Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent reinforcement learning (MARL) has emerged as a powerful paradigm for coordinating swarms of agents in complex decision-making, yet major challenges remain. In competitive settings such as pursuer-evader tasks, simultaneous adaptation can destabilize training; non-kinetic countermeasures often fail under adverse conditions; and policies trained in one configuration rarely generalize to environments with a different number of agents. To address these issues, we propose the Local-Canonicalization Equivariant Graph Neural Networks (LEGO) framework, which integrates seamlessly with popular MARL algorithms such as MAPPO. LEGO employs graph neural networks to capture permutation equivariance and generalization to different agent numbers, canonicalization to enforce E(n)-equivariance, and heterogeneous representations to encode role-specific inductive biases. Experiments on cooperative and competitive swarm benchmarks show that LEGO outperforms strong baselines and improves generalization. In real-world experiments, LEGO demonstrates robustness to varying team sizes and agent failure.
<div id='section'>Paperid: <span id='pid'>996, <a href='https://arxiv.org/pdf/2509.08310.pdf' target='_blank'>https://arxiv.org/pdf/2509.08310.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>S Krishna Niketh, Sagar Babu Mitikiri, V Vignesh, Vedantham Lakshmi Srinivas, Mayukha Pal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.08310">Game-Theoretic Resilience Framework for Cyber-Physical Microgrids using Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The increasing reliance on cyber physical infrastructure in modern power systems has amplified the risk of targeted cyber attacks, necessitating robust and adaptive resilience strategies. This paper presents a mathematically rigorous game theoretic framework to evaluate and enhance microgrid resilience using a combination of quantitative resilience metrics Load Served Ratio LSR, Critical Load Resilience CLR, Topological Survivability Score TSS, and DER Resilience Score DRS. These are integrated into a unified payoff matrix using the Analytic Hierarchy Process AHP to assess attack defense interactions. The framework is formalized as a finite horizon Markov Decision Process MDP with formal convergence guarantees and computational complexity bounds. Three case studies are developed 1. static attacks analyzed via Nash equilibrium, 2. severe attacks incorporating high impact strategies, and 3. adaptive attacks using Stackelberg games, regret matching, softmax heuristics, and Multi Agent Q Learning. Rigorous theoretical analysis provides convergence proofs with explicit rates , PAC learning sample complexity bounds, and computational complexity analysis. The framework is tested on an enhanced IEEE 33bus distribution system with DERs and control switches, demonstrating the effectiveness of adaptive and strategic defenses in improving cyber physical resilience with statistically significant improvements of 18.7% 2.1% over static approaches.
<div id='section'>Paperid: <span id='pid'>997, <a href='https://arxiv.org/pdf/2509.08310.pdf' target='_blank'>https://arxiv.org/pdf/2509.08310.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>S Krishna Niketh, Sagar Babu Mitikiri, V Vignesh, Vedantham Lakshmi Srinivas, Mayukha Pal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.08310">Game-Theoretic Resilience Framework for Cyber-Physical Microgrids using Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The increasing reliance on cyber physical infrastructure in modern power systems has amplified the risk of targeted cyber attacks, necessitating robust and adaptive resilience strategies. This paper presents a mathematically rigorous game theoretic framework to evaluate and enhance microgrid resilience using a combination of quantitative resilience metrics Load Served Ratio LSR, Critical Load Resilience CLR, Topological Survivability Score TSS, and DER Resilience Score DRS. These are integrated into a unified payoff matrix using the Analytic Hierarchy Process AHP to assess attack defense interactions. The framework is formalized as a finite horizon Markov Decision Process MDP with formal convergence guarantees and computational complexity bounds. Three case studies are developed 1. static attacks analyzed via Nash equilibrium, 2. severe attacks incorporating high impact strategies, and 3. adaptive attacks using Stackelberg games, regret matching, softmax heuristics, and Multi Agent Q Learning. Rigorous theoretical analysis provides convergence proofs with explicit rates , PAC learning sample complexity bounds, and computational complexity analysis. The framework is tested on an enhanced IEEE 33bus distribution system with DERs and control switches, demonstrating the effectiveness of adaptive and strategic defenses in improving cyber physical resilience with statistically significant improvements of 18.7% 2.1% over static approaches.
<div id='section'>Paperid: <span id='pid'>998, <a href='https://arxiv.org/pdf/2509.06053.pdf' target='_blank'>https://arxiv.org/pdf/2509.06053.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mingrui Lv, Hangzhi Liu, Zhi Luo, Hongjie Zhang, Jie Ou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.06053">PolicyEvolve: Evolving Programmatic Policies by LLMs for multi-player games via Population-Based Training</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent reinforcement learning (MARL) has achieved significant progress in solving complex multi-player games through self-play. However, training effective adversarial policies requires millions of experience samples and substantial computational resources. Moreover, these policies lack interpretability, hindering their practical deployment. Recently, researchers have successfully leveraged Large Language Models (LLMs) to generate programmatic policies for single-agent tasks, transforming neural network-based policies into interpretable rule-based code with high execution efficiency. Inspired by this, we propose PolicyEvolve, a general framework for generating programmatic policies in multi-player games. PolicyEvolve significantly reduces reliance on manually crafted policy code, achieving high-performance policies with minimal environmental interactions. The framework comprises four modules: Global Pool, Local Pool, Policy Planner, and Trajectory Critic. The Global Pool preserves elite policies accumulated during iterative training. The Local Pool stores temporary policies for the current iteration; only sufficiently high-performing policies from this pool are promoted to the Global Pool. The Policy Planner serves as the core policy generation module. It samples the top three policies from the Global Pool, generates an initial policy for the current iteration based on environmental information, and refines this policy using feedback from the Trajectory Critic. Refined policies are then deposited into the Local Pool. This iterative process continues until the policy achieves a sufficiently high average win rate against the Global Pool, at which point it is integrated into the Global Pool. The Trajectory Critic analyzes interaction data from the current policy, identifies vulnerabilities, and proposes directional improvements to guide the Policy Planner
<div id='section'>Paperid: <span id='pid'>999, <a href='https://arxiv.org/pdf/2509.06053.pdf' target='_blank'>https://arxiv.org/pdf/2509.06053.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mingrui Lv, Hangzhi Liu, Zhi Luo, Hongjie Zhang, Jie Ou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.06053">PolicyEvolve: Evolving Programmatic Policies by LLMs for multi-player games via Population-Based Training</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent reinforcement learning (MARL) has achieved significant progress in solving complex multi-player games through self-play. However, training effective adversarial policies requires millions of experience samples and substantial computational resources. Moreover, these policies lack interpretability, hindering their practical deployment. Recently, researchers have successfully leveraged Large Language Models (LLMs) to generate programmatic policies for single-agent tasks, transforming neural network-based policies into interpretable rule-based code with high execution efficiency. Inspired by this, we propose PolicyEvolve, a general framework for generating programmatic policies in multi-player games. PolicyEvolve significantly reduces reliance on manually crafted policy code, achieving high-performance policies with minimal environmental interactions. The framework comprises four modules: Global Pool, Local Pool, Policy Planner, and Trajectory Critic. The Global Pool preserves elite policies accumulated during iterative training. The Local Pool stores temporary policies for the current iteration; only sufficiently high-performing policies from this pool are promoted to the Global Pool. The Policy Planner serves as the core policy generation module. It samples the top three policies from the Global Pool, generates an initial policy for the current iteration based on environmental information, and refines this policy using feedback from the Trajectory Critic. Refined policies are then deposited into the Local Pool. This iterative process continues until the policy achieves a sufficiently high average win rate against the Global Pool, at which point it is integrated into the Global Pool. The Trajectory Critic analyzes interaction data from the current policy, identifies vulnerabilities, and proposes directional improvements to guide the Policy Planner
<div id='section'>Paperid: <span id='pid'>1000, <a href='https://arxiv.org/pdf/2509.05051.pdf' target='_blank'>https://arxiv.org/pdf/2509.05051.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aaron Mark Thomas, Yu-Cheng Chen, Hubert Okadome Valencia, Sharu Theresa Jose, Ronin Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.05051">QCA-MolGAN: Quantum Circuit Associative Molecular GAN with Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Navigating the vast chemical space of molecular structures to design novel drug molecules with desired target properties remains a central challenge in drug discovery. Recent advances in generative models offer promising solutions. This work presents a novel quantum circuit Born machine (QCBM)-enabled Generative Adversarial Network (GAN), called QCA-MolGAN, for generating drug-like molecules. The QCBM serves as a learnable prior distribution, which is associatively trained to define a latent space aligning with high-level features captured by the GANs discriminator. Additionally, we integrate a novel multi-agent reinforcement learning network to guide molecular generation with desired targeted properties, optimising key metrics such as quantitative estimate of drug-likeness (QED), octanol-water partition coefficient (LogP) and synthetic accessibility (SA) scores in conjunction with one another. Experimental results demonstrate that our approach enhances the property alignment of generated molecules with the multi-agent reinforcement learning agents effectively balancing chemical properties.
<div id='section'>Paperid: <span id='pid'>1001, <a href='https://arxiv.org/pdf/2509.05051.pdf' target='_blank'>https://arxiv.org/pdf/2509.05051.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aaron Mark Thomas, Yu-Cheng Chen, Hubert Okadome Valencia, Sharu Theresa Jose, Ronin Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.05051">QCA-MolGAN: Quantum Circuit Associative Molecular GAN with Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Navigating the vast chemical space of molecular structures to design novel drug molecules with desired target properties remains a central challenge in drug discovery. Recent advances in generative models offer promising solutions. This work presents a novel quantum circuit Born machine (QCBM)-enabled Generative Adversarial Network (GAN), called QCA-MolGAN, for generating drug-like molecules. The QCBM serves as a learnable prior distribution, which is associatively trained to define a latent space aligning with high-level features captured by the GANs discriminator. Additionally, we integrate a novel multi-agent reinforcement learning network to guide molecular generation with desired targeted properties, optimising key metrics such as quantitative estimate of drug-likeness (QED), octanol-water partition coefficient (LogP) and synthetic accessibility (SA) scores in conjunction with one another. Experimental results demonstrate that our approach enhances the property alignment of generated molecules with the multi-agent reinforcement learning agents effectively balancing chemical properties.
<div id='section'>Paperid: <span id='pid'>1002, <a href='https://arxiv.org/pdf/2509.02579.pdf' target='_blank'>https://arxiv.org/pdf/2509.02579.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mazyar Taghavi, Rahman Farnoosh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.02579">Latent Variable Modeling in Multi-Agent Reinforcement Learning via Expectation-Maximization for UAV-Based Wildlife Protection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Protecting endangered wildlife from illegal poaching presents a critical challenge, particularly in vast and partially observable environments where real-time response is essential. This paper introduces a novel Expectation-Maximization (EM) based latent variable modeling approach in the context of Multi-Agent Reinforcement Learning (MARL) for Unmanned Aerial Vehicle (UAV) coordination in wildlife protection. By modeling hidden environmental factors and inter-agent dynamics through latent variables, our method enhances exploration and coordination under uncertainty.We implement and evaluate our EM-MARL framework using a custom simulation involving 10 UAVs tasked with patrolling protected habitats of the endangered Iranian leopard. Extensive experimental results demonstrate superior performance in detection accuracy, adaptability, and policy convergence when compared to standard algorithms such as Proximal Policy Optimization (PPO) and Deep Deterministic Policy Gradient (DDPG). Our findings underscore the potential of combining EM inference with MARL to improve decentralized decisionmaking in complex, high-stakes conservation scenarios. The full implementation, simulation environment, and training scripts are publicly available on GitHub.
<div id='section'>Paperid: <span id='pid'>1003, <a href='https://arxiv.org/pdf/2509.02579.pdf' target='_blank'>https://arxiv.org/pdf/2509.02579.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mazyar Taghavi, Rahman Farnoosh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.02579">Latent Variable Modeling in Multi-Agent Reinforcement Learning via Expectation-Maximization for UAV-Based Wildlife Protection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Protecting endangered wildlife from illegal poaching presents a critical challenge, particularly in vast and partially observable environments where real-time response is essential. This paper introduces a novel Expectation-Maximization (EM) based latent variable modeling approach in the context of Multi-Agent Reinforcement Learning (MARL) for Unmanned Aerial Vehicle (UAV) coordination in wildlife protection. By modeling hidden environmental factors and inter-agent dynamics through latent variables, our method enhances exploration and coordination under uncertainty.We implement and evaluate our EM-MARL framework using a custom simulation involving 10 UAVs tasked with patrolling protected habitats of the endangered Iranian leopard. Extensive experimental results demonstrate superior performance in detection accuracy, adaptability, and policy convergence when compared to standard algorithms such as Proximal Policy Optimization (PPO) and Deep Deterministic Policy Gradient (DDPG). Our findings underscore the potential of combining EM inference with MARL to improve decentralized decisionmaking in complex, high-stakes conservation scenarios. The full implementation, simulation environment, and training scripts are publicly available on GitHub.
<div id='section'>Paperid: <span id='pid'>1004, <a href='https://arxiv.org/pdf/2509.02579.pdf' target='_blank'>https://arxiv.org/pdf/2509.02579.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mazyar Taghavi, Rahman Farnoosh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.02579">Latent Variable Modeling in Multi-Agent Reinforcement Learning via Expectation-Maximization for UAV-Based Wildlife Protection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Protecting endangered wildlife from illegal poaching presents a critical challenge, particularly in vast and partially observable environments where real-time response is essential. This paper introduces a novel Expectation-Maximization (EM) based latent variable modeling approach in the context of Multi-Agent Reinforcement Learning (MARL) for Unmanned Aerial Vehicle (UAV) coordination in wildlife protection. By modeling hidden environmental factors and inter-agent dynamics through latent variables, our method enhances exploration and coordination under uncertainty.We implement and evaluate our EM-MARL framework using a custom simulation involving 10 UAVs tasked with patrolling protected habitats of the endangered Iranian leopard. Extensive experimental results demonstrate superior performance in detection accuracy, adaptability, and policy convergence when compared to standard algorithms such as Proximal Policy Optimization (PPO) and Deep Deterministic Policy Gradient (DDPG). Our findings underscore the potential of combining EM inference with MARL to improve decentralized decisionmaking in complex, high-stakes conservation scenarios. The full implementation, simulation environment, and training scripts are publicly available on GitHub.
<div id='section'>Paperid: <span id='pid'>1005, <a href='https://arxiv.org/pdf/2508.17341.pdf' target='_blank'>https://arxiv.org/pdf/2508.17341.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Muhammet Anil Yagiz, Zeynep Sude Cengiz, Polat Goktas
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.17341">MetaFed: Advancing Privacy, Performance, and Sustainability in Federated Metaverse Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid expansion of immersive Metaverse applications introduces complex challenges at the intersection of performance, privacy, and environmental sustainability. Centralized architectures fall short in addressing these demands, often resulting in elevated energy consumption, latency, and privacy concerns. This paper proposes MetaFed, a decentralized federated learning (FL) framework that enables sustainable and intelligent resource orchestration for Metaverse environments. MetaFed integrates (i) multi-agent reinforcement learning for dynamic client selection, (ii) privacy-preserving FL using homomorphic encryption, and (iii) carbon-aware scheduling aligned with renewable energy availability. Evaluations on MNIST and CIFAR-10 using lightweight ResNet architectures demonstrate that MetaFed achieves up to 25% reduction in carbon emissions compared to conventional approaches, while maintaining high accuracy and minimal communication overhead. These results highlight MetaFed as a scalable solution for building environmentally responsible and privacy-compliant Metaverse infrastructures.
<div id='section'>Paperid: <span id='pid'>1006, <a href='https://arxiv.org/pdf/2508.17341.pdf' target='_blank'>https://arxiv.org/pdf/2508.17341.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Muhammet Anil Yagiz, Zeynep Sude Cengiz, Polat Goktas
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.17341">MetaFed: Advancing Privacy, Performance, and Sustainability in Federated Metaverse Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid expansion of immersive Metaverse applications introduces complex challenges at the intersection of performance, privacy, and environmental sustainability. Centralized architectures fall short in addressing these demands, often resulting in elevated energy consumption, latency, and privacy concerns. This paper proposes MetaFed, a decentralized federated learning (FL) framework that enables sustainable and intelligent resource orchestration for Metaverse environments. MetaFed integrates (i) multi-agent reinforcement learning for dynamic client selection, (ii) privacy-preserving FL using homomorphic encryption, and (iii) carbon-aware scheduling aligned with renewable energy availability. Evaluations on MNIST and CIFAR-10 using lightweight ResNet architectures demonstrate that MetaFed achieves up to 25% reduction in carbon emissions compared to conventional approaches, while maintaining high accuracy and minimal communication overhead. These results highlight MetaFed as a scalable solution for building environmentally responsible and privacy-compliant Metaverse infrastructures.
<div id='section'>Paperid: <span id='pid'>1007, <a href='https://arxiv.org/pdf/2508.10423.pdf' target='_blank'>https://arxiv.org/pdf/2508.10423.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qi Liu, Xiaopeng Zhang, Mingshan Tan, Shuaikang Ma, Jinliang Ding, Yanjie Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.10423">MASH: Cooperative-Heterogeneous Multi-Agent Reinforcement Learning for Single Humanoid Robot Locomotion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper proposes a novel method to enhance locomotion for a single humanoid robot through cooperative-heterogeneous multi-agent deep reinforcement learning (MARL). While most existing methods typically employ single-agent reinforcement learning algorithms for a single humanoid robot or MARL algorithms for multi-robot system tasks, we propose a distinct paradigm: applying cooperative-heterogeneous MARL to optimize locomotion for a single humanoid robot. The proposed method, multi-agent reinforcement learning for single humanoid locomotion (MASH), treats each limb (legs and arms) as an independent agent that explores the robot's action space while sharing a global critic for cooperative learning. Experiments demonstrate that MASH accelerates training convergence and improves whole-body cooperation ability, outperforming conventional single-agent reinforcement learning methods. This work advances the integration of MARL into single-humanoid-robot control, offering new insights into efficient locomotion strategies.
<div id='section'>Paperid: <span id='pid'>1008, <a href='https://arxiv.org/pdf/2508.08555.pdf' target='_blank'>https://arxiv.org/pdf/2508.08555.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tong Zhang, Yu Gou, Jun Liu, Jun-Hong Cui
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.08555">Traffic Load-Aware Resource Management Strategy for Underwater Wireless Sensor Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Underwater Wireless Sensor Networks (UWSNs) represent a promising technology that enables diverse underwater applications through acoustic communication. However, it encounters significant challenges including harsh communication environments, limited energy supply, and restricted signal transmission. This paper aims to provide efficient and reliable communication in underwater networks with limited energy and communication resources by optimizing the scheduling of communication links and adjusting transmission parameters (e.g., transmit power and transmission rate). The efficient and reliable communication multi-objective optimization problem (ERCMOP) is formulated as a decentralized partially observable Markov decision process (Dec-POMDP). A Traffic Load-Aware Resource Management (TARM) strategy based on deep multi-agent reinforcement learning (MARL) is presented to address this problem. Specifically, a traffic load-aware mechanism that leverages the overhear information from neighboring nodes is designed to mitigate the disparity between partial observations and global states. Moreover, by incorporating a solution space optimization algorithm, the number of candidate solutions for the deep MARL-based decision-making model can be effectively reduced, thereby optimizing the computational complexity. Simulation results demonstrate the adaptability of TARM in various scenarios with different transmission demands and collision probabilities, while also validating the effectiveness of the proposed approach in supporting efficient and reliable communication in underwater networks with limited resources.
<div id='section'>Paperid: <span id='pid'>1009, <a href='https://arxiv.org/pdf/2508.07679.pdf' target='_blank'>https://arxiv.org/pdf/2508.07679.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tong Zhang, Yu Gou, Jun Liu, Shanshan Song, Tingting Yang, Jun-Hong Cui
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.07679">Joint link scheduling and power allocation in imperfect and energy-constrained underwater wireless sensor networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Underwater wireless sensor networks (UWSNs) stand as promising technologies facilitating diverse underwater applications. However, the major design issues of the considered system are the severely limited energy supply and unexpected node malfunctions. This paper aims to provide fair, efficient, and reliable (FER) communication to the imperfect and energy-constrained UWSNs (IC-UWSNs). Therefore, we formulate a FER-communication optimization problem (FERCOP) and propose ICRL-JSA to solve the formulated problem. ICRL-JSA is a deep multi-agent reinforcement learning (MARL)-based optimizer for IC-UWSNs through joint link scheduling and power allocation, which automatically learns scheduling algorithms without human intervention. However, conventional RL methods are unable to address the challenges posed by underwater environments and IC-UWSNs. To construct ICRL-JSA, we integrate deep Q-network into IC-UWSNs and propose an advanced training mechanism to deal with complex acoustic channels, limited energy supplies, and unexpected node malfunctions. Simulation results demonstrate the superiority of the proposed ICRL-JSA scheme with an advanced training mechanism compared to various benchmark algorithms.
<div id='section'>Paperid: <span id='pid'>1010, <a href='https://arxiv.org/pdf/2508.07578.pdf' target='_blank'>https://arxiv.org/pdf/2508.07578.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yu Gou, Tong Zhang, Jun Liu, Tingting Yang, Shanshan Song, Jun-Hong Cui
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.07578">Achieving Fair-Effective Communications and Robustness in Underwater Acoustic Sensor Networks: A Semi-Cooperative Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper investigates the fair-effective communication and robustness in imperfect and energy-constrained underwater acoustic sensor networks (IC-UASNs). Specifically, we investigate the impact of unexpected node malfunctions on the network performance under the time-varying acoustic channels. Each node is expected to satisfy Quality of Service (QoS) requirements. However, achieving individual QoS requirements may interfere with other concurrent communications. Underwater nodes rely excessively on the rationality of other underwater nodes when guided by fully cooperative approaches, making it difficult to seek a trade-off between individual QoS and global fair-effective communications under imperfect conditions. Therefore, this paper presents a SEmi-COoperative Power Allocation approach (SECOPA) that achieves fair-effective communication and robustness in IC-UASNs. The approach is distributed multi-agent reinforcement learning (MARL)-based, and the objectives are twofold. On the one hand, each intelligent node individually decides the transmission power to simultaneously optimize individual and global performance. On the other hand, advanced training algorithms are developed to provide imperfect environments for training robust models that can adapt to the time-varying acoustic channels and handle unexpected node failures in the network. Numerical results are presented to validate our proposed approach.
<div id='section'>Paperid: <span id='pid'>1011, <a href='https://arxiv.org/pdf/2508.02027.pdf' target='_blank'>https://arxiv.org/pdf/2508.02027.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinzheng Wu, Junyi Chen, Shaolingfeng Ye, Wei Jiang, Yong Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.02027">An Evolving Scenario Generation Method based on Dual-modal Driver Model Trained by Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the autonomous driving testing methods based on evolving scenarios, the construction method of the driver model, which determines the driving maneuvers of background vehicles (BVs) in the scenario, plays a critical role in generating safety-critical scenarios. In particular, the cooperative adversarial driving characteristics between BVs can contribute to the efficient generation of safety-critical scenarios with high testing value. In this paper, a multi-agent reinforcement learning (MARL) method is used to train and generate a dual-modal driver model (Dual-DM) with non-adversarial and adversarial driving modalities. The model is then connected to a continuous simulated traffic environment to generate complex, diverse and strong interactive safety-critical scenarios through evolving scenario generation method. After that, the generated evolving scenarios are evaluated in terms of fidelity, test efficiency, complexity and diversity. Results show that without performance degradation in scenario fidelity (>85% similarity to real-world scenarios) and complexity (complexity metric: 0.45, +32.35% and +12.5% over two baselines), Dual-DM achieves a substantial enhancement in the efficiency of generating safety-critical scenarios (efficiency metric: 0.86, +195% over two baselines). Furthermore, statistical analysis and case studies demonstrate the diversity of safety-critical evolving scenarios generated by Dual-DM in terms of the adversarial interaction patterns. Therefore, Dual-DM can greatly improve the performance of the generation of safety-critical scenarios through evolving scenario generation method.
<div id='section'>Paperid: <span id='pid'>1012, <a href='https://arxiv.org/pdf/2508.01522.pdf' target='_blank'>https://arxiv.org/pdf/2508.01522.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jack Zeng, Andreu Matoses Gimenez, Eugene Vinitsky, Javier Alonso-Mora, Sihao Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.01522">Decentralized Aerial Manipulation of a Cable-Suspended Load using Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents the first decentralized method to enable real-world 6-DoF manipulation of a cable-suspended load using a team of Micro-Aerial Vehicles (MAVs). Our method leverages multi-agent reinforcement learning (MARL) to train an outer-loop control policy for each MAV. Unlike state-of-the-art controllers that utilize a centralized scheme, our policy does not require global states, inter-MAV communications, nor neighboring MAV information. Instead, agents communicate implicitly through load pose observations alone, which enables high scalability and flexibility. It also significantly reduces computing costs during inference time, enabling onboard deployment of the policy. In addition, we introduce a new action space design for the MAVs using linear acceleration and body rates. This choice, combined with a robust low-level controller, enables reliable sim-to-real transfer despite significant uncertainties caused by cable tension during dynamic 3D motion. We validate our method in various real-world experiments, including full-pose control under load model uncertainties, showing setpoint tracking performance comparable to the state-of-the-art centralized method. We also demonstrate cooperation amongst agents with heterogeneous control policies, and robustness to the complete in-flight loss of one MAV. Videos of experiments: https://autonomousrobots.nl/paper_websites/aerial-manipulation-marl
<div id='section'>Paperid: <span id='pid'>1013, <a href='https://arxiv.org/pdf/2508.01522.pdf' target='_blank'>https://arxiv.org/pdf/2508.01522.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jack Zeng, Andreu Matoses Gimenez, Eugene Vinitsky, Javier Alonso-Mora, Sihao Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.01522">Decentralized Aerial Manipulation of a Cable-Suspended Load using Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents the first decentralized method to enable real-world 6-DoF manipulation of a cable-suspended load using a team of Micro-Aerial Vehicles (MAVs). Our method leverages multi-agent reinforcement learning (MARL) to train an outer-loop control policy for each MAV. Unlike state-of-the-art controllers that utilize a centralized scheme, our policy does not require global states, inter-MAV communications, nor neighboring MAV information. Instead, agents communicate implicitly through load pose observations alone, which enables high scalability and flexibility. It also significantly reduces computing costs during inference time, enabling onboard deployment of the policy. In addition, we introduce a new action space design for the MAVs using linear acceleration and body rates. This choice, combined with a robust low-level controller, enables reliable sim-to-real transfer despite significant uncertainties caused by cable tension during dynamic 3D motion. We validate our method in various real-world experiments, including full-pose control under load model uncertainties, showing setpoint tracking performance comparable to the state-of-the-art centralized method. We also demonstrate cooperation amongst agents with heterogeneous control policies, and robustness to the complete in-flight loss of one MAV. Videos of experiments: https://autonomousrobots.nl/paper_websites/aerial-manipulation-marl
<div id='section'>Paperid: <span id='pid'>1014, <a href='https://arxiv.org/pdf/2508.01522.pdf' target='_blank'>https://arxiv.org/pdf/2508.01522.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jack Zeng, Andreu Matoses Gimenez, Eugene Vinitsky, Javier Alonso-Mora, Sihao Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.01522">Decentralized Aerial Manipulation of a Cable-Suspended Load using Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents the first decentralized method to enable real-world 6-DoF manipulation of a cable-suspended load using a team of Micro-Aerial Vehicles (MAVs). Our method leverages multi-agent reinforcement learning (MARL) to train an outer-loop control policy for each MAV. Unlike state-of-the-art controllers that utilize a centralized scheme, our policy does not require global states, inter-MAV communications, nor neighboring MAV information. Instead, agents communicate implicitly through load pose observations alone, which enables high scalability and flexibility. It also significantly reduces computing costs during inference time, enabling onboard deployment of the policy. In addition, we introduce a new action space design for the MAVs using linear acceleration and body rates. This choice, combined with a robust low-level controller, enables reliable sim-to-real transfer despite significant uncertainties caused by cable tension during dynamic 3D motion. We validate our method in various real-world experiments, including full-pose control under load model uncertainties, showing setpoint tracking performance comparable to the state-of-the-art centralized method. We also demonstrate cooperation amongst agents with heterogeneous control policies, and robustness to the complete in-flight loss of one MAV. Videos of experiments: https://autonomousrobots.nl/paper_websites/aerial-manipulation-marl
<div id='section'>Paperid: <span id='pid'>1015, <a href='https://arxiv.org/pdf/2507.17188.pdf' target='_blank'>https://arxiv.org/pdf/2507.17188.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lijie Zheng, Ji He, Shih Yu Chang, Yulong Shen, Dusit Niyato
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.17188">LLM Meets the Sky: Heuristic Multi-Agent Reinforcement Learning for Secure Heterogeneous UAV Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work tackles the physical layer security (PLS) problem of maximizing the secrecy rate in heterogeneous UAV networks (HetUAVNs) under propulsion energy constraints. Unlike prior studies that assume uniform UAV capabilities or overlook energy-security trade-offs, we consider a realistic scenario where UAVs with diverse payloads and computation resources collaborate to serve ground terminals in the presence of eavesdroppers. To manage the complex coupling between UAV motion and communication, we propose a hierarchical optimization framework. The inner layer uses a semidefinite relaxation (SDR)-based S2DC algorithm combining penalty functions and difference-of-convex (d.c.) programming to solve the secrecy precoding problem with fixed UAV positions. The outer layer introduces a Large Language Model (LLM)-guided heuristic multi-agent reinforcement learning approach (LLM-HeMARL) for trajectory optimization. LLM-HeMARL efficiently incorporates expert heuristics policy generated by the LLM, enabling UAVs to learn energy-aware, security-driven trajectories without the inference overhead of real-time LLM calls. The simulation results show that our method outperforms existing baselines in secrecy rate and energy efficiency, with consistent robustness across varying UAV swarm sizes and random seeds.
<div id='section'>Paperid: <span id='pid'>1016, <a href='https://arxiv.org/pdf/2507.10913.pdf' target='_blank'>https://arxiv.org/pdf/2507.10913.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuangyao Huang, Haibo Zhang, Zhiyi Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.10913">A Learning Framework For Cooperative Collision Avoidance of UAV Swarms Leveraging Domain Knowledge</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a multi-agent reinforcement learning (MARL) framework for cooperative collision avoidance of UAV swarms leveraging domain knowledge-driven reward. The reward is derived from knowledge in the domain of image processing, approximating contours on a two-dimensional field. By modeling obstacles as maxima on the field, collisions are inherently avoided as contours never go through peaks or intersect. Additionally, counters are smooth and energy-efficient. Our framework enables training with large swarm sizes as the agent interaction is minimized and the need for complex credit assignment schemes or observation sharing mechanisms in state-of-the-art MARL approaches are eliminated. Moreover, UAVs obtain the ability to adapt to complex environments where contours may be non-viable or non-existent through intensive training. Extensive experiments are conducted to evaluate the performances of our framework against state-of-the-art MARL algorithms.
<div id='section'>Paperid: <span id='pid'>1017, <a href='https://arxiv.org/pdf/2507.07320.pdf' target='_blank'>https://arxiv.org/pdf/2507.07320.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dongyu Wei, Xiaoren Xu, Shiwen Mao, Mingzhe Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.07320">Optimizing Communication and Device Clustering for Clustered Federated Learning with Differential Privacy</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, a secure and communication-efficient clustered federated learning (CFL) design is proposed. In our model, several base stations (BSs) with heterogeneous task-handling capabilities and multiple users with non-independent and identically distributed (non-IID) data jointly perform CFL training incorporating differential privacy (DP) techniques. Since each BS can process only a subset of the learning tasks and has limited wireless resource blocks (RBs) to allocate to users for federated learning (FL) model parameter transmission, it is necessary to jointly optimize RB allocation and user scheduling for CFL performance optimization. Meanwhile, our considered CFL method requires devices to use their limited data and FL model information to determine their task identities, which may introduce additional communication overhead. We formulate an optimization problem whose goal is to minimize the training loss of all learning tasks while considering device clustering, RB allocation, DP noise, and FL model transmission delay. To solve the problem, we propose a novel dynamic penalty function assisted value decomposed multi-agent reinforcement learning (DPVD-MARL) algorithm that enables distributed BSs to independently determine their connected users, RBs, and DP noise of the connected users but jointly minimize the training loss of all learning tasks across all BSs. Different from the existing MARL methods that assign a large penalty for invalid actions, we propose a novel penalty assignment scheme that assigns penalty depending on the number of devices that cannot meet communication constraints (e.g., delay), which can guide the MARL scheme to quickly find valid actions, thus improving the convergence speed. Simulation results show that the DPVD-MARL can improve the convergence rate by up to 20% and the ultimate accumulated rewards by 15% compared to independent Q-learning.
<div id='section'>Paperid: <span id='pid'>1018, <a href='https://arxiv.org/pdf/2507.06997.pdf' target='_blank'>https://arxiv.org/pdf/2507.06997.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Deemah H. Tashman, Soumaya Cherkaoui, Walaa Hamouda
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.06997">Federated Learning-based MARL for Strengthening Physical-Layer Security in B5G Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper explores the application of a federated learning-based multi-agent reinforcement learning (MARL) strategy to enhance physical-layer security (PLS) in a multi-cellular network within the context of beyond 5G networks. At each cell, a base station (BS) operates as a deep reinforcement learning (DRL) agent that interacts with the surrounding environment to maximize the secrecy rate of legitimate users in the presence of an eavesdropper. This eavesdropper attempts to intercept the confidential information shared between the BS and its authorized users. The DRL agents are deemed to be federated since they only share their network parameters with a central server and not the private data of their legitimate users. Two DRL approaches, deep Q-network (DQN) and Reinforce deep policy gradient (RDPG), are explored and compared. The results demonstrate that RDPG converges more rapidly than DQN. In addition, we demonstrate that the proposed method outperforms the distributed DRL approach. Furthermore, the outcomes illustrate the trade-off between security and complexity.
<div id='section'>Paperid: <span id='pid'>1019, <a href='https://arxiv.org/pdf/2507.04140.pdf' target='_blank'>https://arxiv.org/pdf/2507.04140.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ho Jae Lee, Se Hwan Jeon, Sangbae Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.04140">Learning Humanoid Arm Motion via Centroidal Momentum Regularized Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humans naturally swing their arms during locomotion to regulate whole-body dynamics, reduce angular momentum, and help maintain balance. Inspired by this principle, we present a limb-level multi-agent reinforcement learning (RL) framework that enables coordinated whole-body control of humanoid robots through emergent arm motion. Our approach employs separate actor-critic structures for the arms and legs, trained with centralized critics but decentralized actors that share only base states and centroidal angular momentum (CAM) observations, allowing each agent to specialize in task-relevant behaviors through modular reward design. The arm agent guided by CAM tracking and damping rewards promotes arm motions that reduce overall angular momentum and vertical ground reaction moments, contributing to improved balance during locomotion or under external perturbations. Comparative studies with single-agent and alternative multi-agent baselines further validate the effectiveness of our approach. Finally, we deploy the learned policy on a humanoid platform, achieving robust performance across diverse locomotion tasks, including flat-ground walking, rough terrain traversal, and stair climbing.
<div id='section'>Paperid: <span id='pid'>1020, <a href='https://arxiv.org/pdf/2506.07829.pdf' target='_blank'>https://arxiv.org/pdf/2506.07829.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jan Corazza, Hadi Partovi Aria, Hyohun Kim, Daniel Neider, Zhe Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.07829">Decentralizing Multi-Agent Reinforcement Learning with Temporal Causal Information</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reinforcement learning (RL) algorithms can find an optimal policy for a single agent to accomplish a particular task. However, many real-world problems require multiple agents to collaborate in order to achieve a common goal. For example, a robot executing a task in a warehouse may require the assistance of a drone to retrieve items from high shelves. In Decentralized Multi-Agent RL (DMARL), agents learn independently and then combine their policies at execution time, but often must satisfy constraints on compatibility of local policies to ensure that they can achieve the global task when combined. In this paper, we study how providing high-level symbolic knowledge to agents can help address unique challenges of this setting, such as privacy constraints, communication limitations, and performance concerns. In particular, we extend the formal tools used to check the compatibility of local policies with the team task, making decentralized training with theoretical guarantees usable in more scenarios. Furthermore, we empirically demonstrate that symbolic knowledge about the temporal evolution of events in the environment can significantly expedite the learning process in DMARL.
<div id='section'>Paperid: <span id='pid'>1021, <a href='https://arxiv.org/pdf/2506.05437.pdf' target='_blank'>https://arxiv.org/pdf/2506.05437.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Julien SoulÃ©, Jean-Paul Jamont, Michel Occello, Louis-Marie Traonouez, Paul ThÃ©ron
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.05437">A MARL-based Approach for Easing MAS Organization Engineering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-Agent Systems (MAS) have been successfully applied in industry for their ability to address complex, distributed problems, especially in IoT-based systems. Their efficiency in achieving given objectives and meeting design requirements is strongly dependent on the MAS organization during the engineering process of an application-specific MAS. To design a MAS that can achieve given goals, available methods rely on the designer's knowledge of the deployment environment. However, high complexity and low readability in some deployment environments make the application of these methods to be costly or raise safety concerns. In order to ease the MAS organization design regarding those concerns, we introduce an original Assisted MAS Organization Engineering Approach (AOMEA). AOMEA relies on combining a Multi-Agent Reinforcement Learning (MARL) process with an organizational model to suggest relevant organizational specifications to help in MAS engineering.
<div id='section'>Paperid: <span id='pid'>1022, <a href='https://arxiv.org/pdf/2505.10484.pdf' target='_blank'>https://arxiv.org/pdf/2505.10484.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Andrea Baisero, Rupali Bhati, Shuo Liu, Aathira Pillai, Christopher Amato
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.10484">Fixing Incomplete Value Function Decomposition for Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Value function decomposition methods for cooperative multi-agent reinforcement learning compose joint values from individual per-agent utilities, and train them using a joint objective. To ensure that the action selection process between individual utilities and joint values remains consistent, it is imperative for the composition to satisfy the individual-global max (IGM) property. Although satisfying IGM itself is straightforward, most existing methods (e.g., VDN, QMIX) have limited representation capabilities and are unable to represent the full class of IGM values, and the one exception that has no such limitation (QPLEX) is unnecessarily complex. In this work, we present a simple formulation of the full class of IGM values that naturally leads to the derivation of QFIX, a novel family of value function decomposition models that expand the representation capabilities of prior models by means of a thin "fixing" layer. We derive multiple variants of QFIX, and implement three variants in two well-known multi-agent frameworks. We perform an empirical evaluation on multiple SMACv2 and Overcooked environments, which confirms that QFIX (i) succeeds in enhancing the performance of prior methods, (ii) learns more stably and performs better than its main competitor QPLEX, and (iii) achieves this while employing the simplest and smallest mixing models.
<div id='section'>Paperid: <span id='pid'>1023, <a href='https://arxiv.org/pdf/2505.01115.pdf' target='_blank'>https://arxiv.org/pdf/2505.01115.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Palok Biswas, Zuzanna Osika, Isidoro Tamassia, Adit Whorra, Jazmin Zatarain-Salazar, Jan Kwakkel, Frans A. Oliehoek, Pradeep K. Murukannaiah
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.01115">Exploring Equity of Climate Policies using Multi-Agent Multi-Objective Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Addressing climate change requires coordinated policy efforts of nations worldwide. These efforts are informed by scientific reports, which rely in part on Integrated Assessment Models (IAMs), prominent tools used to assess the economic impacts of climate policies. However, traditional IAMs optimize policies based on a single objective, limiting their ability to capture the trade-offs among economic growth, temperature goals, and climate justice. As a result, policy recommendations have been criticized for perpetuating inequalities, fueling disagreements during policy negotiations. We introduce Justice, the first framework integrating IAM with Multi-Objective Multi-Agent Reinforcement Learning (MOMARL). By incorporating multiple objectives, Justice generates policy recommendations that shed light on equity while balancing climate and economic goals. Further, using multiple agents can provide a realistic representation of the interactions among the diverse policy actors. We identify equitable Pareto-optimal policies using our framework, which facilitates deliberative decision-making by presenting policymakers with the inherent trade-offs in climate and economic policy.
<div id='section'>Paperid: <span id='pid'>1024, <a href='https://arxiv.org/pdf/2503.23615.pdf' target='_blank'>https://arxiv.org/pdf/2503.23615.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Julien SoulÃ©, Jean-Paul Jamont, Michel Occello, Louis-Marie Traonouez, Paul ThÃ©ron
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.23615">An Organizationally-Oriented Approach to Enhancing Explainability and Control in Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-Agent Reinforcement Learning can lead to the development of collaborative agent behaviors that show similarities with organizational concepts. Pushing forward this perspective, we introduce a novel framework that explicitly incorporates organizational roles and goals from the $\mathcal{M}OISE^+$ model into the MARL process, guiding agents to satisfy corresponding organizational constraints. By structuring training with roles and goals, we aim to enhance both the explainability and control of agent behaviors at the organizational level, whereas much of the literature primarily focuses on individual agents. Additionally, our framework includes a post-training analysis method to infer implicit roles and goals, offering insights into emergent agent behaviors. This framework has been applied across various MARL environments and algorithms, demonstrating coherence between predefined organizational specifications and those inferred from trained agents.
<div id='section'>Paperid: <span id='pid'>1025, <a href='https://arxiv.org/pdf/2503.15615.pdf' target='_blank'>https://arxiv.org/pdf/2503.15615.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Joshua McClellan, Greyson Brothers, Furong Huang, Pratap Tokekar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.15615">PEnGUiN: Partially Equivariant Graph NeUral Networks for Sample Efficient MARL</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Equivariant Graph Neural Networks (EGNNs) have emerged as a promising approach in Multi-Agent Reinforcement Learning (MARL), leveraging symmetry guarantees to greatly improve sample efficiency and generalization. However, real-world environments often exhibit inherent asymmetries arising from factors such as external forces, measurement inaccuracies, or intrinsic system biases. This paper introduces \textit{Partially Equivariant Graph NeUral Networks (PEnGUiN)}, a novel architecture specifically designed to address these challenges. We formally identify and categorize various types of partial equivariance relevant to MARL, including subgroup equivariance, feature-wise equivariance, regional equivariance, and approximate equivariance. We theoretically demonstrate that PEnGUiN is capable of learning both fully equivariant (EGNN) and non-equivariant (GNN) representations within a unified framework. Through extensive experiments on a range of MARL problems incorporating various asymmetries, we empirically validate the efficacy of PEnGUiN. Our results consistently demonstrate that PEnGUiN outperforms both EGNNs and standard GNNs in asymmetric environments, highlighting their potential to improve the robustness and applicability of graph-based MARL algorithms in real-world scenarios.
<div id='section'>Paperid: <span id='pid'>1026, <a href='https://arxiv.org/pdf/2503.13415.pdf' target='_blank'>https://arxiv.org/pdf/2503.13415.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Weiqiang Jin, Hongyang Du, Biao Zhao, Xingwu Tian, Bohang Shi, Guang Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.13415">A Comprehensive Survey on Multi-Agent Cooperative Decision-Making: Scenarios, Approaches, Challenges and Perspectives</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the rapid development of artificial intelligence, intelligent decision-making techniques have gradually surpassed human levels in various human-machine competitions, especially in complex multi-agent cooperative task scenarios. Multi-agent cooperative decision-making involves multiple agents working together to complete established tasks and achieve specific objectives. These techniques are widely applicable in real-world scenarios such as autonomous driving, drone navigation, disaster rescue, and simulated military confrontations. This paper begins with a comprehensive survey of the leading simulation environments and platforms used for multi-agent cooperative decision-making. Specifically, we provide an in-depth analysis for these simulation environments from various perspectives, including task formats, reward allocation, and the underlying technologies employed. Subsequently, we provide a comprehensive overview of the mainstream intelligent decision-making approaches, algorithms and models for multi-agent systems (MAS). Theseapproaches can be broadly categorized into five types: rule-based (primarily fuzzy logic), game theory-based, evolutionary algorithms-based, deep multi-agent reinforcement learning (MARL)-based, and large language models(LLMs)reasoning-based. Given the significant advantages of MARL andLLMs-baseddecision-making methods over the traditional rule, game theory, and evolutionary algorithms, this paper focuses on these multi-agent methods utilizing MARL and LLMs-based techniques. We provide an in-depth discussion of these approaches, highlighting their methodology taxonomies, advantages, and drawbacks. Further, several prominent research directions in the future and potential challenges of multi-agent cooperative decision-making are also detailed.
<div id='section'>Paperid: <span id='pid'>1027, <a href='https://arxiv.org/pdf/2503.05126.pdf' target='_blank'>https://arxiv.org/pdf/2503.05126.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Reginald McLean, Evangelos Chatzaroulas, Jordan Terry, Isaac Woungang, Nariman Farsad, Pablo Samuel Castro
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.05126">Multi-Task Reinforcement Learning Enables Parameter Scaling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-task reinforcement learning (MTRL) aims to endow a single agent with the ability to perform well on multiple tasks. Recent works have focused on developing novel sophisticated architectures to improve performance, often resulting in larger models; it is unclear, however, whether the performance gains are a consequence of the architecture design itself or the extra parameters. We argue that gains are mostly due to scale by demonstrating that naively scaling up a simple MTRL baseline to match parameter counts outperforms the more sophisticated architectures, and these gains benefit most from scaling the critic over the actor. Additionally, we explore the training stability advantages that come with task diversity, demonstrating that increasing the number of tasks can help mitigate plasticity loss. Our findings suggest that MTRL's simultaneous training across multiple tasks provides a natural framework for beneficial parameter scaling in reinforcement learning, challenging the need for complex architectural innovations.
<div id='section'>Paperid: <span id='pid'>1028, <a href='https://arxiv.org/pdf/2503.01458.pdf' target='_blank'>https://arxiv.org/pdf/2503.01458.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xu Wan, Chao Yang, Cheng Yang, Jie Song, Mingyang Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.01458">SrSv: Integrating Sequential Rollouts with Sequential Value Estimation for Multi-agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Although multi-agent reinforcement learning (MARL) has shown its success across diverse domains, extending its application to large-scale real-world systems still faces significant challenges. Primarily, the high complexity of real-world environments exacerbates the credit assignment problem, substantially reducing training efficiency. Moreover, the variability of agent populations in large-scale scenarios necessitates scalable decision-making mechanisms. To address these challenges, we propose a novel framework: Sequential rollout with Sequential value estimation (SrSv). This framework aims to capture agent interdependence and provide a scalable solution for cooperative MARL. Specifically, SrSv leverages the autoregressive property of the Transformer model to handle varying populations through sequential action rollout. Furthermore, to capture the interdependence of policy distributions and value functions among multiple agents, we introduce an innovative sequential value estimation methodology and integrates the value approximation into an attention-based sequential model. We evaluate SrSv on three benchmarks: Multi-Agent MuJoCo, StarCraft Multi-Agent Challenge, and DubinsCars. Experimental results demonstrate that SrSv significantly outperforms baseline methods in terms of training efficiency without compromising convergence performance. Moreover, when implemented in a large-scale DubinsCar system with 1,024 agents, our framework surpasses existing benchmarks, highlighting the excellent scalability of SrSv.
<div id='section'>Paperid: <span id='pid'>1029, <a href='https://arxiv.org/pdf/2502.17046.pdf' target='_blank'>https://arxiv.org/pdf/2502.17046.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinyuan Feng, Min Chen, Zhiqiang Pu, Yifan Xu, Yanyan Liang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.17046">MA2RL: Masked Autoencoders for Generalizable Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To develop generalizable models in multi-agent reinforcement learning, recent approaches have been devoted to discovering task-independent skills for each agent, which generalize across tasks and facilitate agents' cooperation. However, particularly in partially observed settings, such approaches struggle with sample efficiency and generalization capabilities due to two primary challenges: (a) How to incorporate global states into coordinating the skills of different agents? (b) How to learn generalizable and consistent skill semantics when each agent only receives partial observations? To address these challenges, we propose a framework called \textbf{M}asked \textbf{A}utoencoders for \textbf{M}ulti-\textbf{A}gent \textbf{R}einforcement \textbf{L}earning (MA2RL), which encourages agents to infer unobserved entities by reconstructing entity-states from the entity perspective. The entity perspective helps MA2RL generalize to diverse tasks with varying agent numbers and action spaces. Specifically, we treat local entity-observations as masked contexts of the global entity-states, and MA2RL can infer the latent representation of dynamically masked entities, facilitating the assignment of task-independent skills and the learning of skill semantics. Extensive experiments demonstrate that MA2RL achieves significant improvements relative to state-of-the-art approaches, demonstrating extraordinary performance, remarkable zero-shot generalization capabilities and advantageous transferability.
<div id='section'>Paperid: <span id='pid'>1030, <a href='https://arxiv.org/pdf/2502.14606.pdf' target='_blank'>https://arxiv.org/pdf/2502.14606.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Raihana Ferdous, Fitsum Kifetew, Davide Prandi, Angelo Susi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.14606">Curiosity Driven Multi-agent Reinforcement Learning for 3D Game Testing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently testing of games via autonomous agents has shown great promise in tackling challenges faced by the game industry, which mainly relied on either manual testing or record/replay. In particular Reinforcement Learning (RL) solutions have shown potential by learning directly from playing the game without the need for human intervention. In this paper, we present cMarlTest, an approach for testing 3D games through curiosity driven Multi-Agent Reinforcement Learning (MARL). cMarlTest deploys multiple agents that work collaboratively to achieve the testing objective. The use of multiple agents helps resolve issues faced by a single agent approach. We carried out experiments on different levels of a 3D game comparing the performance of cMarlTest with a single agent RL variant. Results are promising where, considering three different types of coverage criteria, cMarlTest achieved higher coverage. cMarlTest was also more efficient in terms of the time taken, with respect to the single agent based variant.
<div id='section'>Paperid: <span id='pid'>1031, <a href='https://arxiv.org/pdf/2502.14200.pdf' target='_blank'>https://arxiv.org/pdf/2502.14200.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao Ma, Zhiqiang Pu, Yi Pan, Boyin Liu, Junlong Gao, Zhenyu Guo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.14200">Causal Mean Field Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Scalability remains a challenge in multi-agent reinforcement learning and is currently under active research. A framework named mean-field reinforcement learning (MFRL) could alleviate the scalability problem by employing the Mean Field Theory to turn a many-agent problem into a two-agent problem. However, this framework lacks the ability to identify essential interactions under nonstationary environments. Causality contains relatively invariant mechanisms behind interactions, though environments are nonstationary. Therefore, we propose an algorithm called causal mean-field Q-learning (CMFQ) to address the scalability problem. CMFQ is ever more robust toward the change of the number of agents though inheriting the compressed representation of MFRL's action-state space. Firstly, we model the causality behind the decision-making process of MFRL into a structural causal model (SCM). Then the essential degree of each interaction is quantified via intervening on the SCM. Furthermore, we design the causality-aware compact representation for behavioral information of agents as the weighted sum of all behavioral information according to their causal effects. We test CMFQ in a mixed cooperative-competitive game and a cooperative game. The result shows that our method has excellent scalability performance in both training in environments containing a large number of agents and testing in environments containing much more agents.
<div id='section'>Paperid: <span id='pid'>1032, <a href='https://arxiv.org/pdf/2502.06060.pdf' target='_blank'>https://arxiv.org/pdf/2502.06060.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bidipta Sarkar, Warren Xia, C. Karen Liu, Dorsa Sadigh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.06060">Training Language Models for Social Deduction with Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Communicating in natural language is a powerful tool in multi-agent settings, as it enables independent agents to share information in partially observable settings and allows zero-shot coordination with humans. However, most prior works are limited as they either rely on training with large amounts of human demonstrations or lack the ability to generate natural and useful communication strategies. In this work, we train language models to have productive discussions about their environment in natural language without any human demonstrations. We decompose the communication problem into listening and speaking. Our key idea is to leverage the agent's goal to predict useful information about the world as a dense reward signal that guides communication. Specifically, we improve a model's listening skills by training them to predict information about the environment based on discussions, and we simultaneously improve a model's speaking skills with multi-agent reinforcement learning by rewarding messages based on their influence on other agents. To investigate the role and necessity of communication in complex social settings, we study an embodied social deduction game based on Among Us, where the key question to answer is the identity of an adversarial imposter. We analyze emergent behaviors due to our technique, such as accusing suspects and providing evidence, and find that it enables strong discussions, doubling the win rates compared to standard RL. We release our code and models at https://socialdeductionllm.github.io/
<div id='section'>Paperid: <span id='pid'>1033, <a href='https://arxiv.org/pdf/2502.04864.pdf' target='_blank'>https://arxiv.org/pdf/2502.04864.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aditya Kapoor, Kale-ab Tessera, Mayank Baranwal, Harshad Khadilkar, Stefano Albrecht, Mingfei Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.04864">$TAR^2$: Temporal-Agent Reward Redistribution for Optimal Policy Preservation in Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In cooperative multi-agent reinforcement learning (MARL), learning effective policies is challenging when global rewards are sparse and delayed. This difficulty arises from the need to assign credit across both agents and time steps, a problem that existing methods often fail to address in episodic, long-horizon tasks. We propose Temporal-Agent Reward Redistribution $TAR^2$, a novel approach that decomposes sparse global rewards into agent-specific, time-step-specific components, thereby providing more frequent and accurate feedback for policy learning. Theoretically, we show that $TAR^2$ (i) aligns with potential-based reward shaping, preserving the same optimal policies as the original environment, and (ii) maintains policy gradient update directions identical to those under the original sparse reward, ensuring unbiased credit signals. Empirical results on two challenging benchmarks, SMACLite and Google Research Football, demonstrate that $TAR^2$ significantly stabilizes and accelerates convergence, outperforming strong baselines like AREL and STAS in both learning speed and final performance. These findings establish $TAR^2$ as a principled and practical solution for agent-temporal credit assignment in sparse-reward multi-agent systems.
<div id='section'>Paperid: <span id='pid'>1034, <a href='https://arxiv.org/pdf/2502.03377.pdf' target='_blank'>https://arxiv.org/pdf/2502.03377.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Abdullahi Isa Ahmed, Jamal Bentahar, El Mehdi Amhoud
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.03377">Energy-Efficient Flying LoRa Gateways: A Multi-Agent Reinforcement Learning Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As next-generation Internet of Things (NG-IoT) networks continue to grow, the number of connected devices is rapidly increasing, along with their energy demands. This creates challenges for resource management and sustainability. Energy-efficient communication, particularly for power-limited IoT devices, is therefore a key research focus. In this paper, we deployed flying LoRa gateways mounted on unmanned aerial vehicles (UAVs) to collect data from LoRa end devices and transmit it to a central server. Our primary objective is to maximize the global system energy efficiency of wireless LoRa networks by joint optimization of transmission power, spreading factor, bandwidth, and user association. To solve this challenging problem, we model the problem as a partially observable Markov decision process (POMDP), where each flying LoRa GW acts as a learning agent using a cooperative multi-agent reinforcement learning (MARL). Simulation results demonstrate that our proposed method, based on the multi-agent proximal policy optimization algorithm, significantly improves the global system energy efficiency and surpasses the popular MARL and other conventional schemes.
<div id='section'>Paperid: <span id='pid'>1035, <a href='https://arxiv.org/pdf/2501.15802.pdf' target='_blank'>https://arxiv.org/pdf/2501.15802.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lanpei Li, Jack Bell, Massimo Coppola, Vincenzo Lomonaco
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.15802">Adaptive AI-based Decentralized Resource Management in the Cloud-Edge Continuum</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The increasing complexity of application requirements and the dynamic nature of the Cloud-Edge Continuum present significant challenges for efficient resource management. These challenges stem from the ever-changing infrastructure, which is characterized by additions, removals, and reconfigurations of nodes and links, as well as the variability of application workloads. Traditional centralized approaches struggle to adapt to these changes due to their static nature, while decentralized solutions face challenges such as limited global visibility and coordination overhead. This paper proposes a hybrid decentralized framework for dynamic application placement and resource management. The framework utilizes Graph Neural Networks (GNNs) to embed resource and application states, enabling comprehensive representation and efficient decision-making. It employs a collaborative multi-agent reinforcement learning (MARL) approach, where local agents optimize resource management in their neighborhoods and a global orchestrator ensures system-wide coordination. By combining decentralized application placement with centralized oversight, our framework addresses the scalability, adaptability, and accuracy challenges inherent in the Cloud-Edge Continuum. This work contributes to the development of decentralized application placement strategies, the integration of GNN embeddings, and collaborative MARL systems, providing a foundation for efficient, adaptive and scalable resource management.
<div id='section'>Paperid: <span id='pid'>1036, <a href='https://arxiv.org/pdf/2501.14451.pdf' target='_blank'>https://arxiv.org/pdf/2501.14451.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Linfeng Liang, Xi Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.14451">MARL-OT: Multi-Agent Reinforcement Learning Guided Online Fuzzing to Detect Safety Violation in Autonomous Driving Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autonomous Driving Systems (ADSs) are safety-critical, as real-world safety violations can result in significant losses. Rigorous testing is essential before deployment, with simulation testing playing a key role. However, ADSs are typically complex, consisting of multiple modules such as perception and planning, or well-trained end-to-end autonomous driving systems. Offline methods, such as the Genetic Algorithm (GA), can only generate predefined trajectories for dynamics, which struggle to cause safety violations for ADSs rapidly and efficiently in different scenarios due to their evolutionary nature. Online methods, such as single-agent reinforcement learning (RL), can quickly adjust the dynamics' trajectory online to adapt to different scenarios, but they struggle to capture complex corner cases of ADS arising from the intricate interplay among multiple vehicles. Multi-agent reinforcement learning (MARL) has a strong ability in cooperative tasks. On the other hand, it faces its own challenges, particularly with convergence. This paper introduces MARL-OT, a scalable framework that leverages MARL to detect safety violations of ADS resulting from surrounding vehicles' cooperation. MARL-OT employs MARL for high-level guidance, triggering various dangerous scenarios for the rule-based online fuzzer to explore potential safety violations of ADS, thereby generating dynamic, realistic safety violation scenarios. Our approach improves the detected safety violation rate by up to 136.2% compared to the state-of-the-art (SOTA) testing technique.
<div id='section'>Paperid: <span id='pid'>1037, <a href='https://arxiv.org/pdf/2501.06454.pdf' target='_blank'>https://arxiv.org/pdf/2501.06454.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Obed Morrison Atsu, Salmane Naoumi, Roberto Bomfin, Marwa Chafii
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.06454">Reinforcement Learning for Enhancing Sensing Estimation in Bistatic ISAC Systems with UAV Swarms</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces a novel Multi-Agent Reinforcement Learning (MARL) framework to enhance integrated sensing and communication (ISAC) networks using unmanned aerial vehicle (UAV) swarms as sensing radars. By framing the positioning and trajectory optimization of UAVs as a Partially Observable Markov Decision Process, we develop a MARL approach that leverages centralized training with decentralized execution to maximize the overall sensing performance. Specifically, we implement a decentralized cooperative MARL strategy to enable UAVs to develop effective communication protocols, therefore enhancing their environmental awareness and operational efficiency. Additionally, we augment the MARL solution with a transmission power adaptation technique to mitigate interference between the communicating drones and optimize the communication protocol efficiency. Moreover, a transmission power adaptation technique is incorporated to mitigate interference and optimize the learned communication protocol efficiency. Despite the increased complexity, our solution demonstrates robust performance and adaptability across various scenarios, providing a scalable and cost-effective enhancement for future ISAC networks.
<div id='section'>Paperid: <span id='pid'>1038, <a href='https://arxiv.org/pdf/2501.05329.pdf' target='_blank'>https://arxiv.org/pdf/2501.05329.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dmytro Kuzmenko, Nadiya Shvai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.05329">Knowledge Transfer in Model-Based Reinforcement Learning Agents for Efficient Multi-Task Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose an efficient knowledge transfer approach for model-based reinforcement learning, addressing the challenge of deploying large world models in resource-constrained environments. Our method distills a high-capacity multi-task agent (317M parameters) into a compact 1M parameter model, achieving state-of-the-art performance on the MT30 benchmark with a normalized score of 28.45, a substantial improvement over the original 1M parameter model's score of 18.93. This demonstrates the ability of our distillation technique to consolidate complex multi-task knowledge effectively. Additionally, we apply FP16 post-training quantization, reducing the model size by 50% while maintaining performance. Our work bridges the gap between the power of large models and practical deployment constraints, offering a scalable solution for efficient and accessible multi-task reinforcement learning in robotics and other resource-limited domains.
<div id='section'>Paperid: <span id='pid'>1039, <a href='https://arxiv.org/pdf/2501.05113.pdf' target='_blank'>https://arxiv.org/pdf/2501.05113.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tobias Kortus, Ralf Keidel, Nicolas R. Gauger, Jan Kieseler
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.05113">Constrained Optimization of Charged Particle Tracking with Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reinforcement learning demonstrated immense success in modelling complex physics-driven systems, providing end-to-end trainable solutions by interacting with a simulated or real environment, maximizing a scalar reward signal. In this work, we propose, building upon previous work, a multi-agent reinforcement learning approach with assignment constraints for reconstructing particle tracks in pixelated particle detectors. Our approach optimizes collaboratively a parametrized policy, functioning as a heuristic to a multidimensional assignment problem, by jointly minimizing the total amount of particle scattering over the reconstructed tracks in a readout frame. To satisfy constraints, guaranteeing a unique assignment of particle hits, we propose a safety layer solving a linear assignment problem for every joint action. Further, to enforce cost margins, increasing the distance of the local policies predictions to the decision boundaries of the optimizer mappings, we recommend the use of an additional component in the blackbox gradient estimation, forcing the policy to solutions with lower total assignment costs. We empirically show on simulated data, generated for a particle detector developed for proton imaging, the effectiveness of our approach, compared to multiple single- and multi-agent baselines. We further demonstrate the effectiveness of constraints with cost margins for both optimization and generalization, introduced by wider regions with high reconstruction performance as well as reduced predictive instabilities. Our results form the basis for further developments in RL-based tracking, offering both enhanced performance with constrained policies and greater flexibility in optimizing tracking algorithms through the option for individual and team rewards.
<div id='section'>Paperid: <span id='pid'>1040, <a href='https://arxiv.org/pdf/2412.14779.pdf' target='_blank'>https://arxiv.org/pdf/2412.14779.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aditya Kapoor, Sushant Swamy, Kale-ab Tessera, Mayank Baranwal, Mingfei Sun, Harshad Khadilkar, Stefano V. Albrecht
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.14779">Agent-Temporal Credit Assignment for Optimal Policy Preservation in Sparse Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In multi-agent environments, agents often struggle to learn optimal policies due to sparse or delayed global rewards, particularly in long-horizon tasks where it is challenging to evaluate actions at intermediate time steps. We introduce Temporal-Agent Reward Redistribution (TAR$^2$), a novel approach designed to address the agent-temporal credit assignment problem by redistributing sparse rewards both temporally and across agents. TAR$^2$ decomposes sparse global rewards into time-step-specific rewards and calculates agent-specific contributions to these rewards. We theoretically prove that TAR$^2$ is equivalent to potential-based reward shaping, ensuring that the optimal policy remains unchanged. Empirical results demonstrate that TAR$^2$ stabilizes and accelerates the learning process. Additionally, we show that when TAR$^2$ is integrated with single-agent reinforcement learning algorithms, it performs as well as or better than traditional multi-agent reinforcement learning methods.
<div id='section'>Paperid: <span id='pid'>1041, <a href='https://arxiv.org/pdf/2412.02091.pdf' target='_blank'>https://arxiv.org/pdf/2412.02091.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kee Siong Ng, Samuel Yang-Zhao, Timothy Cadogan-Cowper
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.02091">The Problem of Social Cost in Multi-Agent General Reinforcement Learning: Survey and Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The AI safety literature is full of examples of powerful AI agents that, in blindly pursuing a specific and usually narrow objective, ends up with unacceptable and even catastrophic collateral damage to others. In this paper, we consider the problem of social harms that can result from actions taken by learning and utility-maximising agents in a multi-agent environment. The problem of measuring social harms or impacts in such multi-agent settings, especially when the agents are artificial generally intelligent (AGI) agents, was listed as an open problem in Everitt et al, 2018. We attempt a partial answer to that open problem in the form of market-based mechanisms to quantify and control the cost of such social harms. The proposed setup captures many well-studied special cases and is more general than existing formulations of multi-agent reinforcement learning with mechanism design in two ways: (i) the underlying environment is a history-based general reinforcement learning environment like in AIXI; (ii) the reinforcement-learning agents participating in the environment can have different learning strategies and planning horizons. To demonstrate the practicality of the proposed setup, we survey some key classes of learning algorithms and present a few applications, including a discussion of the Paperclips problem and pollution control with a cap-and-trade system.
<div id='section'>Paperid: <span id='pid'>1042, <a href='https://arxiv.org/pdf/2411.13942.pdf' target='_blank'>https://arxiv.org/pdf/2411.13942.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ing-Sheng Bernard-Tiong, Yoshihisa Tsurumine, Ryosuke Sota, Kazuki Shibata, Takamitsu Matsubara
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.13942">Cooperative Grasping and Transportation using Multi-agent Reinforcement Learning with Ternary Force Representation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cooperative grasping and transportation require effective coordination to complete the task. This study focuses on the approach leveraging force-sensing feedback, where robots use sensors to detect forces applied by others on an object to achieve coordination. Unlike explicit communication, it avoids delays and interruptions; however, force-sensing is highly sensitive and prone to interference from variations in grasping environment, such as changes in grasping force, grasping pose, object size and geometry, which can interfere with force signals, subsequently undermining coordination. We propose multi-agent reinforcement learning (MARL) with ternary force representation, a force representation that maintains consistent representation against variations in grasping environment. The simulation and real-world experiments demonstrate the robustness of the proposed method to changes in grasping force, object size and geometry as well as inherent sim2real gap.
<div id='section'>Paperid: <span id='pid'>1043, <a href='https://arxiv.org/pdf/2410.02581.pdf' target='_blank'>https://arxiv.org/pdf/2410.02581.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Joshua McClellan, Naveed Haghani, John Winder, Furong Huang, Pratap Tokekar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.02581">Boosting Sample Efficiency and Generalization in Multi-agent Reinforcement Learning via Equivariance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-Agent Reinforcement Learning (MARL) struggles with sample inefficiency and poor generalization [1]. These challenges are partially due to a lack of structure or inductive bias in the neural networks typically used in learning the policy. One such form of structure that is commonly observed in multi-agent scenarios is symmetry. The field of Geometric Deep Learning has developed Equivariant Graph Neural Networks (EGNN) that are equivariant (or symmetric) to rotations, translations, and reflections of nodes. Incorporating equivariance has been shown to improve learning efficiency and decrease error [ 2 ]. In this paper, we demonstrate that EGNNs improve the sample efficiency and generalization in MARL. However, we also show that a naive application of EGNNs to MARL results in poor early exploration due to a bias in the EGNN structure. To mitigate this bias, we present Exploration-enhanced Equivariant Graph Neural Networks or E2GN2. We compare E2GN2 to other common function approximators using common MARL benchmarks MPE and SMACv2. E2GN2 demonstrates a significant improvement in sample efficiency, greater final reward convergence, and a 2x-5x gain in over standard GNNs in our generalization tests. These results pave the way for more reliable and effective solutions in complex multi-agent systems.
<div id='section'>Paperid: <span id='pid'>1044, <a href='https://arxiv.org/pdf/2409.02246.pdf' target='_blank'>https://arxiv.org/pdf/2409.02246.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Matthew Repasky, He Wang, Yao Xie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.02246">Multi-Agent Reinforcement Learning for Joint Police Patrol and Dispatch</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Police patrol units need to split their time between performing preventive patrol and being dispatched to serve emergency incidents. In the existing literature, patrol and dispatch decisions are often studied separately. We consider joint optimization of these two decisions to improve police operations efficiency and reduce response time to emergency calls. Methodology/results: We propose a novel method for jointly optimizing multi-agent patrol and dispatch to learn policies yielding rapid response times. Our method treats each patroller as an independent Q-learner (agent) with a shared deep Q-network that represents the state-action values. The dispatching decisions are chosen using mixed-integer programming and value function approximation from combinatorial action spaces. We demonstrate that this heterogeneous multi-agent reinforcement learning approach is capable of learning joint policies that outperform those optimized for patrol or dispatch alone. Managerial Implications: Policies jointly optimized for patrol and dispatch can lead to more effective service while targeting demonstrably flexible objectives, such as those encouraging efficiency and equity in response.
<div id='section'>Paperid: <span id='pid'>1045, <a href='https://arxiv.org/pdf/2408.16875.pdf' target='_blank'>https://arxiv.org/pdf/2408.16875.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Abdalwhab Abdalwhab, Giovanni Beltrame, Samira Ebrahimi Kahou, David St-Onge
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.16875">Learning Multi-agent Multi-machine Tending by Mobile Robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robotics can help address the growing worker shortage challenge of the manufacturing industry. As such, machine tending is a task collaborative robots can tackle that can also highly boost productivity. Nevertheless, existing robotics systems deployed in that sector rely on a fixed single-arm setup, whereas mobile robots can provide more flexibility and scalability. In this work, we introduce a multi-agent multi-machine tending learning framework by mobile robots based on Multi-agent Reinforcement Learning (MARL) techniques with the design of a suitable observation and reward. Moreover, an attention-based encoding mechanism is developed and integrated into Multi-agent Proximal Policy Optimization (MAPPO) algorithm to boost its performance for machine tending scenarios. Our model (AB-MAPPO) outperformed MAPPO in this new challenging scenario in terms of task success, safety, and resources utilization. Furthermore, we provided an extensive ablation study to support our various design decisions.
<div id='section'>Paperid: <span id='pid'>1046, <a href='https://arxiv.org/pdf/2408.15381.pdf' target='_blank'>https://arxiv.org/pdf/2408.15381.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Enrico Marchesini, Andrea Baisero, Rupali Bhati, Christopher Amato
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.15381">On Stateful Value Factorization in Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Value factorization is a popular paradigm for designing scalable multi-agent reinforcement learning algorithms. However, current factorization methods make choices without full justification that may limit their performance. For example, the theory in prior work uses stateless (i.e., history) functions, while the practical implementations use state information -- making the motivating theory a mismatch for the implementation. Also, methods have built off of previous approaches, inheriting their architectures without exploring other, potentially better ones. To address these concerns, we formally analyze the theory of using the state instead of the history in current methods -- reconnecting theory and practice. We then introduce DuelMIX, a factorization algorithm that learns distinct per-agent utility estimators to improve performance and achieve full expressiveness. Experiments on StarCraft II micromanagement and Box Pushing tasks demonstrate the benefits of our intuitions.
<div id='section'>Paperid: <span id='pid'>1047, <a href='https://arxiv.org/pdf/2408.14597.pdf' target='_blank'>https://arxiv.org/pdf/2408.14597.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xueguang Lyu, Andrea Baisero, Yuchen Xiao, Brett Daley, Christopher Amato
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.14597">On Centralized Critics in Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Centralized Training for Decentralized Execution where agents are trained offline in a centralized fashion and execute online in a decentralized manner, has become a popular approach in Multi-Agent Reinforcement Learning (MARL). In particular, it has become popular to develop actor-critic methods that train decentralized actors with a centralized critic where the centralized critic is allowed access global information of the entire system, including the true system state. Such centralized critics are possible given offline information and are not used for online execution. While these methods perform well in a number of domains and have become a de facto standard in MARL, using a centralized critic in this context has yet to be sufficiently analyzed theoretically or empirically. In this paper, we therefore formally analyze centralized and decentralized critic approaches, and analyze the effect of using state-based critics in partially observable environments. We derive theories contrary to the common intuition: critic centralization is not strictly beneficial, and using state values can be harmful. We further prove that, in particular, state-based critics can introduce unexpected bias and variance compared to history-based critics. Finally, we demonstrate how the theory applies in practice by comparing different forms of critics on a wide range of common multi-agent benchmarks. The experiments show practical issues such as the difficulty of representation learning with partial observability, which highlights why the theoretical problems are often overlooked in the literature.
<div id='section'>Paperid: <span id='pid'>1048, <a href='https://arxiv.org/pdf/2408.13759.pdf' target='_blank'>https://arxiv.org/pdf/2408.13759.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qi Liu, Jingxiang Guo, Sixu Lin, Shuaikang Ma, Jinxuan Zhu, Yanjie Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.13759">MASQ: Multi-Agent Reinforcement Learning for Single Quadruped Robot Locomotion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper proposes a novel method to improve locomotion learning for a single quadruped robot using multi-agent deep reinforcement learning (MARL). Many existing methods use single-agent reinforcement learning for an individual robot or MARL for the cooperative task in multi-robot systems. Unlike existing methods, this paper proposes using MARL for the locomotion learning of a single quadruped robot. We develop a learning structure called Multi-Agent Reinforcement Learning for Single Quadruped Robot Locomotion (MASQ), considering each leg as an agent to explore the action space of the quadruped robot, sharing a global critic, and learning collaboratively. Experimental results indicate that MASQ not only speeds up learning convergence but also enhances robustness in real-world settings, suggesting that applying MASQ to single robots such as quadrupeds could surpass traditional single-robot reinforcement learning approaches. Our study provides insightful guidance on integrating MARL with single-robot locomotion learning.
<div id='section'>Paperid: <span id='pid'>1049, <a href='https://arxiv.org/pdf/2408.03884.pdf' target='_blank'>https://arxiv.org/pdf/2408.03884.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mazyar Taghavi, Rahman Farnoosh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.03884">Quantum Computing and Neuromorphic Computing for Safe, Reliable, and explainable Multi-Agent Reinforcement Learning: Optimal Control in Autonomous Robotics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper investigates the utilization of Quantum Computing and Neuromorphic Computing for Safe, Reliable, and Explainable Multi_Agent Reinforcement Learning (MARL) in the context of optimal control in autonomous robotics. The objective was to address the challenges of optimizing the behavior of autonomous agents while ensuring safety, reliability, and explainability. Quantum Computing techniques, including Quantum Approximate Optimization Algorithm (QAOA), were employed to efficiently explore large solution spaces and find approximate solutions to complex MARL problems. Neuromorphic Computing, inspired by the architecture of the human brain, provided parallel and distributed processing capabilities, which were leveraged to develop intelligent and adaptive systems. The combination of these technologies held the potential to enhance the safety, reliability, and explainability of MARL in autonomous robotics. This research contributed to the advancement of autonomous robotics by exploring cutting-edge technologies and their applications in multi-agent systems. Codes and data are available.
<div id='section'>Paperid: <span id='pid'>1050, <a href='https://arxiv.org/pdf/2407.05876.pdf' target='_blank'>https://arxiv.org/pdf/2407.05876.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Timo Bertram, Johannes FÃ¼rnkranz, Martin MÃ¼ller
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.05876">Efficiently Training Neural Networks for Imperfect Information Games by Sampling Information Sets</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In imperfect information games, the evaluation of a game state not only depends on the observable world but also relies on hidden parts of the environment. As accessing the obstructed information trivialises state evaluations, one approach to tackle such problems is to estimate the value of the imperfect state as a combination of all states in the information set, i.e., all possible states that are consistent with the current imperfect information. In this work, the goal is to learn a function that maps from the imperfect game information state to its expected value. However, constructing a perfect training set, i.e. an enumeration of the whole information set for numerous imperfect states, is often infeasible. To compute the expected values for an imperfect information game like \textit{Reconnaissance Blind Chess}, one would need to evaluate thousands of chess positions just to obtain the training target for a single state. Still, the expected value of a state can already be approximated with appropriate accuracy from a much smaller set of evaluations. Thus, in this paper, we empirically investigate how a budget of perfect information game evaluations should be distributed among training samples to maximise the return. Our results show that sampling a small number of states, in our experiments roughly 3, for a larger number of separate positions is preferable over repeatedly sampling a smaller quantity of states. Thus, we find that in our case, the quantity of different samples seems to be more important than higher target quality.
<div id='section'>Paperid: <span id='pid'>1051, <a href='https://arxiv.org/pdf/2407.05864.pdf' target='_blank'>https://arxiv.org/pdf/2407.05864.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Timo Bertram, Johannes FÃ¼rnkranz, Martin MÃ¼ller
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.05864">Neural Network-based Information Set Weighting for Playing Reconnaissance Blind Chess</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In imperfect information games, the game state is generally not fully observable to players. Therefore, good gameplay requires policies that deal with the different information that is hidden from each player. To combat this, effective algorithms often reason about information sets; the sets of all possible game states that are consistent with a player's observations. While there is no way to distinguish between the states within an information set, this property does not imply that all states are equally likely to occur in play. We extend previous research on assigning weights to the states in an information set in order to facilitate better gameplay in the imperfect information game of Reconnaissance Blind Chess. For this, we train two different neural networks which estimate the likelihood of each state in an information set from historical game data. Experimentally, we find that a Siamese neural network is able to achieve higher accuracy and is more efficient than a classical convolutional neural network for the given domain. Finally, we evaluate an RBC-playing agent that is based on the generated weightings and compare different parameter settings that influence how strongly it should rely on them. The resulting best player is ranked 5th on the public leaderboard.
<div id='section'>Paperid: <span id='pid'>1052, <a href='https://arxiv.org/pdf/2406.14156.pdf' target='_blank'>https://arxiv.org/pdf/2406.14156.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Eric Mazumdar, Kishan Panaganti, Laixi Shi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.14156">Tractable Equilibrium Computation in Markov Games through Risk Aversion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A significant roadblock to the development of principled multi-agent reinforcement learning is the fact that desired solution concepts like Nash equilibria may be intractable to compute. To overcome this obstacle, we take inspiration from behavioral economics and show that -- by imbuing agents with important features of human decision-making like risk aversion and bounded rationality -- a class of risk-averse quantal response equilibria (RQE) become tractable to compute in all $n$-player matrix and finite-horizon Markov games. In particular, we show that they emerge as the endpoint of no-regret learning in suitably adjusted versions of the games. Crucially, the class of computationally tractable RQE is independent of the underlying game structure and only depends on agents' degree of risk-aversion and bounded rationality. To validate the richness of this class of solution concepts we show that it captures peoples' patterns of play in a number of 2-player matrix games previously studied in experimental economics. Furthermore, we give a first analysis of the sample complexity of computing these equilibria in finite-horizon Markov games when one has access to a generative model and validate our findings on a simple multi-agent reinforcement learning benchmark.
<div id='section'>Paperid: <span id='pid'>1053, <a href='https://arxiv.org/pdf/2405.18123.pdf' target='_blank'>https://arxiv.org/pdf/2405.18123.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Martin Balla, George E. M. Long, James Goodman, Raluca D. Gaina, Diego Perez-Liebana
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.18123">PyTAG: Tabletop Games for Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Modern Tabletop Games present various interesting challenges for Multi-agent Reinforcement Learning. In this paper, we introduce PyTAG, a new framework that supports interacting with a large collection of games implemented in the Tabletop Games framework. In this work we highlight the challenges tabletop games provide, from a game-playing agent perspective, along with the opportunities they provide for future research. Additionally, we highlight the technical challenges that involve training Reinforcement Learning agents on these games. To explore the Multi-agent setting provided by PyTAG we train the popular Proximal Policy Optimisation Reinforcement Learning algorithm using self-play on a subset of games and evaluate the trained policies against some simple agents and Monte-Carlo Tree Search implemented in the Tabletop Games framework.
<div id='section'>Paperid: <span id='pid'>1054, <a href='https://arxiv.org/pdf/2405.02161.pdf' target='_blank'>https://arxiv.org/pdf/2405.02161.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Simone Brusatin, Tommaso Padoan, Andrea Coletta, Domenico Delli Gatti, Aldo Glielmo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.02161">Simulating the Economic Impact of Rationality through Reinforcement Learning and Agent-Based Modelling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Agent-based models (ABMs) are simulation models used in economics to overcome some of the limitations of traditional frameworks based on general equilibrium assumptions. However, agents within an ABM follow predetermined 'bounded rational' behavioural rules which can be cumbersome to design and difficult to justify. Here we leverage multi-agent reinforcement learning (RL) to expand the capabilities of ABMs with the introduction of 'fully rational' agents that learn their policy by interacting with the environment and maximising a reward function. Specifically, we propose a 'Rational macro ABM' (R-MABM) framework by extending a paradigmatic macro ABM from the economic literature. We show that gradually substituting ABM firms in the model with RL agents, trained to maximise profits, allows for studying the impact of rationality on the economy. We find that RL agents spontaneously learn three distinct strategies for maximising profits, with the optimal strategy depending on the level of market competition and rationality. We also find that RL agents with independent policies, and without the ability to communicate with each other, spontaneously learn to segregate into different strategic groups, thus increasing market power and overall profits. Finally, we find that a higher number of rational (RL) agents in the economy always improves the macroeconomic environment as measured by total output. Depending on the specific rational policy, this can come at the cost of higher instability. Our R-MABM framework allows for stable multi-agent learning, is available in open source, and represents a principled and robust direction to extend economic simulators.
<div id='section'>Paperid: <span id='pid'>1055, <a href='https://arxiv.org/pdf/2405.00902.pdf' target='_blank'>https://arxiv.org/pdf/2405.00902.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhicheng Zhang, Yancheng Liang, Yi Wu, Fei Fang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.00902">MESA: Cooperative Meta-Exploration in Multi-Agent Learning through Exploiting State-Action Space Structure</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent reinforcement learning (MARL) algorithms often struggle to find strategies close to Pareto optimal Nash Equilibrium, owing largely to the lack of efficient exploration. The problem is exacerbated in sparse-reward settings, caused by the larger variance exhibited in policy learning. This paper introduces MESA, a novel meta-exploration method for cooperative multi-agent learning. It learns to explore by first identifying the agents' high-rewarding joint state-action subspace from training tasks and then learning a set of diverse exploration policies to "cover" the subspace. These trained exploration policies can be integrated with any off-policy MARL algorithm for test-time tasks. We first showcase MESA's advantage in a multi-step matrix game. Furthermore, experiments show that with learned exploration policies, MESA achieves significantly better performance in sparse-reward tasks in several multi-agent particle environments and multi-agent MuJoCo environments, and exhibits the ability to generalize to more challenging tasks at test time.
<div id='section'>Paperid: <span id='pid'>1056, <a href='https://arxiv.org/pdf/2404.15583.pdf' target='_blank'>https://arxiv.org/pdf/2404.15583.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sarah Keren, Chaimaa Essayeh, Stefano V. Albrecht, Thomas Morstyn
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.15583">Multi-Agent Reinforcement Learning for Energy Networks: Computational Challenges, Progress and Open Problems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapidly changing architecture and functionality of electrical networks and the increasing penetration of renewable and distributed energy resources have resulted in various technological and managerial challenges. These have rendered traditional centralized energy-market paradigms insufficient due to their inability to support the dynamic and evolving nature of the network. This survey explores how multi-agent reinforcement learning (MARL) can support the decentralization and decarbonization of energy networks and mitigate the associated challenges. This is achieved by specifying key computational challenges in managing energy networks, reviewing recent research progress on addressing them, and highlighting open challenges that may be addressed using MARL.
<div id='section'>Paperid: <span id='pid'>1057, <a href='https://arxiv.org/pdf/2404.03596.pdf' target='_blank'>https://arxiv.org/pdf/2404.03596.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yannick Molinghen, RaphaÃ«l Avalos, Mark Van Achter, Ann NowÃ©, Tom Lenaerts
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.03596">Laser Learning Environment: A new environment for coordination-critical multi-agent tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce the Laser Learning Environment (LLE), a collaborative multi-agent reinforcement learning environment in which coordination is central. In LLE, agents depend on each other to make progress (interdependence), must jointly take specific sequences of actions to succeed (perfect coordination), and accomplishing those joint actions does not yield any intermediate reward (zero-incentive dynamics). The challenge of such problems lies in the difficulty of escaping state space bottlenecks caused by interdependence steps since escaping those bottlenecks is not rewarded. We test multiple state-of-the-art value-based MARL algorithms against LLE and show that they consistently fail at the collaborative task because of their inability to escape state space bottlenecks, even though they successfully achieve perfect coordination. We show that Q-learning extensions such as prioritized experience replay and n-steps return hinder exploration in environments with zero-incentive dynamics, and find that intrinsic curiosity with random network distillation is not sufficient to escape those bottlenecks. We demonstrate the need for novel methods to solve this problem and the relevance of LLE as cooperative MARL benchmark.
<div id='section'>Paperid: <span id='pid'>1058, <a href='https://arxiv.org/pdf/2403.11914.pdf' target='_blank'>https://arxiv.org/pdf/2403.11914.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shengchao Yan, Lukas KÃ¶nig, Wolfram Burgard
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.11914">Agent-Agnostic Centralized Training for Decentralized Multi-Agent Cooperative Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Active traffic management with autonomous vehicles offers the potential for reduced congestion and improved traffic flow. However, developing effective algorithms for real-world scenarios requires overcoming challenges related to infinite-horizon traffic flow and partial observability. To address these issues and further decentralize traffic management, we propose an asymmetric actor-critic model that learns decentralized cooperative driving policies for autonomous vehicles using single-agent reinforcement learning. By employing attention neural networks with masking, our approach efficiently manages real-world traffic dynamics and partial observability, eliminating the need for predefined agents or agent-specific experience buffers in multi-agent reinforcement learning. Extensive evaluations across various traffic scenarios demonstrate our method's significant potential in improving traffic flow at critical bottleneck points. Moreover, we address the challenges posed by conservative autonomous vehicle driving behaviors that adhere strictly to traffic rules, showing that our cooperative policy effectively alleviates potential slowdowns without compromising safety.
<div id='section'>Paperid: <span id='pid'>1059, <a href='https://arxiv.org/pdf/2403.07890.pdf' target='_blank'>https://arxiv.org/pdf/2403.07890.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Weichao Mao, Haoran Qiu, Chen Wang, Hubertus Franke, Zbigniew Kalbarczyk, Tamer BaÅar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.07890">$\widetilde{O}(T^{-1})$ Convergence to (Coarse) Correlated Equilibria in Full-Information General-Sum Markov Games</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>No-regret learning has a long history of being closely connected to game theory. Recent works have devised uncoupled no-regret learning dynamics that, when adopted by all the players in normal-form games, converge to various equilibrium solutions at a near-optimal rate of $\widetilde{O}(T^{-1})$, a significant improvement over the $O(1/\sqrt{T})$ rate of classic no-regret learners. However, analogous convergence results are scarce in Markov games, a more generic setting that lays the foundation for multi-agent reinforcement learning. In this work, we close this gap by showing that the optimistic-follow-the-regularized-leader (OFTRL) algorithm, together with appropriate value update procedures, can find $\widetilde{O}(T^{-1})$-approximate (coarse) correlated equilibria in full-information general-sum Markov games within $T$ iterations. Numerical results are also included to corroborate our theoretical findings.
<div id='section'>Paperid: <span id='pid'>1060, <a href='https://arxiv.org/pdf/2402.17268.pdf' target='_blank'>https://arxiv.org/pdf/2402.17268.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hong Cheng, Huan Luo, Zhi Liu, Wei Sun, Weitao Li, Qiyue Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.17268">Reinforcement Learning Based Robust Volt/Var Control in Active Distribution Networks With Imprecisely Known Delay</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Active distribution networks (ADNs) incorporating massive photovoltaic (PV) devices encounter challenges of rapid voltage fluctuations and potential violations. Due to the fluctuation and intermittency of PV generation, the state gap, arising from time-inconsistent states and exacerbated by imprecisely known system delays, significantly impacts the accuracy of voltage control. This paper addresses this challenge by introducing a framework for delay adaptive Volt/Var control (VVC) in the presence of imprecisely known system delays to regulate the reactive power of PV inverters. The proposed approach formulates the voltage control, based on predicted system operation states, as a robust VVC problem. It employs sample selection from the state prediction interval to promptly identify the worst-performing system operation state. Furthermore, we leverage the decentralized partially observable Markov decision process (Dec-POMDP) to reformulate the robust VVC problem. We design Multiple Policy Networks and employ Multiple Policy Networks and Reward Shaping-based Multi-agent Twin Delayed Deep Deterministic Policy Gradient (MPNRS-MATD3) algorithm to efficiently address and solve the Dec-POMDP model-based problem. Simulation results show the delay adaption characteristic of our proposed framework, and the MPNRS-MATD3 outperforms other multi-agent reinforcement learning algorithms in robust voltage control.
<div id='section'>Paperid: <span id='pid'>1061, <a href='https://arxiv.org/pdf/2402.05027.pdf' target='_blank'>https://arxiv.org/pdf/2402.05027.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jannis Weil, Zhenghua Bao, Osama Abboud, Tobias Meuser
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.05027">Towards Generalizability of Multi-Agent Reinforcement Learning in Graphs with Recurrent Message Passing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Graph-based environments pose unique challenges to multi-agent reinforcement learning. In decentralized approaches, agents operate within a given graph and make decisions based on partial or outdated observations. The size of the observed neighborhood limits the generalizability to different graphs and affects the reactivity of agents, the quality of the selected actions, and the communication overhead. This work focuses on generalizability and resolves the trade-off in observed neighborhood size with a continuous information flow in the whole graph. We propose a recurrent message-passing model that iterates with the environment's steps and allows nodes to create a global representation of the graph by exchanging messages with their neighbors. Agents receive the resulting learned graph observations based on their location in the graph. Our approach can be used in a decentralized manner at runtime and in combination with a reinforcement learning algorithm of choice. We evaluate our method across 1000 diverse graphs in the context of routing in communication networks and find that it enables agents to generalize and adapt to changes in the graph.
<div id='section'>Paperid: <span id='pid'>1062, <a href='https://arxiv.org/pdf/2401.17405.pdf' target='_blank'>https://arxiv.org/pdf/2401.17405.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziqing Lu, Guanlin Liu, Lifeng Lai, Weiyu Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.17405">Camouflage Adversarial Attacks on Multiple Agent Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The multi-agent reinforcement learning systems (MARL) based on the Markov decision process (MDP) have emerged in many critical applications. To improve the robustness/defense of MARL systems against adversarial attacks, the study of various adversarial attacks on reinforcement learning systems is very important. Previous works on adversarial attacks considered some possible features to attack in MDP, such as the action poisoning attacks, the reward poisoning attacks, and the state perception attacks. In this paper, we propose a brand-new form of attack called the camouflage attack in the MARL systems. In the camouflage attack, the attackers change the appearances of some objects without changing the actual objects themselves; and the camouflaged appearances may look the same to all the targeted recipient (victim) agents. The camouflaged appearances can mislead the recipient agents to misguided actions. We design algorithms that give the optimal camouflage attacks minimizing the rewards of recipient agents. Our numerical and theoretical results show that camouflage attacks can rival the more conventional, but likely more difficult state perception attacks. We also investigate cost-constrained camouflage attacks and showed numerically how cost budgets affect the attack performance.
<div id='section'>Paperid: <span id='pid'>1063, <a href='https://arxiv.org/pdf/2401.12914.pdf' target='_blank'>https://arxiv.org/pdf/2401.12914.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Salwa Mostafa, Mateus P. Mota, Alvaro Valcarce, Mehdi Bennis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.12914">Emergent Communication Protocol Learning for Task Offloading in Industrial Internet of Things</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we leverage a multi-agent reinforcement learning (MARL) framework to jointly learn a computation offloading decision and multichannel access policy with corresponding signaling. Specifically, the base station and industrial Internet of Things mobile devices are reinforcement learning agents that need to cooperate to execute their computation tasks within a deadline constraint. We adopt an emergent communication protocol learning framework to solve this problem. The numerical results illustrate the effectiveness of emergent communication in improving the channel access success rate and the number of successfully computed tasks compared to contention-based, contention-free, and no-communication approaches. Moreover, the proposed task offloading policy outperforms remote and local computation baselines.
<div id='section'>Paperid: <span id='pid'>1064, <a href='https://arxiv.org/pdf/2401.10949.pdf' target='_blank'>https://arxiv.org/pdf/2401.10949.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ali Baheri, Mykel J. Kochenderfer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.10949">The Synergy Between Optimal Transport Theory and Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper explores the integration of optimal transport (OT) theory with multi-agent reinforcement learning (MARL). This integration uses OT to handle distributions and transportation problems to enhance the efficiency, coordination, and adaptability of MARL. There are five key areas where OT can impact MARL: (1) policy alignment, where OT's Wasserstein metric is used to align divergent agent strategies towards unified goals; (2) distributed resource management, employing OT to optimize resource allocation among agents; (3) addressing non-stationarity, using OT to adapt to dynamic environmental shifts; (4) scalable multi-agent learning, harnessing OT for decomposing large-scale learning objectives into manageable tasks; and (5) enhancing energy efficiency, applying OT principles to develop sustainable MARL systems. This paper articulates how the synergy between OT and MARL can address scalability issues, optimize resource distribution, align agent policies in cooperative environments, and ensure adaptability in dynamically changing conditions.
<div id='section'>Paperid: <span id='pid'>1065, <a href='https://arxiv.org/pdf/2312.16609.pdf' target='_blank'>https://arxiv.org/pdf/2312.16609.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Iosif Sakos, Emmanouil-Vasileios Vlatakis-Gkaragkounis, Panayotis Mertikopoulos, Georgios Piliouras
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.16609">Exploiting hidden structures in non-convex games for convergence to Nash equilibrium</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A wide array of modern machine learning applications - from adversarial models to multi-agent reinforcement learning - can be formulated as non-cooperative games whose Nash equilibria represent the system's desired operational states. Despite having a highly non-convex loss landscape, many cases of interest possess a latent convex structure that could potentially be leveraged to yield convergence to equilibrium. Driven by this observation, our paper proposes a flexible first-order method that successfully exploits such "hidden structures" and achieves convergence under minimal assumptions for the transformation connecting the players' control variables to the game's latent, convex-structured layer. The proposed method - which we call preconditioned hidden gradient descent (PHGD) - hinges on a judiciously chosen gradient preconditioning scheme related to natural gradient methods. Importantly, we make no separability assumptions for the game's hidden structure, and we provide explicit convergence rate guarantees for both deterministic and stochastic environments.
<div id='section'>Paperid: <span id='pid'>1066, <a href='https://arxiv.org/pdf/2312.13232.pdf' target='_blank'>https://arxiv.org/pdf/2312.13232.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Vinzenz Thoma, Michael Curry, Niao He, Sven Seuken
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.13232">Learning Best Response Policies in Dynamic Auctions via Deep Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Many real-world auctions are dynamic processes, in which bidders interact and report information over multiple rounds with the auctioneer. The sequential decision making aspect paired with imperfect information renders analyzing the incentive properties of such auctions much more challenging than in the static case. It is clear that bidders often have incentives for manipulation, but the full scope of such strategies is not well-understood. We aim to develop a tool for better understanding the incentive properties in dynamic auctions by using reinforcement learning to learn the optimal strategic behavior for an auction participant. We frame the decision problem as a Markov Decision Process, show its relation to multi-task reinforcement learning and use a soft actor-critic algorithm with experience relabeling to best-respond against several known analytical equilibria as well as to find profitable deviations against exploitable bidder strategies.
<div id='section'>Paperid: <span id='pid'>1067, <a href='https://arxiv.org/pdf/2312.11545.pdf' target='_blank'>https://arxiv.org/pdf/2312.11545.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lebin Yu, Yunbo Qiu, Quanming Yao, Yuan Shen, Xudong Zhang, Jian Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.11545">Robust Communicative Multi-Agent Reinforcement Learning with Active Defense</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Communication in multi-agent reinforcement learning (MARL) has been proven to effectively promote cooperation among agents recently. Since communication in real-world scenarios is vulnerable to noises and adversarial attacks, it is crucial to develop robust communicative MARL technique. However, existing research in this domain has predominantly focused on passive defense strategies, where agents receive all messages equally, making it hard to balance performance and robustness. We propose an active defense strategy, where agents automatically reduce the impact of potentially harmful messages on the final decision. There are two challenges to implement this strategy, that are defining unreliable messages and adjusting the unreliable messages' impact on the final decision properly. To address them, we design an Active Defense Multi-Agent Communication framework (ADMAC), which estimates the reliability of received messages and adjusts their impact on the final decision accordingly with the help of a decomposable decision structure. The superiority of ADMAC over existing methods is validated by experiments in three communication-critical tasks under four types of attacks.
<div id='section'>Paperid: <span id='pid'>1068, <a href='https://arxiv.org/pdf/2311.15858.pdf' target='_blank'>https://arxiv.org/pdf/2311.15858.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lorenzo Mario Amorosa, Marco Skocaj, Roberto Verdone, Deniz GÃ¼ndÃ¼z
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.15858">Multi-Agent Reinforcement Learning for Power Control in Wireless Networks via Adaptive Graphs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The ever-increasing demand for high-quality and heterogeneous wireless communication services has driven extensive research on dynamic optimization strategies in wireless networks. Among several possible approaches, multi-agent deep reinforcement learning (MADRL) has emerged as a promising method to address a wide range of complex optimization problems like power control. However, the seamless application of MADRL to a variety of network optimization problems faces several challenges related to convergence. In this paper, we present the use of graphs as communication-inducing structures among distributed agents as an effective means to mitigate these challenges. Specifically, we harness graph neural networks (GNNs) as neural architectures for policy parameterization to introduce a relational inductive bias in the collective decision-making process. Most importantly, we focus on modeling the dynamic interactions among sets of neighboring agents through the introduction of innovative methods for defining a graph-induced framework for integrated communication and learning. Finally, the superior generalization capabilities of the proposed methodology to larger networks and to networks with different user categories is verified through simulations.
<div id='section'>Paperid: <span id='pid'>1069, <a href='https://arxiv.org/pdf/2311.06144.pdf' target='_blank'>https://arxiv.org/pdf/2311.06144.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Beomyeol Yu, Taeyoung Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.06144">Multi-Agent Reinforcement Learning for the Low-Level Control of a Quadrotor UAV</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>By leveraging the underlying structures of the quadrotor dynamics, we propose multi-agent reinforcement learning frameworks to innovate the low-level control of a quadrotor, where independent agents operate cooperatively to achieve a common goal. While single-agent reinforcement learning has been successfully applied in quadrotor controls, training a large monolithic network is often data-intensive and time-consuming. Moreover, achieving agile yawing control remains a significant challenge due to the strongly coupled nature of the quadrotor dynamics. To address this, we decompose the quadrotor dynamics into translational and yawing components and assign collaborative reinforcement learning agents to each part to facilitate more efficient training. Additionally, we introduce regularization terms to mitigate steady-state errors and prevent excessive maneuvers. Benchmark studies, including sim-to-sim transfer verification, demonstrate that our proposed training schemes substantially improve the convergence rate of training, while enhancing flight control performance and stability compared to traditional single-agent approaches.
<div id='section'>Paperid: <span id='pid'>1070, <a href='https://arxiv.org/pdf/2310.15414.pdf' target='_blank'>https://arxiv.org/pdf/2310.15414.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bidipta Sarkar, Andy Shih, Dorsa Sadigh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.15414">Diverse Conventions for Human-AI Collaboration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Conventions are crucial for strong performance in cooperative multi-agent games, because they allow players to coordinate on a shared strategy without explicit communication. Unfortunately, standard multi-agent reinforcement learning techniques, such as self-play, converge to conventions that are arbitrary and non-diverse, leading to poor generalization when interacting with new partners. In this work, we present a technique for generating diverse conventions by (1) maximizing their rewards during self-play, while (2) minimizing their rewards when playing with previously discovered conventions (cross-play), stimulating conventions to be semantically different. To ensure that learned policies act in good faith despite the adversarial optimization of cross-play, we introduce \emph{mixed-play}, where an initial state is randomly generated by sampling self-play and cross-play transitions and the player learns to maximize the self-play reward from this initial state. We analyze the benefits of our technique on various multi-agent collaborative games, including Overcooked, and find that our technique can adapt to the conventions of humans, surpassing human-level performance when paired with real users.
<div id='section'>Paperid: <span id='pid'>1071, <a href='https://arxiv.org/pdf/2310.05053.pdf' target='_blank'>https://arxiv.org/pdf/2310.05053.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lang Feng, Dong Xing, Junru Zhang, Gang Pan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.05053">FP3O: Enabling Proximal Policy Optimization in Multi-Agent Cooperation with Parameter-Sharing Versatility</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing multi-agent PPO algorithms lack compatibility with different types of parameter sharing when extending the theoretical guarantee of PPO to cooperative multi-agent reinforcement learning (MARL). In this paper, we propose a novel and versatile multi-agent PPO algorithm for cooperative MARL to overcome this limitation. Our approach is achieved upon the proposed full-pipeline paradigm, which establishes multiple parallel optimization pipelines by employing various equivalent decompositions of the advantage function. This procedure successfully formulates the interconnections among agents in a more general manner, i.e., the interconnections among pipelines, making it compatible with diverse types of parameter sharing. We provide a solid theoretical foundation for policy improvement and subsequently develop a practical algorithm called Full-Pipeline PPO (FP3O) by several approximations. Empirical evaluations on Multi-Agent MuJoCo and StarCraftII tasks demonstrate that FP3O outperforms other strong baselines and exhibits remarkable versatility across various parameter-sharing configurations.
<div id='section'>Paperid: <span id='pid'>1072, <a href='https://arxiv.org/pdf/2309.02462.pdf' target='_blank'>https://arxiv.org/pdf/2309.02462.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pol SuÃ¡rez, Francisco AlcÃ¡ntara-Ãvila, Arnau MirÃ³, Jean Rabault, Bernat Font, Oriol Lehmkuhl, R. Vinuesa
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.02462">Active flow control for three-dimensional cylinders through deep reinforcement learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents for the first time successful results of active flow control with multiple independently controlled zero-net-mass-flux synthetic jets. The jets are placed on a three-dimensional cylinder along its span with the aim of reducing the drag coefficient. The method is based on a deep-reinforcement-learning framework that couples a computational-fluid-dynamics solver with an agent using the proximal-policy-optimization algorithm. We implement a multi-agent reinforcement-learning framework which offers numerous advantages: it exploits local invariants, makes the control adaptable to different geometries, facilitates transfer learning and cross-application of agents and results in significant training speedup. In this contribution we report significant drag reduction after applying the DRL-based control in three different configurations of the problem.
<div id='section'>Paperid: <span id='pid'>1073, <a href='https://arxiv.org/pdf/2307.07670.pdf' target='_blank'>https://arxiv.org/pdf/2307.07670.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Guanlin Liu, Lifeng Lai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.07670">Efficient Adversarial Attacks on Online Multi-agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Due to the broad range of applications of multi-agent reinforcement learning (MARL), understanding the effects of adversarial attacks against MARL model is essential for the safe applications of this model. Motivated by this, we investigate the impact of adversarial attacks on MARL. In the considered setup, there is an exogenous attacker who is able to modify the rewards before the agents receive them or manipulate the actions before the environment receives them. The attacker aims to guide each agent into a target policy or maximize the cumulative rewards under some specific reward function chosen by the attacker, while minimizing the amount of manipulation on feedback and action. We first show the limitations of the action poisoning only attacks and the reward poisoning only attacks. We then introduce a mixed attack strategy with both the action poisoning and the reward poisoning. We show that the mixed attack strategy can efficiently attack MARL agents even if the attacker has no prior information about the underlying environment and the agents' algorithms.
<div id='section'>Paperid: <span id='pid'>1074, <a href='https://arxiv.org/pdf/2306.07749.pdf' target='_blank'>https://arxiv.org/pdf/2306.07749.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pragnya Alatur, Giorgia Ramponi, Niao He, Andreas Krause
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.07749">Provably Learning Nash Policies in Constrained Markov Potential Games</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent reinforcement learning (MARL) addresses sequential decision-making problems with multiple agents, where each agent optimizes its own objective. In many real-world instances, the agents may not only want to optimize their objectives, but also ensure safe behavior. For example, in traffic routing, each car (agent) aims to reach its destination quickly (objective) while avoiding collisions (safety). Constrained Markov Games (CMGs) are a natural formalism for safe MARL problems, though generally intractable. In this work, we introduce and study Constrained Markov Potential Games (CMPGs), an important class of CMGs. We first show that a Nash policy for CMPGs can be found via constrained optimization. One tempting approach is to solve it by Lagrangian-based primal-dual methods. As we show, in contrast to the single-agent setting, however, CMPGs do not satisfy strong duality, rendering such approaches inapplicable and potentially unsafe. To solve the CMPG problem, we propose our algorithm Coordinate-Ascent for CMPGs (CA-CMPG), which provably converges to a Nash policy in tabular, finite-horizon CMPGs. Furthermore, we provide the first sample complexity bounds for learning Nash policies in unknown CMPGs, and, which under additional assumptions, guarantee safe exploration.
<div id='section'>Paperid: <span id='pid'>1075, <a href='https://arxiv.org/pdf/2306.01920.pdf' target='_blank'>https://arxiv.org/pdf/2306.01920.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dingyang Chen, Qi Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.01920">Context-Aware Bayesian Network Actor-Critic Methods for Cooperative Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Executing actions in a correlated manner is a common strategy for human coordination that often leads to better cooperation, which is also potentially beneficial for cooperative multi-agent reinforcement learning (MARL). However, the recent success of MARL relies heavily on the convenient paradigm of purely decentralized execution, where there is no action correlation among agents for scalability considerations. In this work, we introduce a Bayesian network to inaugurate correlations between agents' action selections in their joint policy. Theoretically, we establish a theoretical justification for why action dependencies are beneficial by deriving the multi-agent policy gradient formula under such a Bayesian network joint policy and proving its global convergence to Nash equilibria under tabular softmax policy parameterization in cooperative Markov games. Further, by equipping existing MARL algorithms with a recent method of differentiable directed acyclic graphs (DAGs), we develop practical algorithms to learn the context-aware Bayesian network policies in scenarios with partial observability and various difficulty. We also dynamically decrease the sparsity of the learned DAG throughout the training process, which leads to weakly or even purely independent policies for decentralized execution. Empirical results on a range of MARL benchmarks show the benefits of our approach.
<div id='section'>Paperid: <span id='pid'>1076, <a href='https://arxiv.org/pdf/2305.13206.pdf' target='_blank'>https://arxiv.org/pdf/2305.13206.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jannis Weil, Johannes Czech, Tobias Meuser, Kristian Kersting
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.13206">Know your Enemy: Investigating Monte-Carlo Tree Search with Opponent Models in Pommerman</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In combination with Reinforcement Learning, Monte-Carlo Tree Search has shown to outperform human grandmasters in games such as Chess, Shogi and Go with little to no prior domain knowledge. However, most classical use cases only feature up to two players. Scaling the search to an arbitrary number of players presents a computational challenge, especially if decisions have to be planned over a longer time horizon. In this work, we investigate techniques that transform general-sum multiplayer games into single-player and two-player games that consider other agents to act according to given opponent models. For our evaluation, we focus on the challenging Pommerman environment which involves partial observability, a long time horizon and sparse rewards. In combination with our search methods, we investigate the phenomena of opponent modeling using heuristics and self-play. Overall, we demonstrate the effectiveness of our multiplayer search variants both in a supervised learning and reinforcement learning setting.
<div id='section'>Paperid: <span id='pid'>1077, <a href='https://arxiv.org/pdf/2305.05566.pdf' target='_blank'>https://arxiv.org/pdf/2305.05566.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Adam Michalski, Filippos Christianos, Stefano V. Albrecht
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.05566">SMAClite: A Lightweight Environment for Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>There is a lack of standard benchmarks for Multi-Agent Reinforcement Learning (MARL) algorithms. The Starcraft Multi-Agent Challenge (SMAC) has been widely used in MARL research, but is built on top of a heavy, closed-source computer game, StarCraft II. Thus, SMAC is computationally expensive and requires knowledge and the use of proprietary tools specific to the game for any meaningful alteration or contribution to the environment. We introduce SMAClite -- a challenge based on SMAC that is both decoupled from Starcraft II and open-source, along with a framework which makes it possible to create new content for SMAClite without any special knowledge. We conduct experiments to show that SMAClite is equivalent to SMAC, by training MARL algorithms on SMAClite and reproducing SMAC results. We then show that SMAClite outperforms SMAC in both runtime speed and memory.
<div id='section'>Paperid: <span id='pid'>1078, <a href='https://arxiv.org/pdf/2305.02058.pdf' target='_blank'>https://arxiv.org/pdf/2305.02058.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kairui Guo, Adrian Cheng, Yaqi Li, Jun Li, Rob Duffield, Steven W. Su
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.02058">Human Machine Co-adaption Interface via Cooperation Markov Decision Process System</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper aims to develop a new human-machine interface to improve rehabilitation performance from the perspective of both the user (patient) and the machine (robot) by introducing the co-adaption techniques via model-based reinforcement learning. Previous studies focus more on robot assistance, i.e., to improve the control strategy so as to fulfill the objective of Assist-As-Needed. In this study, we treat the full process of robot-assisted rehabilitation as a co-adaptive or mutual learning process and emphasize the adaptation of the user to the machine. To this end, we proposed a Co-adaptive MDPs (CaMDPs) model to quantify the learning rates based on cooperative multi-agent reinforcement learning (MARL) in the high abstraction layer of the systems. We proposed several approaches to cooperatively adjust the Policy Improvement among the two agents in the framework of Policy Iteration. Based on the proposed co-adaptive MDPs, the simulation study indicates the non-stationary problem can be mitigated using various proposed Policy Improvement approaches.
<div id='section'>Paperid: <span id='pid'>1079, <a href='https://arxiv.org/pdf/2303.14061.pdf' target='_blank'>https://arxiv.org/pdf/2303.14061.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Leo Ardon, Daniel Furelos-Blanco, Alessandra Russo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.14061">Learning Reward Machines in Cooperative Multi-Agent Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a novel approach to Multi-Agent Reinforcement Learning (MARL) that combines cooperative task decomposition with the learning of reward machines (RMs) encoding the structure of the sub-tasks. The proposed method helps deal with the non-Markovian nature of the rewards in partially observable environments and improves the interpretability of the learnt policies required to complete the cooperative task. The RMs associated with each sub-task are learnt in a decentralised manner and then used to guide the behaviour of each agent. By doing so, the complexity of a cooperative multi-agent problem is reduced, allowing for more effective learning. The results suggest that our approach is a promising direction for future research in MARL, especially in complex environments with large state spaces and multiple agents.
<div id='section'>Paperid: <span id='pid'>1080, <a href='https://arxiv.org/pdf/2303.12287.pdf' target='_blank'>https://arxiv.org/pdf/2303.12287.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dylan J. Foster, Noah Golowich, Sham M. Kakade
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.12287">Hardness of Independent Learning and Sparse Equilibrium Computation in Markov Games</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We consider the problem of decentralized multi-agent reinforcement learning in Markov games. A fundamental question is whether there exist algorithms that, when adopted by all agents and run independently in a decentralized fashion, lead to no-regret for each player, analogous to celebrated convergence results in normal-form games. While recent work has shown that such algorithms exist for restricted settings (notably, when regret is defined with respect to deviations to Markovian policies), the question of whether independent no-regret learning can be achieved in the standard Markov game framework was open. We provide a decisive negative resolution this problem, both from a computational and statistical perspective. We show that:
  - Under the widely-believed assumption that PPAD-hard problems cannot be solved in polynomial time, there is no polynomial-time algorithm that attains no-regret in general-sum Markov games when executed independently by all players, even when the game is known to the algorithm designer and the number of players is a small constant.
  - When the game is unknown, no algorithm, regardless of computational efficiency, can achieve no-regret without observing a number of episodes that is exponential in the number of players.
  Perhaps surprisingly, our lower bounds hold even for seemingly easier setting in which all agents are controlled by a a centralized algorithm. They are proven via lower bounds for a simpler problem we refer to as SparseCCE, in which the goal is to compute a coarse correlated equilibrium that is sparse in the sense that it can be represented as a mixture of a small number of product policies. The crux of our approach is a novel application of aggregation techniques from online learning, whereby we show that any algorithm for the SparseCCE problem can be used to compute approximate Nash equilibria for non-zero sum normal-form games.
<div id='section'>Paperid: <span id='pid'>1081, <a href='https://arxiv.org/pdf/2302.11793.pdf' target='_blank'>https://arxiv.org/pdf/2302.11793.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Callum Rhys Tilbury, Filippos Christianos, Stefano V. Albrecht
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.11793">Revisiting the Gumbel-Softmax in MADDPG</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>MADDPG is an algorithm in multi-agent reinforcement learning (MARL) that extends the popular single-agent method, DDPG, to multi-agent scenarios. Importantly, DDPG is an algorithm designed for continuous action spaces, where the gradient of the state-action value function exists. For this algorithm to work in discrete action spaces, discrete gradient estimation must be performed. For MADDPG, the Gumbel-Softmax (GS) estimator is used -- a reparameterisation which relaxes a discrete distribution into a similar continuous one. This method, however, is statistically biased, and a recent MARL benchmarking paper suggests that this bias makes MADDPG perform poorly in grid-world situations, where the action space is discrete. Fortunately, many alternatives to the GS exist, boasting a wide range of properties. This paper explores several of these alternatives and integrates them into MADDPG for discrete grid-world scenarios. The corresponding impact on various performance metrics is then measured and analysed. It is found that one of the proposed estimators performs significantly better than the original GS in several tasks, achieving up to 55% higher returns, along with faster convergence.
<div id='section'>Paperid: <span id='pid'>1082, <a href='https://arxiv.org/pdf/2302.09277.pdf' target='_blank'>https://arxiv.org/pdf/2302.09277.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yunbo Qiu, Yue Jin, Lebin Yu, Jian Wang, Xudong Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.09277">Promoting Cooperation in Multi-Agent Reinforcement Learning via Mutual Help</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent reinforcement learning (MARL) has achieved great progress in cooperative tasks in recent years. However, in the local reward scheme, where only local rewards for each agent are given without global rewards shared by all the agents, traditional MARL algorithms lack sufficient consideration of agents' mutual influence. In cooperative tasks, agents' mutual influence is especially important since agents are supposed to coordinate to achieve better performance. In this paper, we propose a novel algorithm Mutual-Help-based MARL (MH-MARL) to instruct agents to help each other in order to promote cooperation. MH-MARL utilizes an expected action module to generate expected other agents' actions for each particular agent. Then, the expected actions are delivered to other agents for selective imitation during training. Experimental results show that MH-MARL improves the performance of MARL both in success rate and cumulative reward.
<div id='section'>Paperid: <span id='pid'>1083, <a href='https://arxiv.org/pdf/2302.05063.pdf' target='_blank'>https://arxiv.org/pdf/2302.05063.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lebin Yu, Yunbo Qiu, Quanming Yao, Xudong Zhang, Jian Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.05063">Improving Zero-Shot Coordination Performance Based on Policy Similarity</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Over these years, multi-agent reinforcement learning has achieved remarkable performance in multi-agent planning and scheduling tasks. It typically follows the self-play setting, where agents are trained by playing with a fixed group of agents. However, in the face of zero-shot coordination, where an agent must coordinate with unseen partners, self-play agents may fail. Several methods have been proposed to handle this problem, but they either take a lot of time or lack generalizability. In this paper, we firstly reveal an important phenomenon: the zero-shot coordination performance is strongly linearly correlated with the similarity between an agent's training partner and testing partner. Inspired by it, we put forward a Similarity-Based Robust Training (SBRT) scheme that improves agents' zero-shot coordination performance by disturbing their partners' actions during training according to a pre-defined policy similarity value. To validate its effectiveness, we apply our scheme to three multi-agent reinforcement learning frameworks and achieve better performance compared with previous methods.
<div id='section'>Paperid: <span id='pid'>1084, <a href='https://arxiv.org/pdf/2302.05055.pdf' target='_blank'>https://arxiv.org/pdf/2302.05055.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lebin Yu, Yunbo Qiu, Qiexiang Wang, Xudong Zhang, Jian Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.05055">Low Entropy Communication in Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Communication in multi-agent reinforcement learning has been drawing attention recently for its significant role in cooperation. However, multi-agent systems may suffer from limitations on communication resources and thus need efficient communication techniques in real-world scenarios. According to the Shannon-Hartley theorem, messages to be transmitted reliably in worse channels require lower entropy. Therefore, we aim to reduce message entropy in multi-agent communication. A fundamental challenge is that the gradients of entropy are either 0 or infinity, disabling gradient-based methods. To handle it, we propose a pseudo gradient descent scheme, which reduces entropy by adjusting the distributions of messages wisely. We conduct experiments on two base communication frameworks with six environment settings and find that our scheme can reduce message entropy by up to 90% with nearly no loss of cooperation performance.
<div id='section'>Paperid: <span id='pid'>1085, <a href='https://arxiv.org/pdf/2302.04944.pdf' target='_blank'>https://arxiv.org/pdf/2302.04944.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Elliot Fosong, Arrasy Rahman, Ignacio Carlucho, Stefano V. Albrecht
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.04944">Learning Complex Teamwork Tasks Using a Given Sub-task Decomposition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Training a team to complete a complex task via multi-agent reinforcement learning can be difficult due to challenges such as policy search in a large joint policy space, and non-stationarity caused by mutually adapting agents. To facilitate efficient learning of complex multi-agent tasks, we propose an approach which uses an expert-provided decomposition of a task into simpler multi-agent sub-tasks. In each sub-task, a subset of the entire team is trained to acquire sub-task-specific policies. The sub-teams are then merged and transferred to the target task, where their policies are collectively fine-tuned to solve the more complex target task. We show empirically that such approaches can greatly reduce the number of timesteps required to solve a complex target task relative to training from-scratch. However, we also identify and investigate two problems with naive implementations of approaches based on sub-task decomposition, and propose a simple and scalable method to address these problems which augments existing actor-critic algorithms. We demonstrate the empirical benefits of our proposed method, enabling sub-task decomposition approaches to be deployed in diverse multi-agent tasks.
<div id='section'>Paperid: <span id='pid'>1086, <a href='https://arxiv.org/pdf/2302.01586.pdf' target='_blank'>https://arxiv.org/pdf/2302.01586.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xu Shen, Francesco Borrelli
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.01586">Reinforcement Learning and Distributed Model Predictive Control for Conflict Resolution in Highly Constrained Spaces</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work presents a distributed algorithm for resolving cooperative multi-vehicle conflicts in highly constrained spaces. By formulating the conflict resolution problem as a Multi-Agent Reinforcement Learning (RL) problem, we can train a policy offline to drive the vehicles towards their destinations safely and efficiently in a simplified discrete environment. During the online execution, each vehicle first simulates the interaction among vehicles with the trained policy to obtain its strategy, which is used to guide the computation of a reference trajectory. A distributed Model Predictive Controller (MPC) is then proposed to track the reference while avoiding collisions. The preliminary results show that the combination of RL and distributed MPC has the potential to guide vehicles to resolve conflicts safely and smoothly while being less computationally demanding than the centralized approach.
<div id='section'>Paperid: <span id='pid'>1087, <a href='https://arxiv.org/pdf/2301.04268.pdf' target='_blank'>https://arxiv.org/pdf/2301.04268.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Quan Nguyen, Nishant A. Mehta
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.04268">Adversarial Online Multi-Task Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We consider the adversarial online multi-task reinforcement learning setting, where in each of $K$ episodes the learner is given an unknown task taken from a finite set of $M$ unknown finite-horizon MDP models. The learner's objective is to minimize its regret with respect to the optimal policy for each task. We assume the MDPs in $\mathcal{M}$ are well-separated under a notion of $Î»$-separability, and show that this notion generalizes many task-separability notions from previous works. We prove a minimax lower bound of $Î©(K\sqrt{DSAH})$ on the regret of any learning algorithm and an instance-specific lower bound of $Î©(\frac{K}{Î»^2})$ in sample complexity for a class of uniformly-good cluster-then-learn algorithms. We use a novel construction called 2-JAO MDP for proving the instance-specific lower bound. The lower bounds are complemented with a polynomial time algorithm that obtains $\tilde{O}(\frac{K}{Î»^2})$ sample complexity guarantee for the clustering phase and $\tilde{O}(\sqrt{MK})$ regret guarantee for the learning phase, indicating that the dependency on $K$ and $\frac{1}{Î»^2}$ is tight.
<div id='section'>Paperid: <span id='pid'>1088, <a href='https://arxiv.org/pdf/2301.00433.pdf' target='_blank'>https://arxiv.org/pdf/2301.00433.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenjing Zhang, Yining Wang, Mingzhe Chen, Tao Luo, Dusit Niyato
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.00433">Optimization of Image Transmission in a Cooperative Semantic Communication Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, a semantic communication framework for image transmission is developed. In the investigated framework, a set of servers cooperatively transmit images to a set of users utilizing semantic communication techniques. To evaluate the performance of studied semantic communication system, a multimodal metric is proposed to measure the correlation between the extracted semantic information and the original image. To meet the ISS requirement of each user, each server must jointly determine the semantic information to be transmitted and the resource blocks (RBs) used for semantic information transmission. We formulate this problem as an optimization problem aiming to minimize each server's transmission latency while reaching the ISS requirement. To solve this problem, a value decomposition based entropy-maximized multi-agent reinforcement learning (RL) is proposed, which enables servers to coordinate for training and execute RB allocation in a distributed manner to approach to a globally optimal performance with less training iterations. Compared to traditional multi-agent RL, the proposed RL improves the valuable action exploration of servers and the probability of finding a globally optimal RB allocation policy based on local observation. Simulation results show that the proposed algorithm can reduce the transmission delay by up to 16.1% compared to traditional multi-agent RL.
<div id='section'>Paperid: <span id='pid'>1089, <a href='https://arxiv.org/pdf/2209.14430.pdf' target='_blank'>https://arxiv.org/pdf/2209.14430.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jikai Jin, Yiping Lu, Jose Blanchet, Lexing Ying
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2209.14430">Minimax Optimal Kernel Operator Learning via Multilevel Training</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Learning mappings between infinite-dimensional function spaces has achieved empirical success in many disciplines of machine learning, including generative modeling, functional data analysis, causal inference, and multi-agent reinforcement learning. In this paper, we study the statistical limit of learning a Hilbert-Schmidt operator between two infinite-dimensional Sobolev reproducing kernel Hilbert spaces. We establish the information-theoretic lower bound in terms of the Sobolev Hilbert-Schmidt norm and show that a regularization that learns the spectral components below the bias contour and ignores the ones that are above the variance contour can achieve the optimal learning rate. At the same time, the spectral components between the bias and variance contours give us flexibility in designing computationally feasible machine learning algorithms. Based on this observation, we develop a multilevel kernel operator learning algorithm that is optimal when learning linear operators between infinite-dimensional function spaces.
<div id='section'>Paperid: <span id='pid'>1090, <a href='https://arxiv.org/pdf/2209.14344.pdf' target='_blank'>https://arxiv.org/pdf/2209.14344.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Filippos Christianos, Georgios Papoudakis, Stefano V. Albrecht
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2209.14344">Pareto Actor-Critic for Equilibrium Selection in Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work focuses on equilibrium selection in no-conflict multi-agent games, where we specifically study the problem of selecting a Pareto-optimal Nash equilibrium among several existing equilibria. It has been shown that many state-of-the-art multi-agent reinforcement learning (MARL) algorithms are prone to converging to Pareto-dominated equilibria due to the uncertainty each agent has about the policy of the other agents during training. To address sub-optimal equilibrium selection, we propose Pareto Actor-Critic (Pareto-AC), which is an actor-critic algorithm that utilises a simple property of no-conflict games (a superset of cooperative games): the Pareto-optimal equilibrium in a no-conflict game maximises the returns of all agents and, therefore, is the preferred outcome for all agents. We evaluate Pareto-AC in a diverse set of multi-agent games and show that it converges to higher episodic returns compared to seven state-of-the-art MARL algorithms and that it successfully converges to a Pareto-optimal equilibrium in a range of matrix games. Finally, we propose PACDCG, a graph neural network extension of Pareto-AC, which is shown to efficiently scale in games with a large number of agents.
<div id='section'>Paperid: <span id='pid'>1091, <a href='https://arxiv.org/pdf/2206.06758.pdf' target='_blank'>https://arxiv.org/pdf/2206.06758.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Matthew Morris, Thomas D. Barrett, Arnu Pretorius
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2206.06758">Universally Expressive Communication in Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Allowing agents to share information through communication is crucial for solving complex tasks in multi-agent reinforcement learning. In this work, we consider the question of whether a given communication protocol can express an arbitrary policy. By observing that many existing protocols can be viewed as instances of graph neural networks (GNNs), we demonstrate the equivalence of joint action selection to node labelling. With standard GNN approaches provably limited in their expressive capacity, we draw from existing GNN literature and consider augmenting agent observations with: (1) unique agent IDs and (2) random noise. We provide a theoretical analysis as to how these approaches yield universally expressive communication, and also prove them capable of targeting arbitrary sets of actions for identical agents. Empirically, these augmentations are found to improve performance on tasks where expressive communication is required, whilst, in general, the optimal communication protocol is found to be task-dependent.
<div id='section'>Paperid: <span id='pid'>1092, <a href='https://arxiv.org/pdf/2204.08594.pdf' target='_blank'>https://arxiv.org/pdf/2204.08594.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuangyao Huang, Haibo Zhang, Zhiyi Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2204.08594">CoDe: A Cooperative and Decentralized Collision Avoidance Algorithm for Small-Scale UAV Swarms Considering Energy Efficiency</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces a cooperative and decentralized collision avoidance algorithm (CoDe) for small-scale UAV swarms consisting of up to three UAVs. CoDe improves energy efficiency of UAVs by achieving effective cooperation among UAVs. Moreover, CoDe is specifically tailored for UAV's operations by addressing the challenges faced by existing schemes, such as ineffectiveness in selecting actions from continuous action spaces and high computational complexity. CoDe is based on Multi-Agent Reinforcement Learning (MARL), and finds cooperative policies by incorporating a novel credit assignment scheme. The novel credit assignment scheme estimates the contribution of an individual by subtracting a baseline from the joint action value for the swarm. The credit assignment scheme in CoDe outperforms other benchmarks as the baseline takes into account not only the importance of a UAV's action but also the interrelation between UAVs. Furthermore, extensive experiments are conducted against existing MARL-based and conventional heuristic-based algorithms to demonstrate the advantages of the proposed algorithm.
<div id='section'>Paperid: <span id='pid'>1093, <a href='https://arxiv.org/pdf/2204.02013.pdf' target='_blank'>https://arxiv.org/pdf/2204.02013.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>S. VenkataKeerthy, Siddharth Jain, Anilava Kundu, Rohit Aggarwal, Albert Cohen, Ramakrishna Upadrasta
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2204.02013">RL4ReAl: Reinforcement Learning for Register Allocation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We aim to automate decades of research and experience in register allocation, leveraging machine learning. We tackle this problem by embedding a multi-agent reinforcement learning algorithm within LLVM, training it with the state of the art techniques. We formalize the constraints that precisely define the problem for a given instruction-set architecture, while ensuring that the generated code preserves semantic correctness. We also develop a gRPC based framework providing a modular and efficient compiler interface for training and inference. Our approach is architecture independent: we show experimental results targeting Intel x86 and ARM AArch64. Our results match or out-perform the heavily tuned, production-grade register allocators of LLVM.
<div id='section'>Paperid: <span id='pid'>1094, <a href='https://arxiv.org/pdf/2111.11229.pdf' target='_blank'>https://arxiv.org/pdf/2111.11229.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>MichaÅ Zawalski, BÅaÅ¼ej OsiÅski, Henryk Michalewski, Piotr MiÅoÅ
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2111.11229">Off-Policy Correction For Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent reinforcement learning (MARL) provides a framework for problems involving multiple interacting agents. Despite apparent similarity to the single-agent case, multi-agent problems are often harder to train and analyze theoretically. In this work, we propose MA-Trace, a new on-policy actor-critic algorithm, which extends V-Trace to the MARL setting. The key advantage of our algorithm is its high scalability in a multi-worker setting. To this end, MA-Trace utilizes importance sampling as an off-policy correction method, which allows distributing the computations with no impact on the quality of training. Furthermore, our algorithm is theoretically grounded - we prove a fixed-point theorem that guarantees convergence. We evaluate the algorithm extensively on the StarCraft Multi-Agent Challenge, a standard benchmark for multi-agent algorithms. MA-Trace achieves high performance on all its tasks and exceeds state-of-the-art results on some of them.
<div id='section'>Paperid: <span id='pid'>1095, <a href='https://arxiv.org/pdf/2110.02784.pdf' target='_blank'>https://arxiv.org/pdf/2110.02784.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhaoming Qin, Nanqing Dong, Di Liu, Zhefan Wang, Junwei Cao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2110.02784">Scalable Multi-Agent Reinforcement Learning for Residential Load Scheduling under Data Governance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As a data-driven approach, multi-agent reinforcement learning (MARL) has made remarkable advances in solving cooperative residential load scheduling problems. However, centralized training, the most common paradigm for MARL, limits large-scale deployment in communication-constrained cloud-edge environments. As a remedy, distributed training shows unparalleled advantages in real-world applications but still faces challenge with system scalability, e.g., the high cost of communication overhead during coordinating individual agents, and needs to comply with data governance in terms of privacy. In this work, we propose a novel MARL solution to address these two practical issues. Our proposed approach is based on actor-critic methods, where the global critic is a learned function of individual critics computed solely based on local observations of households. This scheme preserves household privacy completely and significantly reduces communication cost. Simulation experiments demonstrate that the proposed framework achieves comparable performance to the state-of-the-art actor-critic framework without data governance and communication constraints.
<div id='section'>Paperid: <span id='pid'>1096, <a href='https://arxiv.org/pdf/2510.08607.pdf' target='_blank'>https://arxiv.org/pdf/2510.08607.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhaoqilin Yang, Chanchan Li, Tianqi Liu, Hongxin Zhao, Youliang Tian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.08607">GRPO-GCC: Enhancing Cooperation in Spatial Public Goods Games via Group Relative Policy Optimization with Global Cooperation Constraint</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Inspired by the principle of self-regulating cooperation in collective institutions, we propose the Group Relative Policy Optimization with Global Cooperation Constraint (GRPO-GCC) framework. This work is the first to introduce GRPO into spatial public goods games, establishing a new deep reinforcement learning baseline for structured populations. GRPO-GCC integrates group relative policy optimization with a global cooperation constraint that strengthens incentives at intermediate cooperation levels while weakening them at extremes. This mechanism aligns local decision making with sustainable collective outcomes and prevents collapse into either universal defection or unconditional cooperation. The framework advances beyond existing approaches by combining group-normalized advantage estimation, a reference-anchored KL penalty, and a global incentive term that dynamically adjusts cooperative payoffs. As a result, it achieves accelerated cooperation onset, stabilized policy adaptation, and long-term sustainability. GRPO-GCC demonstrates how a simple yet global signal can reshape incentives toward resilient cooperation, and provides a new paradigm for multi-agent reinforcement learning in socio-technical systems.
<div id='section'>Paperid: <span id='pid'>1097, <a href='https://arxiv.org/pdf/2510.07888.pdf' target='_blank'>https://arxiv.org/pdf/2510.07888.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinren Zhang, Sixi Cheng, Zixin Zhong, Jiadong Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.07888">Network Topology and Information Efficiency of Multi-Agent Systems: Study based on MARL</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent systems (MAS) solve complex problems through coordinated autonomous entities with individual decision-making capabilities. While Multi-Agent Reinforcement Learning (MARL) enables these agents to learn intelligent strategies, it faces challenges of non-stationarity and partial observability. Communications among agents offer a solution, but questions remain about its optimal structure and evaluation. This paper explores two underexamined aspects: communication topology and information efficiency. We demonstrate that directed and sequential topologies improve performance while reducing communication overhead across both homogeneous and heterogeneous tasks. Additionally, we introduce two metrics -- Information Entropy Efficiency Index (IEI) and Specialization Efficiency Index (SEI) -- to evaluate message compactness and role differentiation. Incorporating these metrics into training objectives improves success rates and convergence speed. Our findings highlight that designing adaptive communication topologies with information-efficient messaging is essential for effective coordination in complex MAS.
<div id='section'>Paperid: <span id='pid'>1098, <a href='https://arxiv.org/pdf/2510.07888.pdf' target='_blank'>https://arxiv.org/pdf/2510.07888.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinren Zhang, Sixi Cheng, Zixin Zhong, Jiadong Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.07888">Network Topology and Information Efficiency of Multi-Agent Systems: Study based on MARL</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent systems (MAS) solve complex problems through coordinated autonomous entities with individual decision-making capabilities. While Multi-Agent Reinforcement Learning (MARL) enables these agents to learn intelligent strategies, it faces challenges of non-stationarity and partial observability. Communications among agents offer a solution, but questions remain about its optimal structure and evaluation. This paper explores two underexamined aspects: communication topology and information efficiency. We demonstrate that directed and sequential topologies improve performance while reducing communication overhead across both homogeneous and heterogeneous tasks. Additionally, we introduce two metrics -- Information Entropy Efficiency Index (IEI) and Specialization Efficiency Index (SEI) -- to evaluate message compactness and role differentiation. Incorporating these metrics into training objectives improves success rates and convergence speed. Our findings highlight that designing adaptive communication topologies with information-efficient messaging is essential for effective coordination in complex MAS.
<div id='section'>Paperid: <span id='pid'>1099, <a href='https://arxiv.org/pdf/2509.23905.pdf' target='_blank'>https://arxiv.org/pdf/2509.23905.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianjiao Sun, Ningyan Guo, Haozhe Gu, Yanyan Peng, Zhiyong Feng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.23905">Integrated Communication and Control for Energy-Efficient UAV Swarms: A Multi-Agent Reinforcement Learning Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The deployment of unmanned aerial vehicle (UAV) swarm-assisted communication networks has become an increasingly vital approach for remediating coverage limitations in infrastructure-deficient environments, with especially pressing applications in temporary scenarios, such as emergency rescue, military and security operations, and remote area coverage. However, complex geographic environments lead to unpredictable and highly dynamic wireless channel conditions, resulting in frequent interruptions of air-to-ground (A2G) links that severely constrain the reliability and quality of service in UAV swarm-assisted mobile communications. To improve the quality of UAV swarm-assisted communications in complex geographic environments, we propose an integrated communication and control co-design mechanism. Given the stringent energy constraints inherent in UAV swarms, our proposed mechanism is designed to optimize energy efficiency while maintaining an equilibrium between equitable communication rates for mobile ground users (GUs) and UAV energy expenditure. We formulate the joint resource allocation and 3D trajectory control problem as a Markov decision process (MDP), and develop a multi-agent reinforcement learning (MARL) framework to enable real-time coordinated actions across the UAV swarm. To optimize the action policy of UAV swarms, we propose a novel multi-agent hybrid proximal policy optimization with action masking (MAHPPO-AM) algorithm, specifically designed to handle complex hybrid action spaces. The algorithm incorporates action masking to enforce hard constraints in high-dimensional action spaces. Experimental results demonstrate that our approach achieves a fairness index of 0.99 while reducing energy consumption by up to 25% compared to baseline methods.
<div id='section'>Paperid: <span id='pid'>1100, <a href='https://arxiv.org/pdf/2509.23905.pdf' target='_blank'>https://arxiv.org/pdf/2509.23905.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianjiao Sun, Ningyan Guo, Haozhe Gu, Yanyan Peng, Zhiyong Feng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.23905">Integrated Communication and Control for Energy-Efficient UAV Swarms: A Multi-Agent Reinforcement Learning Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The deployment of unmanned aerial vehicle (UAV) swarm-assisted communication networks has become an increasingly vital approach for remediating coverage limitations in infrastructure-deficient environments, with especially pressing applications in temporary scenarios, such as emergency rescue, military and security operations, and remote area coverage. However, complex geographic environments lead to unpredictable and highly dynamic wireless channel conditions, resulting in frequent interruptions of air-to-ground (A2G) links that severely constrain the reliability and quality of service in UAV swarm-assisted mobile communications. To improve the quality of UAV swarm-assisted communications in complex geographic environments, we propose an integrated communication and control co-design mechanism. Given the stringent energy constraints inherent in UAV swarms, our proposed mechanism is designed to optimize energy efficiency while maintaining an equilibrium between equitable communication rates for mobile ground users (GUs) and UAV energy expenditure. We formulate the joint resource allocation and 3D trajectory control problem as a Markov decision process (MDP), and develop a multi-agent reinforcement learning (MARL) framework to enable real-time coordinated actions across the UAV swarm. To optimize the action policy of UAV swarms, we propose a novel multi-agent hybrid proximal policy optimization with action masking (MAHPPO-AM) algorithm, specifically designed to handle complex hybrid action spaces. The algorithm incorporates action masking to enforce hard constraints in high-dimensional action spaces. Experimental results demonstrate that our approach achieves a fairness index of 0.99 while reducing energy consumption by up to 25% compared to baseline methods.
<div id='section'>Paperid: <span id='pid'>1101, <a href='https://arxiv.org/pdf/2509.23157.pdf' target='_blank'>https://arxiv.org/pdf/2509.23157.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yanqing Fu, Chao Huang, Chenrun Wang, Zhuping Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.23157">Grouped Satisficing Paths in Pure Strategy Games: a Topological Perspective</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In game theory and multi-agent reinforcement learning (MARL), each agent selects a strategy, interacts with the environment and other agents, and subsequently updates its strategy based on the received payoff. This process generates a sequence of joint strategies $(s^t)_{t \geq 0}$, where $s^t$ represents the strategy profile of all agents at time step $t$. A widely adopted principle in MARL algorithms is "win-stay, lose-shift", which dictates that an agent retains its current strategy if it achieves the best response. This principle exhibits a fixed-point property when the joint strategy has become an equilibrium. The sequence of joint strategies under this principle is referred to as a satisficing path, a concept first introduced in [40] and explored in the context of $N$-player games in [39]. A fundamental question arises regarding this principle: Under what conditions does every initial joint strategy $s$ admit a finite-length satisficing path $(s^t)_{0 \leq t \leq T}$ where $s^0=s$ and $s^T$ is an equilibrium? This paper establishes a sufficient condition for such a property, and demonstrates that any finite-state Markov game, as well as any $N$-player game, guarantees the existence of a finite-length satisficing path from an arbitrary initial strategy to some equilibrium. These results provide a stronger theoretical foundation for the design of MARL algorithms.
<div id='section'>Paperid: <span id='pid'>1102, <a href='https://arxiv.org/pdf/2509.23157.pdf' target='_blank'>https://arxiv.org/pdf/2509.23157.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yanqing Fu, Chao Huang, Chenrun Wang, Zhuping Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.23157">Grouped Satisficing Paths in Pure Strategy Games: a Topological Perspective</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In game theory and multi-agent reinforcement learning (MARL), each agent selects a strategy, interacts with the environment and other agents, and subsequently updates its strategy based on the received payoff. This process generates a sequence of joint strategies $(s^t)_{t \geq 0}$, where $s^t$ represents the strategy profile of all agents at time step $t$. A widely adopted principle in MARL algorithms is "win-stay, lose-shift", which dictates that an agent retains its current strategy if it achieves the best response. This principle exhibits a fixed-point property when the joint strategy has become an equilibrium. The sequence of joint strategies under this principle is referred to as a satisficing path, a concept first introduced in [40] and explored in the context of $N$-player games in [39]. A fundamental question arises regarding this principle: Under what conditions does every initial joint strategy $s$ admit a finite-length satisficing path $(s^t)_{0 \leq t \leq T}$ where $s^0=s$ and $s^T$ is an equilibrium? This paper establishes a sufficient condition for such a property, and demonstrates that any finite-state Markov game, as well as any $N$-player game, guarantees the existence of a finite-length satisficing path from an arbitrary initial strategy to some equilibrium. These results provide a stronger theoretical foundation for the design of MARL algorithms.
<div id='section'>Paperid: <span id='pid'>1103, <a href='https://arxiv.org/pdf/2509.09135.pdf' target='_blank'>https://arxiv.org/pdf/2509.09135.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuefeng Wang, Lei Zhang, Henglin Pu, Ahmed H. Qureshi, Husheng Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.09135">Continuous-Time Value Iteration for Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing reinforcement learning (RL) methods struggle with complex dynamical systems that demand interactions at high frequencies or irregular time intervals. Continuous-time RL (CTRL) has emerged as a promising alternative by replacing discrete-time Bellman recursion with differential value functions defined as viscosity solutions of the Hamilton--Jacobi--Bellman (HJB) equation. While CTRL has shown promise, its applications have been largely limited to the single-agent domain. This limitation stems from two key challenges: (i) conventional solution methods for HJB equations suffer from the curse of dimensionality (CoD), making them intractable in high-dimensional systems; and (ii) even with HJB-based learning approaches, accurately approximating centralized value functions in multi-agent settings remains difficult, which in turn destabilizes policy training. In this paper, we propose a CT-MARL framework that uses physics-informed neural networks (PINNs) to approximate HJB-based value functions at scale. To ensure the value is consistent with its differential structure, we align value learning with value-gradient learning by introducing a Value Gradient Iteration (VGI) module that iteratively refines value gradients along trajectories. This improves gradient fidelity, in turn yielding more accurate values and stronger policy learning. We evaluate our method using continuous-time variants of standard benchmarks, including multi-agent particle environment (MPE) and multi-agent MuJoCo. Our results demonstrate that our approach consistently outperforms existing continuous-time RL baselines and scales to complex multi-agent dynamics.
<div id='section'>Paperid: <span id='pid'>1104, <a href='https://arxiv.org/pdf/2509.09135.pdf' target='_blank'>https://arxiv.org/pdf/2509.09135.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuefeng Wang, Lei Zhang, Henglin Pu, Ahmed H. Qureshi, Husheng Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.09135">Continuous-Time Value Iteration for Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing reinforcement learning (RL) methods struggle with complex dynamical systems that demand interactions at high frequencies or irregular time intervals. Continuous-time RL (CTRL) has emerged as a promising alternative by replacing discrete-time Bellman recursion with differential value functions defined as viscosity solutions of the Hamilton--Jacobi--Bellman (HJB) equation. While CTRL has shown promise, its applications have been largely limited to the single-agent domain. This limitation stems from two key challenges: (i) conventional solution methods for HJB equations suffer from the curse of dimensionality (CoD), making them intractable in high-dimensional systems; and (ii) even with HJB-based learning approaches, accurately approximating centralized value functions in multi-agent settings remains difficult, which in turn destabilizes policy training. In this paper, we propose a CT-MARL framework that uses physics-informed neural networks (PINNs) to approximate HJB-based value functions at scale. To ensure the value is consistent with its differential structure, we align value learning with value-gradient learning by introducing a Value Gradient Iteration (VGI) module that iteratively refines value gradients along trajectories. This improves gradient fidelity, in turn yielding more accurate values and stronger policy learning. We evaluate our method using continuous-time variants of standard benchmarks, including multi-agent particle environment (MPE) and multi-agent MuJoCo. Our results demonstrate that our approach consistently outperforms existing continuous-time RL baselines and scales to complex multi-agent dynamics.
<div id='section'>Paperid: <span id='pid'>1105, <a href='https://arxiv.org/pdf/2509.05446.pdf' target='_blank'>https://arxiv.org/pdf/2509.05446.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Iftekhar Haider Chowdhury, Zaed Ikbal Syed, Ahmed Faizul Haque Dhrubo, Mohammad Abdul Qayum
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.05446">Dynamic Sensitivity Filter Pruning using Multi-Agent Reinforcement Learning For DCNN's</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep Convolutional Neural Networks have achieved state of the art performance across various computer vision tasks, however their practical deployment is limited by computational and memory overhead. This paper introduces Differential Sensitivity Fusion Pruning, a novel single shot filter pruning framework that focuses on evaluating the stability and redundancy of filter importance scores across multiple criteria. Differential Sensitivity Fusion Pruning computes a differential sensitivity score for each filter by fusing the discrepancies among gradient based sensitivity, first order Taylor expansion, and KL divergence of activation distributions. An exponential scaling mechanism is applied to emphasize filters with inconsistent importance across metrics, identifying candidates that are structurally unstable or less critical to the model performance. Unlike iterative or reinforcement learning based pruning strategies, Differential Sensitivity Fusion Pruning is efficient and deterministic, requiring only a single forward-backward pass for scoring and pruning. Extensive experiments across varying pruning rates between 50 to 70 percent demonstrate that Differential Sensitivity Fusion Pruning significantly reduces model complexity, achieving over 80 percent Floating point Operations Per Seconds reduction while maintaining high accuracy. For instance, at 70 percent pruning, our approach retains up to 98.23 percent of baseline accuracy, surpassing traditional heuristics in both compression and generalization. The proposed method presents an effective solution for scalable and adaptive Deep Convolutional Neural Networks compression, paving the way for efficient deployment on edge and mobile platforms.
<div id='section'>Paperid: <span id='pid'>1106, <a href='https://arxiv.org/pdf/2509.05446.pdf' target='_blank'>https://arxiv.org/pdf/2509.05446.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Iftekhar Haider Chowdhury, Zaed Ikbal Syed, Ahmed Faizul Haque Dhrubo, Mohammad Abdul Qayum
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.05446">Dynamic Sensitivity Filter Pruning using Multi-Agent Reinforcement Learning For DCNN's</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep Convolutional Neural Networks have achieved state of the art performance across various computer vision tasks, however their practical deployment is limited by computational and memory overhead. This paper introduces Differential Sensitivity Fusion Pruning, a novel single shot filter pruning framework that focuses on evaluating the stability and redundancy of filter importance scores across multiple criteria. Differential Sensitivity Fusion Pruning computes a differential sensitivity score for each filter by fusing the discrepancies among gradient based sensitivity, first order Taylor expansion, and KL divergence of activation distributions. An exponential scaling mechanism is applied to emphasize filters with inconsistent importance across metrics, identifying candidates that are structurally unstable or less critical to the model performance. Unlike iterative or reinforcement learning based pruning strategies, Differential Sensitivity Fusion Pruning is efficient and deterministic, requiring only a single forward-backward pass for scoring and pruning. Extensive experiments across varying pruning rates between 50 to 70 percent demonstrate that Differential Sensitivity Fusion Pruning significantly reduces model complexity, achieving over 80 percent Floating point Operations Per Seconds reduction while maintaining high accuracy. For instance, at 70 percent pruning, our approach retains up to 98.23 percent of baseline accuracy, surpassing traditional heuristics in both compression and generalization. The proposed method presents an effective solution for scalable and adaptive Deep Convolutional Neural Networks compression, paving the way for efficient deployment on edge and mobile platforms.
<div id='section'>Paperid: <span id='pid'>1107, <a href='https://arxiv.org/pdf/2509.01257.pdf' target='_blank'>https://arxiv.org/pdf/2509.01257.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Andrea Fox, Francesco De Pellegrini, Eitan Altman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.01257">Multi-Agent Reinforcement Learning for Task Offloading in Wireless Edge Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In edge computing systems, autonomous agents must make fast local decisions while competing for shared resources. Existing MARL methods often resume to centralized critics or frequent communication, which fail under limited observability and communication constraints. We propose a decentralized framework in which each agent solves a constrained Markov decision process (CMDP), coordinating implicitly through a shared constraint vector. For the specific case of offloading, e.g., constraints prevent overloading shared server resources. Coordination constraints are updated infrequently and act as a lightweight coordination mechanism. They enable agents to align with global resource usage objectives but require little direct communication. Using safe reinforcement learning, agents learn policies that meet both local and global goals. We establish theoretical guarantees under mild assumptions and validate our approach experimentally, showing improved performance over centralized and independent baselines, especially in large-scale settings.
<div id='section'>Paperid: <span id='pid'>1108, <a href='https://arxiv.org/pdf/2509.01257.pdf' target='_blank'>https://arxiv.org/pdf/2509.01257.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Andrea Fox, Francesco De Pellegrini, Eitan Altman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.01257">Multi-Agent Reinforcement Learning for Task Offloading in Wireless Edge Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In edge computing systems, autonomous agents must make fast local decisions while competing for shared resources. Existing MARL methods often resume to centralized critics or frequent communication, which fail under limited observability and communication constraints. We propose a decentralized framework in which each agent solves a constrained Markov decision process (CMDP), coordinating implicitly through a shared constraint vector. For the specific case of offloading, e.g., constraints prevent overloading shared server resources. Coordination constraints are updated infrequently and act as a lightweight coordination mechanism. They enable agents to align with global resource usage objectives but require little direct communication. Using safe reinforcement learning, agents learn policies that meet both local and global goals. We establish theoretical guarantees under mild assumptions and validate our approach experimentally, showing improved performance over centralized and independent baselines, especially in large-scale settings.
<div id='section'>Paperid: <span id='pid'>1109, <a href='https://arxiv.org/pdf/2508.14679.pdf' target='_blank'>https://arxiv.org/pdf/2508.14679.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Parham Soltani, Mehrshad Eskandarpour, Amir Ahmadizad, Hossein Soleimani
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.14679">Energy-Efficient Routing Algorithm for Wireless Sensor Networks: A Multi-Agent Reinforcement Learning Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Efficient energy management is essential in Wireless Sensor Networks (WSNs) to extend network lifetime and ensure reliable data transmission. This paper presents a novel method using reinforcement learning-based cluster-head selection and a hybrid multi-hop routing algorithm, which leverages Q-learning within a multi-agent system to dynamically adapt transmission paths based on the energy distribution across sensor nodes. Each sensor node is modeled as an autonomous agent that observes local state parameters, such as residual energy, distance to sink, hop count, and hotspot proximity, and selects routing actions that maximize long-term energy efficiency. After computing the optimal paths, each sensor aggregates sensed data and forwards it through intermediate nodes to a selected transmitter node, chosen based on the highest remaining State of Charge (SoC), thereby avoiding premature node depletion. To promote efficient learning, a carefully designed reward function incentivizes balanced load distribution, hotspot avoidance, and energy-aware forwarding while maintaining signal quality. The learning process occurs either in a decentralized manner or via a cloud-based controller that offloads computation in large-scale deployments. Moreover, the RL-driven routing decisions are fused with classical graph-based methods, Minimum Energy Routing Algorithm (MERA) and Minimum Spanning Tree (MST), to optimize energy consumption and load balancing. Simulations confirm that the proposed approach significantly improves node survival rate, reduces SoC variance, and enhances network resilience, making it a scalable and adaptive solution for energy-constrained WSNs in dynamic sensor deployments and IoT applications.
<div id='section'>Paperid: <span id='pid'>1110, <a href='https://arxiv.org/pdf/2508.14676.pdf' target='_blank'>https://arxiv.org/pdf/2508.14676.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Parham Soltani, Mehrshad Eskandarpour, Sina Heidari, Farnaz Alizadeh, Hossein Soleimani
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.14676">Adaptive Vision-Based Coverage Optimization in Mobile Wireless Sensor Networks: A Multi-Agent Deep Reinforcement Learning Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Traditional Wireless Sensor Networks (WSNs) typically rely on pre-analysis of the target area, network size, and sensor coverage to determine initial deployment. This often results in significant overlap to ensure continued network operation despite sensor energy depletion. With the emergence of Mobile Wireless Sensor Networks (MWSNs), issues such as sensor failure and static coverage limitations can be more effectively addressed through mobility. This paper proposes a novel deployment strategy in which mobile sensors autonomously position themselves to maximize area coverage, eliminating the need for predefined policies. A live camera system, combined with deep reinforcement learning (DRL), monitors the network by detecting sensor LED indicators and evaluating real-time coverage. Rewards based on coverage efficiency and sensor movement are computed at each learning step and shared across the network through a Multi-Agent Reinforcement Learning (MARL) framework, enabling decentralized, cooperative sensor control. Key contributions include a vision-based, low-cost coverage evaluation method; a scalable MARL-DRL framework for autonomous deployment; and a self-reconfigurable system that adjusts sensor positioning in response to energy depletion. Compared to traditional distance-based localization, the proposed method achieves a 26.5% improvement in coverage, a 32% reduction in energy consumption, and a 22% decrease in redundancy, extending network lifetime by 45%. This approach significantly enhances adaptability, energy efficiency, and robustness in MWSNs, offering a practical deployment solution within the IoT framework.
<div id='section'>Paperid: <span id='pid'>1111, <a href='https://arxiv.org/pdf/2508.13661.pdf' target='_blank'>https://arxiv.org/pdf/2508.13661.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Maciej Wojtala, Bogusz StefaÅczyk, Dominik Bogucki, Åukasz Lepak, Jakub Strykowski, PaweÅ WawrzyÅski
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.13661">MACTAS: Self-Attention-Based Module for Inter-Agent Communication in Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Communication is essential for the collective execution of complex tasks by human agents, motivating interest in communication mechanisms for multi-agent reinforcement learning (MARL). However, existing communication protocols in MARL are often complex and non-differentiable. In this work, we introduce a self-attention-based communication module that exchanges information between the agents in MARL. Our proposed approach is fully differentiable, allowing agents to learn to generate messages in a reward-driven manner. The module can be seamlessly integrated with any action-value function decomposition method and can be viewed as an extension of such decompositions. Notably, it includes a fixed number of trainable parameters, independent of the number of agents. Experimental results on the SMAC benchmark demonstrate the effectiveness of our approach, which achieves state-of-the-art performance on several maps.
<div id='section'>Paperid: <span id='pid'>1112, <a href='https://arxiv.org/pdf/2508.12633.pdf' target='_blank'>https://arxiv.org/pdf/2508.12633.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yaqi Xu, Yan Shi, Jin Tian, Fanzeng Xia, Tongxin Li, Shanzhi Chen, Yuming Ge
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.12633">DCT-MARL: A Dynamic Communication Topology-Based MARL Algorithm for Connected Vehicle Platoon Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the rapid advancement of vehicular communication facilities and autonomous driving technologies, connected vehicle platooning has emerged as a promising approach to improve traffic efficiency and driving safety. Reliable Vehicle-to-Vehicle (V2V) communication is critical to achieving efficient cooperative control. However, in the real-world traffic environment, V2V communication may suffer from time-varying delay and packet loss, leading to degraded control performance and even safety risks. To mitigate the adverse effects of non-ideal communication, this paper proposes a Dynamic Communication Topology based Multi-Agent Reinforcement Learning (DCT-MARL) algorithm for robust cooperative platoon control. Specifically, the state space is augmented with historical control action and delay to enhance robustness against communication delay. To mitigate the impact of packet loss, a multi-key gated communication mechanism is introduced, which dynamically adjusts the communication topology based on the correlation between vehicles and their current communication status. Simulation results demonstrate that the proposed DCT-MARL significantly outperforms state-of-the-art methods in terms of string stability and driving comfort, validating its superior robustness and effectiveness.
<div id='section'>Paperid: <span id='pid'>1113, <a href='https://arxiv.org/pdf/2508.06871.pdf' target='_blank'>https://arxiv.org/pdf/2508.06871.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aleksandar Todorov, Juan Cardenas-Cartagena, Rafael F. Cunha, Marco Zullich, Matthia Sabatelli
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.06871">Sparsity-Driven Plasticity in Multi-Task Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Plasticity loss, a diminishing capacity to adapt as training progresses, is a critical challenge in deep reinforcement learning. We examine this issue in multi-task reinforcement learning (MTRL), where higher representational flexibility is crucial for managing diverse and potentially conflicting task demands. We systematically explore how sparsification methods, particularly Gradual Magnitude Pruning (GMP) and Sparse Evolutionary Training (SET), enhance plasticity and consequently improve performance in MTRL agents. We evaluate these approaches across distinct MTRL architectures (shared backbone, Mixture of Experts, Mixture of Orthogonal Experts) on standardized MTRL benchmarks, comparing against dense baselines, and a comprehensive range of alternative plasticity-inducing or regularization methods. Our results demonstrate that both GMP and SET effectively mitigate key indicators of plasticity degradation, such as neuron dormancy and representational collapse. These plasticity improvements often correlate with enhanced multi-task performance, with sparse agents frequently outperforming dense counterparts and achieving competitive results against explicit plasticity interventions. Our findings offer insights into the interplay between plasticity, network sparsity, and MTRL designs, highlighting dynamic sparsification as a robust but context-sensitive tool for developing more adaptable MTRL systems.
<div id='section'>Paperid: <span id='pid'>1114, <a href='https://arxiv.org/pdf/2508.06061.pdf' target='_blank'>https://arxiv.org/pdf/2508.06061.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ainur Zhaikhan, Malek Khammassi, Ali H. Sayed
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.06061">Policy Optimization in Multi-Agent Settings under Partially Observable Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work leverages adaptive social learning to estimate partially observable global states in multi-agent reinforcement learning (MARL) problems. Unlike existing methods, the proposed approach enables the concurrent operation of social learning and reinforcement learning. Specifically, it alternates between a single step of social learning and a single step of MARL, eliminating the need for the time- and computation-intensive two-timescale learning frameworks. Theoretical guarantees are provided to support the effectiveness of the proposed method. Simulation results verify that the performance of the proposed methodology can approach that of reinforcement learning when the true state is known.
<div id='section'>Paperid: <span id='pid'>1115, <a href='https://arxiv.org/pdf/2508.02948.pdf' target='_blank'>https://arxiv.org/pdf/2508.02948.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zain Ulabedeen Farhat, Debamita Ghosh, George K. Atia, Yue Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.02948">Online Robust Multi-Agent Reinforcement Learning under Model Uncertainties</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Well-trained multi-agent systems can fail when deployed in real-world environments due to model mismatches between the training and deployment environments, caused by environment uncertainties including noise or adversarial attacks. Distributionally Robust Markov Games (DRMGs) enhance system resilience by optimizing for worst-case performance over a defined set of environmental uncertainties. However, current methods are limited by their dependence on simulators or large offline datasets, which are often unavailable. This paper pioneers the study of online learning in DRMGs, where agents learn directly from environmental interactions without prior data. We introduce the {\it Robust Optimistic Nash Value Iteration (RONAVI)} algorithm and provide the first provable guarantees for this setting. Our theoretical analysis demonstrates that the algorithm achieves low regret and efficiently finds the optimal robust policy for uncertainty sets measured by Total Variation divergence and Kullback-Leibler divergence. These results establish a new, practical path toward developing truly robust multi-agent systems.
<div id='section'>Paperid: <span id='pid'>1116, <a href='https://arxiv.org/pdf/2508.02912.pdf' target='_blank'>https://arxiv.org/pdf/2508.02912.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Brennen A. Hill, Mant Koh En Wei, Thangavel Jishnuanandh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.02912">Engineered over Emergent Communication in MARL for Scalable and Sample-Efficient Cooperative Task Allocation in a Partially Observable Grid</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We compare the efficacy of learned versus engineered communication strategies in a cooperative multi-agent reinforcement learning (MARL) environment. For the learned approach, we introduce Learned Direct Communication (LDC), where agents generate messages and actions concurrently via a neural network. Our engineered approach, Intention Communication, employs an Imagined Trajectory Generation Module (ITGM) and a Message Generation Network (MGN) to formulate messages based on predicted future states. Both strategies are evaluated on their success rates in cooperative tasks under fully and partially observable conditions. Our findings indicate that while emergent communication is viable, the engineered approach demonstrates superior performance and scalability, particularly as environmental complexity increases.
<div id='section'>Paperid: <span id='pid'>1117, <a href='https://arxiv.org/pdf/2508.02912.pdf' target='_blank'>https://arxiv.org/pdf/2508.02912.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Brennen A. Hill, Mant Koh En Wei, Thangavel Jishnuanandh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.02912">Communicating Plans, Not Percepts: Scalable Multi-Agent Coordination with Embodied World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robust coordination is critical for effective decision-making in multi-agent systems, especially under partial observability. A central question in Multi-Agent Reinforcement Learning (MARL) is whether to engineer communication protocols or learn them end-to-end. We investigate this dichotomy using embodied world models. We propose and compare two communication strategies for a cooperative task-allocation problem. The first, Learned Direct Communication (LDC), learns a protocol end-to-end, with agents generating messages and actions concurrently. The second, Intention Communication, uses an engineered inductive bias: a compact, learned world model, the Imagined Trajectory Generation Module (ITGM), to simulate future states. Agents then communicate a summary of this plan. We evaluate these approaches on goal-directed interaction in a grid world, a canonical abstraction for embodied AI problems. Our experiments reveal that while emergent communication is viable in simple settings, the engineered, world model-based approach shows superior performance, sample efficiency, and scalability as complexity increases. These findings advocate for integrating structured, predictive models into MARL agents to enable active, goal-driven coordination.
<div id='section'>Paperid: <span id='pid'>1118, <a href='https://arxiv.org/pdf/2508.02912.pdf' target='_blank'>https://arxiv.org/pdf/2508.02912.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Brennen A. Hill, Mant Koh En Wei, Thangavel Jishnuanandh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.02912">Communicating Plans, Not Percepts: Scalable Multi-Agent Coordination with Embodied World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robust coordination is critical for effective decision-making in multi-agent systems, especially under partial observability. A central question in Multi-Agent Reinforcement Learning (MARL) is whether to engineer communication protocols or learn them end-to-end. We investigate this dichotomy using embodied world models. We propose and compare two communication strategies for a cooperative task-allocation problem. The first, Learned Direct Communication (LDC), learns a protocol end-to-end, with agents generating messages and actions concurrently. The second, Intention Communication, uses an engineered inductive bias: a compact, learned world model, the Imagined Trajectory Generation Module (ITGM), to simulate future states. Agents then communicate a summary of this plan. We evaluate these approaches on goal-directed interaction in a grid world, a canonical abstraction for embodied AI problems. Our experiments reveal that while emergent communication is viable in simple settings, the engineered, world model-based approach shows superior performance, sample efficiency, and scalability as complexity increases. These findings advocate for integrating structured, predictive models into MARL agents to enable active, goal-driven coordination.
<div id='section'>Paperid: <span id='pid'>1119, <a href='https://arxiv.org/pdf/2507.16796.pdf' target='_blank'>https://arxiv.org/pdf/2507.16796.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mian Ibad Ali Shah, Enda Barrett, Karl Mason
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.16796">Uncertainty-Aware Knowledge Transformers for Peer-to-Peer Energy Trading with Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a novel framework for Peer-to-Peer (P2P) energy trading that integrates uncertainty-aware prediction with multi-agent reinforcement learning (MARL), addressing a critical gap in current literature. In contrast to previous works relying on deterministic forecasts, the proposed approach employs a heteroscedastic probabilistic transformer-based prediction model called Knowledge Transformer with Uncertainty (KTU) to explicitly quantify prediction uncertainty, which is essential for robust decision-making in the stochastic environment of P2P energy trading. The KTU model leverages domain-specific features and is trained with a custom loss function that ensures reliable probabilistic forecasts and confidence intervals for each prediction. Integrating these uncertainty-aware forecasts into the MARL framework enables agents to optimize trading strategies with a clear understanding of risk and variability. Experimental results show that the uncertainty-aware Deep Q-Network (DQN) reduces energy purchase costs by up to 5.7% without P2P trading and 3.2% with P2P trading, while increasing electricity sales revenue by 6.4% and 44.7%, respectively. Additionally, peak hour grid demand is reduced by 38.8% without P2P and 45.6% with P2P. These improvements are even more pronounced when P2P trading is enabled, highlighting the synergy between advanced forecasting and market mechanisms for resilient, economically efficient energy communities.
<div id='section'>Paperid: <span id='pid'>1120, <a href='https://arxiv.org/pdf/2507.15351.pdf' target='_blank'>https://arxiv.org/pdf/2507.15351.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zijian Zhao, Sen Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.15351">One Step is Enough: Multi-Agent Reinforcement Learning based on One-Step Policy Optimization for Order Dispatch on Ride-Sharing Platforms</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>On-demand ride-sharing platforms face the fundamental challenge of dynamically bundling passengers with diverse origins and destinations and matching them with vehicles in real time, all under significant uncertainty. Recently, MARL has emerged as a promising solution for this problem, leveraging decentralized learning to address the curse of dimensionality caused by the large number of agents in the ride-hailing market and the resulting expansive state and action spaces. However, conventional MARL-based ride-sharing approaches heavily rely on the accurate estimation of Q-values or V-values, which becomes problematic in large-scale, highly uncertain environments. Specifically, most of these approaches adopt an independent paradigm, exacerbating this issue, as each agent treats others as part of the environment, leading to unstable training and substantial estimation bias in value functions. To address these challenges, we propose two novel alternative methods that bypass value function estimation. First, we adapt GRPO to ride-sharing, replacing the PPO baseline with the group average reward to eliminate critic estimation errors and reduce training bias. Second, inspired by GRPO's full utilization of group reward information, we customize the PPO framework for ride-sharing platforms and show that, under a homogeneous fleet, the optimal policy can be trained using only one-step rewards - a method we term One-Step Policy Optimization (OSPO). Experiments on a real-world Manhattan ride-hailing dataset demonstrate that both GRPO and OSPO achieve superior performance across most scenarios, efficiently optimizing pickup times and the number of served orders using simple MLP networks.
<div id='section'>Paperid: <span id='pid'>1121, <a href='https://arxiv.org/pdf/2507.11566.pdf' target='_blank'>https://arxiv.org/pdf/2507.11566.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fuda van Diggelen, Tugay Alperen KaragÃ¼zel, Andres Garcia Rincon, A. E. Eiben, Dario Floreano, Eliseo Ferrante
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.11566">Emergent Heterogeneous Swarm Control Through Hebbian Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we introduce Hebbian learning as a novel method for swarm robotics, enabling the automatic emergence of heterogeneity. Hebbian learning presents a biologically inspired form of neural adaptation that solely relies on local information. By doing so, we resolve several major challenges for learning heterogeneous control: 1) Hebbian learning removes the complexity of attributing emergent phenomena to single agents through local learning rules, thus circumventing the micro-macro problem; 2) uniform Hebbian learning rules across all swarm members limit the number of parameters needed, mitigating the curse of dimensionality with scaling swarm sizes; and 3) evolving Hebbian learning rules based on swarm-level behaviour minimises the need for extensive prior knowledge typically required for optimising heterogeneous swarms. This work demonstrates that with Hebbian learning heterogeneity naturally emerges, resulting in swarm-level behavioural switching and in significantly improved swarm capabilities. It also demonstrates how the evolution of Hebbian learning rules can be a valid alternative to Multi Agent Reinforcement Learning in standard benchmarking tasks.
<div id='section'>Paperid: <span id='pid'>1122, <a href='https://arxiv.org/pdf/2507.10251.pdf' target='_blank'>https://arxiv.org/pdf/2507.10251.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenjing Zhang, Wei Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.10251">ToMacVF : Temporal Macro-action Value Factorization for Asynchronous Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing asynchronous MARL methods based on MacDec-POMDP typically construct training trajectory buffers by simply sampling limited and biased data at the endpoints of macro-actions, and directly apply conventional MARL methods on the buffers. As a result, these methods lead to an incomplete and inaccurate representation of the macro-action execution process, along with unsuitable credit assignments. To solve these problems, the Temporal Macro-action Value Factorization (ToMacVF) is proposed to achieve fine-grained temporal credit assignment for macro-action contributions. A centralized training buffer, called Macro-action Segmented Joint Experience Replay Trajectory (Mac-SJERT), is designed to incorporate with ToMacVF to collect accurate and complete macro-action execution information, supporting a more comprehensive and precise representation of the macro-action process. To ensure principled and fine-grained asynchronous value factorization, the consistency requirement between joint and individual macro-action selection called Temporal Macro-action based IGM (To-Mac-IGM) is formalized, proving that it generalizes the synchronous cases. Based on To-Mac-IGM, a modularized ToMacVF architecture, which satisfies CTDE principle, is designed to conveniently integrate previous value factorization methods. Next, the ToMacVF algorithm is devised as an implementation of the ToMacVF architecture. Experimental results demonstrate that, compared to asynchronous baselines, our ToMacVF algorithm not only achieves optimal performance but also exhibits strong adaptability and robustness across various asynchronous multi-agent experimental scenarios.
<div id='section'>Paperid: <span id='pid'>1123, <a href='https://arxiv.org/pdf/2507.08610.pdf' target='_blank'>https://arxiv.org/pdf/2507.08610.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Parag Dutta, Ambedkar Dukkipati
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.08610">Emergent Natural Language with Communication Games for Improving Image Captioning Capabilities without Additional Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Image captioning is an important problem in developing various AI systems, and these tasks require large volumes of annotated images to train the models. Since all existing labelled datasets are already used for training the large Vision Language Models (VLMs), it becomes challenging to improve the performance of the same. Considering this, it is essential to consider the unsupervised image captioning performance, which remains relatively under-explored. To that end, we propose LoGIC (Lewis Communication Game for Image Captioning), a Multi-agent Reinforcement Learning game. The proposed method consists of two agents, a 'speaker' and a 'listener', with the objective of learning a strategy for communicating in natural language. We train agents in the cooperative common-reward setting using the GRPO algorithm and show that improvement in image captioning performance emerges as a consequence of the agents learning to play the game. We show that using pre-trained VLMs as the 'speaker' and Large Language Model (LLM) for language understanding in the 'listener', we achieved a $46$ BLEU score after fine-tuning using LoGIC without additional labels, a $2$ units advantage in absolute metrics compared to the $44$ BLEU score of the vanilla VLM. Additionally, we replace the VLM from the 'speaker' with lightweight components: (i) a ViT for image perception and (ii) a GPT2 language generation, and train them from scratch using LoGIC, obtaining a $31$ BLEU score in the unsupervised setting, a $10$ points advantage over existing unsupervised image-captioning methods.
<div id='section'>Paperid: <span id='pid'>1124, <a href='https://arxiv.org/pdf/2507.02698.pdf' target='_blank'>https://arxiv.org/pdf/2507.02698.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Thomas Hazenberg, Yao Ma, Seyed Sahand Mohammadi Ziabari, Marijn van Rijswijk
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.02698">Multi-Agent Reinforcement Learning for Dynamic Pricing in Supply Chains: Benchmarking Strategic Agent Behaviours under Realistically Simulated Market Conditions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This study investigates how Multi-Agent Reinforcement Learning (MARL) can improve dynamic pricing strategies in supply chains, particularly in contexts where traditional ERP systems rely on static, rule-based approaches that overlook strategic interactions among market actors. While recent research has applied reinforcement learning to pricing, most implementations remain single-agent and fail to model the interdependent nature of real-world supply chains. This study addresses that gap by evaluating the performance of three MARL algorithms: MADDPG, MADQN, and QMIX against static rule-based baselines, within a simulated environment informed by real e-commerce transaction data and a LightGBM demand prediction model. Results show that rule-based agents achieve near-perfect fairness (Jain's Index: 0.9896) and the highest price stability (volatility: 0.024), but they fully lack competitive dynamics. Among MARL agents, MADQN exhibits the most aggressive pricing behaviour, with the highest volatility and the lowest fairness (0.5844). MADDPG provides a more balanced approach, supporting market competition (share volatility: 9.5 pp) while maintaining relatively high fairness (0.8819) and stable pricing. These findings suggest that MARL introduces emergent strategic behaviour not captured by static pricing rules and may inform future developments in dynamic pricing.
<div id='section'>Paperid: <span id='pid'>1125, <a href='https://arxiv.org/pdf/2506.05236.pdf' target='_blank'>https://arxiv.org/pdf/2506.05236.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Maxime Toquebiau, Jae-Yun Jun, FaÃ¯z Benamar, Nicolas Bredeche
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.05236">Towards Language-Augmented Multi-Agent Deep Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Most prior works on communication in multi-agent reinforcement learning have focused on emergent communication, which often results in inefficient and non-interpretable systems. Inspired by the role of language in natural intelligence, we investigate how grounding agents in a human-defined language can improve the learning and coordination of embodied agents. We propose a framework in which agents are trained not only to act but also to produce and interpret natural language descriptions of their observations. This language-augmented learning serves a dual role: enabling efficient and interpretable communication between agents, and guiding representation learning. We demonstrate that language-augmented agents outperform emergent communication baselines across various tasks. Our analysis reveals that language grounding leads to more informative internal representations, better generalization to new partners, and improved capability for human-agent interaction. These findings demonstrate the effectiveness of integrating structured language into multi-agent learning and open avenues for more interpretable and capable multi-agent systems.
<div id='section'>Paperid: <span id='pid'>1126, <a href='https://arxiv.org/pdf/2506.04195.pdf' target='_blank'>https://arxiv.org/pdf/2506.04195.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Elena Zamaraeva, Christopher M. Collins, George R. Darling, Matthew S. Dyer, Bei Peng, Rahul Savani, Dmytro Antypov, Vladimir V. Gusev, Judith Clymo, Paul G. Spirakis, Matthew J. Rosseinsky
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.04195">MACS: Multi-Agent Reinforcement Learning for Optimization of Crystal Structures</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Geometry optimization of atomic structures is a common and crucial task in computational chemistry and materials design. Following the learning to optimize paradigm, we propose a new multi-agent reinforcement learning method called Multi-Agent Crystal Structure optimization (MACS) to address periodic crystal structure optimization. MACS treats geometry optimization as a partially observable Markov game in which atoms are agents that adjust their positions to collectively discover a stable configuration. We train MACS across various compositions of reported crystalline materials to obtain a policy that successfully optimizes structures from the training compositions as well as structures of larger sizes and unseen compositions, confirming its excellent scalability and zero-shot transferability. We benchmark our approach against a broad range of state-of-the-art optimization methods and demonstrate that MACS optimizes periodic crystal structures significantly faster, with fewer energy calculations, and the lowest failure rate.
<div id='section'>Paperid: <span id='pid'>1127, <a href='https://arxiv.org/pdf/2505.19837.pdf' target='_blank'>https://arxiv.org/pdf/2505.19837.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Christoph R. Landolt, Christoph WÃ¼rsch, Roland Meier, Alain Mermoud, Julian Jang-Jaccard
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.19837">Multi-Agent Reinforcement Learning in Cybersecurity: From Fundamentals to Applications</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-Agent Reinforcement Learning (MARL) has shown great potential as an adaptive solution for addressing modern cybersecurity challenges. MARL enables decentralized, adaptive, and collaborative defense strategies and provides an automated mechanism to combat dynamic, coordinated, and sophisticated threats. This survey investigates the current state of research in MARL applications for automated cyber defense (ACD), focusing on intruder detection and lateral movement containment. Additionally, it examines the role of Autonomous Intelligent Cyber-defense Agents (AICA) and Cyber Gyms in training and validating MARL agents. Finally, the paper outlines existing challenges, such as scalability and adversarial robustness, and proposes future research directions. This also discusses how MARL integrates in AICA to provide adaptive, scalable, and dynamic solutions to counter the increasingly sophisticated landscape of cyber threats. It highlights the transformative potential of MARL in areas like intrusion detection and lateral movement containment, and underscores the value of Cyber Gyms for training and validation of AICA.
<div id='section'>Paperid: <span id='pid'>1128, <a href='https://arxiv.org/pdf/2505.08222.pdf' target='_blank'>https://arxiv.org/pdf/2505.08222.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Matteo Gallici, Ivan Masmitja, Mario MartÃ­n
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.08222">Scaling Multi Agent Reinforcement Learning for Underwater Acoustic Tracking via Autonomous Vehicles</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autonomous vehicles (AV) offer a cost-effective solution for scientific missions such as underwater tracking. Recently, reinforcement learning (RL) has emerged as a powerful method for controlling AVs in complex marine environments. However, scaling these techniques to a fleet--essential for multi-target tracking or targets with rapid, unpredictable motion--presents significant computational challenges. Multi-Agent Reinforcement Learning (MARL) is notoriously sample-inefficient, and while high-fidelity simulators like Gazebo's LRAUV provide 100x faster-than-real-time single-robot simulations, they offer no significant speedup for multi-vehicle scenarios, making MARL training impractical. To address these limitations, we propose an iterative distillation method that transfers high-fidelity simulations into a simplified, GPU-accelerated environment while preserving high-level dynamics. This approach achieves up to a 30,000x speedup over Gazebo through parallelization, enabling efficient training via end-to-end GPU acceleration. Additionally, we introduce a novel Transformer-based architecture (TransfMAPPO) that learns multi-agent policies invariant to the number of agents and targets, significantly improving sample efficiency. Following large-scale curriculum learning conducted entirely on GPU, we perform extensive evaluations in Gazebo, demonstrating that our method maintains tracking errors below 5 meters over extended durations, even in the presence of multiple fast-moving targets. This work bridges the gap between large-scale MARL training and high-fidelity deployment, providing a scalable framework for autonomous fleet control in real-world sea missions.
<div id='section'>Paperid: <span id='pid'>1129, <a href='https://arxiv.org/pdf/2505.03288.pdf' target='_blank'>https://arxiv.org/pdf/2505.03288.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Francesco Morri, HÃ©lÃ¨ne Le Cadre, Pierre Gruet, Luce Brotcorne
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.03288">Game Theory and Multi-Agent Reinforcement Learning for Zonal Ancillary Markets</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We characterize zonal ancillary market coupling relying on noncooperative game theory. To that purpose, we formulate the ancillary market as a multi-leader single follower bilevel problem, that we subsequently cast as a generalized Nash game with side constraints and nonconvex feasibility sets. We determine conditions for equilibrium existence and show that the game has a generalized potential game structure. To compute market equilibrium, we rely on two exact approaches: an integrated optimization approach and Gauss-Seidel best-response, that we compare against multi-agent deep reinforcement learning. On real data from Germany and Austria, simulations indicate that multi-agent deep reinforcement learning achieves the smallest convergence rate but requires pretraining, while best-response is the slowest. On the economics side, multi-agent deep reinforcement learning results in smaller market costs compared to the exact methods, but at the cost of higher variability in the profit allocation among stakeholders. Further, stronger coupling between zones tends to reduce costs for larger zones.
<div id='section'>Paperid: <span id='pid'>1130, <a href='https://arxiv.org/pdf/2505.03288.pdf' target='_blank'>https://arxiv.org/pdf/2505.03288.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Francesco Morri, HÃ©lÃ¨ne Le Cadre, Pierre Gruet, Luce Brotcorne
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.03288">Game Theory and Multi-Agent Reinforcement Learning for Zonal Ancillary Markets</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We characterize zonal ancillary market coupling relying on noncooperative game theory. To that purpose, we formulate the ancillary market as a multi-leader single follower bilevel problem, that we subsequently cast as a generalized Nash game with side constraints and nonconvex feasibility sets. We determine conditions for equilibrium existence and show that the game has a generalized potential game structure. To compute market equilibrium, we rely on two exact approaches: an integrated optimization approach and Gauss-Seidel best-response, that we compare against multi-agent deep reinforcement learning. On real data from Germany and Austria, simulations indicate that multi-agent deep reinforcement learning achieves the smallest convergence rate but requires pretraining, while best-response is the slowest. On the economics side, multi-agent deep reinforcement learning results in smaller market costs compared to the exact methods, but at the cost of higher variability in the profit allocation among stakeholders. Further, stronger coupling between zones tends to reduce costs for larger zones.
<div id='section'>Paperid: <span id='pid'>1131, <a href='https://arxiv.org/pdf/2504.13424.pdf' target='_blank'>https://arxiv.org/pdf/2504.13424.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yang Shen, Shuqi Chai, Bing Li, Xiaodong Luo, Qingjiang Shi, Rongqing Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.13424">Decentralized Handover Parameter Optimization with MARL for Load Balancing in 5G Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In cellular networks, cell handover refers to the process where a device switches from one base station to another, and this mechanism is crucial for balancing the load among different cells. Traditionally, engineers would manually adjust parameters based on experience. However, the explosive growth in the number of cells has rendered manual tuning impractical. Existing research tends to overlook critical engineering details in order to simplify handover problems. In this paper, we classify cell handover into three types, and jointly model their mutual influence. To achieve load balancing, we propose a multi-agent-reinforcement-learning (MARL)-based scheme to automatically optimize the parameters. To reduce the agent interaction costs, a distributed training is implemented based on consensus approximation of global average load, and it is shown that the approximation error is bounded. Experimental results show that our proposed scheme outperforms existing benchmarks in balancing load and improving network performance.
<div id='section'>Paperid: <span id='pid'>1132, <a href='https://arxiv.org/pdf/2504.11258.pdf' target='_blank'>https://arxiv.org/pdf/2504.11258.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Liam Welsh, Udit Grover, Sebastian Jaimungal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.11258">Multi-Agent Reinforcement Learning for Greenhouse Gas Offset Credit Markets</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Climate change is a major threat to the future of humanity, and its impacts are being intensified by excess man-made greenhouse gas emissions. One method governments can employ to control these emissions is to provide firms with emission limits and penalize any excess emissions above the limit. Excess emissions may also be offset by firms who choose to invest in carbon reducing and capturing projects. These projects generate offset credits which can be submitted to a regulating agency to offset a firm's excess emissions, or they can be traded with other firms. In this work, we characterize the finite-agent Nash equilibrium for offset credit markets. As computing Nash equilibria is an NP-hard problem, we utilize the modern reinforcement learning technique Nash-DQN to efficiently estimate the market's Nash equilibria. We demonstrate not only the validity of employing reinforcement learning methods applied to climate themed financial markets, but also the significant financial savings emitting firms may achieve when abiding by the Nash equilibria through numerical experiments.
<div id='section'>Paperid: <span id='pid'>1133, <a href='https://arxiv.org/pdf/2504.11258.pdf' target='_blank'>https://arxiv.org/pdf/2504.11258.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Liam Welsh, Udit Grover, Sebastian Jaimungal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.11258">Multi-Agent Reinforcement Learning for Greenhouse Gas Offset Credit Markets</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Climate change is a major threat to the future of humanity, and its impacts are being intensified by excess man-made greenhouse gas emissions. One method governments can employ to control these emissions is to provide firms with emission limits and penalize any excess emissions above the limit. Excess emissions may also be offset by firms who choose to invest in carbon reducing and capturing projects. These projects generate offset credits which can be submitted to a regulating agency to offset a firm's excess emissions, or they can be traded with other firms. In this work, we characterize the finite-agent Nash equilibrium for offset credit markets. As computing Nash equilibria is an NP-hard problem, we utilize the modern reinforcement learning technique Nash-DQN to efficiently estimate the market's Nash equilibria. We demonstrate not only the validity of employing reinforcement learning methods applied to climate themed financial markets, but also the significant financial savings emitting firms may achieve when abiding by the Nash equilibria through numerical experiments.
<div id='section'>Paperid: <span id='pid'>1134, <a href='https://arxiv.org/pdf/2504.11258.pdf' target='_blank'>https://arxiv.org/pdf/2504.11258.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Liam Welsh, Udit Grover, Sebastian Jaimungal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.11258">Multi-Agent Reinforcement Learning for Greenhouse Gas Offset Credit Markets</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Climate change is a major threat to the future of humanity, and its impacts are being intensified by excess man-made greenhouse gas emissions. One method governments can employ to control these emissions is to provide firms with emission limits and penalize any excess emissions above the limit. Excess emissions may also be offset by firms who choose to invest in carbon reducing and capturing projects. These projects generate offset credits which can be submitted to a regulating agency to offset a firm's excess emissions, or they can be traded with other firms. In this work, we characterize the finite-agent Nash equilibrium for offset credit markets. As computing Nash equilibria is an NP-hard problem, we utilize the modern reinforcement learning technique Nash-DQN to efficiently estimate the market's Nash equilibria. We demonstrate not only the validity of employing reinforcement learning methods applied to climate themed financial markets, but also the significant financial savings emitting firms may achieve when abiding by the Nash equilibria through numerical experiments.
<div id='section'>Paperid: <span id='pid'>1135, <a href='https://arxiv.org/pdf/2504.04438.pdf' target='_blank'>https://arxiv.org/pdf/2504.04438.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wang Zhang, Chenguang Liu, Yue Pi, Yong Zhang, Hairong Huang, Baoquan Rao, Yulong Ding, Shuanghua Yang, Jie Jiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.04438">DRAMA: A Dynamic Packet Routing Algorithm using Multi-Agent Reinforcement Learning with Emergent Communication</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The continuous expansion of network data presents a pressing challenge for conventional routing algorithms. As the demand escalates, these algorithms are struggling to cope. In this context, reinforcement learning (RL) and multi-agent reinforcement learning (MARL) algorithms emerge as promising solutions. However, the urgency and importance of the problem are clear, as existing RL/MARL-based routing approaches lack effective communication in run time among routers, making it challenging for individual routers to adapt to complex and dynamic changing networks. More importantly, they lack the ability to deal with dynamically changing network topology, especially the addition of the router, due to the non-scalability of their neural networks. This paper proposes a novel dynamic routing algorithm, DRAMA, incorporating emergent communication in multi-agent reinforcement learning. Through emergent communication, routers could learn how to communicate effectively to maximize the optimization objectives. Meanwhile, a new Q-network and graph-based emergent communication are introduced to dynamically adapt to the changing network topology without retraining while ensuring robust performance. Experimental results showcase DRAMA's superior performance over the traditional routing algorithm and other RL/MARL-based algorithms, achieving a higher delivery rate and lower latency in diverse network scenarios, including dynamic network load and topology. Moreover, an ablation experiment validates the prospect of emergent communication in facilitating packet routing.
<div id='section'>Paperid: <span id='pid'>1136, <a href='https://arxiv.org/pdf/2504.04160.pdf' target='_blank'>https://arxiv.org/pdf/2504.04160.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alexandre Oliveira, Katarina Dyreby, Francisco Caldas, Cláudia Soares
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.04160">OrbitZoo: Multi-Agent Reinforcement Learning Environment for Orbital Dynamics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The increasing number of satellites and orbital debris has made space congestion a critical issue, threatening satellite safety and sustainability. Challenges such as collision avoidance, station-keeping, and orbital maneuvering require advanced techniques to handle dynamic uncertainties and multi-agent interactions. Reinforcement learning (RL) has shown promise in this domain, enabling adaptive, autonomous policies for space operations; however, many existing RL frameworks rely on custom-built environments developed from scratch, which often use simplified models and require significant time to implement and validate the orbital dynamics, limiting their ability to fully capture real-world complexities. To address this, we introduce OrbitZoo, a versatile multi-agent RL environment built on a high-fidelity industry standard library, that enables realistic data generation, supports scenarios like collision avoidance and cooperative maneuvers, and ensures robust and accurate orbital dynamics. The environment is validated against a real satellite constellation, Starlink, achieving a Mean Absolute Percentage Error (MAPE) of 0.16% compared to real-world data. This validation ensures reliability for generating high-fidelity simulations and enabling autonomous and independent satellite operations.
<div id='section'>Paperid: <span id='pid'>1137, <a href='https://arxiv.org/pdf/2503.22867.pdf' target='_blank'>https://arxiv.org/pdf/2503.22867.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Huiwen Yan, Mushuang Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.22867">Markov Potential Game Construction and Multi-Agent Reinforcement Learning with Applications to Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Markov games (MGs) serve as the mathematical foundation for multi-agent reinforcement learning (MARL), enabling self-interested agents to learn their optimal policies while interacting with others in a shared environment. However, due to the complexities of an MG problem, seeking (Markov perfect) Nash equilibrium (NE) is often very challenging for a general-sum MG. Markov potential games (MPGs), which are a special class of MGs, have appealing properties such as guaranteed existence of pure NEs and guaranteed convergence of gradient play algorithms, thereby leading to desirable properties for many MARL algorithms in their NE-seeking processes. However, the question of how to construct MPGs has been open. This paper provides sufficient conditions on the reward design and on the Markov decision process (MDP), under which an MG is an MPG. Numerical results on autonomous driving applications are reported.
<div id='section'>Paperid: <span id='pid'>1138, <a href='https://arxiv.org/pdf/2503.22162.pdf' target='_blank'>https://arxiv.org/pdf/2503.22162.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ning Liu, Sen Shen, Xiangrui Kong, Hongtao Zhang, Thomas BrÃ¤unl
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.22162">Cooperative Hybrid Multi-Agent Pathfinding Based on Shared Exploration Maps</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-Agent Pathfinding is used in areas including multi-robot formations, warehouse logistics, and intelligent vehicles. However, many environments are incomplete or frequently change, making it difficult for standard centralized planning or pure reinforcement learning to maintain both global solution quality and local flexibility. This paper introduces a hybrid framework that integrates D* Lite global search with multi-agent reinforcement learning, using a switching mechanism and a freeze-prevention strategy to handle dynamic conditions and crowded settings. We evaluate the framework in the discrete POGEMA environment and compare it with baseline methods. Experimental outcomes indicate that the proposed framework substantially improves success rate, collision rate, and path efficiency. The model is further tested on the EyeSim platform, where it maintains feasible Pathfinding under frequent changes and large-scale robot deployments.
<div id='section'>Paperid: <span id='pid'>1139, <a href='https://arxiv.org/pdf/2503.17803.pdf' target='_blank'>https://arxiv.org/pdf/2503.17803.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Giovanni Briglia, Stefano Mariani, Franco Zambonelli
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.17803">A Roadmap Towards Improving Multi-Agent Reinforcement Learning With Causal Discovery And Inference</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Causal reasoning is increasingly used in Reinforcement Learning (RL) to improve the learning process in several dimensions: efficacy of learned policies, efficiency of convergence, generalisation capabilities, safety and interpretability of behaviour. However, applications of causal reasoning to Multi-Agent RL (MARL) are still mostly unexplored. In this paper, we take the first step in investigating the opportunities and challenges of applying causal reasoning in MARL. We measure the impact of a simple form of causal augmentation in state-of-the-art MARL scenarios increasingly requiring cooperation, and with state-of-the-art MARL algorithms exploiting various degrees of collaboration between agents. Then, we discuss the positive as well as negative results achieved, giving us the chance to outline the areas where further research may help to successfully transfer causal RL to the multi-agent setting.
<div id='section'>Paperid: <span id='pid'>1140, <a href='https://arxiv.org/pdf/2503.10907.pdf' target='_blank'>https://arxiv.org/pdf/2503.10907.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xueting Luo, Hao Deng, Jihong Yang, Yao Shen, Huanhuan Guo, Zhiyuan Sun, Mingqing Liu, Jiming Wei, Shengjie Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.10907">H2-MARL: Multi-Agent Reinforcement Learning for Pareto Optimality in Hospital Capacity Strain and Human Mobility during Epidemic</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The necessity of achieving an effective balance between minimizing the losses associated with restricting human mobility and ensuring hospital capacity has gained significant attention in the aftermath of COVID-19. Reinforcement learning (RL)-based strategies for human mobility management have recently advanced in addressing the dynamic evolution of cities and epidemics; however, they still face challenges in achieving coordinated control at the township level and adapting to cities of varying scales. To address the above issues, we propose a multi-agent RL approach that achieves Pareto optimality in managing hospital capacity and human mobility (H2-MARL), applicable across cities of different scales. We first develop a township-level infection model with online-updatable parameters to simulate disease transmission and construct a city-wide dynamic spatiotemporal epidemic simulator. On this basis, H2-MARL is designed to treat each division as an agent, with a trade-off dual-objective reward function formulated and an experience replay buffer enriched with expert knowledge built. To evaluate the effectiveness of the model, we construct a township-level human mobility dataset containing over one billion records from four representative cities of varying scales. Extensive experiments demonstrate that H2-MARL has the optimal dual-objective trade-off capability, which can minimize hospital capacity strain while minimizing human mobility restriction loss. Meanwhile, the applicability of the proposed model to epidemic control in cities of varying scales is verified, which showcases its feasibility and versatility in practical applications.
<div id='section'>Paperid: <span id='pid'>1141, <a href='https://arxiv.org/pdf/2503.05092.pdf' target='_blank'>https://arxiv.org/pdf/2503.05092.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Adam Labiosa, Josiah P. Hanna
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.05092">Multi-Robot Collaboration through Reinforcement Learning and Abstract Simulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Teams of people coordinate to perform complex tasks by forming abstract mental models of world and agent dynamics. The use of abstract models contrasts with much recent work in robot learning that uses a high-fidelity simulator and reinforcement learning (RL) to obtain policies for physical robots. Motivated by this difference, we investigate the extent to which so-called abstract simulators can be used for multi-agent reinforcement learning (MARL) and the resulting policies successfully deployed on teams of physical robots. An abstract simulator models the robot's target task at a high-level of abstraction and discards many details of the world that could impact optimal decision-making. Policies are trained in an abstract simulator then transferred to the physical robot by making use of separately-obtained low-level perception and motion control modules. We identify three key categories of modifications to the abstract simulator that enable policy transfer to physical robots: simulation fidelity enhancements, training optimizations and simulation stochasticity. We then run an empirical study with extensive ablations to determine the value of each modification category for enabling policy transfer in cooperative robot soccer tasks. We also compare the performance of policies produced by our method with a well-tuned non-learning-based behavior architecture from the annual RoboCup competition and find that our approach leads to a similar level of performance. Broadly we show that MARL can be use to train cooperative physical robot behaviors using highly abstract models of the world.
<div id='section'>Paperid: <span id='pid'>1142, <a href='https://arxiv.org/pdf/2503.04094.pdf' target='_blank'>https://arxiv.org/pdf/2503.04094.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Seth Karten, Andy Luu Nguyen, Chi Jin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.04094">PokÃ©Champ: an Expert-level Minimax Language Agent</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce PokÃ©Champ, a minimax agent powered by Large Language Models (LLMs) for PokÃ©mon battles. Built on a general framework for two-player competitive games, PokÃ©Champ leverages the generalist capabilities of LLMs to enhance minimax tree search. Specifically, LLMs replace three key modules: (1) player action sampling, (2) opponent modeling, and (3) value function estimation, enabling the agent to effectively utilize gameplay history and human knowledge to reduce the search space and address partial observability. Notably, our framework requires no additional LLM training. We evaluate PokÃ©Champ in the popular Gen 9 OU format. When powered by GPT-4o, it achieves a win rate of 76% against the best existing LLM-based bot and 84% against the strongest rule-based bot, demonstrating its superior performance. Even with an open-source 8-billion-parameter Llama 3.1 model, PokÃ©Champ consistently outperforms the previous best LLM-based bot, PokÃ©llmon powered by GPT-4o, with a 64% win rate. PokÃ©Champ attains a projected Elo of 1300-1500 on the PokÃ©mon Showdown online ladder, placing it among the top 30%-10% of human players. In addition, this work compiles the largest real-player PokÃ©mon battle dataset, featuring over 3 million games, including more than 500k high-Elo matches. Based on this dataset, we establish a series of battle benchmarks and puzzles to evaluate specific battling skills. We further provide key updates to the local game engine. We hope this work fosters further research that leverage PokÃ©mon battle as benchmark to integrate LLM technologies with game-theoretic algorithms addressing general multiagent problems. Videos, code, and dataset available at https://sites.google.com/view/pokechamp-llm.
<div id='section'>Paperid: <span id='pid'>1143, <a href='https://arxiv.org/pdf/2503.02189.pdf' target='_blank'>https://arxiv.org/pdf/2503.02189.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dickness Kakitahi Kwesiga, Angshuman Guin, Michael Hunter
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.02189">Adaptive Traffic Signal Control based on Multi-Agent Reinforcement Learning. Case Study on a simulated real-world corridor</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Previous studies that have formulated multi-agent reinforcement learning (RL) algorithms for adaptive traffic signal control have primarily used value-based RL methods. However, recent literature has shown that policy-based methods may perform better in partially observable environments. Additionally, RL methods remain largely untested for real-world normally signal timing plans because of the simplifying assumptions common in the literature. The current study attempts to address these gaps and formulates a multi-agent proximal policy optimization (MA-PPO) algorithm to implement adaptive and coordinated traffic control along an arterial corridor. The formulated MA-PPO has a centralized-critic architecture under a centralized training and decentralized execution framework. Agents are designed to allow selection and implementation of up to eight signal phases, as commonly implemented in field controllers. The formulated algorithm is tested on a simulated real-world seven intersection corridor. The speed of convergence for each agent was found to depend on the size of the action space, which depends on the number and sequence of signal phases. The performance of the formulated MA-PPO adaptive control algorithm is compared with the field implemented actuated-coordinated signal control (ASC), modeled using PTV-Vissim-MaxTime software in the loop simulation (SILs). The trained MA-PPO performed significantly better than the ASC for all movements. Compared to ASC the MA-PPO showed 2% and 24% improvements in travel time in the primary and secondary coordination directions, respectively. For cross streets movements MA-PPO also showed significant crossing time reductions. Volume sensitivity experiments revealed that the formulated MA-PPO demonstrated good stability, robustness, and adaptability to changes in traffic demand.
<div id='section'>Paperid: <span id='pid'>1144, <a href='https://arxiv.org/pdf/2503.01069.pdf' target='_blank'>https://arxiv.org/pdf/2503.01069.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kareem Eissa, Rayal Prasad, Sarith Mohan, Ankur Kapoor, Dorin Comaniciu, Vivek Singh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.01069">Multi-Agent Reinforcement Learning with Long-Term Performance Objectives for Service Workforce Optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Workforce optimization plays a crucial role in efficient organizational operations where decision-making may span several different administrative and time scales. For instance, dispatching personnel to immediate service requests while managing talent acquisition with various expertise sets up a highly dynamic optimization problem. Existing work focuses on specific sub-problems such as resource allocation and facility location, which are solved with heuristics like local-search and, more recently, deep reinforcement learning. However, these may not accurately represent real-world scenarios where such sub-problems are not fully independent. Our aim is to fill this gap by creating a simulator that models a unified workforce optimization problem. Specifically, we designed a modular simulator to support the development of reinforcement learning methods for integrated workforce optimization problems. We focus on three interdependent aspects: personnel dispatch, workforce management, and personnel positioning. The simulator provides configurable parameterizations to help explore dynamic scenarios with varying levels of stochasticity and non-stationarity. To facilitate benchmarking and ablation studies, we also include heuristic and RL baselines for the above mentioned aspects.
<div id='section'>Paperid: <span id='pid'>1145, <a href='https://arxiv.org/pdf/2502.20462.pdf' target='_blank'>https://arxiv.org/pdf/2502.20462.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Leopoldo Agorio, Sean Van Alen, Santiago Paternain, Miguel Calvo-Fullana, Juan Andres Bazerque
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.20462">Cooperative Multi-Agent Assignment over Stochastic Graphs via Constrained Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Constrained multi-agent reinforcement learning offers the framework to design scalable and almost surely feasible solutions for teams of agents operating in dynamic environments to carry out conflicting tasks. We address the challenges of multi-agent coordination through an unconventional formulation in which the dual variables are not driven to convergence but are free to cycle, enabling agents to adapt their policies dynamically based on real-time constraint satisfaction levels. The coordination relies on a light single-bit communication protocol over a network with stochastic connectivity. Using this gossiped information, agents update local estimates of the dual variables. Furthermore, we modify the local dual dynamics by introducing a contraction factor, which lets us use finite communication buffers and keep the estimation error bounded. Under this model, we provide theoretical guarantees of almost sure feasibility and corroborate them with numerical experiments in which a team of robots successfully patrols multiple regions, communicating under a time-varying ad-hoc network.
<div id='section'>Paperid: <span id='pid'>1146, <a href='https://arxiv.org/pdf/2502.20068.pdf' target='_blank'>https://arxiv.org/pdf/2502.20068.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianyang Qi, Shibo Chen, Jun Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.20068">A Generative Model Enhanced Multi-Agent Reinforcement Learning Method for Electric Vehicle Charging Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the widespread adoption of electric vehicles (EVs), navigating for EV drivers to select a cost-effective charging station has become an important yet challenging issue due to dynamic traffic conditions, fluctuating electricity prices, and potential competition from other EVs. The state-of-the-art deep reinforcement learning (DRL) algorithms for solving this task still require global information about all EVs at the execution stage, which not only increases communication costs but also raises privacy issues among EV drivers. To overcome these drawbacks, we introduce a novel generative model-enhanced multi-agent DRL algorithm that utilizes only the EV's local information while achieving performance comparable to these state-of-the-art algorithms. Specifically, the policy network is implemented on the EV side, and a Conditional Variational Autoencoder-Long Short Term Memory (CVAE-LSTM)-based recommendation model is developed to provide recommendation information. Furthermore, a novel future charging competition encoder is designed to effectively compress global information, enhancing training performance. The multi-gradient descent algorithm (MGDA) is also utilized to adaptively balance the weight between the two parts of the training objective, resulting in a more stable training process. Simulations are conducted based on a practical area in XiÃ¡n, China. Experimental results show that our proposed algorithm, which relies on local information, outperforms existing local information-based methods and achieves less than 8\% performance loss compared to global information-based methods.
<div id='section'>Paperid: <span id='pid'>1147, <a href='https://arxiv.org/pdf/2502.12605.pdf' target='_blank'>https://arxiv.org/pdf/2502.12605.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kyeonghyeon Park, David Molina Concha, Hyun-Rok Lee, Chi-Guhn Lee, Taesik Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.12605">Hypernetwork-based approach for optimal composition design in partially controlled multi-agent systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Partially Controlled Multi-Agent Systems (PCMAS) are comprised of controllable agents, managed by a system designer, and uncontrollable agents, operating autonomously. This study addresses an optimal composition design problem in PCMAS, which involves the system designer's problem, determining the optimal number and policies of controllable agents, and the uncontrollable agents' problem, identifying their best-response policies. Solving this bi-level optimization problem is computationally intensive, as it requires repeatedly solving multi-agent reinforcement learning problems under various compositions for both types of agents. To address these challenges, we propose a novel hypernetwork-based framework that jointly optimizes the system's composition and agent policies. Unlike traditional methods that train separate policy networks for each composition, the proposed framework generates policies for both controllable and uncontrollable agents through a unified hypernetwork. This approach enables efficient information sharing across similar configurations, thereby reducing computational overhead. Additional improvements are achieved by incorporating reward parameter optimization and mean action networks. Using real-world New York City taxi data, we demonstrate that our framework outperforms existing methods in approximating equilibrium policies. Our experimental results show significant improvements in key performance metrics, such as order response rate and served demand, highlighting the practical utility of controlling agents and their potential to enhance decision-making in PCMAS.
<div id='section'>Paperid: <span id='pid'>1148, <a href='https://arxiv.org/pdf/2502.08681.pdf' target='_blank'>https://arxiv.org/pdf/2502.08681.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Barbera de Mol, Davide Barbieri, Jan Viebahn, Davide Grossi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.08681">Centrally Coordinated Multi-Agent Reinforcement Learning for Power Grid Topology Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Power grid operation is becoming more complex due to the increase in generation of renewable energy. The recent series of Learning To Run a Power Network (L2RPN) competitions have encouraged the use of artificial agents to assist human dispatchers in operating power grids. However, the combinatorial nature of the action space poses a challenge to both conventional optimizers and learned controllers. Action space factorization, which breaks down decision-making into smaller sub-tasks, is one approach to tackle the curse of dimensionality. In this study, we propose a centrally coordinated multi-agent (CCMA) architecture for action space factorization. In this approach, regional agents propose actions and subsequently a coordinating agent selects the final action. We investigate several implementations of the CCMA architecture, and benchmark in different experimental settings against various L2RPN baseline approaches. The CCMA architecture exhibits higher sample efficiency and superior final performance than the baseline approaches. The results suggest high potential of the CCMA approach for further application in higher-dimensional L2RPN as well as real-world power grid settings.
<div id='section'>Paperid: <span id='pid'>1149, <a href='https://arxiv.org/pdf/2502.05573.pdf' target='_blank'>https://arxiv.org/pdf/2502.05573.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Beining Zhang, Aditya Kapoor, Mingfei Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.05573">Low-Rank Agent-Specific Adaptation (LoRASA) for Multi-Agent Policy Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent reinforcement learning (MARL) often relies on \emph{parameter sharing (PS)} to scale efficiently. However, purely shared policies can stifle each agent's unique specialization, reducing overall performance in heterogeneous environments. We propose \textbf{Low-Rank Agent-Specific Adaptation (LoRASA)}, a novel approach that treats each agent's policy as a specialized ``task'' fine-tuned from a shared backbone. Drawing inspiration from parameter-efficient transfer methods, LoRASA appends small, low-rank adaptation matrices to each layer of the shared policy, naturally inducing \emph{parameter-space sparsity} that promotes both specialization and scalability. We evaluate LoRASA on challenging benchmarks including the StarCraft Multi-Agent Challenge (SMAC) and Multi-Agent MuJoCo (MAMuJoCo), implementing it atop widely used algorithms such as MAPPO and A2PO. Across diverse tasks, LoRASA matches or outperforms existing baselines \emph{while reducing memory and computational overhead}. Ablation studies on adapter rank, placement, and timing validate the method's flexibility and efficiency. Our results suggest LoRASA's potential to establish a new norm for MARL policy parameterization: combining a shared foundation for coordination with low-rank agent-specific refinements for individual specialization.
<div id='section'>Paperid: <span id='pid'>1150, <a href='https://arxiv.org/pdf/2502.04773.pdf' target='_blank'>https://arxiv.org/pdf/2502.04773.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>George Papadopoulos, Andreas Kontogiannis, Foteini Papadopoulou, Chaido Poulianou, Ioannis Koumentis, George Vouros
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.04773">An Extended Benchmarking of Multi-Agent Reinforcement Learning Algorithms in Complex Fully Cooperative Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-Agent Reinforcement Learning (MARL) has recently emerged as a significant area of research. However, MARL evaluation often lacks systematic diversity, hindering a comprehensive understanding of algorithms' capabilities. In particular, cooperative MARL algorithms are predominantly evaluated on benchmarks such as SMAC and GRF, which primarily feature team game scenarios without assessing adequately various aspects of agents' capabilities required in fully cooperative real-world tasks such as multi-robot cooperation and warehouse, resource management, search and rescue, and human-AI cooperation. Moreover, MARL algorithms are mainly evaluated on low dimensional state spaces, and thus their performance on high-dimensional (e.g., image) observations is not well-studied. To fill this gap, this paper highlights the crucial need for expanding systematic evaluation across a wider array of existing benchmarks. To this end, we conduct extensive evaluation and comparisons of well-known MARL algorithms on complex fully cooperative benchmarks, including tasks with images as agents' observations. Interestingly, our analysis shows that many algorithms, hailed as state-of-the-art on SMAC and GRF, may underperform standard MARL baselines on fully cooperative benchmarks. Finally, towards more systematic and better evaluation of cooperative MARL algorithms, we have open-sourced PyMARLzoo+, an extension of the widely used (E)PyMARL libraries, which addresses an open challenge from [TBG++21], facilitating seamless integration and support with all benchmarks of PettingZoo, as well as Overcooked, PressurePlate, Capture Target and Box Pushing.
<div id='section'>Paperid: <span id='pid'>1151, <a href='https://arxiv.org/pdf/2502.03506.pdf' target='_blank'>https://arxiv.org/pdf/2502.03506.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruoning Zhang, Siying Wang, Wenyu Chen, Yang Zhou, Zhitong Zhao, Zixuan Zhang, Ruijie Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.03506">Optimistic Îµ-Greedy Exploration for Cooperative Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The Centralized Training with Decentralized Execution (CTDE) paradigm is widely used in cooperative multi-agent reinforcement learning. However, due to the representational limitations of traditional monotonic value decomposition methods, algorithms can underestimate optimal actions, leading policies to suboptimal solutions. To address this challenge, we propose Optimistic $Îµ$-Greedy Exploration, focusing on enhancing exploration to correct value estimations. The underestimation arises from insufficient sampling of optimal actions during exploration, as our analysis indicated. We introduce an optimistic updating network to identify optimal actions and sample actions from its distribution with a probability of $Îµ$ during exploration, increasing the selection frequency of optimal actions. Experimental results in various environments reveal that the Optimistic $Îµ$-Greedy Exploration effectively prevents the algorithm from suboptimal solutions and significantly improves its performance compared to other algorithms.
<div id='section'>Paperid: <span id='pid'>1152, <a href='https://arxiv.org/pdf/2502.03125.pdf' target='_blank'>https://arxiv.org/pdf/2502.03125.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yang Zhou, Siying Wang, Wenyu Chen, Ruoning Zhang, Zhitong Zhao, Zixuan Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.03125">Double Distillation Network for Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent reinforcement learning typically employs a centralized training-decentralized execution (CTDE) framework to alleviate the non-stationarity in environment. However, the partial observability during execution may lead to cumulative gap errors gathered by agents, impairing the training of effective collaborative policies. To overcome this challenge, we introduce the Double Distillation Network (DDN), which incorporates two distillation modules aimed at enhancing robust coordination and facilitating the collaboration process under constrained information. The external distillation module uses a global guiding network and a local policy network, employing distillation to reconcile the gap between global training and local execution. In addition, the internal distillation module introduces intrinsic rewards, drawn from state information, to enhance the exploration capabilities of agents. Extensive experiments demonstrate that DDN significantly improves performance across multiple scenarios.
<div id='section'>Paperid: <span id='pid'>1153, <a href='https://arxiv.org/pdf/2502.02875.pdf' target='_blank'>https://arxiv.org/pdf/2502.02875.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Siying Wang, Yang Zhou, Zhitong Zhao, Ruoning Zhang, Jinliang Shao, Wenyu Chen, Yuhua Cheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.02875">Heterogeneous Value Decomposition Policy Fusion for Multi-Agent Cooperation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Value decomposition (VD) has become one of the most prominent solutions in cooperative multi-agent reinforcement learning. Most existing methods generally explore how to factorize the joint value and minimize the discrepancies between agent observations and characteristics of environmental states. However, direct decomposition may result in limited representation or difficulty in optimization. Orthogonal to designing a new factorization scheme, in this paper, we propose Heterogeneous Policy Fusion (HPF) to integrate the strengths of various VD methods. We construct a composite policy set to select policies for interaction adaptively. Specifically, this adaptive mechanism allows agents' trajectories to benefit from diverse policy transitions while incorporating the advantages of each factorization method. Additionally, HPF introduces a constraint between these heterogeneous policies to rectify the misleading update caused by the unexpected exploratory or suboptimal non-cooperation. Experimental results on cooperative tasks show HPF's superior performance over multiple baselines, proving its effectiveness and ease of implementation.
<div id='section'>Paperid: <span id='pid'>1154, <a href='https://arxiv.org/pdf/2502.01971.pdf' target='_blank'>https://arxiv.org/pdf/2502.01971.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianyu Ren, Xuan Yao, Yang Li, Xiao-Jun Zeng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.01971">Bottom-Up Reputation Promotes Cooperation with Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reputation serves as a powerful mechanism for promoting cooperation in multi-agent systems, as agents are more inclined to cooperate with those of good social standing. While existing multi-agent reinforcement learning methods typically rely on predefined social norms to assign reputations, the question of how a population reaches a consensus on judgement when agents hold private, independent views remains unresolved. In this paper, we propose a novel bottom-up reputation learning method, Learning with Reputation Reward (LR2), designed to promote cooperative behaviour through rewards shaping based on assigned reputation. Our agent architecture includes a dilemma policy that determines cooperation by considering the impact on neighbours, and an evaluation policy that assigns reputations to affect the actions of neighbours while optimizing self-objectives. It operates using local observations and interaction-based rewards, without relying on centralized modules or predefined norms. Our findings demonstrate the effectiveness and adaptability of LR2 across various spatial social dilemma scenarios. Interestingly, we find that LR2 stabilizes and enhances cooperation not only with reward reshaping from bottom-up reputation but also by fostering strategy clustering in structured populations, thereby creating environments conducive to sustained cooperation.
<div id='section'>Paperid: <span id='pid'>1155, <a href='https://arxiv.org/pdf/2502.01568.pdf' target='_blank'>https://arxiv.org/pdf/2502.01568.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Benjamin A. Spiegel, Lucas Gelfond, George Konidaris
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.01568">Visual Theory of Mind Enables the Invention of Proto-Writing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Symbolic writing systems are graphical semiotic codes that are ubiquitous in modern society but are otherwise absent in the animal kingdom. Anthropological evidence suggests that the earliest forms of some writing systems originally consisted of iconic pictographs, which signify their referent via visual resemblance. While previous studies have examined the emergence and, separately, the evolution of pictographic systems through a computational lens, most employ non-naturalistic methodologies that make it difficult to draw clear analogies to human and animal cognition. We develop a multi-agent reinforcement learning testbed for emergent communication called a Signification Game, and formulate a model of inferential communication that enables agents to leverage visual theory of mind to communicate actions using pictographs. Our model, which is situated within a broader formalism for animal communication, sheds light on the cognitive and cultural processes underlying the emergence of proto-writing.
<div id='section'>Paperid: <span id='pid'>1156, <a href='https://arxiv.org/pdf/2501.12061.pdf' target='_blank'>https://arxiv.org/pdf/2501.12061.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Somnath Hazra, Pallab Dasgupta, Soumyajit Dey
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.12061">Tackling Uncertainties in Multi-Agent Reinforcement Learning through Integration of Agent Termination Dynamics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-Agent Reinforcement Learning (MARL) has gained significant traction for solving complex real-world tasks, but the inherent stochasticity and uncertainty in these environments pose substantial challenges to efficient and robust policy learning. While Distributional Reinforcement Learning has been successfully applied in single-agent settings to address risk and uncertainty, its application in MARL is substantially limited. In this work, we propose a novel approach that integrates distributional learning with a safety-focused loss function to improve convergence in cooperative MARL tasks. Specifically, we introduce a Barrier Function based loss that leverages safety metrics, identified from inherent faults in the system, into the policy learning process. This additional loss term helps mitigate risks and encourages safer exploration during the early stages of training. We evaluate our method in the StarCraft II micromanagement benchmark, where our approach demonstrates improved convergence and outperforms state-of-the-art baselines in terms of both safety and task completion. Our results suggest that incorporating safety considerations can significantly enhance learning performance in complex, multi-agent environments.
<div id='section'>Paperid: <span id='pid'>1157, <a href='https://arxiv.org/pdf/2412.20475.pdf' target='_blank'>https://arxiv.org/pdf/2412.20475.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sheng Cen, Qiying Pan, Yifei Zhu, Bo Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.20475">SatFlow: Scalable Network Planning for LEO Mega-Constellations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Low-earth-orbit (LEO) satellite communication networks have evolved into mega-constellations with hundreds to thousands of satellites inter-connecting with inter-satellite links (ISLs). Network planning, which plans for network resources and architecture to improve the network performance and save operational costs, is crucial for satellite network management. However, due to the large scale of mega-constellations, high dynamics of satellites, and complex distribution of real-world traffic, it is extremely challenging to conduct scalable network planning on mega-constellations with high performance. In this paper, we propose SatFlow, a distributed and hierarchical network planning framework to plan for the network topology, traffic allocation, and fine-grained ISL terminal power allocation for mega-constellations. To tackle the hardness of the original problem, we decompose the grand problem into two hierarchical sub-problems, tackled by two-tier modules. A multi-agent reinforcement learning approach is proposed for the upper-level module so that the overall laser energy consumption and ISL operational costs can be minimized; A distributed alternating step algorithm is proposed for the lower-level module so that the laser energy consumption could be minimized with low time complexity for a given topology. Extensive simulations on various mega-constellations validate SatFlow's scalability on the constellation size, reducing the flow violation ratio by up to 21.0% and reducing the total costs by up to 89.4%, compared with various state-of-the-art benchmarks.
<div id='section'>Paperid: <span id='pid'>1158, <a href='https://arxiv.org/pdf/2412.19538.pdf' target='_blank'>https://arxiv.org/pdf/2412.19538.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuan Zhou, Xiang Shi, Lele Zhang, Chen Chen, Hongbo Li, Lin Ma, Fang Deng, Jie Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.19538">Scalable Hierarchical Reinforcement Learning for Hyper Scale Multi-Robot Task Planning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To improve the efficiency of warehousing system and meet huge customer orders, we aim to solve the challenges of dimension disaster and dynamic properties in hyper scale multi-robot task planning (MRTP) for robotic mobile fulfillment system (RMFS). Existing research indicates that hierarchical reinforcement learning (HRL) is an effective method to reduce these challenges. Based on that, we construct an efficient multi-stage HRL-based multi-robot task planner for hyper scale MRTP in RMFS, and the planning process is represented with a special temporal graph topology. To ensure optimality, the planner is designed with a centralized architecture, but it also brings the challenges of scaling up and generalization that require policies to maintain performance for various unlearned scales and maps. To tackle these difficulties, we first construct a hierarchical temporal attention network (HTAN) to ensure basic ability of handling inputs with unfixed lengths, and then design multi-stage curricula for hierarchical policy learning to further improve the scaling up and generalization ability while avoiding catastrophic forgetting. Additionally, we notice that policies with hierarchical structure suffer from unfair credit assignment that is similar to that in multi-agent reinforcement learning, inspired of which, we propose a hierarchical reinforcement learning algorithm with counterfactual rollout baseline to improve learning performance. Experimental results demonstrate that our planner outperform other state-of-the-art methods on various MRTP instances in both simulated and real-world RMFS. Also, our planner can successfully scale up to hyper scale MRTP instances in RMFS with up to 200 robots and 1000 retrieval racks on unlearned maps while keeping superior performance over other methods.
<div id='section'>Paperid: <span id='pid'>1159, <a href='https://arxiv.org/pdf/2412.15700.pdf' target='_blank'>https://arxiv.org/pdf/2412.15700.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Guangchong Zhou, Zeren Zhang, Guoliang Fan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.15700">AIR: Unifying Individual and Collective Exploration in Cooperative Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Exploration in cooperative multi-agent reinforcement learning (MARL) remains challenging for value-based agents due to the absence of an explicit policy. Existing approaches include individual exploration based on uncertainty towards the system and collective exploration through behavioral diversity among agents. However, the introduction of additional structures often leads to reduced training efficiency and infeasible integration of these methods. In this paper, we propose Adaptive exploration via Identity Recognition~(AIR), which consists of two adversarial components: a classifier that recognizes agent identities from their trajectories, and an action selector that adaptively adjusts the mode and degree of exploration. We theoretically prove that AIR can facilitate both individual and collective exploration during training, and experiments also demonstrate the efficiency and effectiveness of AIR across various tasks.
<div id='section'>Paperid: <span id='pid'>1160, <a href='https://arxiv.org/pdf/2412.15388.pdf' target='_blank'>https://arxiv.org/pdf/2412.15388.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sharlin Utke, Jeremie Houssineau, Giovanni Montana
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.15388">Investigating Relational State Abstraction in Collaborative MARL</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper explores the impact of relational state abstraction on sample efficiency and performance in collaborative Multi-Agent Reinforcement Learning. The proposed abstraction is based on spatial relationships in environments where direct communication between agents is not allowed, leveraging the ubiquity of spatial reasoning in real-world multi-agent scenarios. We introduce MARC (Multi-Agent Relational Critic), a simple yet effective critic architecture incorporating spatial relational inductive biases by transforming the state into a spatial graph and processing it through a relational graph neural network. The performance of MARC is evaluated across six collaborative tasks, including a novel environment with heterogeneous agents. We conduct a comprehensive empirical analysis, comparing MARC against state-of-the-art MARL baselines, demonstrating improvements in both sample efficiency and asymptotic performance, as well as its potential for generalization. Our findings suggest that a minimal integration of spatial relational inductive biases as abstraction can yield substantial benefits without requiring complex designs or task-specific engineering. This work provides insights into the potential of relational state abstraction to address sample efficiency, a key challenge in MARL, offering a promising direction for developing more efficient algorithms in spatially complex environments.
<div id='section'>Paperid: <span id='pid'>1161, <a href='https://arxiv.org/pdf/2411.19359.pdf' target='_blank'>https://arxiv.org/pdf/2411.19359.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dickness Kakitahi Kwesiga, Suyash Chandra Vishnoi, Angshuman Guin, Michael Hunter
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.19359">Integrating Transit Signal Priority into Multi-Agent Reinforcement Learning based Traffic Signal Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This study integrates Transit Signal Priority (TSP) into multi-agent reinforcement learning (MARL) based traffic signal control. The first part of the study develops adaptive signal control based on MARL for a pair of coordinated intersections in a microscopic simulation environment. The two agents, one for each intersection, are centrally trained using a value decomposition network (VDN) architecture. The trained agents show slightly better performance compared to coordinated actuated signal control based on overall intersection delay at v/c of 0.95. In the second part of the study the trained signal control agents are used as background signal controllers while developing event-based TSP agents. In one variation, independent TSP agents are formulated and trained under a decentralized training and decentralized execution (DTDE) framework to implement TSP at each intersection. In the second variation, the two TSP agents are centrally trained under a centralized training and decentralized execution (CTDE) framework and VDN architecture to select and implement coordinated TSP strategies across the two intersections. In both cases the agents converge to the same bus delay value, but independent agents show high instability throughout the training process. For the test runs, the two independent agents reduce bus delay across the two intersections by 22% compared to the no TSP case while the coordinated TSP agents achieve 27% delay reduction. In both cases, there is only a slight increase in delay for a majority of the side street movements.
<div id='section'>Paperid: <span id='pid'>1162, <a href='https://arxiv.org/pdf/2411.12130.pdf' target='_blank'>https://arxiv.org/pdf/2411.12130.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kejun Chen, Truc Nguyen, Malik Hassanaly
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.12130">Adversarial Multi-Agent Reinforcement Learning for Proactive False Data Injection Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Smart inverters are instrumental in the integration of renewable and distributed energy resources (DERs) into the electric grid. Such inverters rely on communication layers for continuous control and monitoring, potentially exposing them to cyber-physical attacks such as false data injection attacks (FDIAs). We propose to construct a defense strategy against a priori unknown FDIAs with a multi-agent reinforcement learning (MARL) framework. The first agent is an adversary that simulates and discovers various FDIA strategies, while the second agent is a defender in charge of detecting and localizing FDIAs. This approach enables the defender to be trained against new FDIAs continuously generated by the adversary. The numerical results demonstrate that the proposed MARL defender outperforms a supervised offline defender. Additionally, we show that the detection skills of an MARL defender can be combined with that of an offline defender through a transfer learning approach.
<div id='section'>Paperid: <span id='pid'>1163, <a href='https://arxiv.org/pdf/2411.06601.pdf' target='_blank'>https://arxiv.org/pdf/2411.06601.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rohit Bokade, Xiaoning Jin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.06601">OffLight: An Offline Multi-Agent Reinforcement Learning Framework for Traffic Signal Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Efficient traffic control (TSC) is essential for urban mobility, but traditional systems struggle to handle the complexity of real-world traffic. Multi-agent Reinforcement Learning (MARL) offers adaptive solutions, but online MARL requires extensive interactions with the environment, making it costly and impractical. Offline MARL mitigates these challenges by using historical traffic data for training but faces significant difficulties with heterogeneous behavior policies in real-world datasets, where mixed-quality data complicates learning. We introduce OffLight, a novel offline MARL framework designed to handle heterogeneous behavior policies in TSC datasets. To improve learning efficiency, OffLight incorporates Importance Sampling (IS) to correct for distributional shifts and Return-Based Prioritized Sampling (RBPS) to focus on high-quality experiences. OffLight utilizes a Gaussian Mixture Variational Graph Autoencoder (GMM-VGAE) to capture the diverse distribution of behavior policies from local observations. Extensive experiments across real-world urban traffic scenarios show that OffLight outperforms existing offline RL methods, achieving up to a 7.8% reduction in average travel time and 11.2% decrease in queue length. Ablation studies confirm the effectiveness of OffLight's components in handling heterogeneous data and improving policy performance. These results highlight OffLight's scalability and potential to improve urban traffic management without the risks of online learning.
<div id='section'>Paperid: <span id='pid'>1164, <a href='https://arxiv.org/pdf/2410.18202.pdf' target='_blank'>https://arxiv.org/pdf/2410.18202.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rohit Bokade, Xiaoning Jin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.18202">PyTSC: A Unified Platform for Multi-Agent Reinforcement Learning in Traffic Signal Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-Agent Reinforcement Learning (MARL) presents a promising approach for addressing the complexity of Traffic Signal Control (TSC) in urban environments. However, existing platforms for MARL-based TSC research face challenges such as slow simulation speeds and convoluted, difficult-to-maintain codebases. To address these limitations, we introduce PyTSC, a robust and flexible simulation environment that facilitates the training and evaluation of MARL algorithms for TSC. PyTSC integrates multiple simulators, such as SUMO and CityFlow, and offers a streamlined API, empowering researchers to explore a broad spectrum of MARL approaches efficiently. PyTSC accelerates experimentation and provides new opportunities for advancing intelligent traffic management systems in real-world applications.
<div id='section'>Paperid: <span id='pid'>1165, <a href='https://arxiv.org/pdf/2410.17373.pdf' target='_blank'>https://arxiv.org/pdf/2410.17373.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dongsu Lee, Minhae Kwon
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.17373">Episodic Future Thinking Mechanism for Multi-agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding cognitive processes in multi-agent interactions is a primary goal in cognitive science. It can guide the direction of artificial intelligence (AI) research toward social decision-making in multi-agent systems, which includes uncertainty from character heterogeneity. In this paper, we introduce an episodic future thinking (EFT) mechanism for a reinforcement learning (RL) agent, inspired by cognitive processes observed in animals. To enable future thinking functionality, we first develop a multi-character policy that captures diverse characters with an ensemble of heterogeneous policies. Here, the character of an agent is defined as a different weight combination on reward components, representing distinct behavioral preferences. The future thinking agent collects observation-action trajectories of the target agents and uses the pre-trained multi-character policy to infer their characters. Once the character is inferred, the agent predicts the upcoming actions of target agents and simulates the potential future scenario. This capability allows the agent to adaptively select the optimal action, considering the predicted future scenario in multi-agent interactions. To evaluate the proposed mechanism, we consider the multi-agent autonomous driving scenario with diverse driving traits and multiple particle environments. Simulation results demonstrate that the EFT mechanism with accurate character inference leads to a higher reward than existing multi-agent solutions. We also confirm that the effect of reward improvement remains valid across societies with different levels of character diversity.
<div id='section'>Paperid: <span id='pid'>1166, <a href='https://arxiv.org/pdf/2410.01763.pdf' target='_blank'>https://arxiv.org/pdf/2410.01763.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rebekah A. GelpÃ­, Yikai Tang, Ethan C. Jackson, William A. Cunningham
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.01763">Social coordination perpetuates stereotypic expectations and behaviors across generations in deep multi-agent reinforcement learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite often being perceived as morally objectionable, stereotypes are a common feature of social groups, a phenomenon that has often been attributed to biased motivations or limits on the ability to process information. We argue that one reason for this continued prevalence is that pre-existing expectations about how others will behave, in the context of social coordination, can change the behaviors of one's social partners, creating the very stereotype one expected to see, even in the absence of other potential sources of stereotyping. We use a computational model of dynamic social coordination to illustrate how this "feedback loop" can emerge, engendering and entrenching stereotypic behavior, and then show that human behavior on the task generates a comparable feedback loop. Notably, people's choices on the task are not related to social dominance or system justification, suggesting biased motivations are not necessary to maintain these stereotypes.
<div id='section'>Paperid: <span id='pid'>1167, <a href='https://arxiv.org/pdf/2410.01364.pdf' target='_blank'>https://arxiv.org/pdf/2410.01364.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yutian Zhang, Guohong Zheng, Zhiyuan Liu, Quan Li, Haipeng Zeng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.01364">MARLens: Understanding Multi-agent Reinforcement Learning for Traffic Signal Control via Visual Analytics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The issue of traffic congestion poses a significant obstacle to the development of global cities. One promising solution to tackle this problem is intelligent traffic signal control (TSC). Recently, TSC strategies leveraging reinforcement learning (RL) have garnered attention among researchers. However, the evaluation of these models has primarily relied on fixed metrics like reward and queue length. This limited evaluation approach provides only a narrow view of the model's decision-making process, impeding its practical implementation. Moreover, effective TSC necessitates coordinated actions across multiple intersections. Existing visual analysis solutions fall short when applied in multi-agent settings. In this study, we delve into the challenge of interpretability in multi-agent reinforcement learning (MARL), particularly within the context of TSC. We propose MARLens a visual analytics system tailored to understand MARL-based TSC. Our system serves as a versatile platform for both RL and TSC researchers. It empowers them to explore the model's features from various perspectives, revealing its decision-making processes and shedding light on interactions among different agents. To facilitate quick identification of critical states, we have devised multiple visualization views, complemented by a traffic simulation module that allows users to replay specific training scenarios. To validate the utility of our proposed system, we present three comprehensive case studies, incorporate insights from domain experts through interviews, and conduct a user study. These collective efforts underscore the feasibility and effectiveness of MARLens in enhancing our understanding of MARL-based TSC systems and pave the way for more informed and efficient traffic management strategies.
<div id='section'>Paperid: <span id='pid'>1168, <a href='https://arxiv.org/pdf/2409.13571.pdf' target='_blank'>https://arxiv.org/pdf/2409.13571.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jaeyeon Jang, Diego Klabjan, Han Liu, Nital S. Patel, Xiuqi Li, Balakrishnan Ananthanarayanan, Husam Dauod, Tzung-Han Juang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.13571">Scalable Multi-agent Reinforcement Learning for Factory-wide Dynamic Scheduling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Real-time dynamic scheduling is a crucial but notoriously challenging task in modern manufacturing processes due to its high decision complexity. Recently, reinforcement learning (RL) has been gaining attention as an impactful technique to handle this challenge. However, classical RL methods typically rely on human-made dispatching rules, which are not suitable for large-scale factory-wide scheduling. To bridge this gap, this paper applies a leader-follower multi-agent RL (MARL) concept to obtain desired coordination after decomposing the scheduling problem into a set of sub-problems that are handled by each individual agent for scalability. We further strengthen the procedure by proposing a rule-based conversion algorithm to prevent catastrophic loss of production capacity due to an agent's error. Our experimental results demonstrate that the proposed model outperforms the state-of-the-art deep RL-based scheduling models in various aspects. Additionally, the proposed model provides the most robust scheduling performance to demand changes. Overall, the proposed MARL-based scheduling model presents a promising solution to the real-time scheduling problem, with potential applications in various manufacturing industries.
<div id='section'>Paperid: <span id='pid'>1169, <a href='https://arxiv.org/pdf/2408.11751.pdf' target='_blank'>https://arxiv.org/pdf/2408.11751.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>David Molina Concha, Jiping Li, Haoran Yin, Kyeonghyeon Park, Hyun-Rok Lee, Taesik Lee, Dhruv Sirohi, Chi-Guhn Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.11751">Bayesian Optimization Framework for Efficient Fleet Design in Autonomous Multi-Robot Exploration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This study addresses the challenge of fleet design optimization in the context of heterogeneous multi-robot fleets, aiming to obtain feasible designs that balance performance and costs. In the domain of autonomous multi-robot exploration, reinforcement learning agents play a central role, offering adaptability to complex terrains and facilitating collaboration among robots. However, modifying the fleet composition results in changes in the learned behavior, and training multi-robot systems using multi-agent reinforcement learning is expensive. Therefore, an exhaustive evaluation of each potential fleet design is infeasible. To tackle these hurdles, we introduce Bayesian Optimization for Fleet Design (BOFD), a framework leveraging multi-objective Bayesian Optimization to explore fleets on the Pareto front of performance and cost while accounting for uncertainty in the design space. Moreover, we establish a sub-linear bound for cumulative regret, supporting BOFD's robustness and efficacy. Extensive benchmark experiments in synthetic and simulated environments demonstrate the superiority of our framework over state-of-the-art methods, achieving efficient fleet designs with minimal fleet evaluations.
<div id='section'>Paperid: <span id='pid'>1170, <a href='https://arxiv.org/pdf/2408.09686.pdf' target='_blank'>https://arxiv.org/pdf/2408.09686.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>David Molina Concha, Kyeonghyeon Park, Hyun-Rok Lee, Taesik Lee, Chi-Guhn Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.09686">Algorithmic Contract Design with Reinforcement Learning Agents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce a novel problem setting for algorithmic contract design, named the principal-MARL contract design problem. This setting extends traditional contract design to account for dynamic and stochastic environments using Markov Games and Multi-Agent Reinforcement Learning. To tackle this problem, we propose a Multi-Objective Bayesian Optimization (MOBO) framework named Constrained Pareto Maximum Entropy Search (cPMES). Our approach integrates MOBO and MARL to explore the highly constrained contract design space, identifying promising incentive and recruitment decisions. cPMES transforms the principal-MARL contract design problem into an unconstrained multi-objective problem, leveraging the probability of feasibility as part of the objectives and ensuring promising designs predicted on the feasibility border are included in the Pareto front. By focusing the entropy prediction on designs within the Pareto set, cPMES mitigates the risk of the search strategy being overwhelmed by entropy from constraints. We demonstrate the effectiveness of cPMES through extensive benchmark studies in synthetic and simulated environments, showing its ability to find feasible contract designs that maximize the principal's objectives. Additionally, we provide theoretical support with a sub-linear regret bound concerning the number of iterations.
<div id='section'>Paperid: <span id='pid'>1171, <a href='https://arxiv.org/pdf/2407.19144.pdf' target='_blank'>https://arxiv.org/pdf/2407.19144.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yasin Findik, Hunter Hasenfus, Reza Azadeh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.19144">Collaborative Adaptation for Recovery from Unforeseen Malfunctions in Discrete and Continuous MARL Domains</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cooperative multi-agent learning plays a crucial role for developing effective strategies to achieve individual or shared objectives in multi-agent teams. In real-world settings, agents may face unexpected failures, such as a robot's leg malfunctioning or a teammate's battery running out. These malfunctions decrease the team's ability to accomplish assigned task(s), especially if they occur after the learning algorithms have already converged onto a collaborative strategy. Current leading approaches in Multi-Agent Reinforcement Learning (MARL) often recover slowly -- if at all -- from such malfunctions. To overcome this limitation, we present the Collaborative Adaptation (CA) framework, highlighting its unique capability to operate in both continuous and discrete domains. Our framework enhances the adaptability of agents to unexpected failures by integrating inter-agent relationships into their learning processes, thereby accelerating the recovery from malfunctions. We evaluated our framework's performance through experiments in both discrete and continuous environments. Empirical results reveal that in scenarios involving unforeseen malfunction, although state-of-the-art algorithms often converge on sub-optimal solutions, the proposed CA framework mitigates and recovers more effectively.
<div id='section'>Paperid: <span id='pid'>1172, <a href='https://arxiv.org/pdf/2407.10332.pdf' target='_blank'>https://arxiv.org/pdf/2407.10332.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ryan Hare, Ying Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.10332">Ontology-driven Reinforcement Learning for Personalized Student Support</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the search for more effective education, there is a widespread effort to develop better approaches to personalize student education. Unassisted, educators often do not have time or resources to personally support every student in a given classroom. Motivated by this issue, and by recent advancements in artificial intelligence, this paper presents a general-purpose framework for personalized student support, applicable to any virtual educational system such as a serious game or an intelligent tutoring system. To fit any educational situation, we apply ontologies for their semantic organization, combining them with data collection considerations and multi-agent reinforcement learning. The result is a modular system that can be adapted to any virtual educational software to provide useful personalized assistance to students.
<div id='section'>Paperid: <span id='pid'>1173, <a href='https://arxiv.org/pdf/2407.04974.pdf' target='_blank'>https://arxiv.org/pdf/2407.04974.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ainur Zhaikhan, Ali H. Sayed
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.04974">Multi-agent Off-policy Actor-Critic Reinforcement Learning for Partially Observable Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This study proposes the use of a social learning method to estimate a global state within a multi-agent off-policy actor-critic algorithm for reinforcement learning (RL) operating in a partially observable environment. We assume that the network of agents operates in a fully-decentralized manner, possessing the capability to exchange variables with their immediate neighbors. The proposed design methodology is supported by an analysis demonstrating that the difference between final outcomes, obtained when the global state is fully observed versus estimated through the social learning method, is $\varepsilon$-bounded when an appropriate number of iterations of social learning updates are implemented. Unlike many existing dec-POMDP-based RL approaches, the proposed algorithm is suitable for model-free multi-agent reinforcement learning as it does not require knowledge of a transition model. Furthermore, experimental results illustrate the efficacy of the algorithm and demonstrate its superiority over the current state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>1174, <a href='https://arxiv.org/pdf/2406.13930.pdf' target='_blank'>https://arxiv.org/pdf/2406.13930.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wen-Tse Chen, Yuxuan Li, Shiyu Huang, Jiayu Chen, Jeff Schneider
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.13930">ME-IGM: Individual-Global-Max in Maximum Entropy Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent credit assignment is a fundamental challenge for cooperative multi-agent reinforcement learning (MARL), where a team of agents learn from shared reward signals. The Individual-Global-Max (IGM) condition is a widely used principle for multi-agent credit assignment, requiring that the joint action determined by individual Q-functions maximizes the global Q-value. Meanwhile, the principle of maximum entropy has been leveraged to enhance exploration in MARL. However, we identify a critical limitation in existing maximum entropy MARL methods: a misalignment arises between local policies and the joint policy that maximizes the global Q-value, leading to violations of the IGM condition. To address this misalignment, we propose an order-preserving transformation. Building on it, we introduce ME-IGM, a novel maximum entropy MARL algorithm compatible with any credit assignment mechanism that satisfies the IGM condition while enjoying the benefits of maximum entropy exploration. We empirically evaluate two variants of ME-IGM: ME-QMIX and ME-QPLEX, in non-monotonic matrix games, and demonstrate their state-of-the-art performance across 17 scenarios in SMAC-v2 and Overcooked.
<div id='section'>Paperid: <span id='pid'>1175, <a href='https://arxiv.org/pdf/2405.12236.pdf' target='_blank'>https://arxiv.org/pdf/2405.12236.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Maad Ebrahim, Abdelhakim Hafid
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.12236">Fully Distributed Fog Load Balancing with Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Real-time Internet of Things (IoT) applications require real-time support to handle the ever-growing demand for computing resources to process IoT workloads. Fog Computing provides high availability of such resources in a distributed manner. However, these resources must be efficiently managed to distribute unpredictable traffic demands among heterogeneous Fog resources. This paper proposes a fully distributed load-balancing solution with Multi-Agent Reinforcement Learning (MARL) that intelligently distributes IoT workloads to optimize the waiting time while providing fair resource utilization in the Fog network. These agents use transfer learning for life-long self-adaptation to dynamic changes in the environment. By leveraging distributed decision-making, MARL agents effectively minimize the waiting time compared to a single centralized agent solution and other baselines, enhancing end-to-end execution delay. Besides performance gain, a fully distributed solution allows for a global-scale implementation where agents can work independently in small collaboration regions, leveraging nearby local resources. Furthermore, we analyze the impact of a realistic frequency to observe the state of the environment, unlike the unrealistic common assumption in the literature of having observations readily available in real-time for every required action. The findings highlight the trade-off between realism and performance using an interval-based Gossip-based multi-casting protocol against assuming real-time observation availability for every generated workload.
<div id='section'>Paperid: <span id='pid'>1176, <a href='https://arxiv.org/pdf/2405.11106.pdf' target='_blank'>https://arxiv.org/pdf/2405.11106.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chuanneng Sun, Songjun Huang, Dario Pompili
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.11106">LLM-based Multi-Agent Reinforcement Learning: Current and Future Directions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, Large Language Models (LLMs) have shown great abilities in various tasks, including question answering, arithmetic problem solving, and poem writing, among others. Although research on LLM-as-an-agent has shown that LLM can be applied to Reinforcement Learning (RL) and achieve decent results, the extension of LLM-based RL to Multi-Agent System (MAS) is not trivial, as many aspects, such as coordination and communication between agents, are not considered in the RL frameworks of a single agent. To inspire more research on LLM-based MARL, in this letter, we survey the existing LLM-based single-agent and multi-agent RL frameworks and provide potential research directions for future research. In particular, we focus on the cooperative tasks of multiple agents with a common goal and communication among them. We also consider human-in/on-the-loop scenarios enabled by the language component in the framework.
<div id='section'>Paperid: <span id='pid'>1177, <a href='https://arxiv.org/pdf/2405.08443.pdf' target='_blank'>https://arxiv.org/pdf/2405.08443.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yang Qu, Jinming Ma, Feng Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.08443">Safety Constrained Multi-Agent Reinforcement Learning for Active Voltage Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Active voltage control presents a promising avenue for relieving power congestion and enhancing voltage quality, taking advantage of the distributed controllable generators in the power network, such as roof-top photovoltaics. While Multi-Agent Reinforcement Learning (MARL) has emerged as a compelling approach to address this challenge, existing MARL approaches tend to overlook the constrained optimization nature of this problem, failing in guaranteeing safety constraints. In this paper, we formalize the active voltage control problem as a constrained Markov game and propose a safety-constrained MARL algorithm. We expand the primal-dual optimization RL method to multi-agent settings, and augment it with a novel approach of double safety estimation to learn the policy and to update the Lagrange-multiplier. In addition, we proposed different cost functions and investigated their influences on the behavior of our constrained MARL method. We evaluate our approach in the power distribution network simulation environment with real-world scale scenarios. Experimental results demonstrate the effectiveness of the proposed method compared with the state-of-the-art MARL methods. This paper is published at \url{https://www.ijcai.org/Proceedings/2024/}.
<div id='section'>Paperid: <span id='pid'>1178, <a href='https://arxiv.org/pdf/2405.07621.pdf' target='_blank'>https://arxiv.org/pdf/2405.07621.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kaushik Dey, Satheesh K. Perepu, Abir Das, Pallab Dasgupta
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.07621">Towards Adaptive IMFs -- Generalization of utility functions in Multi-Agent Frameworks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Intent Management Function (IMF) is an integral part of future-generation networks. In recent years, there has been some work on AI-based IMFs that can handle conflicting intents and prioritize the global objective based on apriori definition of the utility function and accorded priorities for competing intents. Some of the earlier works use Multi-Agent Reinforcement Learning (MARL) techniques with AdHoc Teaming (AHT) approaches for efficient conflict handling in IMF. However, the success of such frameworks in real-life scenarios requires them to be flexible to business situations. The intent priorities can change and the utility function, which measures the extent of intent fulfilment, may also vary in definition. This paper proposes a novel mechanism whereby the IMF can generalize to different forms of utility functions and change of intent priorities at run-time without additional training. Such generalization ability, without additional training requirements, would help to deploy IMF in live networks where customer intents and priorities change frequently. Results on the network emulator demonstrate the efficacy of the approach, scalability for new intents, outperforming existing techniques that require additional training to achieve the same degree of flexibility thereby saving cost, and increasing efficiency and adaptability.
<div id='section'>Paperid: <span id='pid'>1179, <a href='https://arxiv.org/pdf/2405.02654.pdf' target='_blank'>https://arxiv.org/pdf/2405.02654.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianyu Ren, Xiao-Jun Zeng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.02654">Enhancing Cooperation through Selective Interaction and Long-term Experiences in Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The significance of network structures in promoting group cooperation within social dilemmas has been widely recognized. Prior studies attribute this facilitation to the assortment of strategies driven by spatial interactions. Although reinforcement learning has been employed to investigate the impact of dynamic interaction on the evolution of cooperation, there remains a lack of understanding about how agents develop neighbour selection behaviours and the formation of strategic assortment within an explicit interaction structure. To address this, our study introduces a computational framework based on multi-agent reinforcement learning in the spatial Prisoner's Dilemma game. This framework allows agents to select dilemma strategies and interacting neighbours based on their long-term experiences, differing from existing research that relies on preset social norms or external incentives. By modelling each agent using two distinct Q-networks, we disentangle the coevolutionary dynamics between cooperation and interaction. The results indicate that long-term experience enables agents to develop the ability to identify non-cooperative neighbours and exhibit a preference for interaction with cooperative ones. This emergent self-organizing behaviour leads to the clustering of agents with similar strategies, thereby increasing network reciprocity and enhancing group cooperation.
<div id='section'>Paperid: <span id='pid'>1180, <a href='https://arxiv.org/pdf/2404.13654.pdf' target='_blank'>https://arxiv.org/pdf/2404.13654.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shengbo Wang, Chuan Lin, Guangjie Han, Shengchao Zhu, Zhixian Li, Zhenyu Wang, Yunpeng Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.13654">Multi-AUV Cooperative Underwater Multi-Target Tracking Based on Dynamic-Switching-enabled Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, autonomous underwater vehicle (AUV) swarms are gradually becoming popular and have been widely promoted in ocean exploration or underwater tracking, etc. In this paper, we propose a multi-AUV cooperative underwater multi-target tracking algorithm especially when the real underwater factors are taken into account. We first give normally modelling approach for the underwater sonar-based detection and the ocean current interference on the target tracking process. Then, based on software-defined networking (SDN), we regard the AUV swarm as a underwater ad-hoc network and propose a hierarchical software-defined multi-AUV reinforcement learning (HSARL) architecture. Based on the proposed HSARL architecture, we propose the "Dynamic-Switching" mechanism, it includes "Dynamic-Switching Attention" and "Dynamic-Switching Resampling" mechanisms which accelerate the HSARL algorithm's convergence speed and effectively prevents it from getting stuck in a local optimum state. Additionally, we introduce the reward reshaping mechanism for further accelerating the convergence speed of the proposed HSARL algorithm in early phase. Finally, based on a proposed AUV classification method, we propose a cooperative tracking algorithm called Dynamic-Switching-Based MARL (DSBM)-driven tracking algorithm. Evaluation results demonstrate that our proposed DSBM tracking algorithm can perform precise underwater multi-target tracking, comparing with many of recent research products in terms of various important metrics.
<div id='section'>Paperid: <span id='pid'>1181, <a href='https://arxiv.org/pdf/2404.12520.pdf' target='_blank'>https://arxiv.org/pdf/2404.12520.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Amin Shojaeighadikolaei, Zsolt Talata, Morteza Hashemi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.12520">Centralized vs. Decentralized Multi-Agent Reinforcement Learning for Enhanced Control of Electric Vehicle Charging Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The widespread adoption of electric vehicles (EVs) poses several challenges to power distribution networks and smart grid infrastructure due to the possibility of significantly increasing electricity demands, especially during peak hours. Furthermore, when EVs participate in demand-side management programs, charging expenses can be reduced by using optimal charging control policies that fully utilize real-time pricing schemes. However, devising optimal charging methods and control strategies for EVs is challenging due to various stochastic and uncertain environmental factors. Currently, most EV charging controllers operate based on a centralized model. In this paper, we introduce a novel approach for distributed and cooperative charging strategy using a Multi-Agent Reinforcement Learning (MARL) framework. Our method is built upon the Deep Deterministic Policy Gradient (DDPG) algorithm for a group of EVs in a residential community, where all EVs are connected to a shared transformer. This method, referred to as CTDE-DDPG, adopts a Centralized Training Decentralized Execution (CTDE) approach to establish cooperation between agents during the training phase, while ensuring a distributed and privacy-preserving operation during execution. We theoretically examine the performance of centralized and decentralized critics for the DDPG-based MARL implementation and demonstrate their trade-offs. Furthermore, we numerically explore the efficiency, scalability, and performance of centralized and decentralized critics. Our theoretical and numerical results indicate that, despite higher policy gradient variances and training complexity, the CTDE-DDPG framework significantly improves charging efficiency by reducing total variation by approximately %36 and charging cost by around %9.1 on average...
<div id='section'>Paperid: <span id='pid'>1182, <a href='https://arxiv.org/pdf/2404.03869.pdf' target='_blank'>https://arxiv.org/pdf/2404.03869.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xudong Guo, Daming Shi, Junjie Yu, Wenhui Fan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.03869">Heterogeneous Multi-Agent Reinforcement Learning for Zero-Shot Scalable Collaboration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The emergence of multi-agent reinforcement learning (MARL) is significantly transforming various fields like autonomous vehicle networks. However, real-world multi-agent systems typically contain multiple roles, and the scale of these systems dynamically fluctuates. Consequently, in order to achieve zero-shot scalable collaboration, it is essential that strategies for different roles can be updated flexibly according to the scales, which is still a challenge for current MARL frameworks. To address this, we propose a novel MARL framework named Scalable and Heterogeneous Proximal Policy Optimization (SHPPO), integrating heterogeneity into parameter-shared PPO-based MARL networks. We first leverage a latent network to learn strategy patterns for each agent adaptively. Second, we introduce a heterogeneous layer to be inserted into decision-making networks, whose parameters are specifically generated by the learned latent variables. Our approach is scalable as all the parameters are shared except for the heterogeneous layer, and gains both inter-individual and temporal heterogeneity, allowing SHPPO to adapt effectively to varying scales. SHPPO exhibits superior performance in classic MARL environments like Starcraft Multi-Agent Challenge (SMAC) and Google Research Football (GRF), showcasing enhanced zero-shot scalability, and offering insights into the learned latent variables' impact on team performance by visualization.
<div id='section'>Paperid: <span id='pid'>1183, <a href='https://arxiv.org/pdf/2403.15935.pdf' target='_blank'>https://arxiv.org/pdf/2403.15935.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fnu Hairi, Zifan Zhang, Jia Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.15935">Sample and Communication Efficient Fully Decentralized MARL Policy Evaluation via a New Approach: Local TD update</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In actor-critic framework for fully decentralized multi-agent reinforcement learning (MARL), one of the key components is the MARL policy evaluation (PE) problem, where a set of $N$ agents work cooperatively to evaluate the value function of the global states for a given policy through communicating with their neighbors. In MARL-PE, a critical challenge is how to lower the sample and communication complexities, which are defined as the number of training samples and communication rounds needed to converge to some $Îµ$-stationary point. To lower communication complexity in MARL-PE, a "natural'' idea is to perform multiple local TD-update steps between each consecutive rounds of communication to reduce the communication frequency. However, the validity of the local TD-update approach remains unclear due to the potential "agent-drift'' phenomenon resulting from heterogeneous rewards across agents in general. This leads to an interesting open question: Can the local TD-update approach entail low sample and communication complexities? In this paper, we make the first attempt to answer this fundamental question. We focus on the setting of MARL-PE with average reward, which is motivated by many multi-agent network optimization problems. Our theoretical and experimental results confirm that allowing multiple local TD-update steps is indeed an effective approach in lowering the sample and communication complexities of MARL-PE compared to consensus-based MARL-PE algorithms. Specifically, the local TD-update steps between two consecutive communication rounds can be as large as $\mathcal{O}(1/Îµ^{1/2}\log{(1/Îµ)})$ in order to converge to an $Îµ$-stationary point of MARL-PE. Moreover, we show theoretically that in order to reach the optimal sample complexity, the communication complexity of local TD-update approach is $\mathcal{O}(1/Îµ^{1/2}\log{(1/Îµ)})$.
<div id='section'>Paperid: <span id='pid'>1184, <a href='https://arxiv.org/pdf/2403.08906.pdf' target='_blank'>https://arxiv.org/pdf/2403.08906.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuksel Arslantas, Ege Yuceel, Muhammed O. Sayin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.08906">Strategizing against Q-learners: A Control-theoretical Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we explore the susceptibility of the independent Q-learning algorithms (a classical and widely used multi-agent reinforcement learning method) to strategic manipulation of sophisticated opponents in normal-form games played repeatedly. We quantify how much strategically sophisticated agents can exploit naive Q-learners if they know the opponents' Q-learning algorithm. To this end, we formulate the strategic actors' interactions as a stochastic game (whose state encompasses Q-function estimates of the Q-learners) as if the Q-learning algorithms are the underlying dynamical system. We also present a quantization-based approximation scheme to tackle the continuum state space and analyze its performance for two competing strategic actors and a single strategic actor both analytically and numerically.
<div id='section'>Paperid: <span id='pid'>1185, <a href='https://arxiv.org/pdf/2403.07017.pdf' target='_blank'>https://arxiv.org/pdf/2403.07017.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Long Wang, Feng Fu, Xingru Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.07017">Mathematics of multi-agent learning systems at the interface of game theory and artificial intelligence</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Evolutionary Game Theory (EGT) and Artificial Intelligence (AI) are two fields that, at first glance, might seem distinct, but they have notable connections and intersections. The former focuses on the evolution of behaviors (or strategies) in a population, where individuals interact with others and update their strategies based on imitation (or social learning). The more successful a strategy is, the more prevalent it becomes over time. The latter, meanwhile, is centered on machine learning algorithms and (deep) neural networks. It is often from a single-agent perspective but increasingly involves multi-agent environments, in which intelligent agents adjust their strategies based on feedback and experience, somewhat akin to the evolutionary process yet distinct in their self-learning capacities. In light of the key components necessary to address real-world problems, including (i) learning and adaptation, (ii) cooperation and competition, (iii) robustness and stability, and altogether (iv) population dynamics of individual agents whose strategies evolve, the cross-fertilization of ideas between both fields will contribute to the advancement of mathematics of multi-agent learning systems, in particular, to the nascent domain of ``collective cooperative intelligence'' bridging evolutionary dynamics and multi-agent reinforcement learning.
<div id='section'>Paperid: <span id='pid'>1186, <a href='https://arxiv.org/pdf/2403.06397.pdf' target='_blank'>https://arxiv.org/pdf/2403.06397.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuefeng Wang, Henglin Pu, Hyung Jun Kim, Husheng Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.06397">DeepSafeMPC: Deep Learning-Based Model Predictive Control for Safe Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Safe Multi-agent reinforcement learning (safe MARL) has increasingly gained attention in recent years, emphasizing the need for agents to not only optimize the global return but also adhere to safety requirements through behavioral constraints. Some recent work has integrated control theory with multi-agent reinforcement learning to address the challenge of ensuring safety. However, there have been only very limited applications of Model Predictive Control (MPC) methods in this domain, primarily due to the complex and implicit dynamics characteristic of multi-agent environments. To bridge this gap, we propose a novel method called Deep Learning-Based Model Predictive Control for Safe Multi-Agent Reinforcement Learning (DeepSafeMPC). The key insight of DeepSafeMPC is leveraging a entralized deep learning model to well predict environmental dynamics. Our method applies MARL principles to search for optimal solutions. Through the employment of MPC, the actions of agents can be restricted within safe states concurrently. We demonstrate the effectiveness of our approach using the Safe Multi-agent MuJoCo environment, showcasing significant advancements in addressing safety concerns in MARL.
<div id='section'>Paperid: <span id='pid'>1187, <a href='https://arxiv.org/pdf/2403.01816.pdf' target='_blank'>https://arxiv.org/pdf/2403.01816.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenjing Zhang, Wei Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.01816">SMAUG: A Sliding Multidimensional Task Window-Based MARL Framework for Adaptive Real-Time Subtask Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Instead of making behavioral decisions directly from the exponentially expanding joint observational-action space, subtask-based multi-agent reinforcement learning (MARL) methods enable agents to learn how to tackle different subtasks. Most existing subtask-based MARL methods are based on hierarchical reinforcement learning (HRL). However, these approaches often limit the number of subtasks, perform subtask recognition periodically, and can only identify and execute a specific subtask within the predefined fixed time period, which makes them inflexible and not suitable for diverse and dynamic scenarios with constantly changing subtasks. To break through above restrictions, a \textbf{S}liding \textbf{M}ultidimensional t\textbf{A}sk window based m\textbf{U}ti-agent reinforcement learnin\textbf{G} framework (SMAUG) is proposed for adaptive real-time subtask recognition. It leverages a sliding multidimensional task window to extract essential information of subtasks from trajectory segments concatenated based on observed and predicted trajectories in varying lengths. An inference network is designed to iteratively predict future trajectories with the subtask-oriented policy network. Furthermore, intrinsic motivation rewards are defined to promote subtask exploration and behavior diversity. SMAUG can be integrated with any Q-learning-based approach. Experiments on StarCraft II show that SMAUG not only demonstrates performance superiority in comparison with all baselines but also presents a more prominent and swift rise in rewards during the initial training stage.
<div id='section'>Paperid: <span id='pid'>1188, <a href='https://arxiv.org/pdf/2402.19420.pdf' target='_blank'>https://arxiv.org/pdf/2402.19420.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Greg d'Eon, Neil Newman, Kevin Leyton-Brown
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.19420">Understanding Iterative Combinatorial Auction Designs via Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Iterative combinatorial auctions are widely used in high stakes settings such as spectrum auctions. Such auctions can be hard to analyze, making it difficult for bidders to determine how to behave and for designers to optimize auction rules to ensure desirable outcomes such as high revenue or welfare. In this paper, we investigate whether multi-agent reinforcement learning (MARL) algorithms can be used to understand iterative combinatorial auctions, given that these algorithms have recently shown empirical success in several other domains. We find that MARL can indeed benefit auction analysis, but that deploying it effectively is nontrivial. We begin by describing modelling decisions that keep the resulting game tractable without sacrificing important features such as imperfect information or asymmetry between bidders. We also discuss how to navigate pitfalls of various MARL algorithms, how to overcome challenges in verifying convergence, and how to generate and interpret multiple equilibria. We illustrate the promise of our resulting approach by using it to evaluate a specific rule change to a clock auction, finding substantially different auction outcomes due to complex changes in bidders' behavior.
<div id='section'>Paperid: <span id='pid'>1189, <a href='https://arxiv.org/pdf/2402.17773.pdf' target='_blank'>https://arxiv.org/pdf/2402.17773.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yaniv Cohen, Tomer Gafni, Ronen Greenberg, Kobi Cohen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.17773">SINR-Aware Deep Reinforcement Learning for Distributed Dynamic Channel Allocation in Cognitive Interference Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We consider the problem of dynamic channel allocation (DCA) in cognitive communication networks with the goal of maximizing a global signal-to-interference-plus-noise ratio (SINR) measure under a specified target quality of service (QoS)-SINR for each network. The shared bandwidth is partitioned into K channels with frequency separation. In contrast to the majority of existing studies that assume perfect orthogonality or a one- to-one user-channel allocation mapping, this paper focuses on real-world systems experiencing inter-carrier interference (ICI) and channel reuse by multiple large-scale networks. This realistic scenario significantly increases the problem dimension, rendering existing algorithms inefficient. We propose a novel multi-agent reinforcement learning (RL) framework for distributed DCA, named Channel Allocation RL To Overlapped Networks (CARLTON). The CARLTON framework is based on the Centralized Training with Decentralized Execution (CTDE) paradigm, utilizing the DeepMellow value-based RL algorithm. To ensure robust performance in the interference-laden environment we address, CARLTON employs a low-dimensional representation of observations, generating a QoS-type measure while maximizing a global SINR measure and ensuring the target QoS-SINR for each network. Our results demonstrate exceptional performance and robust generalization, showcasing superior efficiency compared to alternative state-of-the-art methods, while achieving a marginally diminished performance relative to a fully centralized approach.
<div id='section'>Paperid: <span id='pid'>1190, <a href='https://arxiv.org/pdf/2402.13440.pdf' target='_blank'>https://arxiv.org/pdf/2402.13440.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chitra Subramanian, Miao Liu, Naweed Khan, Jonathan Lenchner, Aporva Amarnath, Sarathkrishna Swaminathan, Ryan Riegel, Alexander Gray
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.13440">A Neuro-Symbolic Approach to Multi-Agent RL for Interpretability and Probabilistic Decision Making</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent reinforcement learning (MARL) is well-suited for runtime decision-making in optimizing the performance of systems where multiple agents coexist and compete for shared resources. However, applying common deep learning-based MARL solutions to real-world problems suffers from issues of interpretability, sample efficiency, partial observability, etc. To address these challenges, we present an event-driven formulation, where decision-making is handled by distributed co-operative MARL agents using neuro-symbolic methods. The recently introduced neuro-symbolic Logical Neural Networks (LNN) framework serves as a function approximator for the RL, to train a rules-based policy that is both logical and interpretable by construction. To enable decision-making under uncertainty and partial observability, we developed a novel probabilistic neuro-symbolic framework, Probabilistic Logical Neural Networks (PLNN), which combines the capabilities of logical reasoning with probabilistic graphical models. In PLNN, the upward/downward inference strategy, inherited from LNN, is coupled with belief bounds by setting the activation function for the logical operator associated with each neural network node to a probability-respecting generalization of the FrÃ©chet inequalities. These PLNN nodes form the unifying element that combines probabilistic logic and Bayes Nets, permitting inference for variables with unobserved states. We demonstrate our contributions by addressing key MARL challenges for power sharing in a system-on-chip application.
<div id='section'>Paperid: <span id='pid'>1191, <a href='https://arxiv.org/pdf/2402.00515.pdf' target='_blank'>https://arxiv.org/pdf/2402.00515.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhenglong Li, Vincent Tam, Kwan L. Yeung
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.00515">Developing A Multi-Agent and Self-Adaptive Framework with Deep Reinforcement Learning for Dynamic Portfolio Risk Management</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep or reinforcement learning (RL) approaches have been adapted as reactive agents to quickly learn and respond with new investment strategies for portfolio management under the highly turbulent financial market environments in recent years. In many cases, due to the very complex correlations among various financial sectors, and the fluctuating trends in different financial markets, a deep or reinforcement learning based agent can be biased in maximising the total returns of the newly formulated investment portfolio while neglecting its potential risks under the turmoil of various market conditions in the global or regional sectors. Accordingly, a multi-agent and self-adaptive framework namely the MASA is proposed in which a sophisticated multi-agent reinforcement learning (RL) approach is adopted through two cooperating and reactive agents to carefully and dynamically balance the trade-off between the overall portfolio returns and their potential risks. Besides, a very flexible and proactive agent as the market observer is integrated into the MASA framework to provide some additional information on the estimated market trends as valuable feedbacks for multi-agent RL approach to quickly adapt to the ever-changing market conditions. The obtained empirical results clearly reveal the potential strengths of our proposed MASA framework based on the multi-agent RL approach against many well-known RL-based approaches on the challenging data sets of the CSI 300, Dow Jones Industrial Average and S&P 500 indexes over the past 10 years. More importantly, our proposed MASA framework shed lights on many possible directions for future investigation.
<div id='section'>Paperid: <span id='pid'>1192, <a href='https://arxiv.org/pdf/2401.12258.pdf' target='_blank'>https://arxiv.org/pdf/2401.12258.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ram Rachum, Yonatan Nakar, Bill Tomlinson, Nitay Alon, Reuth Mirsky
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.12258">Emergent Dominance Hierarchies in Reinforcement Learning Agents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Modern Reinforcement Learning (RL) algorithms are able to outperform humans in a wide variety of tasks. Multi-agent reinforcement learning (MARL) settings present additional challenges, and successful cooperation in mixed-motive groups of agents depends on a delicate balancing act between individual and group objectives. Social conventions and norms, often inspired by human institutions, are used as tools for striking this balance.
  In this paper, we examine a fundamental, well-studied social convention that underlies cooperation in both animal and human societies: dominance hierarchies.
  We adapt the ethological theory of dominance hierarchies to artificial agents, borrowing the established terminology and definitions with as few amendments as possible. We demonstrate that populations of RL agents, operating without explicit programming or intrinsic rewards, can invent, learn, enforce, and transmit a dominance hierarchy to new populations. The dominance hierarchies that emerge have a similar structure to those studied in chickens, mice, fish, and other species.
<div id='section'>Paperid: <span id='pid'>1193, <a href='https://arxiv.org/pdf/2312.12095.pdf' target='_blank'>https://arxiv.org/pdf/2312.12095.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yanwen Ba, Xuan Liu, Xinning Chen, Hao Wang, Yang Xu, Kenli Li, Shigeng Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.12095">Cautiously-Optimistic Knowledge Sharing for Cooperative Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While decentralized training is attractive in multi-agent reinforcement learning (MARL) for its excellent scalability and robustness, its inherent coordination challenges in collaborative tasks result in numerous interactions for agents to learn good policies. To alleviate this problem, action advising methods make experienced agents share their knowledge about what to do, while less experienced agents strictly follow the received advice. However, this method of sharing and utilizing knowledge may hinder the team's exploration of better states, as agents can be unduly influenced by suboptimal or even adverse advice, especially in the early stages of learning. Inspired by the fact that humans can learn not only from the success but also from the failure of others, this paper proposes a novel knowledge sharing framework called Cautiously-Optimistic kNowledge Sharing (CONS). CONS enables each agent to share both positive and negative knowledge and cautiously assimilate knowledge from others, thereby enhancing the efficiency of early-stage exploration and the agents' robustness to adverse advice. Moreover, considering the continuous improvement of policies, agents value negative knowledge more in the early stages of learning and shift their focus to positive knowledge in the later stages. Our framework can be easily integrated into existing Q-learning based methods without introducing additional training costs. We evaluate CONS in several challenging multi-agent tasks and find it excels in environments where optimal behavioral patterns are difficult to discover, surpassing the baselines in terms of convergence rate and final performance.
<div id='section'>Paperid: <span id='pid'>1194, <a href='https://arxiv.org/pdf/2311.06255.pdf' target='_blank'>https://arxiv.org/pdf/2311.06255.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Parham Gohari, Matthew Hale, Ufuk Topcu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.06255">Privacy-Engineered Value Decomposition Networks for Cooperative Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In cooperative multi-agent reinforcement learning (Co-MARL), a team of agents must jointly optimize the team's long-term rewards to learn a designated task. Optimizing rewards as a team often requires inter-agent communication and data sharing, leading to potential privacy implications. We assume privacy considerations prohibit the agents from sharing their environment interaction data. Accordingly, we propose Privacy-Engineered Value Decomposition Networks (PE-VDN), a Co-MARL algorithm that models multi-agent coordination while provably safeguarding the confidentiality of the agents' environment interaction data. We integrate three privacy-engineering techniques to redesign the data flows of the VDN algorithm, an existing Co-MARL algorithm that consolidates the agents' environment interaction data to train a central controller that models multi-agent coordination, and develop PE-VDN. In the first technique, we design a distributed computation scheme that eliminates Vanilla VDN's dependency on sharing environment interaction data. Then, we utilize a privacy-preserving multi-party computation protocol to guarantee that the data flows of the distributed computation scheme do not pose new privacy risks. Finally, we enforce differential privacy to preempt inference threats against the agents' training data, past environment interactions, when they take actions based on their neural network predictions. We implement PE-VDN in StarCraft Multi-Agent Competition (SMAC) and show that it achieves 80% of Vanilla VDN's win rate while maintaining differential privacy levels that provide meaningful privacy guarantees. The results demonstrate that PE-VDN can safeguard the confidentiality of agents' environment interaction data without sacrificing multi-agent coordination.
<div id='section'>Paperid: <span id='pid'>1195, <a href='https://arxiv.org/pdf/2311.04240.pdf' target='_blank'>https://arxiv.org/pdf/2311.04240.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Farinaz Alamiyan-Harandi, Pouria Ramazi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.04240">Environmental-Impact Based Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To promote cooperation and strengthen the individual impact on the collective outcome in social dilemmas, we propose the Environmental-impact Multi-Agent Reinforcement Learning (EMuReL) method where each agent estimates the "environmental impact" of every other agent, that is, the difference in the current environment state compared to the hypothetical environment in the absence of that other agent. Inspired by the Inequity Aversion model, the agent then compares its own reward with those of its fellows multiplied by their environmental impacts. If its reward exceeds the scaled reward of one of its fellows, the agent takes "social responsibility" toward that fellow by reducing its own reward. Therefore, the less influential an agent is in reaching the current state, the more social responsibility is taken by other agents. Experiments in the Cleanup (resp. Harvest) test environment demonstrate that agents trained based on EMuReL learn to cooperate more effectively and obtain $54\%$ ($39\%$) and $20\%$ ($44\%$) more total rewards while preserving the same cooperation levels compared to when they are trained based on the two state-of-the-art reward reshaping methods inequity aversion and social influence.
<div id='section'>Paperid: <span id='pid'>1196, <a href='https://arxiv.org/pdf/2311.04239.pdf' target='_blank'>https://arxiv.org/pdf/2311.04239.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Farinaz Alamiyan-Harandi, Mersad Hassanjani, Pouria Ramazi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.04239">Kindness in Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In human societies, people often incorporate fairness in their decisions and treat reciprocally by being kind to those who act kindly. They evaluate the kindness of others' actions not only by monitoring the outcomes but also by considering the intentions. This behavioral concept can be adapted to train cooperative agents in Multi-Agent Reinforcement Learning (MARL). We propose the KindMARL method, where agents' intentions are measured by counterfactual reasoning over the environmental impact of the actions that were available to the agents. More specifically, the current environment state is compared with the estimation of the current environment state provided that the agent had chosen another action. The difference between each agent's reward, as the outcome of its action, with that of its fellow, multiplied by the intention of the fellow is then taken as the fellow's "kindness". If the result of each reward-comparison confirms the agent's superiority, it perceives the fellow's kindness and reduces its own reward. Experimental results in the Cleanup and Harvest environments show that training based on the KindMARL method enabled the agents to earn 89\% (resp. 37\%) and 44% (resp. 43\%) more total rewards than training based on the Inequity Aversion and Social Influence methods. The effectiveness of KindMARL is further supported by experiments in a traffic light control problem.
<div id='section'>Paperid: <span id='pid'>1197, <a href='https://arxiv.org/pdf/2310.17416.pdf' target='_blank'>https://arxiv.org/pdf/2310.17416.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kaushik Dey, Satheesh K. Perepu, Abir Das
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.17416">Goals are Enough: Inducing AdHoc cooperation among unseen Multi-Agent systems in IMFs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Intent-based management will play a critical role in achieving customers' expectations in the next-generation mobile networks. Traditional methods cannot perform efficient resource management since they tend to handle each expectation independently. Existing approaches, e.g., based on multi-agent reinforcement learning (MARL) allocate resources in an efficient fashion when there are conflicting expectations on the network slice. However, in reality, systems are often far more complex to be addressed by a standalone MARL formulation. Often there exists a hierarchical structure of intent fulfilment where multiple pre-trained, self-interested agents may need to be further orchestrated by a supervisor or controller agent. Such agents may arrive in the system adhoc, which then needs to be orchestrated along with other available agents. Retraining the whole system every time is often infeasible given the associated time and cost. Given the challenges, such adhoc coordination of pre-trained systems could be achieved through an intelligent supervisor agent which incentivizes pre-trained RL/MARL agents through sets of dynamic contracts (goals or bonuses) and encourages them to act as a cohesive unit towards fulfilling a global expectation. Some approaches use a rule-based supervisor agent and deploy the hierarchical constituent agents sequentially, based on human-coded rules.
  In the current work, we propose a framework whereby pre-trained agents can be orchestrated in parallel leveraging an AI-based supervisor agent. For this, we propose to use Adhoc-Teaming approaches which assign optimal goals to the MARL agents and incentivize them to exhibit certain desired behaviours. Results on the network emulator show that the proposed approach results in faster and improved fulfilment of expectations when compared to rule-based approaches and even generalizes to changes in environments.
<div id='section'>Paperid: <span id='pid'>1198, <a href='https://arxiv.org/pdf/2310.16772.pdf' target='_blank'>https://arxiv.org/pdf/2310.16772.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kejiang Qian, Lingjun Mao, Xin Liang, Yimin Ding, Jin Gao, Xinran Wei, Ziyi Guo, Jiajie Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.16772">AI Agent as Urban Planner: Steering Stakeholder Dynamics in Urban Planning via Consensus-based Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In urban planning, land use readjustment plays a pivotal role in aligning land use configurations with the current demands for sustainable urban development. However, present-day urban planning practices face two main issues. Firstly, land use decisions are predominantly dependent on human experts. Besides, while resident engagement in urban planning can promote urban sustainability and livability, it is challenging to reconcile the diverse interests of stakeholders. To address these challenges, we introduce a Consensus-based Multi-Agent Reinforcement Learning framework for real-world land use readjustment. This framework serves participatory urban planning, allowing diverse intelligent agents as stakeholder representatives to vote for preferred land use types. Within this framework, we propose a novel consensus mechanism in reward design to optimize land utilization through collective decision making. To abstract the structure of the complex urban system, the geographic information of cities is transformed into a spatial graph structure and then processed by graph neural networks. Comprehensive experiments on both traditional top-down planning and participatory planning methods from real-world communities indicate that our computational framework enhances global benefits and accommodates diverse interests, leading to improved satisfaction across different demographic groups. By integrating Multi-Agent Reinforcement Learning, our framework ensures that participatory urban planning decisions are more dynamic and adaptive to evolving community needs and provides a robust platform for automating complex real-world urban planning processes.
<div id='section'>Paperid: <span id='pid'>1199, <a href='https://arxiv.org/pdf/2310.10948.pdf' target='_blank'>https://arxiv.org/pdf/2310.10948.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xianyue Peng, Shenyang Chen, Hang Gao, Hao Wang, H. Michael Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.10948">Combat Urban Congestion via Collaboration: Heterogeneous GNN-based MARL for Coordinated Platooning and Traffic Signal Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Over the years, reinforcement learning has emerged as a popular approach to develop signal control and vehicle platooning strategies either independently or in a hierarchical way. However, jointly controlling both in real-time to alleviate traffic congestion presents new challenges, such as the inherent physical and behavioral heterogeneity between signal control and platooning, as well as coordination between them. This paper proposes an innovative solution to tackle these challenges based on heterogeneous graph multi-agent reinforcement learning and traffic theories. Our approach involves: 1) designing platoon and signal control as distinct reinforcement learning agents with their own set of observations, actions, and reward functions to optimize traffic flow; 2) designing coordination by incorporating graph neural networks within multi-agent reinforcement learning to facilitate seamless information exchange among agents on a regional scale; 3) applying alternating optimization for training, allowing agents to update their own policies and adapt to other agents' policies. We evaluate our approach through SUMO simulations, which show convergent results in terms of both travel time and fuel consumption, and superior performance compared to other adaptive signal control methods.
<div id='section'>Paperid: <span id='pid'>1200, <a href='https://arxiv.org/pdf/2310.02435.pdf' target='_blank'>https://arxiv.org/pdf/2310.02435.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rohit Bokade, Xiaoning Jin, Christopher Amato
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.02435">Multi-Agent Reinforcement Learning Based on Representational Communication for Large-Scale Traffic Signal Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Traffic signal control (TSC) is a challenging problem within intelligent transportation systems and has been tackled using multi-agent reinforcement learning (MARL). While centralized approaches are often infeasible for large-scale TSC problems, decentralized approaches provide scalability but introduce new challenges, such as partial observability. Communication plays a critical role in decentralized MARL, as agents must learn to exchange information using messages to better understand the system and achieve effective coordination. Deep MARL has been used to enable inter-agent communication by learning communication protocols in a differentiable manner. However, many deep MARL communication frameworks proposed for TSC allow agents to communicate with all other agents at all times, which can add to the existing noise in the system and degrade overall performance. In this study, we propose a communication-based MARL framework for large-scale TSC. Our framework allows each agent to learn a communication policy that dictates "which" part of the message is sent "to whom". In essence, our framework enables agents to selectively choose the recipients of their messages and exchange variable length messages with them. This results in a decentralized and flexible communication mechanism in which agents can effectively use the communication channel only when necessary. We designed two networks, a synthetic $4 \times 4$ grid network and a real-world network based on the Pasubio neighborhood in Bologna. Our framework achieved the lowest network congestion compared to related methods, with agents utilizing $\sim 47-65 \%$ of the communication channel. Ablation studies further demonstrated the effectiveness of the communication policies learned within our framework.
<div id='section'>Paperid: <span id='pid'>1201, <a href='https://arxiv.org/pdf/2309.04615.pdf' target='_blank'>https://arxiv.org/pdf/2309.04615.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhizun Wang, David Meger
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.04615">Leveraging World Model Disentanglement in Value-Based Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we propose a novel model-based multi-agent reinforcement learning approach named Value Decomposition Framework with Disentangled World Model to address the challenge of achieving a common goal of multiple agents interacting in the same environment with reduced sample complexity. Due to scalability and non-stationarity problems posed by multi-agent systems, model-free methods rely on a considerable number of samples for training. In contrast, we use a modularized world model, composed of action-conditioned, action-free, and static branches, to unravel the environment dynamics and produce imagined outcomes based on past experience, without sampling directly from the real environment. We employ variational auto-encoders and variational graph auto-encoders to learn the latent representations for the world model, which is merged with a value-based framework to predict the joint action-value function and optimize the overall training objective. We present experimental results in Easy, Hard, and Super-Hard StarCraft II micro-management challenges to demonstrate that our method achieves high sample efficiency and exhibits superior performance in defeating the enemy armies compared to other baselines.
<div id='section'>Paperid: <span id='pid'>1202, <a href='https://arxiv.org/pdf/2309.04615.pdf' target='_blank'>https://arxiv.org/pdf/2309.04615.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhizun Wang, David Meger
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.04615">VDFD: Multi-Agent Value Decomposition Framework with Disentangled World Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we propose a novel model-based multi-agent reinforcement learning approach named Value Decomposition Framework with Disentangled World Model to address the challenge of achieving a common goal of multiple agents interacting in the same environment with reduced sample complexity. Due to scalability and non-stationarity problems posed by multi-agent systems, model-free methods rely on a considerable number of samples for training. In contrast, we use a modularized world model, composed of action-conditioned, action-free, and static branches, to unravel the complicated environment dynamics. Our model produces imagined outcomes based on past experience, without sampling directly from the real environment. We employ variational auto-encoders and variational graph auto-encoders to learn the latent representations for the world model, which is merged with a value-based framework to predict the joint action-value function and optimize the overall training objective. Experimental results on StarCraft II micro-management, Multi-Agent MuJoCo, and Level-Based Foraging challenges demonstrate that our method achieves high sample efficiency and exhibits superior performance compared to other baselines across a wide range of multi-agent learning tasks.
<div id='section'>Paperid: <span id='pid'>1203, <a href='https://arxiv.org/pdf/2309.04615.pdf' target='_blank'>https://arxiv.org/pdf/2309.04615.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhizun Wang, David Meger
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.04615">VDFD: Multi-Agent Value Decomposition Framework with Disentangled World Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we propose a novel model-based multi-agent reinforcement learning approach named Value Decomposition Framework with Disentangled World Model to address the challenge of achieving a common goal of multiple agents interacting in the same environment with reduced sample complexity. Due to scalability and non-stationarity problems posed by multi-agent systems, model-free methods rely on a considerable number of samples for training. In contrast, we use a modularized world model, composed of action-conditioned, action-free, and static branches, to unravel the complicated environment dynamics. Our model produces imagined outcomes based on past experience, without sampling directly from the real environment. We employ variational auto-encoders and variational graph auto-encoders to learn the latent representations for the world model, which is merged with a value-based framework to predict the joint action-value function and optimize the overall training objective. Experimental results on StarCraft II micro-management, Multi-Agent MuJoCo, and Level-Based Foraging challenges demonstrate that our method achieves high sample efficiency and exhibits superior performance compared to other baselines across a wide range of multi-agent learning tasks.
<div id='section'>Paperid: <span id='pid'>1204, <a href='https://arxiv.org/pdf/2308.12985.pdf' target='_blank'>https://arxiv.org/pdf/2308.12985.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiajie Yu, Pierre-Antoine Laharotte, Yu Han, Wei Ma, Ludovic Leclercq
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.12985">Perimeter Control with Heterogeneous Metering Rates for Cordon Signals: A Physics-Regularized Multi-Agent Reinforcement Learning Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Perimeter Control (PC) strategies have been proposed to address urban road network control in oversaturated situations by regulating the transfer flow of the Protected Network (PN) based on the Macroscopic Fundamental Diagram (MFD). The uniform metering rate for cordon signals in most existing studies overlooks the variance of local traffic states at the intersection level, which may cause severe local traffic congestion and degradation of the network stability. PC strategies with heterogeneous metering rates for cordon signals allow precise control for the perimeter but the complexity of the problem increases exponentially with the scale of the PN. This paper leverages a Multi-Agent Reinforcement Learning (MARL)-based traffic signal control framework to decompose this PC problem, which considers heterogeneous metering rates for cordon signals, into multi-agent cooperation tasks. Each agent controls an individual signal located in the cordon, decreasing the dimension of action space for the controller compared to centralized methods. A physics regularization approach for the MARL framework is proposed to ensure the distributed cordon signal controllers are aware of the global network state by encoding MFD-based knowledge into the action-value functions of the local agents. The proposed PC strategy is operated as a two-stage system, with a feedback PC strategy detecting the overall traffic state within the PN and then distributing local instructions to cordon signals controllers in the MARL framework via the physics regularization. Through numerical tests with different demand patterns in a microscopic traffic environment, the proposed PC strategy shows promising robustness and transferability. It outperforms state-of-the-art feedback PC strategies in increasing network throughput, decreasing distributed delay for gate links, and reducing carbon emissions.
<div id='section'>Paperid: <span id='pid'>1205, <a href='https://arxiv.org/pdf/2308.12921.pdf' target='_blank'>https://arxiv.org/pdf/2308.12921.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Amin Shojaeighadikolaei, Morteza Hashemi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.12921">An Efficient Distributed Multi-Agent Reinforcement Learning for EV Charging Network Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The increasing trend in adopting electric vehicles (EVs) will significantly impact the residential electricity demand, which results in an increased risk of transformer overload in the distribution grid. To mitigate such risks, there are urgent needs to develop effective EV charging controllers. Currently, the majority of the EV charge controllers are based on a centralized approach for managing individual EVs or a group of EVs. In this paper, we introduce a decentralized Multi-agent Reinforcement Learning (MARL) charging framework that prioritizes the preservation of privacy for EV owners. We employ the Centralized Training Decentralized Execution-Deep Deterministic Policy Gradient (CTDE-DDPG) scheme, which provides valuable information to users during training while maintaining privacy during execution. Our results demonstrate that the CTDE framework improves the performance of the charging network by reducing the network costs. Moreover, we show that the Peak-to-Average Ratio (PAR) of the total demand is reduced, which, in turn, reduces the risk of transformer overload during the peak hours.
<div id='section'>Paperid: <span id='pid'>1206, <a href='https://arxiv.org/pdf/2308.11272.pdf' target='_blank'>https://arxiv.org/pdf/2308.11272.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yonghyeon Jo, Sunwoo Lee, Junghyuk Yeom, Seungyul Han
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.11272">FoX: Formation-aware exploration in multi-agent reinforcement learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, deep multi-agent reinforcement learning (MARL) has gained significant popularity due to its success in various cooperative multi-agent tasks. However, exploration still remains a challenging problem in MARL due to the partial observability of the agents and the exploration space that can grow exponentially as the number of agents increases. Firstly, in order to address the scalability issue of the exploration space, we define a formation-based equivalence relation on the exploration space and aim to reduce the search space by exploring only meaningful states in different formations. Then, we propose a novel formation-aware exploration (FoX) framework that encourages partially observable agents to visit the states in diverse formations by guiding them to be well aware of their current formation solely based on their own observations. Numerical results show that the proposed FoX framework significantly outperforms the state-of-the-art MARL algorithms on Google Research Football (GRF) and sparse Starcraft II multi-agent challenge (SMAC) tasks.
<div id='section'>Paperid: <span id='pid'>1207, <a href='https://arxiv.org/pdf/2308.08705.pdf' target='_blank'>https://arxiv.org/pdf/2308.08705.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiangyu Liu, Kaiqing Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.08705">Partially Observable Multi-Agent Reinforcement Learning with Information Sharing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We study provable multi-agent reinforcement learning (RL) in the general framework of partially observable stochastic games (POSGs). To circumvent the known hardness results and the use of computationally intractable oracles, we advocate leveraging the potential \emph{information-sharing} among agents, a common practice in empirical multi-agent RL, and a standard model for multi-agent control systems with communications. We first establish several computational complexity results to justify the necessity of information-sharing, as well as the observability assumption that has enabled quasi-efficient single-agent RL with partial observations, for efficiently solving POSGs. {Inspired by the inefficiency of planning in the ground-truth model,} we then propose to further \emph{approximate} the shared common information to construct an {approximate model} of the POSG, in which planning an approximate \emph{equilibrium} (in terms of solving the original POSG) can be quasi-efficient, i.e., of quasi-polynomial-time, under the aforementioned assumptions. Furthermore, we develop a partially observable multi-agent RL algorithm that is \emph{both} statistically and computationally quasi-efficient. {Finally, beyond equilibrium learning, we extend our algorithmic framework to finding the \emph{team-optimal solution} in cooperative POSGs, i.e., decentralized partially observable Markov decision processes, a much more challenging goal. We establish concrete computational and sample complexities under several common structural assumptions of the model.} We hope our study could open up the possibilities of leveraging and even designing different \emph{information structures}, a well-studied notion in control theory, for developing both sample- and computation-efficient partially observable multi-agent RL.
<div id='section'>Paperid: <span id='pid'>1208, <a href='https://arxiv.org/pdf/2307.16536.pdf' target='_blank'>https://arxiv.org/pdf/2307.16536.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nouman Khan, Vijay Subramanian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.16536">Cooperative Multi-Agent Constrained POMDPs: Strong Duality and Primal-Dual Reinforcement Learning with Approximate Information States</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We study the problem of decentralized constrained POMDPs in a team-setting where the multiple non-strategic agents have asymmetric information. Strong duality is established for the setting of infinite-horizon expected total discounted costs when the observations lie in a countable space, the actions are chosen from a finite space, and the immediate cost functions are bounded. Following this, connections with the common-information and approximate information-state approaches are established. The approximate information-states are characterized independent of the Lagrange-multipliers vector so that adaptations of the multiplier (during learning) will not necessitate new representations. Finally, a primal-dual multi-agent reinforcement learning (MARL) framework based on centralized training distributed execution (CTDE) and three time-scale stochastic approximation is developed with the aid of recurrent and feedforward neural-networks as function-approximators.
<div id='section'>Paperid: <span id='pid'>1209, <a href='https://arxiv.org/pdf/2307.08897.pdf' target='_blank'>https://arxiv.org/pdf/2307.08897.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mehrad Jaloli, Marzia Cescon
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.08897">Basal-Bolus Advisor for Type 1 Diabetes (T1D) Patients Using Multi-Agent Reinforcement Learning (RL) Methodology</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a novel multi-agent reinforcement learning (RL) approach for personalized glucose control in individuals with type 1 diabetes (T1D). The method employs a closed-loop system consisting of a blood glucose (BG) metabolic model and a multi-agent soft actor-critic RL model acting as the basal-bolus advisor. Performance evaluation is conducted in three scenarios, comparing the RL agents to conventional therapy. Evaluation metrics include glucose levels (minimum, maximum, and mean), time spent in different BG ranges, and average daily bolus and basal insulin dosages. Results demonstrate that the RL-based basal-bolus advisor significantly improves glucose control, reducing glycemic variability and increasing time spent within the target range (70-180 mg/dL). Hypoglycemia events are effectively prevented, and severe hyperglycemia events are reduced. The RL approach also leads to a statistically significant reduction in average daily basal insulin dosage compared to conventional therapy. These findings highlight the effectiveness of the multi-agent RL approach in achieving better glucose control and mitigating the risk of severe hyperglycemia in individuals with T1D.
<div id='section'>Paperid: <span id='pid'>1210, <a href='https://arxiv.org/pdf/2307.07529.pdf' target='_blank'>https://arxiv.org/pdf/2307.07529.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jaeyeon Jang, Diego Klabjan, Han Liu, Nital S. Patel, Xiuqi Li, Balakrishnan Ananthanarayanan, Husam Dauod, Tzung-Han Juang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.07529">Learning Multiple Coordinated Agents under Directed Acyclic Graph Constraints</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper proposes a novel multi-agent reinforcement learning (MARL) method to learn multiple coordinated agents under directed acyclic graph (DAG) constraints. Unlike existing MARL approaches, our method explicitly exploits the DAG structure between agents to achieve more effective learning performance. Theoretically, we propose a novel surrogate value function based on a MARL model with synthetic rewards (MARLM-SR) and prove that it serves as a lower bound of the optimal value function. Computationally, we propose a practical training algorithm that exploits new notion of leader agent and reward generator and distributor agent to guide the decomposed follower agents to better explore the parameter space in environments with DAG constraints. Empirically, we exploit four DAG environments including a real-world scheduling for one of Intel's high volume packaging and test factory to benchmark our methods and show it outperforms the other non-DAG approaches.
<div id='section'>Paperid: <span id='pid'>1211, <a href='https://arxiv.org/pdf/2307.01862.pdf' target='_blank'>https://arxiv.org/pdf/2307.01862.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jack Garbus, Jordan Pollack
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.01862">Emergent Resource Exchange and Tolerated Theft Behavior using Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>For decades, the evolution of cooperation has piqued the interest of numerous academic disciplines such as game theory, economics, biology, and computer science. In this work, we demonstrate the emergence of a novel and effective resource exchange protocol formed by dropping and picking up resources in a foraging environment. This form of cooperation is made possible by the introduction of a campfire, which adds an extended period of congregation and downtime for agents to explore otherwise unlikely interactions. We find that the agents learn to avoid getting cheated by their exchange partners, but not always from a third party. We also observe the emergence of behavior analogous to tolerated theft, despite the lack of any punishment, combat, or larceny mechanism in the environment.
<div id='section'>Paperid: <span id='pid'>1212, <a href='https://arxiv.org/pdf/2306.13979.pdf' target='_blank'>https://arxiv.org/pdf/2306.13979.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaoliang Hu, Pengcheng Guo, Yadong Li, Guanyu Li, Zhen Cui, Jian Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.13979">TVDO: Tchebycheff Value-Decomposition Optimization for Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In cooperative multiagent reinforcement learning (MARL), centralized training with decentralized execution (CTDE) has recently attracted more attention due to the physical demand. However, the most dilemma therein is the inconsistency between jointly-trained policies and individually-executed actions. In this article, we propose a factorized Tchebycheff value-decomposition optimization (TVDO) method to overcome the trouble of inconsistency. In particular, a nonlinear Tchebycheff aggregation function is formulated to realize the global optimum by tightly constraining the upper bound of individual action-value bias, which is inspired by the Tchebycheff method of multi-objective optimization. We theoretically prove that, under no extra limitations, the factorized value decomposition with Tchebycheff aggregation satisfies the sufficiency and necessity of Individual-Global-Max (IGM), which guarantees the consistency between the global and individual optimal action-value function. Empirically, in the climb and penalty game, we verify that TVDO precisely expresses the global-to-individual value decomposition with a guarantee of policy consistency. Meanwhile, we evaluate TVDO in the SMAC benchmark, and extensive experiments demonstrate that TVDO achieves a significant performance superiority over some SOTA MARL baselines.
<div id='section'>Paperid: <span id='pid'>1213, <a href='https://arxiv.org/pdf/2306.11433.pdf' target='_blank'>https://arxiv.org/pdf/2306.11433.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ho Jung Lee, Sang-Bin Jeon, Yong-Hun Cho, In-Kwon Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.11433">Multi-user Reset Controller for Redirected Walking Using Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The reset technique of Redirected Walking (RDW) forcibly reorients the user's direction overtly to avoid collisions with boundaries, obstacles, or other users in the physical space. However, excessive resetting can decrease the user's sense of immersion and presence. Several RDW studies have been conducted to address this issue. Among them, much research has been done on reset techniques that reduce the number of resets by devising reset direction rules (e.g.,~ 2:1-turn, reset-to-center) or optimizing them for a given environment. However, existing optimization studies on reset techniques have mainly focused on a single-user environment. In a multi-user environment, the dynamic movement of other users and static obstacles in the physical space increase the possibility of resetting. In this study, we propose a multi-user reset controller (MRC) that resets the user taking into account both physical obstacles and multi-user movement to minimize the number of resets. MRC is trained using multi-agent reinforcement learning to determine the optimal reset direction in different environments. This approach enables MRC to effectively account for different environmental contexts, including arbitrary physical obstacles and the dynamic movements of other users in the same physical space. We compared MRC with other reset techniques through simulation tests and user studies, and our results show that MRC reduces the mean number of resets by up to 55\%. Overall, our study confirmed that MRC is an effective reset technique in multi-user environments. Supplemental materials are available at an anonymous link: (https://osf.io/rpftu/?view_only=8230f344502f4013af2a5229db5e21c3).
<div id='section'>Paperid: <span id='pid'>1214, <a href='https://arxiv.org/pdf/2306.10134.pdf' target='_blank'>https://arxiv.org/pdf/2306.10134.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qingshuang Sun, Denis Steckelmacher, Yuan Yao, Ann NowÃ©, RaphaÃ«l Avalos
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.10134">Dynamic Size Message Scheduling for Multi-Agent Communication under Limited Bandwidth</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Communication plays a vital role in multi-agent systems, fostering collaboration and coordination. However, in real-world scenarios where communication is bandwidth-limited, existing multi-agent reinforcement learning (MARL) algorithms often provide agents with a binary choice: either transmitting a fixed number of bytes or no information at all. This limitation hinders the ability to effectively utilize the available bandwidth. To overcome this challenge, we present the Dynamic Size Message Scheduling (DSMS) method, which introduces a finer-grained approach to scheduling by considering the actual size of the information to be exchanged. Our contribution lies in adaptively adjusting message sizes using Fourier transform-based compression techniques, enabling agents to tailor their messages to match the allocated bandwidth while striking a balance between information loss and transmission efficiency. Receiving agents can reliably decompress the messages using the inverse Fourier transform. Experimental results demonstrate that DSMS significantly improves performance in multi-agent cooperative tasks by optimizing the utilization of bandwidth and effectively balancing information value.
<div id='section'>Paperid: <span id='pid'>1215, <a href='https://arxiv.org/pdf/2305.11358.pdf' target='_blank'>https://arxiv.org/pdf/2305.11358.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Manuel Rios, Nicanor Quijano, Luis Felipe Giraldo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.11358">Understanding the World to Solve Social Dilemmas Using Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Social dilemmas are situations where groups of individuals can benefit from mutual cooperation but conflicting interests impede them from doing so. This type of situations resembles many of humanity's most critical challenges, and discovering mechanisms that facilitate the emergence of cooperative behaviors is still an open problem. In this paper, we study the behavior of self-interested rational agents that learn world models in a multi-agent reinforcement learning (RL) setting and that coexist in environments where social dilemmas can arise. Our simulation results show that groups of agents endowed with world models outperform all the other tested ones when dealing with scenarios where social dilemmas can arise. We exploit the world model architecture to qualitatively assess the learnt dynamics and confirm that each agent's world model is capable to encode information of the behavior of the changing environment and the other agent's actions. This is the first work that shows that world models facilitate the emergence of complex coordinated behaviors that enable interacting agents to ``understand'' both environmental and social dynamics.
<div id='section'>Paperid: <span id='pid'>1216, <a href='https://arxiv.org/pdf/2304.09547.pdf' target='_blank'>https://arxiv.org/pdf/2304.09547.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ainur Zhaikhan, Ali H. Sayed
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.09547">Graph Exploration for Effective Multi-agent Q-Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper proposes an exploration technique for multi-agent reinforcement learning (MARL) with graph-based communication among agents. We assume the individual rewards received by the agents are independent of the actions by the other agents, while their policies are coupled. In the proposed framework, neighbouring agents collaborate to estimate the uncertainty about the state-action space in order to execute more efficient explorative behaviour. Different from existing works, the proposed algorithm does not require counting mechanisms and can be applied to continuous-state environments without requiring complex conversion techniques. Moreover, the proposed scheme allows agents to communicate in a fully decentralized manner with minimal information exchange. And for continuous-state scenarios, each agent needs to exchange only a single parameter vector. The performance of the algorithm is verified with theoretical results for discrete-state scenarios and with experiments for continuous ones.
<div id='section'>Paperid: <span id='pid'>1217, <a href='https://arxiv.org/pdf/2303.16737.pdf' target='_blank'>https://arxiv.org/pdf/2303.16737.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Danish Rizvi, David Boyle
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.16737">Multi-Agent Reinforcement Learning with Action Masking for UAV-enabled Mobile Communications</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Unmanned Aerial Vehicles (UAVs) are increasingly used as aerial base stations to provide ad hoc communications infrastructure. Building upon prior research efforts which consider either static nodes, 2D trajectories or single UAV systems, this paper focuses on the use of multiple UAVs for providing wireless communication to mobile users in the absence of terrestrial communications infrastructure. In particular, we jointly optimize UAV 3D trajectory and NOMA power allocation to maximize system throughput. Firstly, a weighted K-means-based clustering algorithm establishes UAV-user associations at regular intervals. The efficacy of training a novel Shared Deep Q-Network (SDQN) with action masking is then explored. Unlike training each UAV separately using DQN, the SDQN reduces training time by using the experiences of multiple UAVs instead of a single agent. We also show that SDQN can be used to train a multi-agent system with differing action spaces. Simulation results confirm that: 1) training a shared DQN outperforms a conventional DQN in terms of maximum system throughput (+20%) and training time (-10%); 2) it can converge for agents with different action spaces, yielding a 9% increase in throughput compared to mutual learning algorithms; and 3) combining NOMA with an SDQN architecture enables the network to achieve a better sum rate compared with existing baseline schemes.
<div id='section'>Paperid: <span id='pid'>1218, <a href='https://arxiv.org/pdf/2303.09070.pdf' target='_blank'>https://arxiv.org/pdf/2303.09070.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lokesh Chandra Das, Myounggyu Won
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.09070">LCS-TF: Multi-Agent Deep Reinforcement Learning-Based Intelligent Lane-Change System for Improving Traffic Flow</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Discretionary lane-change is one of the critical challenges for autonomous vehicle (AV) design due to its significant impact on traffic efficiency. Existing intelligent lane-change solutions have primarily focused on optimizing the performance of the ego-vehicle, thereby suffering from limited generalization performance. Recent research has seen an increased interest in multi-agent reinforcement learning (MARL)-based approaches to address the limitation of the ego vehicle-based solutions through close coordination of multiple agents. Although MARL-based approaches have shown promising results, the potential impact of lane-change decisions on the overall traffic flow of a road segment has not been fully considered. In this paper, we present a novel hybrid MARL-based intelligent lane-change system for AVs designed to jointly optimize the local performance for the ego vehicle, along with the global performance focused on the overall traffic flow of a given road segment. With a careful review of the relevant transportation literature, a novel state space is designed to integrate both the critical local traffic information pertaining to the surrounding vehicles of the ego vehicle, as well as the global traffic information obtained from a road-side unit (RSU) responsible for managing a road segment. We create a reward function to ensure that the agents make effective lane-change decisions by considering the performance of the ego vehicle and the overall improvement of traffic flow. A multi-agent deep Q-network (DQN) algorithm is designed to determine the optimal policy for each agent to effectively cooperate in performing lane-change maneuvers. LCS-TF's performance was evaluated through extensive simulations in comparison with state-of-the-art MARL models. In all aspects of traffic efficiency, driving safety, and driver comfort, the results indicate that LCS-TF exhibits superior performance.
<div id='section'>Paperid: <span id='pid'>1219, <a href='https://arxiv.org/pdf/2303.00177.pdf' target='_blank'>https://arxiv.org/pdf/2303.00177.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pedro Cisneros-Velarde, Sanmi Koyejo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.00177">Finite-sample Guarantees for Nash Q-learning with Linear Function Approximation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Nash Q-learning may be considered one of the first and most known algorithms in multi-agent reinforcement learning (MARL) for learning policies that constitute a Nash equilibrium of an underlying general-sum Markov game. Its original proof provided asymptotic guarantees and was for the tabular case. Recently, finite-sample guarantees have been provided using more modern RL techniques for the tabular case. Our work analyzes Nash Q-learning using linear function approximation -- a representation regime introduced when the state space is large or continuous -- and provides finite-sample guarantees that indicate its sample efficiency. We find that the obtained performance nearly matches an existing efficient result for single-agent RL under the same representation and has a polynomial gap when compared to the best-known result for the tabular case.
<div id='section'>Paperid: <span id='pid'>1220, <a href='https://arxiv.org/pdf/2302.14094.pdf' target='_blank'>https://arxiv.org/pdf/2302.14094.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Arman Ghasemi, Amin Shojaeighadikolaei, Morteza Hashemi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.14094">Combating Uncertainties in Wind and Distributed PV Energy Sources Using Integrated Reinforcement Learning and Time-Series Forecasting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Renewable energy sources, such as wind and solar power, are increasingly being integrated into smart grid systems. However, when compared to traditional energy resources, the unpredictability of renewable energy generation poses significant challenges for both electricity providers and utility companies. Furthermore, the large-scale integration of distributed energy resources (such as PV systems) creates new challenges for energy management in microgrids. To tackle these issues, we propose a novel framework with two objectives: (i) combating uncertainty of renewable energy in smart grid by leveraging time-series forecasting with Long-Short Term Memory (LSTM) solutions, and (ii) establishing distributed and dynamic decision-making framework with multi-agent reinforcement learning using Deep Deterministic Policy Gradient (DDPG) algorithm. The proposed framework considers both objectives concurrently to fully integrate them, while considering both wholesale and retail markets, thereby enabling efficient energy management in the presence of uncertain and distributed renewable energy sources. Through extensive numerical simulations, we demonstrate that the proposed solution significantly improves the profit of load serving entities (LSE) by providing a more accurate wind generation forecast. Furthermore, our results demonstrate that households with PV and battery installations can increase their profits by using intelligent battery charge/discharge actions determined by the DDPG agents.
<div id='section'>Paperid: <span id='pid'>1221, <a href='https://arxiv.org/pdf/2302.04376.pdf' target='_blank'>https://arxiv.org/pdf/2302.04376.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Volodymyr Tkachuk, Seyed Alireza Bakhtiari, Johannes Kirschner, Matej Jusup, Ilija Bogunovic, Csaba SzepesvÃ¡ri
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.04376">Efficient Planning in Combinatorial Action Spaces with Applications to Cooperative Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A practical challenge in reinforcement learning are combinatorial action spaces that make planning computationally demanding. For example, in cooperative multi-agent reinforcement learning, a potentially large number of agents jointly optimize a global reward function, which leads to a combinatorial blow-up in the action space by the number of agents. As a minimal requirement, we assume access to an argmax oracle that allows to efficiently compute the greedy policy for any Q-function in the model class. Building on recent work in planning with local access to a simulator and linear function approximation, we propose efficient algorithms for this setting that lead to polynomial compute and query complexity in all relevant problem parameters. For the special case where the feature decomposition is additive, we further improve the bounds and extend the results to the kernelized setting with an efficient algorithm.
<div id='section'>Paperid: <span id='pid'>1222, <a href='https://arxiv.org/pdf/2302.00671.pdf' target='_blank'>https://arxiv.org/pdf/2302.00671.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Grace Zhang, Ayush Jain, Injune Hwang, Shao-Hua Sun, Joseph J. Lim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.00671">QMP: Q-switch Mixture of Policies for Multi-Task Behavior Sharing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-task reinforcement learning (MTRL) aims to learn several tasks simultaneously for better sample efficiency than learning them separately. Traditional methods achieve this by sharing parameters or relabeled data between tasks. In this work, we introduce a new framework for sharing behavioral policies across tasks, which can be used in addition to existing MTRL methods. The key idea is to improve each task's off-policy data collection by employing behaviors from other task policies. Selectively sharing helpful behaviors acquired in one task to collect training data for another task can lead to higher-quality trajectories, leading to more sample-efficient MTRL. Thus, we introduce a simple and principled framework called Q-switch mixture of policies (QMP) that selectively shares behavior between different task policies by using the task's Q-function to evaluate and select useful shareable behaviors. We theoretically analyze how QMP improves the sample efficiency of the underlying RL algorithm. Our experiments show that QMP's behavioral policy sharing provides complementary gains over many popular MTRL algorithms and outperforms alternative ways to share behaviors in various manipulation, locomotion, and navigation environments. Videos are available at https://qmp-mtrl.github.io.
<div id='section'>Paperid: <span id='pid'>1223, <a href='https://arxiv.org/pdf/2301.11153.pdf' target='_blank'>https://arxiv.org/pdf/2301.11153.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sriram Ganapathi Subramanian, Matthew E. Taylor, Kate Larson, Mark Crowley
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.11153">Learning from Multiple Independent Advisors in Multi-agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent reinforcement learning typically suffers from the problem of sample inefficiency, where learning suitable policies involves the use of many data samples. Learning from external demonstrators is a possible solution that mitigates this problem. However, most prior approaches in this area assume the presence of a single demonstrator. Leveraging multiple knowledge sources (i.e., advisors) with expertise in distinct aspects of the environment could substantially speed up learning in complex environments. This paper considers the problem of simultaneously learning from multiple independent advisors in multi-agent reinforcement learning. The approach leverages a two-level Q-learning architecture, and extends this framework from single-agent to multi-agent settings. We provide principled algorithms that incorporate a set of advisors by both evaluating the advisors at each state and subsequently using the advisors to guide action selection. We also provide theoretical convergence and sample complexity guarantees. Experimentally, we validate our approach in three different test-beds and show that our algorithms give better performances than baselines, can effectively integrate the combined expertise of different advisors, and learn to ignore bad advice.
<div id='section'>Paperid: <span id='pid'>1224, <a href='https://arxiv.org/pdf/2301.05873.pdf' target='_blank'>https://arxiv.org/pdf/2301.05873.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Paramita Koley, Aurghya Maiti, Niloy Ganguly, Sourangshu Bhattacharya
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.05873">Opponent-aware Role-based Learning in Team Competitive Markov Games</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Team competition in multi-agent Markov games is an increasingly important setting for multi-agent reinforcement learning, due to its general applicability in modeling many real-life situations. Multi-agent actor-critic methods are the most suitable class of techniques for learning optimal policies in the team competition setting, due to their flexibility in learning agent-specific critic functions, which can also learn from other agents. In many real-world team competitive scenarios, the roles of the agents naturally emerge, in order to aid in coordination and collaboration within members of the teams. However, existing methods for learning emergent roles rely heavily on the Q-learning setup which does not allow learning of agent-specific Q-functions. In this paper, we propose RAC, a novel technique for learning the emergent roles of agents within a team that are diverse and dynamic. In the proposed method, agents also benefit from predicting the roles of the agents in the opponent team. RAC uses the actor-critic framework with role encoder and opponent role predictors for learning an optimal policy. Experimentation using 2 games demonstrates that the policies learned by RAC achieve higher rewards than those learned using state-of-the-art baselines. Moreover, experiments suggest that the agents in a team learn diverse and opponent-aware policies.
<div id='section'>Paperid: <span id='pid'>1225, <a href='https://arxiv.org/pdf/2301.01919.pdf' target='_blank'>https://arxiv.org/pdf/2301.01919.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xudong Guo, Daming Shi, Wenhui Fan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.01919">Scalable Communication for Multi-Agent Reinforcement Learning via Transformer-Based Email Mechanism</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Communication can impressively improve cooperation in multi-agent reinforcement learning (MARL), especially for partially-observed tasks. However, existing works either broadcast the messages leading to information redundancy, or learn targeted communication by modeling all the other agents as targets, which is not scalable when the number of agents varies. In this work, to tackle the scalability problem of MARL communication for partially-observed tasks, we propose a novel framework Transformer-based Email Mechanism (TEM). The agents adopt local communication to send messages only to the ones that can be observed without modeling all the agents. Inspired by human cooperation with email forwarding, we design message chains to forward information to cooperate with the agents outside the observation range. We introduce Transformer to encode and decode the message chain to choose the next receiver selectively. Empirically, TEM outperforms the baselines on multiple cooperative MARL benchmarks. When the number of agents varies, TEM maintains superior performance without further training.
<div id='section'>Paperid: <span id='pid'>1226, <a href='https://arxiv.org/pdf/2207.07751.pdf' target='_blank'>https://arxiv.org/pdf/2207.07751.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lishuo Pan, Sandeep Manjanna, M. Ani Hsieh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2207.07751">MARLAS: Multi Agent Reinforcement Learning for cooperated Adaptive Sampling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The multi-robot adaptive sampling problem aims at finding trajectories for a team of robots to efficiently sample the phenomenon of interest within a given endurance budget of the robots. In this paper, we propose a robust and scalable approach using Multi-Agent Reinforcement Learning for cooperated Adaptive Sampling (MARLAS) of quasi-static environmental processes. Given a prior on the field being sampled, the proposed method learns decentralized policies for a team of robots to sample high-utility regions within a fixed budget. The multi-robot adaptive sampling problem requires the robots to coordinate with each other to avoid overlapping sampling trajectories. Therefore, we encode the estimates of neighbor positions and intermittent communication between robots into the learning process. We evaluated MARLAS over multiple performance metrics and found it to outperform other baseline multi-robot sampling techniques. Additionally, we demonstrate scalability with both the size of the robot team and the size of the region being sampled. We further demonstrate robustness to communication failures and robot failures. The experimental evaluations are conducted both in simulations on real data and in real robot experiments on demo environmental setup.
<div id='section'>Paperid: <span id='pid'>1227, <a href='https://arxiv.org/pdf/2206.05733.pdf' target='_blank'>https://arxiv.org/pdf/2206.05733.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qijun Luo, Xiao Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2206.05733">Finite-Time Analysis of Fully Decentralized Single-Timescale Actor-Critic</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Decentralized Actor-Critic (AC) algorithms have been widely utilized for multi-agent reinforcement learning (MARL) and have achieved remarkable success. Apart from its empirical success, the theoretical convergence property of decentralized AC algorithms is largely unexplored. Most of the existing finite-time convergence results are derived based on either double-loop update or two-timescale step sizes rule, and this is the case even for centralized AC algorithm under a single-agent setting. In practice, the \emph{single-timescale} update is widely utilized, where actor and critic are updated in an alternating manner with step sizes being of the same order. In this work, we study a decentralized \emph{single-timescale} AC algorithm.Theoretically, using linear approximation for value and reward estimation, we show that the algorithm has sample complexity of $\tilde{\mathcal{O}}(\varepsilon^{-2})$ under Markovian sampling, which matches the optimal complexity with a double-loop implementation (here, $\tilde{\mathcal{O}}$ hides a logarithmic term). When we reduce to the single-agent setting, our result yields new sample complexity for centralized AC using a single-timescale update scheme. The central to establishing our complexity results is \emph{the hidden smoothness of the optimal critic variable} we revealed. We also provide a local action privacy-preserving version of our algorithm and its analysis. Finally, we conduct experiments to show the superiority of our algorithm over the existing decentralized AC algorithms.
<div id='section'>Paperid: <span id='pid'>1228, <a href='https://arxiv.org/pdf/2205.15859.pdf' target='_blank'>https://arxiv.org/pdf/2205.15859.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziyi Liu, Xian Guo, Yongchun Fang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2205.15859">Learning Generalizable Risk-Sensitive Policies to Coordinate in Decentralized Multi-Agent General-Sum Games</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While various multi-agent reinforcement learning methods have been proposed in cooperative settings, few works investigate how self-interested learning agents achieve mutual coordination in decentralized general-sum games and generalize pre-trained policies to non-cooperative opponents during execution. In this paper, we present Generalizable Risk-Sensitive Policy (GRSP). GRSP learns the distributions over agent's return and estimate a dynamic risk-seeking bonus to discover risky coordination strategies. Furthermore, to avoid overfitting to training opponents, GRSP learns an auxiliary opponent modeling task to infer opponents' types and dynamically alter corresponding strategies during execution. Empirically, agents trained via GRSP can achieve mutual coordination during training stably and avoid being exploited by non-cooperative opponents during execution. To the best of our knowledge, it is the first method to learn coordination strategies between agents both in iterated prisoner's dilemma (IPD) and iterated stag hunt (ISH) without shaping opponents or rewards, and firstly consider generalization during execution. Furthermore, we show that GRSP can be scaled to high-dimensional settings.
<div id='section'>Paperid: <span id='pid'>1229, <a href='https://arxiv.org/pdf/2202.13046.pdf' target='_blank'>https://arxiv.org/pdf/2202.13046.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gangshan Jing, He Bai, Jemin George, Aranya Chakrabortty, Piyush K. Sharma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2202.13046">Distributed Multi-Agent Reinforcement Learning Based on Graph-Induced Local Value Functions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Achieving distributed reinforcement learning (RL) for large-scale cooperative multi-agent systems (MASs) is challenging because: (i) each agent has access to only limited information; (ii) issues on convergence or computational complexity emerge due to the curse of dimensionality. In this paper, we propose a general computationally efficient distributed framework for cooperative multi-agent reinforcement learning (MARL) by utilizing the structures of graphs involved in this problem. We introduce three coupling graphs describing three types of inter-agent couplings in MARL, namely, the state graph, the observation graph and the reward graph. By further considering a communication graph, we propose two distributed RL approaches based on local value-functions derived from the coupling graphs. The first approach is able to reduce sample complexity significantly under specific conditions on the aforementioned four graphs. The second approach provides an approximate solution and can be efficient even for problems with dense coupling graphs. Here there is a trade-off between minimizing the approximation error and reducing the computational complexity. Simulations show that our RL algorithms have a significantly improved scalability to large-scale MASs compared with centralized and consensus-based distributed RL algorithms.
<div id='section'>Paperid: <span id='pid'>1230, <a href='https://arxiv.org/pdf/2111.00345.pdf' target='_blank'>https://arxiv.org/pdf/2111.00345.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sriram Ganapathi Subramanian, Matthew E. Taylor, Kate Larson, Mark Crowley
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2111.00345">Multi-Agent Advisor Q-Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the last decade, there have been significant advances in multi-agent reinforcement learning (MARL) but there are still numerous challenges, such as high sample complexity and slow convergence to stable policies, that need to be overcome before wide-spread deployment is possible. However, many real-world environments already, in practice, deploy sub-optimal or heuristic approaches for generating policies. An interesting question that arises is how to best use such approaches as advisors to help improve reinforcement learning in multi-agent domains. In this paper, we provide a principled framework for incorporating action recommendations from online sub-optimal advisors in multi-agent settings. We describe the problem of ADvising Multiple Intelligent Reinforcement Agents (ADMIRAL) in nonrestrictive general-sum stochastic game environments and present two novel Q-learning based algorithms: ADMIRAL - Decision Making (ADMIRAL-DM) and ADMIRAL - Advisor Evaluation (ADMIRAL-AE), which allow us to improve learning by appropriately incorporating advice from an advisor (ADMIRAL-DM), and evaluate the effectiveness of an advisor (ADMIRAL-AE). We analyze the algorithms theoretically and provide fixed-point guarantees regarding their learning in general-sum stochastic games. Furthermore, extensive experiments illustrate that these algorithms: can be used in a variety of environments, have performances that compare favourably to other related baselines, can scale to large state-action spaces, and are robust to poor advice from advisors.
<div id='section'>Paperid: <span id='pid'>1231, <a href='https://arxiv.org/pdf/2012.11258.pdf' target='_blank'>https://arxiv.org/pdf/2012.11258.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jacopo Castellini, Sam Devlin, Frans A. Oliehoek, Rahul Savani
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2012.11258">Difference Rewards Policy Gradients</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Policy gradient methods have become one of the most popular classes of algorithms for multi-agent reinforcement learning. A key challenge, however, that is not addressed by many of these methods is multi-agent credit assignment: assessing an agent's contribution to the overall performance, which is crucial for learning good policies. We propose a novel algorithm called Dr.Reinforce that explicitly tackles this by combining difference rewards with policy gradients to allow for learning decentralized policies when the reward function is known. By differencing the reward function directly, Dr.Reinforce avoids difficulties associated with learning the Q-function as done by Counterfactual Multiagent Policy Gradients (COMA), a state-of-the-art difference rewards method. For applications where the reward function is unknown, we show the effectiveness of a version of Dr.Reinforce that learns an additional reward network that is used to estimate the difference rewards.
<div id='section'>Paperid: <span id='pid'>1232, <a href='https://arxiv.org/pdf/1902.07497.pdf' target='_blank'>https://arxiv.org/pdf/1902.07497.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jacopo Castellini, Frans A. Oliehoek, Rahul Savani, Shimon Whiteson
</span></div><div id="title">Title: <a href="https://arxiv.org/html/1902.07497">Analysing Factorizations of Action-Value Networks for Cooperative Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent years have seen the application of deep reinforcement learning techniques to cooperative multi-agent systems, with great empirical success. However, given the lack of theoretical insight, it remains unclear what the employed neural networks are learning, or how we should enhance their learning power to address the problems on which they fail. In this work, we empirically investigate the learning power of various network architectures on a series of one-shot games. Despite their simplicity, these games capture many of the crucial problems that arise in the multi-agent setting, such as an exponential number of joint actions or the lack of an explicit coordination mechanism. Our results extend those in [4] and quantify how well various approaches can represent the requisite value functions, and help us identify the reasons that can impede good performance, like sparsity of the values or too tight coordination requirements.
<div id='section'>Paperid: <span id='pid'>1233, <a href='https://arxiv.org/pdf/2510.05048.pdf' target='_blank'>https://arxiv.org/pdf/2510.05048.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ondřej Kubíček, Viliam Lisý
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.05048">Look-ahead Reasoning with a Learned Model in Imperfect Information Games</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Test-time reasoning significantly enhances pre-trained AI agents' performance. However, it requires an explicit environment model, often unavailable or overly complex in real-world scenarios. While MuZero enables effective model learning for search in perfect information games, extending this paradigm to imperfect information games presents substantial challenges due to more nuanced look-ahead reasoning techniques and large number of states relevant for individual decisions. This paper introduces an algorithm LAMIR that learns an abstracted model of an imperfect information game directly from the agent-environment interaction. During test time, this trained model is used to perform look-ahead reasoning. The learned abstraction limits the size of each subgame to a manageable size, making theoretically principled look-ahead reasoning tractable even in games where previous methods could not scale. We empirically demonstrate that with sufficient capacity, LAMIR learns the exact underlying game structure, and with limited capacity, it still learns a valuable abstraction, which improves game playing performance of the pre-trained agents even in large games.
<div id='section'>Paperid: <span id='pid'>1234, <a href='https://arxiv.org/pdf/2510.05048.pdf' target='_blank'>https://arxiv.org/pdf/2510.05048.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ondřej Kubíček, Viliam Lisý
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.05048">Look-ahead Reasoning with a Learned Model in Imperfect Information Games</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Test-time reasoning significantly enhances pre-trained AI agents' performance. However, it requires an explicit environment model, often unavailable or overly complex in real-world scenarios. While MuZero enables effective model learning for search in perfect information games, extending this paradigm to imperfect information games presents substantial challenges due to more nuanced look-ahead reasoning techniques and large number of states relevant for individual decisions. This paper introduces an algorithm LAMIR that learns an abstracted model of an imperfect information game directly from the agent-environment interaction. During test time, this trained model is used to perform look-ahead reasoning. The learned abstraction limits the size of each subgame to a manageable size, making theoretically principled look-ahead reasoning tractable even in games where previous methods could not scale. We empirically demonstrate that with sufficient capacity, LAMIR learns the exact underlying game structure, and with limited capacity, it still learns a valuable abstraction, which improves game playing performance of the pre-trained agents even in large games.
<div id='section'>Paperid: <span id='pid'>1235, <a href='https://arxiv.org/pdf/2510.03823.pdf' target='_blank'>https://arxiv.org/pdf/2510.03823.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Adam Haroon, Tristan Schuler
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.03823">Distributed Area Coverage with High Altitude Balloons Using Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>High Altitude Balloons (HABs) can leverage stratospheric wind layers for limited horizontal control, enabling applications in reconnaissance, environmental monitoring, and communications networks. Existing multi-agent HAB coordination approaches use deterministic methods like Voronoi partitioning and extremum seeking control for large global constellations, which perform poorly for smaller teams and localized missions. While single-agent HAB control using reinforcement learning has been demonstrated on HABs, coordinated multi-agent reinforcement learning (MARL) has not yet been investigated. This work presents the first systematic application of multi-agent reinforcement learning (MARL) to HAB coordination for distributed area coverage. We extend our previously developed reinforcement learning simulation environment (RLHAB) to support cooperative multi-agent learning, enabling multiple agents to operate simultaneously in realistic atmospheric conditions. We adapt QMIX for HAB area coverage coordination, leveraging Centralized Training with Decentralized Execution to address atmospheric vehicle coordination challenges. Our approach employs specialized observation spaces providing individual state, environmental context, and teammate data, with hierarchical rewards prioritizing coverage while encouraging spatial distribution. We demonstrate that QMIX achieves similar performance to the theoretically optimal geometric deterministic method for distributed area coverage, validating the MARL approach and providing a foundation for more complex autonomous multi-HAB missions where deterministic methods become intractable.
<div id='section'>Paperid: <span id='pid'>1236, <a href='https://arxiv.org/pdf/2510.03823.pdf' target='_blank'>https://arxiv.org/pdf/2510.03823.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Adam Haroon, Tristan Schuler
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.03823">Distributed Area Coverage with High Altitude Balloons Using Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>High Altitude Balloons (HABs) can leverage stratospheric wind layers for limited horizontal control, enabling applications in reconnaissance, environmental monitoring, and communications networks. Existing multi-agent HAB coordination approaches use deterministic methods like Voronoi partitioning and extremum seeking control for large global constellations, which perform poorly for smaller teams and localized missions. While single-agent HAB control using reinforcement learning has been demonstrated on HABs, coordinated multi-agent reinforcement learning (MARL) has not yet been investigated. This work presents the first systematic application of multi-agent reinforcement learning (MARL) to HAB coordination for distributed area coverage. We extend our previously developed reinforcement learning simulation environment (RLHAB) to support cooperative multi-agent learning, enabling multiple agents to operate simultaneously in realistic atmospheric conditions. We adapt QMIX for HAB area coverage coordination, leveraging Centralized Training with Decentralized Execution to address atmospheric vehicle coordination challenges. Our approach employs specialized observation spaces providing individual state, environmental context, and teammate data, with hierarchical rewards prioritizing coverage while encouraging spatial distribution. We demonstrate that QMIX achieves similar performance to the theoretically optimal geometric deterministic method for distributed area coverage, validating the MARL approach and providing a foundation for more complex autonomous multi-HAB missions where deterministic methods become intractable.
<div id='section'>Paperid: <span id='pid'>1237, <a href='https://arxiv.org/pdf/2510.03534.pdf' target='_blank'>https://arxiv.org/pdf/2510.03534.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nicolò Dal Fabbro, Milad Mesbahi, Renato Mendes, João Borges de Sousa, George J. Pappas
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.03534">Long-Term Mapping of the Douro River Plume with Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We study the problem of long-term (multiple days) mapping of a river plume using multiple autonomous underwater vehicles (AUVs), focusing on the Douro river representative use-case. We propose an energy - and communication - efficient multi-agent reinforcement learning approach in which a central coordinator intermittently communicates with the AUVs, collecting measurements and issuing commands. Our approach integrates spatiotemporal Gaussian process regression (GPR) with a multi-head Q-network controller that regulates direction and speed for each AUV. Simulations using the Delft3D ocean model demonstrate that our method consistently outperforms both single- and multi-agent benchmarks, with scaling the number of agents both improving mean squared error (MSE) and operational endurance. In some instances, our algorithm demonstrates that doubling the number of AUVs can more than double endurance while maintaining or improving accuracy, underscoring the benefits of multi-agent coordination. Our learned policies generalize across unseen seasonal regimes over different months and years, demonstrating promise for future developments of data-driven long-term monitoring of dynamic plume environments.
<div id='section'>Paperid: <span id='pid'>1238, <a href='https://arxiv.org/pdf/2510.03534.pdf' target='_blank'>https://arxiv.org/pdf/2510.03534.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nicolò Dal Fabbro, Milad Mesbahi, Renato Mendes, João Borges de Sousa, George J. Pappas
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.03534">Long-Term Mapping of the Douro River Plume with Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We study the problem of long-term (multiple days) mapping of a river plume using multiple autonomous underwater vehicles (AUVs), focusing on the Douro river representative use-case. We propose an energy - and communication - efficient multi-agent reinforcement learning approach in which a central coordinator intermittently communicates with the AUVs, collecting measurements and issuing commands. Our approach integrates spatiotemporal Gaussian process regression (GPR) with a multi-head Q-network controller that regulates direction and speed for each AUV. Simulations using the Delft3D ocean model demonstrate that our method consistently outperforms both single- and multi-agent benchmarks, with scaling the number of agents both improving mean squared error (MSE) and operational endurance. In some instances, our algorithm demonstrates that doubling the number of AUVs can more than double endurance while maintaining or improving accuracy, underscoring the benefits of multi-agent coordination. Our learned policies generalize across unseen seasonal regimes over different months and years, demonstrating promise for future developments of data-driven long-term monitoring of dynamic plume environments.
<div id='section'>Paperid: <span id='pid'>1239, <a href='https://arxiv.org/pdf/2510.00274.pdf' target='_blank'>https://arxiv.org/pdf/2510.00274.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Maisha Maliha, Dean Hougen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.00274">MAGIC-MASK: Multi-Agent Guided Inter-Agent Collaboration with Mask-Based Explainability for Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding the decision-making process of Deep Reinforcement Learning agents remains a key challenge for deploying these systems in safety-critical and multi-agent environments. While prior explainability methods like StateMask, have advanced the identification of critical states, they remain limited by computational cost, exploration coverage, and lack of adaptation to multi-agent settings. To overcome these limitations, we propose a mathematically grounded framework, MAGIC-MASK (Multi-Agent Guided Inter-agent Collaboration with Mask-Based Explainability for Reinforcement Learning), that extends perturbation-based explanation to Multi-Agent Reinforcement Learning. Our method integrates Proximal Policy Optimization, adaptive epsilon-greedy exploration, and lightweight inter-agent collaboration to share masked state information and peer experience. This collaboration enables each agent to perform saliency-guided masking and share reward-based insights with peers, reducing the time required for critical state discovery, improving explanation fidelity, and leading to faster and more robust learning. The core novelty of our approach lies in generalizing explainability from single-agent to multi-agent systems through a unified mathematical formalism built on trajectory perturbation, reward fidelity analysis, and Kullback-Leibler divergence regularization. This framework yields localized, interpretable explanations grounded in probabilistic modeling and multi-agent Markov decision processes. We validate our framework on both single-agent and multi-agent benchmarks, including a multi-agent highway driving environment and Google Research Football, demonstrating that MAGIC-MASK consistently outperforms state-of-the-art baselines in fidelity, learning efficiency, and policy robustness while offering interpretable and transferable explanations.
<div id='section'>Paperid: <span id='pid'>1240, <a href='https://arxiv.org/pdf/2510.00274.pdf' target='_blank'>https://arxiv.org/pdf/2510.00274.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Maisha Maliha, Dean Hougen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.00274">MAGIC-MASK: Multi-Agent Guided Inter-Agent Collaboration with Mask-Based Explainability for Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding the decision-making process of Deep Reinforcement Learning agents remains a key challenge for deploying these systems in safety-critical and multi-agent environments. While prior explainability methods like StateMask, have advanced the identification of critical states, they remain limited by computational cost, exploration coverage, and lack of adaptation to multi-agent settings. To overcome these limitations, we propose a mathematically grounded framework, MAGIC-MASK (Multi-Agent Guided Inter-agent Collaboration with Mask-Based Explainability for Reinforcement Learning), that extends perturbation-based explanation to Multi-Agent Reinforcement Learning. Our method integrates Proximal Policy Optimization, adaptive epsilon-greedy exploration, and lightweight inter-agent collaboration to share masked state information and peer experience. This collaboration enables each agent to perform saliency-guided masking and share reward-based insights with peers, reducing the time required for critical state discovery, improving explanation fidelity, and leading to faster and more robust learning. The core novelty of our approach lies in generalizing explainability from single-agent to multi-agent systems through a unified mathematical formalism built on trajectory perturbation, reward fidelity analysis, and Kullback-Leibler divergence regularization. This framework yields localized, interpretable explanations grounded in probabilistic modeling and multi-agent Markov decision processes. We validate our framework on both single-agent and multi-agent benchmarks, including a multi-agent highway driving environment and Google Research Football, demonstrating that MAGIC-MASK consistently outperforms state-of-the-art baselines in fidelity, learning efficiency, and policy robustness while offering interpretable and transferable explanations.
<div id='section'>Paperid: <span id='pid'>1241, <a href='https://arxiv.org/pdf/2509.22707.pdf' target='_blank'>https://arxiv.org/pdf/2509.22707.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinqi Yan, Fang He, Qianlong Sang, Bifeng Tong, Peng Sun, Yili Gong, Chuang Hu, Dazhao Cheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.22707">Metadata-Guided Adaptable Frequency Scaling across Heterogeneous Applications and Devices</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Dynamic Voltage and Frequency Scaling is essential for enhancing energy efficiency in mobile platforms. However, traditional heuristic-based governors are increasingly inadequate for managing the complexity of heterogeneous System-on-Chip designs and diverse application workloads. Although reinforcement learning approaches offer improved performance, their poor generalization capability and reliance on extensive retraining for each hardware and application combination leads to significant deployment costs. In this work, we observe that device and application metadata inherently encapsulate valuable knowledge for DVFS, presenting an opportunity to overcome these limitations. We formulate DVFS for heterogeneous devices and applications as a multi-task reinforcement learning problem. We introduce MetaDVFS, which is a metadata-guided framework that systematically leverages metadata to discover and transfer shared knowledge across DVFS tasks. MetaDVFS can output a set of DVFS models with significant generalization capability for various applications of heterogeneous devices. Evaluations on five Google Pixel devices running six applications show that MetaDVFS achieves up to 17% improvement in Performance-Power Ratio and up to 26% improvement in Quality of Experience. Compared to state-of-the-art methods, MetaDVFS delivers 70.8% faster adaptation and 5.8-27.6% higher performance over standalone device-application specific training, while avoiding negative transfer effects. These results establish MetaDVFS as an effective and scalable solution for DVFS deployment in heterogeneous mobile environments.
<div id='section'>Paperid: <span id='pid'>1242, <a href='https://arxiv.org/pdf/2509.22707.pdf' target='_blank'>https://arxiv.org/pdf/2509.22707.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinqi Yan, Fang He, Qianlong Sang, Bifeng Tong, Peng Sun, Yili Gong, Chuang Hu, Dazhao Cheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.22707">Metadata-Guided Adaptable Frequency Scaling across Heterogeneous Applications and Devices</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Dynamic Voltage and Frequency Scaling is essential for enhancing energy efficiency in mobile platforms. However, traditional heuristic-based governors are increasingly inadequate for managing the complexity of heterogeneous System-on-Chip designs and diverse application workloads. Although reinforcement learning approaches offer improved performance, their poor generalization capability and reliance on extensive retraining for each hardware and application combination leads to significant deployment costs. In this work, we observe that device and application metadata inherently encapsulate valuable knowledge for DVFS, presenting an opportunity to overcome these limitations. We formulate DVFS for heterogeneous devices and applications as a multi-task reinforcement learning problem. We introduce MetaDVFS, which is a metadata-guided framework that systematically leverages metadata to discover and transfer shared knowledge across DVFS tasks. MetaDVFS can output a set of DVFS models with significant generalization capability for various applications of heterogeneous devices. Evaluations on five Google Pixel devices running six applications show that MetaDVFS achieves up to 17% improvement in Performance-Power Ratio and up to 26% improvement in Quality of Experience. Compared to state-of-the-art methods, MetaDVFS delivers 70.8% faster adaptation and 5.8-27.6% higher performance over standalone device-application specific training, while avoiding negative transfer effects. These results establish MetaDVFS as an effective and scalable solution for DVFS deployment in heterogeneous mobile environments.
<div id='section'>Paperid: <span id='pid'>1243, <a href='https://arxiv.org/pdf/2509.19512.pdf' target='_blank'>https://arxiv.org/pdf/2509.19512.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Charles Dansereau, Junior-Samuel Lopez-Yepez, Karthik Soma, Antoine Fagette
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.19512">The Heterogeneous Multi-Agent Challenge</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-Agent Reinforcement Learning (MARL) is a growing research area which gained significant traction in recent years, extending Deep RL applications to a much wider range of problems. A particularly challenging class of problems in this domain is Heterogeneous Multi-Agent Reinforcement Learning (HeMARL), where agents with different sensors, resources, or capabilities must cooperate based on local information. The large number of real-world situations involving heterogeneous agents makes it an attractive research area, yet underexplored, as most MARL research focuses on homogeneous agents (e.g., a swarm of identical robots). In MARL and single-agent RL, standardized environments such as ALE and SMAC have allowed to establish recognized benchmarks to measure progress. However, there is a clear lack of such standardized testbed for cooperative HeMARL. As a result, new research in this field often uses simple environments, where most algorithms perform near optimally, or uses weakly heterogeneous MARL environments.
<div id='section'>Paperid: <span id='pid'>1244, <a href='https://arxiv.org/pdf/2509.19512.pdf' target='_blank'>https://arxiv.org/pdf/2509.19512.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Charles Dansereau, Junior-Samuel Lopez-Yepez, Karthik Soma, Antoine Fagette
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.19512">The Heterogeneous Multi-Agent Challenge</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-Agent Reinforcement Learning (MARL) is a growing research area which gained significant traction in recent years, extending Deep RL applications to a much wider range of problems. A particularly challenging class of problems in this domain is Heterogeneous Multi-Agent Reinforcement Learning (HeMARL), where agents with different sensors, resources, or capabilities must cooperate based on local information. The large number of real-world situations involving heterogeneous agents makes it an attractive research area, yet underexplored, as most MARL research focuses on homogeneous agents (e.g., a swarm of identical robots). In MARL and single-agent RL, standardized environments such as ALE and SMAC have allowed to establish recognized benchmarks to measure progress. However, there is a clear lack of such standardized testbed for cooperative HeMARL. As a result, new research in this field often uses simple environments, where most algorithms perform near optimally, or uses weakly heterogeneous MARL environments.
<div id='section'>Paperid: <span id='pid'>1245, <a href='https://arxiv.org/pdf/2509.15519.pdf' target='_blank'>https://arxiv.org/pdf/2509.15519.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chao Li, Bingkun Bao, Yang Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.15519">Fully Decentralized Cooperative Multi-Agent Reinforcement Learning is A Context Modeling Problem</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper studies fully decentralized cooperative multi-agent reinforcement learning, where each agent solely observes the states, its local actions, and the shared rewards. The inability to access other agents' actions often leads to non-stationarity during value function updates and relative overgeneralization during value function estimation, hindering effective cooperative policy learning. However, existing works fail to address both issues simultaneously, due to their inability to model the joint policy of other agents in a fully decentralized setting. To overcome this limitation, we propose a novel method named Dynamics-Aware Context (DAC), which formalizes the task, as locally perceived by each agent, as an Contextual Markov Decision Process, and further addresses both non-stationarity and relative overgeneralization through dynamics-aware context modeling. Specifically, DAC attributes the non-stationary local task dynamics of each agent to switches between unobserved contexts, each corresponding to a distinct joint policy. Then, DAC models the step-wise dynamics distribution using latent variables and refers to them as contexts. For each agent, DAC introduces a context-based value function to address the non-stationarity issue during value function update. For value function estimation, an optimistic marginal value is derived to promote the selection of cooperative actions, thereby addressing the relative overgeneralization issue. Experimentally, we evaluate DAC on various cooperative tasks (including matrix game, predator and prey, and SMAC), and its superior performance against multiple baselines validates its effectiveness.
<div id='section'>Paperid: <span id='pid'>1246, <a href='https://arxiv.org/pdf/2509.15519.pdf' target='_blank'>https://arxiv.org/pdf/2509.15519.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chao Li, Bingkun Bao, Yang Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.15519">Fully Decentralized Cooperative Multi-Agent Reinforcement Learning is A Context Modeling Problem</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper studies fully decentralized cooperative multi-agent reinforcement learning, where each agent solely observes the states, its local actions, and the shared rewards. The inability to access other agents' actions often leads to non-stationarity during value function updates and relative overgeneralization during value function estimation, hindering effective cooperative policy learning. However, existing works fail to address both issues simultaneously, due to their inability to model the joint policy of other agents in a fully decentralized setting. To overcome this limitation, we propose a novel method named Dynamics-Aware Context (DAC), which formalizes the task, as locally perceived by each agent, as an Contextual Markov Decision Process, and further addresses both non-stationarity and relative overgeneralization through dynamics-aware context modeling. Specifically, DAC attributes the non-stationary local task dynamics of each agent to switches between unobserved contexts, each corresponding to a distinct joint policy. Then, DAC models the step-wise dynamics distribution using latent variables and refers to them as contexts. For each agent, DAC introduces a context-based value function to address the non-stationarity issue during value function update. For value function estimation, an optimistic marginal value is derived to promote the selection of cooperative actions, thereby addressing the relative overgeneralization issue. Experimentally, we evaluate DAC on various cooperative tasks (including matrix game, predator and prey, and SMAC), and its superior performance against multiple baselines validates its effectiveness.
<div id='section'>Paperid: <span id='pid'>1247, <a href='https://arxiv.org/pdf/2509.12927.pdf' target='_blank'>https://arxiv.org/pdf/2509.12927.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xingxing Hong, Yungong Wang, Dexin Jin, Ye Yuan, Ximing Huang, Zijian Wu, Wenxin Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.12927">HLSMAC: A New StarCraft Multi-Agent Challenge for High-Level Strategic Decision-Making</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Benchmarks are crucial for assessing multi-agent reinforcement learning (MARL) algorithms. While StarCraft II-related environments have driven significant advances in MARL, existing benchmarks like SMAC focus primarily on micromanagement, limiting comprehensive evaluation of high-level strategic intelligence. To address this, we introduce HLSMAC, a new cooperative MARL benchmark with 12 carefully designed StarCraft II scenarios based on classical stratagems from the Thirty-Six Stratagems. Each scenario corresponds to a specific stratagem and is designed to challenge agents with diverse strategic elements, including tactical maneuvering, timing coordination, and deception, thereby opening up avenues for evaluating high-level strategic decision-making capabilities. We also propose novel metrics across multiple dimensions beyond conventional win rate, such as ability utilization and advancement efficiency, to assess agents' overall performance within the HLSMAC environment. We integrate state-of-the-art MARL algorithms and LLM-based agents with our benchmark and conduct comprehensive experiments. The results demonstrate that HLSMAC serves as a robust testbed for advancing multi-agent strategic decision-making.
<div id='section'>Paperid: <span id='pid'>1248, <a href='https://arxiv.org/pdf/2509.12927.pdf' target='_blank'>https://arxiv.org/pdf/2509.12927.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xingxing Hong, Yungong Wang, Dexin Jin, Ye Yuan, Ximing Huang, Zijian Wu, Wenxin Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.12927">HLSMAC: A New StarCraft Multi-Agent Challenge for High-Level Strategic Decision-Making</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Benchmarks are crucial for assessing multi-agent reinforcement learning (MARL) algorithms. While StarCraft II-related environments have driven significant advances in MARL, existing benchmarks like SMAC focus primarily on micromanagement, limiting comprehensive evaluation of high-level strategic intelligence. To address this, we introduce HLSMAC, a new cooperative MARL benchmark with 12 carefully designed StarCraft II scenarios based on classical stratagems from the Thirty-Six Stratagems. Each scenario corresponds to a specific stratagem and is designed to challenge agents with diverse strategic elements, including tactical maneuvering, timing coordination, and deception, thereby opening up avenues for evaluating high-level strategic decision-making capabilities. We also propose novel metrics across multiple dimensions beyond conventional win rate, such as ability utilization and advancement efficiency, to assess agents' overall performance within the HLSMAC environment. We integrate state-of-the-art MARL algorithms and LLM-based agents with our benchmark and conduct comprehensive experiments. The results demonstrate that HLSMAC serves as a robust testbed for advancing multi-agent strategic decision-making.
<div id='section'>Paperid: <span id='pid'>1249, <a href='https://arxiv.org/pdf/2509.12048.pdf' target='_blank'>https://arxiv.org/pdf/2509.12048.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hoon Sagong, Heesu Kim, Hanbeen Hong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.12048">Hi-DARTS: Hierarchical Dynamically Adapting Reinforcement Trading System</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Conventional autonomous trading systems struggle to balance computational efficiency and market responsiveness due to their fixed operating frequency. We propose Hi-DARTS, a hierarchical multi-agent reinforcement learning framework that addresses this trade-off. Hi-DARTS utilizes a meta-agent to analyze market volatility and dynamically activate specialized Time Frame Agents for high-frequency or low-frequency trading as needed. During back-testing on AAPL stock from January 2024 to May 2025, Hi-DARTS yielded a cumulative return of 25.17% with a Sharpe Ratio of 0.75. This performance surpasses standard benchmarks, including a passive buy-and-hold strategy on AAPL (12.19% return) and the S&P 500 ETF (SPY) (20.01% return). Our work demonstrates that dynamic, hierarchical agents can achieve superior risk-adjusted returns while maintaining high computational efficiency.
<div id='section'>Paperid: <span id='pid'>1250, <a href='https://arxiv.org/pdf/2509.12048.pdf' target='_blank'>https://arxiv.org/pdf/2509.12048.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hoon Sagong, Heesu Kim, Hanbeen Hong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.12048">Hi-DARTS: Hierarchical Dynamically Adapting Reinforcement Trading System</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Conventional autonomous trading systems struggle to balance computational efficiency and market responsiveness due to their fixed operating frequency. We propose Hi-DARTS, a hierarchical multi-agent reinforcement learning framework that addresses this trade-off. Hi-DARTS utilizes a meta-agent to analyze market volatility and dynamically activate specialized Time Frame Agents for high-frequency or low-frequency trading as needed. During back-testing on AAPL stock from January 2024 to May 2025, Hi-DARTS yielded a cumulative return of 25.17% with a Sharpe Ratio of 0.75. This performance surpasses standard benchmarks, including a passive buy-and-hold strategy on AAPL (12.19% return) and the S&P 500 ETF (SPY) (20.01% return). Our work demonstrates that dynamic, hierarchical agents can achieve superior risk-adjusted returns while maintaining high computational efficiency.
<div id='section'>Paperid: <span id='pid'>1251, <a href='https://arxiv.org/pdf/2509.11508.pdf' target='_blank'>https://arxiv.org/pdf/2509.11508.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tinglong Deng, Hang Tao, Xinxiang Wang, Yinyan Wang, Hanjiang Luo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.11508">SafeDiver: Cooperative AUV-USV Assisted Diver Communication via Multi-agent Reinforcement Learning Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As underwater human activities are increasing, the demand for underwater communication service presents a significant challenge. Existing underwater diver communication methods face hurdles due to inherent disadvantages and complex underwater environments. To address this issue, we propose a scheme that utilizes maritime unmanned systems to assist divers with reliable and high-speed communication. Multiple AUVs are equipped with optical and acoustic multimodal communication devices as relay nodes, providing adaptive communication services based on changes in the diver's activity area. By using a multi-agent reinforcement learning (MARL) approach to control the cooperative movement of AUVs, high-speed and reliable data transmission between divers can be achieved. At the same time, utilizing the advantages of on-demand deployment and wide coverage of unmanned surface vehicles (USVs) as surface relay nodes to coordinate and forward information from AUVs, and controlling AUVs to adaptively select relay USV nodes for data transmission, high-quality communication between divers and surface platform can be achieved. Through simulation verification, the proposed scheme can effectively achieve reliable and high-speed communication for divers.
<div id='section'>Paperid: <span id='pid'>1252, <a href='https://arxiv.org/pdf/2509.11508.pdf' target='_blank'>https://arxiv.org/pdf/2509.11508.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tinglong Deng, Hang Tao, Xinxiang Wang, Yinyan Wang, Hanjiang Luo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.11508">SafeDiver: Cooperative AUV-USV Assisted Diver Communication via Multi-agent Reinforcement Learning Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As underwater human activities are increasing, the demand for underwater communication service presents a significant challenge. Existing underwater diver communication methods face hurdles due to inherent disadvantages and complex underwater environments. To address this issue, we propose a scheme that utilizes maritime unmanned systems to assist divers with reliable and high-speed communication. Multiple AUVs are equipped with optical and acoustic multimodal communication devices as relay nodes, providing adaptive communication services based on changes in the diver's activity area. By using a multi-agent reinforcement learning (MARL) approach to control the cooperative movement of AUVs, high-speed and reliable data transmission between divers can be achieved. At the same time, utilizing the advantages of on-demand deployment and wide coverage of unmanned surface vehicles (USVs) as surface relay nodes to coordinate and forward information from AUVs, and controlling AUVs to adaptively select relay USV nodes for data transmission, high-quality communication between divers and surface platform can be achieved. Through simulation verification, the proposed scheme can effectively achieve reliable and high-speed communication for divers.
<div id='section'>Paperid: <span id='pid'>1253, <a href='https://arxiv.org/pdf/2509.10163.pdf' target='_blank'>https://arxiv.org/pdf/2509.10163.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Francisco Javier Esono Nkulu Andong, Qi Min
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.10163">Federated Multi-Agent Reinforcement Learning for Privacy-Preserving and Energy-Aware Resource Management in 6G Edge Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As sixth-generation (6G) networks move toward ultra-dense, intelligent edge environments, efficient resource management under stringent privacy, mobility, and energy constraints becomes critical. This paper introduces a novel Federated Multi-Agent Reinforcement Learning (Fed-MARL) framework that incorporates cross-layer orchestration of both the MAC layer and application layer for energy-efficient, privacy-preserving, and real-time resource management across heterogeneous edge devices. Each agent uses a Deep Recurrent Q-Network (DRQN) to learn decentralized policies for task offloading, spectrum access, and CPU energy adaptation based on local observations (e.g., queue length, energy, CPU usage, and mobility). To protect privacy, we introduce a secure aggregation protocol based on elliptic curve Diffie Hellman key exchange, which ensures accurate model updates without exposing raw data to semi-honest adversaries. We formulate the resource management problem as a partially observable multi-agent Markov decision process (POMMDP) with a multi-objective reward function that jointly optimizes latency, energy efficiency, spectral efficiency, fairness, and reliability under 6G-specific service requirements such as URLLC, eMBB, and mMTC. Simulation results demonstrate that Fed-MARL outperforms centralized MARL and heuristic baselines in task success rate, latency, energy efficiency, and fairness, while ensuring robust privacy protection and scalability in dynamic, resource-constrained 6G edge networks.
<div id='section'>Paperid: <span id='pid'>1254, <a href='https://arxiv.org/pdf/2509.10163.pdf' target='_blank'>https://arxiv.org/pdf/2509.10163.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Francisco Javier Esono Nkulu Andong, Qi Min
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.10163">Federated Multi-Agent Reinforcement Learning for Privacy-Preserving and Energy-Aware Resource Management in 6G Edge Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As sixth-generation (6G) networks move toward ultra-dense, intelligent edge environments, efficient resource management under stringent privacy, mobility, and energy constraints becomes critical. This paper introduces a novel Federated Multi-Agent Reinforcement Learning (Fed-MARL) framework that incorporates cross-layer orchestration of both the MAC layer and application layer for energy-efficient, privacy-preserving, and real-time resource management across heterogeneous edge devices. Each agent uses a Deep Recurrent Q-Network (DRQN) to learn decentralized policies for task offloading, spectrum access, and CPU energy adaptation based on local observations (e.g., queue length, energy, CPU usage, and mobility). To protect privacy, we introduce a secure aggregation protocol based on elliptic curve Diffie Hellman key exchange, which ensures accurate model updates without exposing raw data to semi-honest adversaries. We formulate the resource management problem as a partially observable multi-agent Markov decision process (POMMDP) with a multi-objective reward function that jointly optimizes latency, energy efficiency, spectral efficiency, fairness, and reliability under 6G-specific service requirements such as URLLC, eMBB, and mMTC. Simulation results demonstrate that Fed-MARL outperforms centralized MARL and heuristic baselines in task success rate, latency, energy efficiency, and fairness, while ensuring robust privacy protection and scalability in dynamic, resource-constrained 6G edge networks.
<div id='section'>Paperid: <span id='pid'>1255, <a href='https://arxiv.org/pdf/2509.03817.pdf' target='_blank'>https://arxiv.org/pdf/2509.03817.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wei Yang, Jesse Thomason
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.03817">Learning to Deliberate: Meta-policy Collaboration for Agentic LLMs with Multi-agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent systems of large language models (LLMs) show promise for complex reasoning, but their effectiveness is often limited by fixed collaboration protocols. These frameworks typically focus on macro-level orchestration while overlooking agents' internal deliberative capabilities. This critical meta-cognitive blindspot treats agents as passive executors unable to adapt their strategy based on internal cognitive states like uncertainty or confidence. We introduce the Meta-Policy Deliberation Framework (MPDF), where agents learn a decentralized policy over a set of high-level meta-cognitive actions: Persist, Refine, and Concede. To overcome the instability of traditional policy gradients in this setting, we develop SoftRankPO, a novel reinforcement learning algorithm. SoftRankPO stabilizes training by shaping advantages based on the rank of rewards mapped through smooth normal quantiles, making the learning process robust to reward variance. Experiments show that MPDF with SoftRankPO achieves a a 4-5% absolute gain in average accuracy across five mathematical and general reasoning benchmarks compared to six state-of-the-art heuristic and learning-based multi-agent reasoning algorithms. Our work presents a paradigm for learning adaptive, meta-cognitive policies for multi-agent LLM systems, shifting the focus from designing fixed protocols to learning dynamic, deliberative strategies.
<div id='section'>Paperid: <span id='pid'>1256, <a href='https://arxiv.org/pdf/2509.03817.pdf' target='_blank'>https://arxiv.org/pdf/2509.03817.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wei Yang, Jesse Thomason
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.03817">Learning to Deliberate: Meta-policy Collaboration for Agentic LLMs with Multi-agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent systems of large language models (LLMs) show promise for complex reasoning, but their effectiveness is often limited by fixed collaboration protocols. These frameworks typically focus on macro-level orchestration while overlooking agents' internal deliberative capabilities. This critical meta-cognitive blindspot treats agents as passive executors unable to adapt their strategy based on internal cognitive states like uncertainty or confidence. We introduce the Meta-Policy Deliberation Framework (MPDF), where agents learn a decentralized policy over a set of high-level meta-cognitive actions: Persist, Refine, and Concede. To overcome the instability of traditional policy gradients in this setting, we develop SoftRankPO, a novel reinforcement learning algorithm. SoftRankPO stabilizes training by shaping advantages based on the rank of rewards mapped through smooth normal quantiles, making the learning process robust to reward variance. Experiments show that MPDF with SoftRankPO achieves a a 4-5% absolute gain in average accuracy across five mathematical and general reasoning benchmarks compared to six state-of-the-art heuristic and learning-based multi-agent reasoning algorithms. Our work presents a paradigm for learning adaptive, meta-cognitive policies for multi-agent LLM systems, shifting the focus from designing fixed protocols to learning dynamic, deliberative strategies.
<div id='section'>Paperid: <span id='pid'>1257, <a href='https://arxiv.org/pdf/2509.03682.pdf' target='_blank'>https://arxiv.org/pdf/2509.03682.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhengyang Li, Qijin Ji, Xinghong Ling, Quan Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.03682">A Comprehensive Review of Multi-Agent Reinforcement Learning in Video Games</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in multi-agent reinforcement learning (MARL) have demonstrated its application potential in modern games. Beginning with foundational work and progressing to landmark achievements such as AlphaStar in StarCraft II and OpenAI Five in Dota 2, MARL has proven capable of achieving superhuman performance across diverse game environments through techniques like self-play, supervised learning, and deep reinforcement learning. With its growing impact, a comprehensive review has become increasingly important in this field. This paper aims to provide a thorough examination of MARL's application from turn-based two-agent games to real-time multi-agent video games including popular genres such as Sports games, First-Person Shooter (FPS) games, Real-Time Strategy (RTS) games and Multiplayer Online Battle Arena (MOBA) games. We further analyze critical challenges posed by MARL in video games, including nonstationary, partial observability, sparse rewards, team coordination, and scalability, and highlight successful implementations in games like Rocket League, Minecraft, Quake III Arena, StarCraft II, Dota 2, Honor of Kings, etc. This paper offers insights into MARL in video game AI systems, proposes a novel method to estimate game complexity, and suggests future research directions to advance MARL and its applications in game development, inspiring further innovation in this rapidly evolving field.
<div id='section'>Paperid: <span id='pid'>1258, <a href='https://arxiv.org/pdf/2509.03682.pdf' target='_blank'>https://arxiv.org/pdf/2509.03682.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhengyang Li, Qijin Ji, Xinghong Ling, Quan Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.03682">A Comprehensive Review of Multi-Agent Reinforcement Learning in Video Games</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in multi-agent reinforcement learning (MARL) have demonstrated its application potential in modern games. Beginning with foundational work and progressing to landmark achievements such as AlphaStar in StarCraft II and OpenAI Five in Dota 2, MARL has proven capable of achieving superhuman performance across diverse game environments through techniques like self-play, supervised learning, and deep reinforcement learning. With its growing impact, a comprehensive review has become increasingly important in this field. This paper aims to provide a thorough examination of MARL's application from turn-based two-agent games to real-time multi-agent video games including popular genres such as Sports games, First-Person Shooter (FPS) games, Real-Time Strategy (RTS) games and Multiplayer Online Battle Arena (MOBA) games. We further analyze critical challenges posed by MARL in video games, including nonstationary, partial observability, sparse rewards, team coordination, and scalability, and highlight successful implementations in games like Rocket League, Minecraft, Quake III Arena, StarCraft II, Dota 2, Honor of Kings, etc. This paper offers insights into MARL in video game AI systems, proposes a novel method to estimate game complexity, and suggests future research directions to advance MARL and its applications in game development, inspiring further innovation in this rapidly evolving field.
<div id='section'>Paperid: <span id='pid'>1259, <a href='https://arxiv.org/pdf/2508.18708.pdf' target='_blank'>https://arxiv.org/pdf/2508.18708.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Promise Osaine Ekpo, Brian La, Thomas Wiener, Saesha Agarwal, Arshia Agrawal, Gonzalo Gonzalez-Pumariega, Lekan P. Molu, Angelique Taylor
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.18708">Skill-Aligned Fairness in Multi-Agent Learning for Collaboration in Healthcare</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Fairness in multi-agent reinforcement learning (MARL) is often framed as a workload balance problem, overlooking agent expertise and the structured coordination required in real-world domains. In healthcare, equitable task allocation requires workload balance or expertise alignment to prevent burnout and overuse of highly skilled agents. Workload balance refers to distributing an approximately equal number of subtasks or equalised effort across healthcare workers, regardless of their expertise. We make two contributions to address this problem. First, we propose FairSkillMARL, a framework that defines fairness as the dual objective of workload balance and skill-task alignment. Second, we introduce MARLHospital, a customizable healthcare-inspired environment for modeling team compositions and energy-constrained scheduling impacts on fairness, as no existing simulators are well-suited for this problem. We conducted experiments to compare FairSkillMARL in conjunction with four standard MARL methods, and against two state-of-the-art fairness metrics. Our results suggest that fairness based solely on equal workload might lead to task-skill mismatches and highlight the need for more robust metrics that capture skill-task misalignment. Our work provides tools and a foundation for studying fairness in heterogeneous multi-agent systems where aligning effort with expertise is critical.
<div id='section'>Paperid: <span id='pid'>1260, <a href='https://arxiv.org/pdf/2508.18708.pdf' target='_blank'>https://arxiv.org/pdf/2508.18708.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Promise Osaine Ekpo, Brian La, Thomas Wiener, Saesha Agarwal, Arshia Agrawal, Gonzalo Gonzalez-Pumariega, Lekan P. Molu, Angelique Taylor
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.18708">Skill-Aligned Fairness in Multi-Agent Learning for Collaboration in Healthcare</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Fairness in multi-agent reinforcement learning (MARL) is often framed as a workload balance problem, overlooking agent expertise and the structured coordination required in real-world domains. In healthcare, equitable task allocation requires workload balance or expertise alignment to prevent burnout and overuse of highly skilled agents. Workload balance refers to distributing an approximately equal number of subtasks or equalised effort across healthcare workers, regardless of their expertise. We make two contributions to address this problem. First, we propose FairSkillMARL, a framework that defines fairness as the dual objective of workload balance and skill-task alignment. Second, we introduce MARLHospital, a customizable healthcare-inspired environment for modeling team compositions and energy-constrained scheduling impacts on fairness, as no existing simulators are well-suited for this problem. We conducted experiments to compare FairSkillMARL in conjunction with four standard MARL methods, and against two state-of-the-art fairness metrics. Our results suggest that fairness based solely on equal workload might lead to task-skill mismatches and highlight the need for more robust metrics that capture skill-task misalignment. Our work provides tools and a foundation for studying fairness in heterogeneous multi-agent systems where aligning effort with expertise is critical.
<div id='section'>Paperid: <span id='pid'>1261, <a href='https://arxiv.org/pdf/2508.15764.pdf' target='_blank'>https://arxiv.org/pdf/2508.15764.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kiarash Kazari, Ezzeldin Shereen, GyÃ¶rgy DÃ¡n
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.15764">Distributed Detection of Adversarial Attacks in Multi-Agent Reinforcement Learning with Continuous Action Space</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We address the problem of detecting adversarial attacks against cooperative multi-agent reinforcement learning with continuous action space. We propose a decentralized detector that relies solely on the local observations of the agents and makes use of a statistical characterization of the normal behavior of observable agents. The proposed detector utilizes deep neural networks to approximate the normal behavior of agents as parametric multivariate Gaussian distributions. Based on the predicted density functions, we define a normality score and provide a characterization of its mean and variance. This characterization allows us to employ a two-sided CUSUM procedure for detecting deviations of the normality score from its mean, serving as a detector of anomalous behavior in real-time. We evaluate our scheme on various multi-agent PettingZoo benchmarks against different state-of-the-art attack methods, and our results demonstrate the effectiveness of our method in detecting impactful adversarial attacks. Particularly, it outperforms the discrete counterpart by achieving AUC-ROC scores of over 0.95 against the most impactful attacks in all evaluated environments.
<div id='section'>Paperid: <span id='pid'>1262, <a href='https://arxiv.org/pdf/2508.09541.pdf' target='_blank'>https://arxiv.org/pdf/2508.09541.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gang Chen, Guoxin Wang, Anton van Beek, Zhenjun Ming, Yan Yan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.09541">Emergence of Hierarchies in Multi-Agent Self-Organizing Systems Pursuing a Joint Objective</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent self-organizing systems (MASOS) exhibit key characteristics including scalability, adaptability, flexibility, and robustness, which have contributed to their extensive application across various fields. However, the self-organizing nature of MASOS also introduces elements of unpredictability in their emergent behaviors. This paper focuses on the emergence of dependency hierarchies during task execution, aiming to understand how such hierarchies arise from agents' collective pursuit of the joint objective, how they evolve dynamically, and what factors govern their development. To investigate this phenomenon, multi-agent reinforcement learning (MARL) is employed to train MASOS for a collaborative box-pushing task. By calculating the gradients of each agent's actions in relation to the states of other agents, the inter-agent dependencies are quantified, and the emergence of hierarchies is analyzed through the aggregation of these dependencies. Our results demonstrate that hierarchies emerge dynamically as agents work towards a joint objective, with these hierarchies evolving in response to changing task requirements. Notably, these dependency hierarchies emerge organically in response to the shared objective, rather than being a consequence of pre-configured rules or parameters that can be fine-tuned to achieve specific results. Furthermore, the emergence of hierarchies is influenced by the task environment and network initialization conditions. Additionally, hierarchies in MASOS emerge from the dynamic interplay between agents' "Talent" and "Effort" within the "Environment." "Talent" determines an agent's initial influence on collective decision-making, while continuous "Effort" within the "Environment" enables agents to shift their roles and positions within the system.
<div id='section'>Paperid: <span id='pid'>1263, <a href='https://arxiv.org/pdf/2508.08669.pdf' target='_blank'>https://arxiv.org/pdf/2508.08669.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yizhou Zhang, Eric Mazumdar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.08669">Convergent Q-Learning for Infinite-Horizon General-Sum Markov Games through Behavioral Economics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Risk-aversion and bounded rationality are two key characteristics of human decision-making. Risk-averse quantal-response equilibrium (RQE) is a solution concept that incorporates these features, providing a more realistic depiction of human decision making in various strategic environments compared to a Nash equilibrium. Furthermore a class of RQE has recently been shown in arXiv:2406.14156 to be universally computationally tractable in all finite-horizon Markov games, allowing for the development of multi-agent reinforcement learning algorithms with convergence guarantees. In this paper, we expand upon the study of RQE and analyze their computation in both two-player normal form games and discounted infinite-horizon Markov games. For normal form games we adopt a monotonicity-based approach allowing us to generalize previous results. We first show uniqueness and Lipschitz continuity of RQE with respect to player's payoff matrices under monotonicity assumptions, and then provide conditions on the players' degrees of risk aversion and bounded rationality that ensure monotonicity. We then focus on discounted infinite-horizon Markov games. We define the risk-averse quantal-response Bellman operator and prove its contraction under further conditions on the players' risk-aversion, bounded rationality, and temporal discounting. This yields a Q-learning based algorithm with convergence guarantees for all infinite-horizon general-sum Markov games.
<div id='section'>Paperid: <span id='pid'>1264, <a href='https://arxiv.org/pdf/2508.04652.pdf' target='_blank'>https://arxiv.org/pdf/2508.04652.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuo Liu, Zeyu Liang, Xueguang Lyu, Christopher Amato
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.04652">LLM Collaboration With Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A large amount of work has been done in Multi-Agent Systems (MAS) for modeling and solving problems with multiple interacting agents. However, most LLMs are pretrained independently and not specifically optimized for coordination. Existing LLM fine-tuning frameworks rely on individual rewards, which require complex reward designs for each agent to encourage collaboration. To address these challenges, we model LLM collaboration as a cooperative Multi-Agent Reinforcement Learning (MARL) problem. We develop a multi-agent, multi-turn algorithm, Multi-Agent Group Relative Policy Optimization (MAGRPO), to solve it, building on current RL approaches for LLMs as well as MARL techniques. Our experiments on LLM writing and coding collaboration demonstrate that fine-tuning MAS with MAGRPO enables agents to generate high-quality responses efficiently through effective cooperation. Our approach opens the door to using other MARL methods for LLMs and highlights the associated challenges.
<div id='section'>Paperid: <span id='pid'>1265, <a href='https://arxiv.org/pdf/2508.01060.pdf' target='_blank'>https://arxiv.org/pdf/2508.01060.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ibrahim Althamary, Chen-Fu Chou, Chih-Wei Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.01060">Connectivity Management in Satellite-Aided Vehicular Networks with Multi-Head Attention-Based State Estimation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Managing connectivity in integrated satellite-terrestrial vehicular networks is critical for 6G, yet is challenged by dynamic conditions and partial observability. This letter introduces the Multi-Agent Actor-Critic with Satellite-Aided Multi-head self-attention (MAAC-SAM), a novel multi-agent reinforcement learning framework that enables vehicles to autonomously manage connectivity across Vehicle-to-Satellite (V2S), Vehicle-to-Infrastructure (V2I), and Vehicle-to-Vehicle (V2V) links. Our key innovation is the integration of a multi-head attention mechanism, which allows for robust state estimation even with fluctuating and limited information sharing among vehicles. The framework further leverages self-imitation learning (SIL) and fingerprinting to improve learning efficiency and real-time decisions. Simulation results, based on realistic SUMO traffic models and 3GPP-compliant configurations, demonstrate that MAAC-SAM outperforms state-of-the-art terrestrial and satellite-assisted baselines by up to 14% in transmission utility and maintains high estimation accuracy across varying vehicle densities and sharing levels.
<div id='section'>Paperid: <span id='pid'>1266, <a href='https://arxiv.org/pdf/2507.18333.pdf' target='_blank'>https://arxiv.org/pdf/2507.18333.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kale-ab Abebe Tessera, Leonard Hinckeldey, Riccardo Zamboni, David Abel, Amos Storkey
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.18333">Remembering the Markov Property in Cooperative MARL</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cooperative multi-agent reinforcement learning (MARL) is typically formalised as a Decentralised Partially Observable Markov Decision Process (Dec-POMDP), where agents must reason about the environment and other agents' behaviour. In practice, current model-free MARL algorithms use simple recurrent function approximators to address the challenge of reasoning about others using partial information. In this position paper, we argue that the empirical success of these methods is not due to effective Markov signal recovery, but rather to learning simple conventions that bypass environment observations and memory. Through a targeted case study, we show that co-adapting agents can learn brittle conventions, which then fail when partnered with non-adaptive agents. Crucially, the same models can learn grounded policies when the task design necessitates it, revealing that the issue is not a fundamental limitation of the learning models but a failure of the benchmark design. Our analysis also suggests that modern MARL environments may not adequately test the core assumptions of Dec-POMDPs. We therefore advocate for new cooperative environments built upon two core principles: (1) behaviours grounded in observations and (2) memory-based reasoning about other agents, ensuring success requires genuine skill rather than fragile, co-adapted agreements.
<div id='section'>Paperid: <span id='pid'>1267, <a href='https://arxiv.org/pdf/2507.14658.pdf' target='_blank'>https://arxiv.org/pdf/2507.14658.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Faizan Contractor, Li Li, Ranwa Al Mallah
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.14658">Learning to Communicate in Multi-Agent Reinforcement Learning for Autonomous Cyber Defence</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Popular methods in cooperative Multi-Agent Reinforcement Learning with partially observable environments typically allow agents to act independently during execution, which may limit the coordinated effect of the trained policies. However, by sharing information such as known or suspected ongoing threats, effective communication can lead to improved decision-making in the cyber battle space. We propose a game design where defender agents learn to communicate and defend against imminent cyber threats by playing training games in the Cyber Operations Research Gym, using the Differentiable Inter Agent Learning algorithm adapted to the cyber operational environment. The tactical policies learned by these autonomous agents are akin to those of human experts during incident responses to avert cyber threats. In addition, the agents simultaneously learn minimal cost communication messages while learning their defence tactical policies.
<div id='section'>Paperid: <span id='pid'>1268, <a href='https://arxiv.org/pdf/2507.13846.pdf' target='_blank'>https://arxiv.org/pdf/2507.13846.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kathrin Korte, Christian Medeiros Adriano, Sona Ghahremani, Holger Giese
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.13846">Causal Knowledge Transfer for Multi-Agent Reinforcement Learning in Dynamic Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>[Context] Multi-agent reinforcement learning (MARL) has achieved notable success in environments where agents must learn coordinated behaviors. However, transferring knowledge across agents remains challenging in non-stationary environments with changing goals. [Problem] Traditional knowledge transfer methods in MARL struggle to generalize, and agents often require costly retraining to adapt. [Approach] This paper introduces a causal knowledge transfer framework that enables RL agents to learn and share compact causal representations of paths within a non-stationary environment. As the environment changes (new obstacles), agents' collisions require adaptive recovery strategies. We model each collision as a causal intervention instantiated as a sequence of recovery actions (a macro) whose effect corresponds to a causal knowledge of how to circumvent the obstacle while increasing the chances of achieving the agent's goal (maximizing cumulative reward). This recovery action macro is transferred online from a second agent and is applied in a zero-shot fashion, i.e., without retraining, just by querying a lookup model with local context information (collisions). [Results] Our findings reveal two key insights: (1) agents with heterogeneous goals were able to bridge about half of the gap between random exploration and a fully retrained policy when adapting to new environments, and (2) the impact of causal knowledge transfer depends on the interplay between environment complexity and agents' heterogeneous goals.
<div id='section'>Paperid: <span id='pid'>1269, <a href='https://arxiv.org/pdf/2507.00550.pdf' target='_blank'>https://arxiv.org/pdf/2507.00550.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bruce Fang, Danyi Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.00550">Collaborative Multi-Agent Reinforcement Learning Approach for Elastic Cloud Resource Scaling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper addresses the challenges of rapid resource variation and highly uncertain task loads in cloud computing environments. It proposes an optimization method for elastic cloud resource scaling based on a multi-agent system. The method deploys multiple autonomous agents to perceive resource states in parallel and make local decisions. While maintaining the distributed nature of the system, it introduces a collaborative value function to achieve global coordination. This improves the responsiveness of resource scheduling and enhances overall system performance. To strengthen system foresight, a lightweight state prediction model is designed. It assists agents in identifying future workload trends and optimizes the selection of scaling actions. For policy training, the method adopts a centralized training and decentralized execution reinforcement learning framework. This enables agents to learn effectively and coordinate strategies under conditions of incomplete information. The paper also constructs typical cloud scenarios, including multi-tenancy and burst traffic, to evaluate the proposed method. The evaluation focuses on resource isolation, service quality assurance, and robustness. Experimental results show that the proposed multi-agent scaling strategy outperforms existing methods in resource utilization, SLA violation control, and scheduling latency. The results demonstrate strong adaptability and intelligent regulation. This provides an efficient and reliable new approach to solving the problem of elastic resource scaling in complex cloud platforms.
<div id='section'>Paperid: <span id='pid'>1270, <a href='https://arxiv.org/pdf/2506.19846.pdf' target='_blank'>https://arxiv.org/pdf/2506.19846.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ai Han, Junxing Hu, Pu Wei, Zhiqian Zhang, Yuhang Guo, Jiawei Lu, Zicheng Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.19846">JoyAgents-R1: Joint Evolution Dynamics for Versatile Multi-LLM Agents with Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent reinforcement learning (MARL) has emerged as a prominent paradigm for increasingly complex tasks. However, joint evolution across heterogeneous agents remains challenging due to cooperative inefficiency and training instability. In this paper, we propose the joint evolution dynamics for MARL called JoyAgents-R1, which first applies Group Relative Policy Optimization (GRPO) to the joint training of heterogeneous multi-agents. By iteratively refining agents' large language models (LLMs) and memories, the method achieves holistic equilibrium with optimal decision-making and memory capabilities. Specifically, JoyAgents-R1 first implements node-wise Monte Carlo sampling on the behavior of each agent across entire reasoning trajectories to enhance GRPO sampling efficiency while maintaining policy diversity. Then, our marginal benefit-driven selection strategy identifies top-$K$ sampling groups with maximal reward fluctuations, enabling targeted agent model updates that improve training stability and maximize joint benefits through cost-effective parameter adjustments. Meanwhile, JoyAgents-R1 introduces an adaptive memory evolution mechanism that repurposes GRPO rewards as cost-free supervisory signals to eliminate repetitive reasoning and accelerate convergence. Experiments across general and domain-specific scenarios demonstrate that JoyAgents-R1 achieves performance comparable to that of larger LLMs while built on smaller open-source models.
<div id='section'>Paperid: <span id='pid'>1271, <a href='https://arxiv.org/pdf/2506.12894.pdf' target='_blank'>https://arxiv.org/pdf/2506.12894.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Naoto Yoshida, Kingson Man
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.12894">Homeostatic Coupling for Prosocial Behavior</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>When regarding the suffering of others, we often experience personal distress and feel compelled to help\footnote{Preprint. Under review.}. Inspired by living systems, we investigate the emergence of prosocial behavior among autonomous agents that are motivated by homeostatic self-regulation. We perform multi-agent reinforcement learning, treating each agent as a vulnerable homeostat charged with maintaining its own well-being. We introduce an empathy-like mechanism to share homeostatic states between agents: an agent can either \emph{observe} their partner's internal state ({\bf cognitive empathy}) or the agent's internal state can be \emph{directly coupled} to that of their partner ({\bf affective empathy}). In three simple multi-agent environments, we show that prosocial behavior arises only under homeostatic coupling - when the distress of a partner can affect one's own well-being. Additionally, we show that empathy can be learned: agents can ``decode" their partner's external emotive states to infer the partner's internal homeostatic states. Assuming some level of physiological similarity, agents reference their own emotion-generation functions to invert the mapping from outward display to internal state. Overall, we demonstrate the emergence of prosocial behavior when homeostatic agents learn to ``read" the emotions of others and then to empathize, or feel as they feel.
<div id='section'>Paperid: <span id='pid'>1272, <a href='https://arxiv.org/pdf/2506.06565.pdf' target='_blank'>https://arxiv.org/pdf/2506.06565.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Emilia Rivas, Sabrina Saika, Ahtesham Bakht, Aritran Piplai, Nathaniel D. Bastian, Ankit Shah
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.06565">Adapting Under Fire: Multi-Agent Reinforcement Learning for Adversarial Drift in Network Security</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Evolving attacks are a critical challenge for the long-term success of Network Intrusion Detection Systems (NIDS). The rise of these changing patterns has exposed the limitations of traditional network security methods. While signature-based methods are used to detect different types of attacks, they often fail to detect unknown attacks. Moreover, the system requires frequent updates with new signatures as the attackers are constantly changing their tactics. In this paper, we design an environment where two agents improve their policies over time. The adversarial agent, referred to as the red agent, perturbs packets to evade the intrusion detection mechanism, whereas the blue agent learns new defensive policies using drift adaptation techniques to counter the attacks. Both agents adapt iteratively: the red agent responds to the evolving NIDS, while the blue agent adjusts to emerging attack patterns. By studying the model's learned policy, we offer concrete insights into drift adaptation techniques with high utility. Experiments show that the blue agent boosts model accuracy by 30% with just 2 to 3 adaptation steps using only 25 to 30 samples each.
<div id='section'>Paperid: <span id='pid'>1273, <a href='https://arxiv.org/pdf/2506.04265.pdf' target='_blank'>https://arxiv.org/pdf/2506.04265.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mengda Ji, Genjiu Xu, Liying Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.04265">CORA: Coalitional Rational Advantage Decomposition for Multi-Agent Policy Gradients</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work focuses on the credit assignment problem in cooperative multi-agent reinforcement learning (MARL). Sharing the global advantage among agents often leads to suboptimal policy updates as it fails to account for the distinct contributions of agents. Although numerous methods consider global or individual contributions for credit assignment, a detailed analysis at the coalition level remains lacking in many approaches. This work analyzes the over-updating problem during multi-agent policy updates from a coalition-level perspective. To address this issue, we propose a credit assignment method called Coalitional Rational Advantage Decomposition (CORA). CORA evaluates coalitional advantages via marginal contributions from all possible coalitions and decomposes advantages using the core solution from cooperative game theory, ensuring coalitional rationality. To reduce computational overhead, CORA employs random coalition sampling. Experiments on matrix games, differential games, and multi-agent collaboration benchmarks demonstrate that CORA outperforms strong baselines, particularly in tasks with multiple local optima. These findings highlight the importance of coalition-aware credit assignment for improving MARL performance.
<div id='section'>Paperid: <span id='pid'>1274, <a href='https://arxiv.org/pdf/2505.24113.pdf' target='_blank'>https://arxiv.org/pdf/2505.24113.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pengcheng Dai, Yuanqiu Mo, Wenwu Yu, Wei Ren
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.24113">Distributed Neural Policy Gradient Algorithm for Global Convergence of Networked Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper studies the networked multi-agent reinforcement learning (NMARL) problem, where the objective of agents is to collaboratively maximize the discounted average cumulative rewards. Different from the existing methods that suffer from poor expression due to linear function approximation, we propose a distributed neural policy gradient algorithm that features two innovatively designed neural networks, specifically for the approximate Q-functions and policy functions of agents. This distributed neural policy gradient algorithm consists of two key components: the distributed critic step and the decentralized actor step. In the distributed critic step, agents receive the approximate Q-function parameters from their neighboring agents via a time-varying communication networks to collaboratively evaluate the joint policy. In contrast, in the decentralized actor step, each agent updates its local policy parameter solely based on its own approximate Q-function. In the convergence analysis, we first establish the global convergence of agents for the joint policy evaluation in the distributed critic step. Subsequently, we rigorously demonstrate the global convergence of the overall distributed neural policy gradient algorithm with respect to the objective function. Finally, the effectiveness of the proposed algorithm is demonstrated by comparing it with a centralized algorithm through simulation in the robot path planning environment.
<div id='section'>Paperid: <span id='pid'>1275, <a href='https://arxiv.org/pdf/2505.20579.pdf' target='_blank'>https://arxiv.org/pdf/2505.20579.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dane Malenfant, Blake A. Richards
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.20579">The challenge of hidden gifts in multi-agent reinforcement learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Sometimes we benefit from actions that others have taken even when we are unaware that they took those actions. For example, if your neighbor chooses not to take a parking spot in front of your house when you are not there, you can benefit, even without being aware that they took this action. These ``hidden gifts'' represent an interesting challenge for multi-agent reinforcement learning (MARL), since assigning credit when the beneficial actions of others are hidden is non-trivial. Here, we study the impact of hidden gifts with a very simple MARL task. In this task, agents in a grid-world environment have individual doors to unlock in order to obtain individual rewards. As well, if all the agents unlock their door the group receives a larger collective reward. However, there is only one key for all of the doors, such that the collective reward can only be obtained when the agents drop the key for others after they use it. Notably, there is nothing to indicate to an agent that the other agents have dropped the key, thus this act for others is a ``hidden gift''. We show that several different state-of-the-art MARL algorithms, including MARL specific architectures, fail to learn how to obtain the collective reward in this simple task. Interestingly, we find that decentralized actor-critic policy gradient agents can succeed when we provide them with information about their own action history, but MARL agents still cannot solve the task with action history. Finally, we derive a correction term for policy gradient agents, inspired by learning aware approaches, which reduces the variance in learning and helps them to converge to collective success more reliably. These results show that credit assignment in multi-agent settings can be particularly challenging in the presence of ``hidden gifts'', and demonstrate that self learning-awareness in decentralized agents can benefit these settings.
<div id='section'>Paperid: <span id='pid'>1276, <a href='https://arxiv.org/pdf/2505.20579.pdf' target='_blank'>https://arxiv.org/pdf/2505.20579.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dane Malenfant, Blake A. Richards
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.20579">The challenge of hidden gifts in multi-agent reinforcement learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Sometimes we benefit from actions that others have taken even when we are unaware that they took those actions. For example, if your neighbor chooses not to take a parking spot in front of your house when you are not there, you can benefit, even without being aware that they took this action. These ``hidden gifts'' represent an interesting challenge for multi-agent reinforcement learning (MARL), since assigning credit when the beneficial actions of others are hidden is non-trivial. Here, we study the impact of hidden gifts with a very simple MARL task. In this task, agents in a grid-world environment have individual doors to unlock in order to obtain individual rewards. As well, if all the agents unlock their door the group receives a larger collective reward. However, there is only one key for all of the doors, such that the collective reward can only be obtained when the agents drop the key for others after they use it. Notably, there is nothing to indicate to an agent that the other agents have dropped the key, thus this act for others is a ``hidden gift''. We show that several different state-of-the-art MARL algorithms, including MARL specific architectures, fail to learn how to obtain the collective reward in this simple task. Interestingly, we find that decentralized actor-critic policy gradient agents can succeed when we provide them with information about their own action history, but MARL agents still cannot solve the task with action history. Finally, we derive a correction term for policy gradient agents, inspired by learning aware approaches, which reduces the variance in learning and helps them to converge to collective success more reliably. These results show that credit assignment in multi-agent settings can be particularly challenging in the presence of ``hidden gifts'', and demonstrate that self learning-awareness in decentralized agents can benefit these settings.
<div id='section'>Paperid: <span id='pid'>1277, <a href='https://arxiv.org/pdf/2505.00787.pdf' target='_blank'>https://arxiv.org/pdf/2505.00787.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lucas N. Alegre, Ana L. C. Bazzan, AndrÃ© Barreto, Bruno C. da Silva
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.00787">Constructing an Optimal Behavior Basis for the Option Keyboard</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-task reinforcement learning aims to quickly identify solutions for new tasks with minimal or no additional interaction with the environment. Generalized Policy Improvement (GPI) addresses this by combining a set of base policies to produce a new one that is at least as good -- though not necessarily optimal -- as any individual base policy. Optimality can be ensured, particularly in the linear-reward case, via techniques that compute a Convex Coverage Set (CCS). However, these are computationally expensive and do not scale to complex domains. The Option Keyboard (OK) improves upon GPI by producing policies that are at least as good -- and often better. It achieves this through a learned meta-policy that dynamically combines base policies. However, its performance critically depends on the choice of base policies. This raises a key question: is there an optimal set of base policies -- an optimal behavior basis -- that enables zero-shot identification of optimal solutions for any linear tasks? We solve this open problem by introducing a novel method that efficiently constructs such an optimal behavior basis. We show that it significantly reduces the number of base policies needed to ensure optimality in new tasks. We also prove that it is strictly more expressive than a CCS, enabling particular classes of non-linear tasks to be solved optimally. We empirically evaluate our technique in challenging domains and show that it outperforms state-of-the-art approaches, increasingly so as task complexity increases.
<div id='section'>Paperid: <span id='pid'>1278, <a href='https://arxiv.org/pdf/2504.12777.pdf' target='_blank'>https://arxiv.org/pdf/2504.12777.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>James Rudd-Jones, Mirco Musolesi, MarÃ­a PÃ©rez-Ortiz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.12777">Multi-Agent Reinforcement Learning Simulation for Environmental Policy Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Climate policy development faces significant challenges due to deep uncertainty, complex system dynamics, and competing stakeholder interests. Climate simulation methods, such as Earth System Models, have become valuable tools for policy exploration. However, their typical use is for evaluating potential polices, rather than directly synthesizing them. The problem can be inverted to optimize for policy pathways, but the traditional optimization approaches often struggle with non-linear dynamics, heterogeneous agents, and comprehensive uncertainty quantification. We propose a framework for augmenting climate simulations with Multi-Agent Reinforcement Learning (MARL) to address these limitations. We identify key challenges at the interface between climate simulations and the application of MARL in the context of policy synthesis, including reward definition, scalability with increasing agents and state spaces, uncertainty propagation across linked systems, and solution validation. Additionally, we discuss challenges in making MARL-derived solutions interpretable and useful for policy-makers. Our framework provides a foundation for more sophisticated climate policy exploration while acknowledging important limitations and areas for future research.
<div id='section'>Paperid: <span id='pid'>1279, <a href='https://arxiv.org/pdf/2503.08728.pdf' target='_blank'>https://arxiv.org/pdf/2503.08728.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yihong Li, Chengwei Zhang, Furui Zhan, Wanting Liu, Kailing Zhou, Longji Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.08728">Enhancing Traffic Signal Control through Model-based Reinforcement Learning and Policy Reuse</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent reinforcement learning (MARL) has shown significant potential in traffic signal control (TSC). However, current MARL-based methods often suffer from insufficient generalization due to the fixed traffic patterns and road network conditions used during training. This limitation results in poor adaptability to new traffic scenarios, leading to high retraining costs and complex deployment. To address this challenge, we propose two algorithms: PLight and PRLight. PLight employs a model-based reinforcement learning approach, pretraining control policies and environment models using predefined source-domain traffic scenarios. The environment model predicts the state transitions, which facilitates the comparison of environmental features. PRLight further enhances adaptability by adaptively selecting pre-trained PLight agents based on the similarity between the source and target domains to accelerate the learning process in the target domain. We evaluated the algorithms through two transfer settings: (1) adaptability to different traffic scenarios within the same road network, and (2) generalization across different road networks. The results show that PRLight significantly reduces the adaptation time compared to learning from scratch in new TSC scenarios, achieving optimal performance using similarities between available and target scenarios.
<div id='section'>Paperid: <span id='pid'>1280, <a href='https://arxiv.org/pdf/2503.07678.pdf' target='_blank'>https://arxiv.org/pdf/2503.07678.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kailing Zhou, Chengwei Zhang, Furui Zhan, Wanting Liu, Yihong Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.07678">Using a single actor to output personalized policy for different intersections</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, with the development of Multi-agent reinforcement learning (MARL), adaptive traffic signal control (ATSC) has achieved satisfactory results. In traffic scenarios with multiple intersections, MARL treats each intersection as an agent and optimizes traffic signal control strategies through learning and real-time decision-making. Considering that observation distributions of intersections might be different in real-world scenarios, shared parameter methods might lack diversity and thus lead to high generalization requirements in the shared-policy network. A typical solution is to increase the size of network parameters. However, simply increasing the scale of the network does not necessarily improve policy generalization, which is validated in our experiments. Accordingly, an approach that considers both the personalization of intersections and the efficiency of parameter sharing is required. To this end, we propose Hyper-Action Multi-Head Proximal Policy Optimization (HAMH-PPO), a Centralized Training with Decentralized Execution (CTDE) MARL method that utilizes a shared PPO policy network to deliver personalized policies for intersections with non-iid observation distributions. The centralized critic in HAMH-PPO uses graph attention units to calculate the graph representations of all intersections and outputs a set of value estimates with multiple output heads for each intersection. The decentralized execution actor takes the local observation history as input and output distributions of action as well as a so-called hyper-action to balance the multiple values estimated from the centralized critic to further guide the updating of TSC policies. The combination of hyper-action and multi-head values enables multiple agents to share a single actor-critic while achieving personalized policies.
<div id='section'>Paperid: <span id='pid'>1281, <a href='https://arxiv.org/pdf/2503.02913.pdf' target='_blank'>https://arxiv.org/pdf/2503.02913.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zilin Zhao, Chishui Chen, Haotian Shi, Jiale Chen, Xuanlin Yue, Zhejian Yang, Yang Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.02913">Towards Robust Multi-UAV Collaboration: MARL with Noise-Resilient Communication and Attention Mechanisms</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Efficient path planning for unmanned aerial vehicles (UAVs) is crucial in remote sensing and information collection. As task scales expand, the cooperative deployment of multiple UAVs significantly improves information collection efficiency. However, collaborative communication and decision-making for multiple UAVs remain major challenges in path planning, especially in noisy environments. To efficiently accomplish complex information collection tasks in 3D space and address robust communication issues, we propose a multi-agent reinforcement learning (MARL) framework for UAV path planning based on the Counterfactual Multi-Agent Policy Gradients (COMA) algorithm. The framework incorporates attention mechanism-based UAV communication protocol and training-deployment system, significantly improving communication robustness and individual decision-making capabilities in noisy conditions. Experiments conducted on both synthetic and real-world datasets demonstrate that our method outperforms existing algorithms in terms of path planning efficiency and robustness, especially in noisy environments, achieving a 78\% improvement in entropy reduction.
<div id='section'>Paperid: <span id='pid'>1282, <a href='https://arxiv.org/pdf/2503.01440.pdf' target='_blank'>https://arxiv.org/pdf/2503.01440.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hyungho Na, Kwanghyeon Lee, Sumin Lee, Il-Chul Moon
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.01440">Trajectory-Class-Aware Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the context of multi-agent reinforcement learning, generalization is a challenge to solve various tasks that may require different joint policies or coordination without relying on policies specialized for each task. We refer to this type of problem as a multi-task, and we train agents to be versatile in this multi-task setting through a single training process. To address this challenge, we introduce TRajectory-class-Aware Multi-Agent reinforcement learning (TRAMA). In TRAMA, agents recognize a task type by identifying the class of trajectories they are experiencing through partial observations, and the agents use this trajectory awareness or prediction as additional information for action policy. To this end, we introduce three primary objectives in TRAMA: (a) constructing a quantized latent space to generate trajectory embeddings that reflect key similarities among them; (b) conducting trajectory clustering using these trajectory embeddings; and (c) building a trajectory-class-aware policy. Specifically for (c), we introduce a trajectory-class predictor that performs agent-wise predictions on the trajectory class; and we design a trajectory-class representation model for each trajectory class. Each agent takes actions based on this trajectory-class representation along with its partial observation for task-aware execution. The proposed method is evaluated on various tasks, including multi-task problems built upon StarCraft II. Empirical results show further performance improvements over state-of-the-art baselines.
<div id='section'>Paperid: <span id='pid'>1283, <a href='https://arxiv.org/pdf/2503.00372.pdf' target='_blank'>https://arxiv.org/pdf/2503.00372.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yugu Li, Zehong Cao, Jianglin Qiao, Siyi Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.00372">Nucleolus Credit Assignment for Effective Coalitions in Multi-agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In cooperative multi-agent reinforcement learning (MARL), agents typically form a single grand coalition based on credit assignment to tackle a composite task, often resulting in suboptimal performance. This paper proposed a nucleolus-based credit assignment grounded in cooperative game theory, enabling the autonomous partitioning of agents into multiple small coalitions that can effectively identify and complete subtasks within a larger composite task. Specifically, our designed nucleolus Q-learning could assign fair credits to each agent, and the nucleolus Q-operator provides theoretical guarantees with interpretability for both learning convergence and the stability of the formed small coalitions. Through experiments on Predator-Prey and StarCraft scenarios across varying difficulty levels, our approach demonstrated the emergence of multiple effective coalitions during MARL training, leading to faster learning and superior performance in terms of win rate and cumulative rewards especially in hard and super-hard environments, compared to four baseline methods. Our nucleolus-based credit assignment showed the promise for complex composite tasks requiring effective subteams of agents.
<div id='section'>Paperid: <span id='pid'>1284, <a href='https://arxiv.org/pdf/2502.02901.pdf' target='_blank'>https://arxiv.org/pdf/2502.02901.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Christine Konicki, Mithun Chakraborty, Michael P. Wellman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.02901">Policy Abstraction and Nash Refinement in Tree-Exploiting PSRO</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Policy Space Response Oracles (PSRO) interleaves empirical game-theoretic analysis with deep reinforcement learning (DRL) to solve games too complex for traditional analytic methods. Tree-exploiting PSRO (TE-PSRO) is a variant of this approach that iteratively builds a coarsened empirical game model in extensive form using data obtained from querying a simulator that represents a detailed description of the game. We make two main methodological advances to TE-PSRO that enhance its applicability to complex games of imperfect information. First, we introduce a scalable representation for the empirical game tree where edges correspond to implicit policies learned through DRL. These policies cover conditions in the underlying game abstracted in the game model, supporting sustainable growth of the tree over epochs. Second, we leverage extensive form in the empirical model by employing refined Nash equilibria to direct strategy exploration. To enable this, we give a modular and scalable algorithm based on generalized backward induction for computing a subgame perfect equilibrium (SPE) in an imperfect-information game. We experimentally evaluate our approach on a suite of games including an alternating-offer bargaining game with outside offers; our results demonstrate that TE-PSRO converges toward equilibrium faster when new strategies are generated based on SPE rather than Nash equilibrium, and with reasonable time/memory requirements for the growing empirical model.
<div id='section'>Paperid: <span id='pid'>1285, <a href='https://arxiv.org/pdf/2502.02071.pdf' target='_blank'>https://arxiv.org/pdf/2502.02071.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yan Chen, Cheng Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.02071">Sequential Multi-objective Multi-agent Reinforcement Learning Approach for Predictive Maintenance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing predictive maintenance (PdM) methods typically focus solely on whether to replace system components without considering the costs incurred by inspection. However, a well-considered approach should be able to minimize Remaining Useful Life (RUL) at engine replacement while maximizing inspection interval. To achieve this, multi-agent reinforcement learning (MARL) can be introduced. However, due to the sequential and mutually constraining nature of these 2 objectives, conventional MARL is not applicable. Therefore, this paper introduces a novel framework and develops a Sequential Multi-objective Multi-agent Proximal Policy Optimization (SMOMA-PPO) algorithm. Furthermore, to provide comprehensive and effective degradation information to RL agents, we also employed Gated Recurrent Unit, quantile regression, and probability distribution fitting to develop a GRU-based RUL Prediction (GRP) model. Experiments demonstrate that the GRP method significantly improves the accuracy of RUL predictions in the later stages of system operation compared to existing methods. When incorporating its output into SMOMA-PPO, we achieve at least a 15% reduction in average RUL without unscheduled replacements (UR), nearly a 10% increase in inspection interval, and an overall decrease in maintenance costs. Importantly, our approach offers a new perspective for addressing multi-objective maintenance planning with sequential constraints, effectively enhancing system reliability and reducing maintenance expenses.
<div id='section'>Paperid: <span id='pid'>1286, <a href='https://arxiv.org/pdf/2501.13592.pdf' target='_blank'>https://arxiv.org/pdf/2501.13592.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Claire Bizon Monroc, Ana BuÅ¡iÄ, Donatien Dubuc, Jiamin Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.13592">WFCRL: A Multi-Agent Reinforcement Learning Benchmark for Wind Farm Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The wind farm control problem is challenging, since conventional model-based control strategies require tractable models of complex aerodynamical interactions between the turbines and suffer from the curse of dimension when the number of turbines increases. Recently, model-free and multi-agent reinforcement learning approaches have been used to address this challenge. In this article, we introduce WFCRL (Wind Farm Control with Reinforcement Learning), the first open suite of multi-agent reinforcement learning environments for the wind farm control problem. WFCRL frames a cooperative Multi-Agent Reinforcement Learning (MARL) problem: each turbine is an agent and can learn to adjust its yaw, pitch or torque to maximize the common objective (e.g. the total power production of the farm). WFCRL also offers turbine load observations that will allow to optimize the farm performance while limiting turbine structural damages. Interfaces with two state-of-the-art farm simulators are implemented in WFCRL: a static simulator (FLORIS) and a dynamic simulator (FAST.Farm). For each simulator, $10$ wind layouts are provided, including $5$ real wind farms. Two state-of-the-art online MARL algorithms are implemented to illustrate the scaling challenges. As learning online on FAST.Farm is highly time-consuming, WFCRL offers the possibility of designing transfer learning strategies from FLORIS to FAST.Farm.
<div id='section'>Paperid: <span id='pid'>1287, <a href='https://arxiv.org/pdf/2501.13448.pdf' target='_blank'>https://arxiv.org/pdf/2501.13448.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yulong Hu, Siyuan Feng, Sen Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.13448">BMG-Q: Localized Bipartite Match Graph Attention Q-Learning for Ride-Pooling Order Dispatch</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces Localized Bipartite Match Graph Attention Q-Learning (BMG-Q), a novel Multi-Agent Reinforcement Learning (MARL) algorithm framework tailored for ride-pooling order dispatch. BMG-Q advances ride-pooling decision-making process with the localized bipartite match graph underlying the Markov Decision Process, enabling the development of novel Graph Attention Double Deep Q Network (GATDDQN) as the MARL backbone to capture the dynamic interactions among ride-pooling vehicles in fleet. Our approach enriches the state information for each agent with GATDDQN by leveraging a localized bipartite interdependence graph and enables a centralized global coordinator to optimize order matching and agent behavior using Integer Linear Programming (ILP). Enhanced by gradient clipping and localized graph sampling, our GATDDQN improves scalability and robustness. Furthermore, the inclusion of a posterior score function in the ILP captures the online exploration-exploitation trade-off and reduces the potential overestimation bias of agents, thereby elevating the quality of the derived solutions. Through extensive experiments and validation, BMG-Q has demonstrated superior performance in both training and operations for thousands of vehicle agents, outperforming benchmark reinforcement learning frameworks by around 10% in accumulative rewards and showing a significant reduction in overestimation bias by over 50%. Additionally, it maintains robustness amidst task variations and fleet size changes, establishing BMG-Q as an effective, scalable, and robust framework for advancing ride-pooling order dispatch operations.
<div id='section'>Paperid: <span id='pid'>1288, <a href='https://arxiv.org/pdf/2501.08020.pdf' target='_blank'>https://arxiv.org/pdf/2501.08020.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Juan Palma-Borda, Eduardo GuzmÃ¡n, MarÃ­a-Victoria Belmonte
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.08020">Cooperative Patrol Routing: Optimizing Urban Crime Surveillance through Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The effective design of patrol strategies is a difficult and complex problem, especially in medium and large areas. The objective is to plan, in a coordinated manner, the optimal routes for a set of patrols in a given area, in order to achieve maximum coverage of the area, while also trying to minimize the number of patrols. In this paper, we propose a multi-agent reinforcement learning (MARL) model, based on a decentralized partially observable Markov decision process, to plan unpredictable patrol routes within an urban environment represented as an undirected graph. The model attempts to maximize a target function that characterizes the environment within a given time frame. Our model has been tested to optimize police patrol routes in three medium-sized districts of the city of Malaga. The aim was to maximize surveillance coverage of the most crime-prone areas, based on actual crime data in the city. To address this problem, several MARL algorithms have been studied, and among these the Value Decomposition Proximal Policy Optimization (VDPPO) algorithm exhibited the best performance. We also introduce a novel metric, the coverage index, for the evaluation of the coverage performance of the routes generated by our model. This metric is inspired by the predictive accuracy index (PAI), which is commonly used in criminology to detect hotspots. Using this metric, we have evaluated the model under various scenarios in which the number of agents (or patrols), their starting positions, and the level of information they can observe in the environment have been modified. Results show that the coordinated routes generated by our model achieve a coverage of more than $90\%$ of the $3\%$ of graph nodes with the highest crime incidence, and $65\%$ for $20\%$ of these nodes; $3\%$ and $20\%$ represent the coverage standards for police resource allocation.
<div id='section'>Paperid: <span id='pid'>1289, <a href='https://arxiv.org/pdf/2412.15639.pdf' target='_blank'>https://arxiv.org/pdf/2412.15639.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lunjun Liu, Weilai Jiang, Yaonan Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.15639">Tacit Learning with Adaptive Information Selection for Cooperative Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In multi-agent reinforcement learning (MARL), the centralized training with decentralized execution (CTDE) framework has gained widespread adoption due to its strong performance. However, the further development of CTDE faces two key challenges. First, agents struggle to autonomously assess the relevance of input information for cooperative tasks, impairing their decision-making abilities. Second, in communication-limited scenarios with partial observability, agents are unable to access global information, restricting their ability to collaborate effectively from a global perspective. To address these challenges, we introduce a novel cooperative MARL framework based on information selection and tacit learning. In this framework, agents gradually develop implicit coordination during training, enabling them to infer the cooperative behavior of others in a discrete space without communication, relying solely on local information. Moreover, we integrate gating and selection mechanisms, allowing agents to adaptively filter information based on environmental changes, thereby enhancing their decision-making capabilities. Experiments on popular MARL benchmarks show that our framework can be seamlessly integrated with state-of-the-art algorithms, leading to significant performance improvements.
<div id='section'>Paperid: <span id='pid'>1290, <a href='https://arxiv.org/pdf/2412.12547.pdf' target='_blank'>https://arxiv.org/pdf/2412.12547.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziang Wang, Lei Wang, Qi Yi, Yimin Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.12547">A MARL Based Multi-Target Tracking Algorithm Under Jamming Against Radar</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Unmanned aerial vehicles (UAVs) have played an increasingly important role in military operations and social life. Among all application scenarios, multi-target tracking tasks accomplished by UAV swarms have received extensive attention. However, when UAVs use radar to track targets, the tracking performance can be severely compromised by jammers. To track targets in the presence of jammers, UAVs can use passive radar to position the jammer. This paper proposes a system where a UAV swarm selects the radar's active or passive work mode to track multiple differently located and potentially jammer-carrying targets. After presenting the optimization problem and proving its solving difficulty, we use a multi-agent reinforcement learning algorithm to solve this control problem. We also propose a mechanism based on simulated annealing algorithm to avoid cases where UAV actions violate constraints. Simulation experiments demonstrate the effectiveness of the proposed algorithm.
<div id='section'>Paperid: <span id='pid'>1291, <a href='https://arxiv.org/pdf/2412.12103.pdf' target='_blank'>https://arxiv.org/pdf/2412.12103.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Naoto Yoshida, Kingson Man
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.12103">Empathic Coupling of Homeostatic States for Intrinsic Prosociality</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>When regarding the suffering of others, we often experience personal distress and feel compelled to help. Inspired by living systems, we investigate the emergence of prosocial behavior among autonomous agents that are motivated by homeostatic self-regulation. We perform multi-agent reinforcement learning, treating each agent as a vulnerable homeostat charged with maintaining its own well-being. We introduce an empathy-like mechanism to share homeostatic states between agents: an agent can either \emph{observe} their partner's internal state (cognitive empathy) or the agent's internal state can be \emph{directly coupled} to that of their partner's (affective empathy). In three simple multi-agent environments, we show that prosocial behavior arises only under homeostatic coupling - when the distress of a partner can affect one's own well-being. Our findings specify the type and role of empathy in artificial agents capable of prosocial behavior.
<div id='section'>Paperid: <span id='pid'>1292, <a href='https://arxiv.org/pdf/2412.00661.pdf' target='_blank'>https://arxiv.org/pdf/2412.00661.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Emile Anand, Ishani Karmarkar, Guannan Qu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.00661">Mean-Field Sampling for Cooperative Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Designing efficient algorithms for multi-agent reinforcement learning (MARL) is fundamentally challenging because the size of the joint state and action spaces grows exponentially in the number of agents. These difficulties are exacerbated when balancing sequential global decision-making with local agent interactions. In this work, we propose a new algorithm $\texttt{SUBSAMPLE-MFQ}$ ($\textbf{Subsample}$-$\textbf{M}$ean-$\textbf{F}$ield-$\textbf{Q}$-learning) and a decentralized randomized policy for a system with $n$ agents. For any $k\leq n$, our algorithm learns a policy for the system in time polynomial in $k$. We prove that this learned policy converges to the optimal policy on the order of $\tilde{O}(1/\sqrt{k})$ as the number of subsampled agents $k$ increases. In particular, this bound is independent of the number of agents $n$.
<div id='section'>Paperid: <span id='pid'>1293, <a href='https://arxiv.org/pdf/2411.07634.pdf' target='_blank'>https://arxiv.org/pdf/2411.07634.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Maria Zampella, Urtzi Otamendi, Xabier Belaunzaran, Arkaitz Artetxe, Igor G. Olaizola, Giuseppe Longo, Basilio Sierra
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.07634">Exploring Multi-Agent Reinforcement Learning for Unrelated Parallel Machine Scheduling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Scheduling problems pose significant challenges in resource, industry, and operational management. This paper addresses the Unrelated Parallel Machine Scheduling Problem (UPMS) with setup times and resources using a Multi-Agent Reinforcement Learning (MARL) approach. The study introduces the Reinforcement Learning environment and conducts empirical analyses, comparing MARL with Single-Agent algorithms. The experiments employ various deep neural network policies for single- and Multi-Agent approaches. Results demonstrate the efficacy of the Maskable extension of the Proximal Policy Optimization (PPO) algorithm in Single-Agent scenarios and the Multi-Agent PPO algorithm in Multi-Agent setups. While Single-Agent algorithms perform adequately in reduced scenarios, Multi-Agent approaches reveal challenges in cooperative learning but a scalable capacity. This research contributes insights into applying MARL techniques to scheduling optimization, emphasizing the need for algorithmic sophistication balanced with scalability for intelligent scheduling solutions.
<div id='section'>Paperid: <span id='pid'>1294, <a href='https://arxiv.org/pdf/2411.05683.pdf' target='_blank'>https://arxiv.org/pdf/2411.05683.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Indranil Sur, Aswin Raghavan, Abrar Rahman, James Z Hare, Daniel Cassenti, Carl Busart
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.05683">Data-Driven Distributed Common Operational Picture from Heterogeneous Platforms using Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The integration of unmanned platforms equipped with advanced sensors promises to enhance situational awareness and mitigate the "fog of war" in military operations. However, managing the vast influx of data from these platforms poses a significant challenge for Command and Control (C2) systems. This study presents a novel multi-agent learning framework to address this challenge. Our method enables autonomous and secure communication between agents and humans, which in turn enables real-time formation of an interpretable Common Operational Picture (COP). Each agent encodes its perceptions and actions into compact vectors, which are then transmitted, received and decoded to form a COP encompassing the current state of all agents (friendly and enemy) on the battlefield. Using Deep Reinforcement Learning (DRL), we jointly train COP models and agent's action selection policies. We demonstrate resilience to degraded conditions such as denied GPS and disrupted communications. Experimental validation is performed in the Starcraft-2 simulation environment to evaluate the precision of the COPs and robustness of policies. We report less than 5% error in COPs and policies resilient to various adversarial conditions. In summary, our contributions include a method for autonomous COP formation, increased resilience through distributed prediction, and joint training of COP models and multi-agent RL policies. This research advances adaptive and resilient C2, facilitating effective control of heterogeneous unmanned platforms.
<div id='section'>Paperid: <span id='pid'>1295, <a href='https://arxiv.org/pdf/2411.04681.pdf' target='_blank'>https://arxiv.org/pdf/2411.04681.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sven Banisch, Dennis Jacob, Tom Willaert, Eckehard Olbrich
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.04681">A dynamical model of platform choice and online segregation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In order to truly understand how social media might shape online discourses or contribute to societal polarization, we need refined models of platform choice, that is: models that help us understand why users prefer one social media platform over another. This study develops a dynamic model of platform selection, extending Social Feedback Theory by incorporating multi-agent reinforcement learning to capture how user decisions are shaped by past rewards across different platforms. A key parameter ($Î¼$) in the model governs users' tendencies to either seek approval from like-minded peers or engage with opposing views. Our findings reveal that online environments can evolve into suboptimal states characterized by polarized, strongly opinionated echo chambers, even when users prefer diverse perspectives. Interestingly, this polarizing state coexists with another equilibrium, where users gravitate toward a single dominant platform, marginalizing other platforms into extremity. Using agent-based simulations and dynamical systems analysis, our model underscores the complex interplay of user preferences and platform dynamics, offering insights into how digital spaces might be better managed to foster diverse discourse.
<div id='section'>Paperid: <span id='pid'>1296, <a href='https://arxiv.org/pdf/2410.21521.pdf' target='_blank'>https://arxiv.org/pdf/2410.21521.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sriniketh Vangaru, Daniel Rosen, Dylan Green, Raphael Rodriguez, Maxwell Wiecek, Amos Johnson, Alyse M. Jones, William C. Headley
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.21521">A Multi-Agent Reinforcement Learning Testbed for Cognitive Radio Applications</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Technological trends show that Radio Frequency Reinforcement Learning (RFRL) will play a prominent role in the wireless communication systems of the future. Applications of RFRL range from military communications jamming to enhancing WiFi networks. Before deploying algorithms for these purposes, they must be trained in a simulation environment to ensure adequate performance. For this reason, we previously created the RFRL Gym: a standardized, accessible tool for the development and testing of reinforcement learning (RL) algorithms in the wireless communications space. This environment leveraged the OpenAI Gym framework and featured customizable simulation scenarios within the RF spectrum. However, the RFRL Gym was limited to training a single RL agent per simulation; this is not ideal, as most real-world RF scenarios will contain multiple intelligent agents in cooperative, competitive, or mixed settings, which is a natural consequence of spectrum congestion. Therefore, through integration with Ray RLlib, multi-agent reinforcement learning (MARL) functionality for training and assessment has been added to the RFRL Gym, making it even more of a robust tool for RF spectrum simulation. This paper provides an overview of the updated RFRL Gym environment. In this work, the general framework of the tool is described relative to comparable existing resources, highlighting the significant additions and refactoring we have applied to the Gym. Afterward, results from testing various RF scenarios in the MARL environment and future additions are discussed.
<div id='section'>Paperid: <span id='pid'>1297, <a href='https://arxiv.org/pdf/2410.18112.pdf' target='_blank'>https://arxiv.org/pdf/2410.18112.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rui Du, Kai Zhao, Jinlong Hou, Qiang Zhang, Peter Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.18112">OPTIMA: Optimized Policy for Intelligent Multi-Agent Systems Enables Coordination-Aware Autonomous Vehicles</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Coordination among connected and autonomous vehicles (CAVs) is advancing due to developments in control and communication technologies. However, much of the current work is based on oversimplified and unrealistic task-specific assumptions, which may introduce vulnerabilities. This is critical because CAVs not only interact with their environment but are also integral parts of it. Insufficient exploration can result in policies that carry latent risks, highlighting the need for methods that explore the environment both extensively and efficiently. This work introduces OPTIMA, a novel distributed reinforcement learning framework for cooperative autonomous vehicle tasks. OPTIMA alternates between thorough data sampling from environmental interactions and multi-agent reinforcement learning algorithms to optimize CAV cooperation, emphasizing both safety and efficiency. Our goal is to improve the generality and performance of CAVs in highly complex and crowded scenarios. Furthermore, the industrial-scale distributed training system easily adapts to different algorithms, reward functions, and strategies.
<div id='section'>Paperid: <span id='pid'>1298, <a href='https://arxiv.org/pdf/2410.07426.pdf' target='_blank'>https://arxiv.org/pdf/2410.07426.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kamil Khan, Sudeep Pasricha
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.07426">CAFEEN: A Cooperative Approach for Energy Efficient NoCs with Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In emerging high-performance Network-on-Chip (NoC) architectures, efficient power management is crucial to minimize energy consumption. We propose a novel framework called CAFEEN that employs both heuristic-based fine-grained and machine learning-based coarse-grained power-gating for energy-efficient NoCs. CAFEEN uses a fine-grained method to activate only essential NoC buffers during lower network loads. It switches to a coarse-grained method at peak loads to minimize compounding wake-up overhead using multi-agent reinforcement learning. Results show that CAFEEN adaptively balances power-efficiency with performance, reducing total energy by 2.60x for single application workloads and 4.37x for multi-application workloads, compared to state-of-the-art NoC power-gating frameworks.
<div id='section'>Paperid: <span id='pid'>1299, <a href='https://arxiv.org/pdf/2409.17443.pdf' target='_blank'>https://arxiv.org/pdf/2409.17443.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Cameron Mehlman, Joseph Abramov, Gregory Falco
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.17443">Cat-and-Mouse Satellite Dynamics: Divergent Adversarial Reinforcement Learning for Contested Multi-Agent Space Operations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As space becomes increasingly crowded and contested, robust autonomous capabilities for multi-agent environments are gaining critical importance. Current autonomous systems in space primarily rely on optimization-based path planning or long-range orbital maneuvers, which have not yet proven effective in adversarial scenarios where one satellite is actively pursuing another. We introduce Divergent Adversarial Reinforcement Learning (DARL), a two-stage Multi-Agent Reinforcement Learning (MARL) approach designed to train autonomous evasion strategies for satellites engaged with multiple adversarial spacecraft. Our method enhances exploration during training by promoting diverse adversarial strategies, leading to more robust and adaptable evader models. We validate DARL through a cat-and-mouse satellite scenario, modeled as a partially observable multi-agent capture the flag game where two adversarial `cat' spacecraft pursue a single `mouse' evader. DARL's performance is compared against several benchmarks, including an optimization-based satellite path planner, demonstrating its ability to produce highly robust models for adversarial multi-agent space environments.
<div id='section'>Paperid: <span id='pid'>1300, <a href='https://arxiv.org/pdf/2409.05291.pdf' target='_blank'>https://arxiv.org/pdf/2409.05291.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Feng Zhu, Robert W. Heath, Aritra Mitra
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.05291">Towards Fast Rates for Federated and Multi-Task Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We consider a setting involving $N$ agents, where each agent interacts with an environment modeled as a Markov Decision Process (MDP). The agents' MDPs differ in their reward functions, capturing heterogeneous objectives/tasks. The collective goal of the agents is to communicate intermittently via a central server to find a policy that maximizes the average of long-term cumulative rewards across environments. The limited existing work on this topic either only provide asymptotic rates, or generate biased policies, or fail to establish any benefits of collaboration. In response, we propose Fast-FedPG - a novel federated policy gradient algorithm with a carefully designed bias-correction mechanism. Under a gradient-domination condition, we prove that our algorithm guarantees (i) fast linear convergence with exact gradients, and (ii) sub-linear rates that enjoy a linear speedup w.r.t. the number of agents with noisy, truncated policy gradients. Notably, in each case, the convergence is to a globally optimal policy with no heterogeneity-induced bias. In the absence of gradient-domination, we establish convergence to a first-order stationary point at a rate that continues to benefit from collaboration.
<div id='section'>Paperid: <span id='pid'>1301, <a href='https://arxiv.org/pdf/2408.15337.pdf' target='_blank'>https://arxiv.org/pdf/2408.15337.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Congzhou Li, Zhouxiang Wu, Divya Khanure, Jason P. Jue
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.15337">A Multi-Agent Reinforcement Learning Scheme for SFC Placement in Edge Computing Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the 5G era and beyond, it is favorable to deploy latency-sensitive and reliability-aware services on edge computing networks in which the computing and network resources are more limited compared to cloud and core networks but can respond more promptly. These services can be composed as Service Function Chains (SFCs) which consist of a sequence of ordered Virtual Network Functions (VNFs). To achieve efficient edge resources allocation for SFC requests and optimal profit for edge service providers, we formulate the SFC placement problem in an edge environment and propose a multi-agent Reinforcement Learning (RL) scheme to address the problem. The proposed scheme employs a set of RL agents to collaboratively make SFC placement decisions, such as path selection, VNF configuration, and VNF deployment. Simulation results show our model can improve the profit of edge service providers by 12\% compared with a heuristic solution.
<div id='section'>Paperid: <span id='pid'>1302, <a href='https://arxiv.org/pdf/2408.07098.pdf' target='_blank'>https://arxiv.org/pdf/2408.07098.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Songchen Fu, Shaojing Zhao, Ta Li, YongHong Yan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.07098">QTypeMix: Enhancing Multi-Agent Cooperative Strategies through Heterogeneous and Homogeneous Value Decomposition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In multi-agent cooperative tasks, the presence of heterogeneous agents is familiar. Compared to cooperation among homogeneous agents, collaboration requires considering the best-suited sub-tasks for each agent. However, the operation of multi-agent systems often involves a large amount of complex interaction information, making it more challenging to learn heterogeneous strategies. Related multi-agent reinforcement learning methods sometimes use grouping mechanisms to form smaller cooperative groups or leverage prior domain knowledge to learn strategies for different roles. In contrast, agents should learn deeper role features without relying on additional information. Therefore, we propose QTypeMix, which divides the value decomposition process into homogeneous and heterogeneous stages. QTypeMix learns to extract type features from local historical observations through the TE loss. In addition, we introduce advanced network structures containing attention mechanisms and hypernets to enhance the representation capability and achieve the value decomposition process. The results of testing the proposed method on 14 maps from SMAC and SMACv2 show that QTypeMix achieves state-of-the-art performance in tasks of varying difficulty.
<div id='section'>Paperid: <span id='pid'>1303, <a href='https://arxiv.org/pdf/2408.04295.pdf' target='_blank'>https://arxiv.org/pdf/2408.04295.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aditya Kapoor, Benjamin Freed, Howie Choset, Jeff Schneider
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.04295">Assigning Credit with Partial Reward Decoupling in Multi-Agent Proximal Policy Optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent proximal policy optimization (MAPPO) has recently demonstrated state-of-the-art performance on challenging multi-agent reinforcement learning tasks. However, MAPPO still struggles with the credit assignment problem, wherein the sheer difficulty in ascribing credit to individual agents' actions scales poorly with team size. In this paper, we propose a multi-agent reinforcement learning algorithm that adapts recent developments in credit assignment to improve upon MAPPO. Our approach leverages partial reward decoupling (PRD), which uses a learned attention mechanism to estimate which of a particular agent's teammates are relevant to its learning updates. We use this estimate to dynamically decompose large groups of agents into smaller, more manageable subgroups. We empirically demonstrate that our approach, PRD-MAPPO, decouples agents from teammates that do not influence their expected future reward, thereby streamlining credit assignment. We additionally show that PRD-MAPPO yields significantly higher data efficiency and asymptotic performance compared to both MAPPO and other state-of-the-art methods across several multi-agent tasks, including StarCraft II. Finally, we propose a version of PRD-MAPPO that is applicable to \textit{shared} reward settings, where PRD was previously not applicable, and empirically show that this also leads to performance improvements over MAPPO.
<div id='section'>Paperid: <span id='pid'>1304, <a href='https://arxiv.org/pdf/2407.20041.pdf' target='_blank'>https://arxiv.org/pdf/2407.20041.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Veit-Lorenz Heuthe, Emanuele Panizon, Hongri Gu, Clemens Bechinger
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.20041">Counterfactual rewards promote collective transport using individually controlled swarm microrobots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Swarm robots offer fascinating opportunities to perform complex tasks beyond the capabilities of individual machines. Just as a swarm of ants collectively moves a large object, similar functions can emerge within a group of robots through individual strategies based on local sensing. However, realizing collective functions with individually controlled microrobots is particularly challenging due to their micrometer size, large number of degrees of freedom, strong thermal noise relative to the propulsion speed, complex physical coupling between neighboring microrobots, and surface collisions. Here, we implement Multi-Agent Reinforcement Learning (MARL) to generate a control strategy for up to 200 microrobots whose motions are individually controlled by laser spots. During the learning process, we employ so-called counterfactual rewards that automatically assign credit to the individual microrobots, which allows for fast and unbiased training. With the help of this efficient reward scheme, swarm microrobots learn to collectively transport a large cargo object to an arbitrary position and orientation, similar to ant swarms. We demonstrate that this flexible and versatile swarm robotic system is robust to variations in group size, the presence of malfunctioning units, and environmental noise. Such control strategies can potentially enable complex and automated assembly of mobile micromachines, programmable drug delivery capsules, and other advanced lab-on-a-chip applications.
<div id='section'>Paperid: <span id='pid'>1305, <a href='https://arxiv.org/pdf/2406.06041.pdf' target='_blank'>https://arxiv.org/pdf/2406.06041.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hafez Ghaemi, Shirin Jamshidi, Mohammad Mashreghi, Majid Nili Ahmadabadi, Hamed Kebriaei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.06041">Risk Sensitivity in Markov Games and Multi-Agent Reinforcement Learning: A Systematic Review</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Markov games (MGs) and multi-agent reinforcement learning (MARL) are studied to model decision making in multi-agent systems. Traditionally, the objective in MG and MARL has been risk-neutral, i.e., agents are assumed to optimize a performance metric such as expected return, without taking into account subjective or cognitive preferences of themselves or of other agents. However, ignoring such preferences leads to inaccurate models of decision making in many real-world scenarios in finance, operations research, and behavioral economics. Therefore, when these preferences are present, it is necessary to incorporate a suitable measure of risk into the optimization objective of agents, which opens the door to risk-sensitive MG and MARL. In this paper, we systemically review the literature on risk sensitivity in MG and MARL that has been growing in recent years alongside other areas of reinforcement learning and game theory. We define and mathematically describe different risk measures used in MG and MARL and individually for each measure, discuss articles that incorporate it. Finally, we identify recent trends in theoretical and applied works in the field and discuss possible directions of future research.
<div id='section'>Paperid: <span id='pid'>1306, <a href='https://arxiv.org/pdf/2405.19998.pdf' target='_blank'>https://arxiv.org/pdf/2405.19998.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hyungho Na, Il-chul Moon
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.19998">LAGMA: LAtent Goal-guided Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In cooperative multi-agent reinforcement learning (MARL), agents collaborate to achieve common goals, such as defeating enemies and scoring a goal. However, learning goal-reaching paths toward such a semantic goal takes a considerable amount of time in complex tasks and the trained model often fails to find such paths. To address this, we present LAtent Goal-guided Multi-Agent reinforcement learning (LAGMA), which generates a goal-reaching trajectory in latent space and provides a latent goal-guided incentive to transitions toward this reference trajectory. LAGMA consists of three major components: (a) quantized latent space constructed via a modified VQ-VAE for efficient sample utilization, (b) goal-reaching trajectory generation via extended VQ codebook, and (c) latent goal-guided intrinsic reward generation to encourage transitions towards the sampled goal-reaching path. The proposed method is evaluated by StarCraft II with both dense and sparse reward settings and Google Research Football. Empirical results show further performance improvement over state-of-the-art baselines.
<div id='section'>Paperid: <span id='pid'>1307, <a href='https://arxiv.org/pdf/2405.07318.pdf' target='_blank'>https://arxiv.org/pdf/2405.07318.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ananya Hazarika, Mehdi Rahmati
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.07318">AdaptNet: Rethinking Sensing and Communication for a Seamless Internet of Drones Experience</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the evolving era of Unmanned Aerial Vehicles (UAVs), the emphasis has moved from mere data collection to strategically obtaining timely and relevant data within the Internet of Drones (IoDs) ecosystem. However, the unpredictable conditions in dynamic IoDs pose safety challenges for drones. Addressing this, our approach introduces a multi-UAV framework using spatial-temporal clustering and the Frechet distance for enhancing reliability. Seamlessly coupled with Integrated Sensing and Communication (ISAC), it enhances the precision and agility of UAV networks. Our Multi-Agent Reinforcement Learning (MARL) mechanism ensures UAVs adapt strategies through ongoing environmental interactions and enhancing intelligent sensing. This focus ensures operational safety and efficiency, considering data capture and transmission viability. By evaluating the relevance of the sensed information, we can communicate only the most crucial data variations beyond a set threshold and optimize bandwidth usage. Our methodology transforms the UAV domain, transitioning drones from data gatherers to adept information orchestrators, establishing a benchmark for efficiency and adaptability in modern aerial systems.
<div id='section'>Paperid: <span id='pid'>1308, <a href='https://arxiv.org/pdf/2404.13860.pdf' target='_blank'>https://arxiv.org/pdf/2404.13860.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Huan Bao, Kaimin Wei, Yongdong Wu, Jin Qian, Robert H. Deng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.13860">Distributional Black-Box Model Inversion Attack with Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A Model Inversion (MI) attack based on Generative Adversarial Networks (GAN) aims to recover the private training data from complex deep learning models by searching codes in the latent space. However, they merely search a deterministic latent space such that the found latent code is usually suboptimal. In addition, the existing distributional MI schemes assume that an attacker can access the structures and parameters of the target model, which is not always viable in practice. To overcome the above shortcomings, this paper proposes a novel Distributional Black-Box Model Inversion (DBB-MI) attack by constructing the probabilistic latent space for searching the target privacy data. Specifically, DBB-MI does not need the target model parameters or specialized GAN training. Instead, it finds the latent probability distribution by combining the output of the target model with multi-agent reinforcement learning techniques. Then, it randomly chooses latent codes from the latent probability distribution for recovering the private data. As the latent probability distribution closely aligns with the target privacy data in latent space, the recovered data will leak the privacy of training samples of the target model significantly. Abundant experiments conducted on diverse datasets and networks show that the present DBB-MI has better performance than state-of-the-art in attack accuracy, K-nearest neighbor feature distance, and Peak Signal-to-Noise Ratio.
<div id='section'>Paperid: <span id='pid'>1309, <a href='https://arxiv.org/pdf/2404.06387.pdf' target='_blank'>https://arxiv.org/pdf/2404.06387.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nancirose Piazza, Amirhossein Karimia, Behnia Soleymanib, Vahid Behzadan, Stefan Sarkadi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.06387">Robust Coordination under Misaligned Communication via Power Regularization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Effective communication in Multi-Agent Reinforcement Learning (MARL) can significantly enhance coordination and collaborative performance in complex and partially observable environments. However, reliance on communication can also introduce vulnerabilities when agents are misaligned, potentially leading to adversarial interactions that exploit implicit assumptions of cooperative intent. Prior work has addressed adversarial behavior through power regularization through controlling the influence one agent exerts over another, but has largely overlooked the role of communication in these dynamics. This paper introduces Communicative Power Regularization (CPR), extending power regularization specifically to communication channels. By explicitly quantifying and constraining agents' communicative influence during training, CPR actively mitigates vulnerabilities arising from misaligned or adversarial communications. Evaluations across benchmark environments Red-Door-Blue-Door, Predator-Prey, and Grid Coverage demonstrate that our approach significantly enhances robustness to adversarial communication while preserving cooperative performance, offering a practical framework for secure and resilient cooperative MARL systems.
<div id='section'>Paperid: <span id='pid'>1310, <a href='https://arxiv.org/pdf/2404.02362.pdf' target='_blank'>https://arxiv.org/pdf/2404.02362.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yusei Naito, Tomohiko Jimbo, Tadashi Odashima, Takamitsu Matsubara
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.02362">Task-priority Intermediated Hierarchical Distributed Policies: Reinforcement Learning of Adaptive Multi-robot Cooperative Transport</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-robot cooperative transport is crucial in logistics, housekeeping, and disaster response. However, it poses significant challenges in environments where objects of various weights are mixed and the number of robots and objects varies. This paper presents Task-priority Intermediated Hierarchical Distributed Policies (TIHDP), a multi-agent Reinforcement Learning (RL) framework that addresses these challenges through a hierarchical policy structure. TIHDP consists of three layers: task allocation policy (higher layer), dynamic task priority (intermediate layer), and robot control policy (lower layer). Whereas the dynamic task priority layer can manipulate the priority of any object to be transported by receiving global object information and communicating with other robots, the task allocation and robot control policies are restricted by local observations/actions so that they are not affected by changes in the number of objects and robots. Through simulations and real-robot demonstrations, TIHDP shows promising adaptability and performance of the learned multi-robot cooperative transport, even in environments with varying numbers of robots and objects. Video is available at https://youtu.be/Rmhv5ovj0xM
<div id='section'>Paperid: <span id='pid'>1311, <a href='https://arxiv.org/pdf/2403.15916.pdf' target='_blank'>https://arxiv.org/pdf/2403.15916.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Albin Larsson Forsberg, Alexandros Nikou, Aneta Vulgarakis Feljan, Jana Tumova
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.15916">Multi-agent transformer-accelerated RL for satisfaction of STL specifications</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>One of the main challenges in multi-agent reinforcement learning is scalability as the number of agents increases. This issue is further exacerbated if the problem considered is temporally dependent. State-of-the-art solutions today mainly follow centralized training with decentralized execution paradigm in order to handle the scalability concerns. In this paper, we propose time-dependent multi-agent transformers which can solve the temporally dependent multi-agent problem efficiently with a centralized approach via the use of transformers that proficiently handle the large input. We highlight the efficacy of this method on two problems and use tools from statistics to verify the probability that the trajectories generated under the policy satisfy the task. The experiments show that our approach has superior performance against the literature baseline algorithms in both cases.
<div id='section'>Paperid: <span id='pid'>1312, <a href='https://arxiv.org/pdf/2403.13639.pdf' target='_blank'>https://arxiv.org/pdf/2403.13639.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiyue Luo, Jun Xu, Fanglin Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.13639">Multi-agent Reinforcement Traffic Signal Control based on Interpretable Influence Mechanism and Biased ReLU Approximation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Traffic signal control is important in intelligent transportation system, of which cooperative control is difficult to realize but yet vital. Many methods model multi-intersection traffic networks as grids and address the problem using multi-agent reinforcement learning (RL). Despite these existing studies, there is an opportunity to further enhance our understanding of the connectivity and globality of the traffic networks by capturing the spatiotemporal traffic information with efficient neural networks in deep RL. In this paper, we propose a novel multi-agent actor-critic framework based on an interpretable influence mechanism with a centralized learning and decentralized execution method. Specifically, we first construct an actor-critic framework, for which the piecewise linear neural network (PWLNN), named biased ReLU (BReLU), is used as the function approximator to obtain a more accurate and theoretically grounded approximation. Finally, our proposed framework is validated on two synthetic traffic networks to coordinate signal control between intersections, achieving lower traffic delays across the entire traffic network compared to state-of-the-art (SOTA) performance.
<div id='section'>Paperid: <span id='pid'>1313, <a href='https://arxiv.org/pdf/2403.04416.pdf' target='_blank'>https://arxiv.org/pdf/2403.04416.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Debasmita Dey, Nirnay Ghosh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.04416">iTRPL: An Intelligent and Trusted RPL Protocol based on Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Routing Protocol for Low Power and Lossy Networks (RPL) is the de-facto routing standard in IoT networks. It enables nodes to collaborate and autonomously build ad-hoc networks modeled by tree-like destination-oriented direct acyclic graphs (DODAG). Despite its widespread usage in industry and healthcare domains, RPL is susceptible to insider attacks. Although the state-of-the-art RPL ensures that only authenticated nodes participate in DODAG, such hard security measures are still inadequate to prevent insider threats. This entails a need to integrate soft security mechanisms to support decision-making. This paper proposes iTRPL, an intelligent and behavior-based framework that incorporates trust to segregate honest and malicious nodes within a DODAG. It also leverages multi-agent reinforcement learning (MARL) to make autonomous decisions concerning the DODAG. The framework enables a parent node to compute the trust for its child and decide if the latter can join the DODAG. It tracks the behavior of the child node, updates the trust, computes the rewards (or penalties), and shares with the root. The root aggregates the rewards/penalties of all nodes, computes the overall return, and decides via its $Îµ$-Greedy MARL module if the DODAG will be retained or modified for the future. A simulation-based performance evaluation demonstrates that iTRPL learns to make optimal decisions with time.
<div id='section'>Paperid: <span id='pid'>1314, <a href='https://arxiv.org/pdf/2403.01112.pdf' target='_blank'>https://arxiv.org/pdf/2403.01112.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hyungho Na, Yunkyeong Seo, Il-chul Moon
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.01112">Efficient Episodic Memory Utilization of Cooperative Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In cooperative multi-agent reinforcement learning (MARL), agents aim to achieve a common goal, such as defeating enemies or scoring a goal. Existing MARL algorithms are effective but still require significant learning time and often get trapped in local optima by complex tasks, subsequently failing to discover a goal-reaching policy. To address this, we introduce Efficient episodic Memory Utilization (EMU) for MARL, with two primary objectives: (a) accelerating reinforcement learning by leveraging semantically coherent memory from an episodic buffer and (b) selectively promoting desirable transitions to prevent local convergence. To achieve (a), EMU incorporates a trainable encoder/decoder structure alongside MARL, creating coherent memory embeddings that facilitate exploratory memory recall. To achieve (b), EMU introduces a novel reward structure called episodic incentive based on the desirability of states. This reward improves the TD target in Q-learning and acts as an additional incentive for desirable transitions. We provide theoretical support for the proposed incentive and demonstrate the effectiveness of EMU compared to conventional episodic control. The proposed method is evaluated in StarCraft II and Google Research Football, and empirical results indicate further performance improvement over state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>1315, <a href='https://arxiv.org/pdf/2402.10803.pdf' target='_blank'>https://arxiv.org/pdf/2402.10803.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Johann Lussange, Stefano Vrizzi, Stefano Palminteri, Boris Gutkin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.10803">Modelling crypto markets by multi-agent reinforcement learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Building on a previous foundation work (Lussange et al. 2020), this study introduces a multi-agent reinforcement learning (MARL) model simulating crypto markets, which is calibrated to the Binance's daily closing prices of $153$ cryptocurrencies that were continuously traded between 2018 and 2022. Unlike previous agent-based models (ABM) or multi-agent systems (MAS) which relied on zero-intelligence agents or single autonomous agent methodologies, our approach relies on endowing agents with reinforcement learning (RL) techniques in order to model crypto markets. This integration is designed to emulate, with a bottom-up approach to complexity inference, both individual and collective agents, ensuring robustness in the recent volatile conditions of such markets and during the COVID-19 era. A key feature of our model also lies in the fact that its autonomous agents perform asset price valuation based on two sources of information: the market prices themselves, and the approximation of the crypto assets fundamental values beyond what those market prices are. Our MAS calibration against real market data allows for an accurate emulation of crypto markets microstructure and probing key market behaviors, in both the bearish and bullish regimes of that particular time period.
<div id='section'>Paperid: <span id='pid'>1316, <a href='https://arxiv.org/pdf/2402.10222.pdf' target='_blank'>https://arxiv.org/pdf/2402.10222.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chenhao Tong, Maria A. Rodriguez, Richard O. Sinnott
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.10222">Autonomous Vehicle Patrolling Through Deep Reinforcement Learning: Learning to Communicate and Cooperate</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autonomous vehicles are suited for continuous area patrolling problems. Finding an optimal patrolling strategy can be challenging due to unknown environmental factors, such as wind or landscape; or autonomous vehicles' constraints, such as limited battery life or hardware failures. Importantly, patrolling large areas often requires multiple agents to collectively coordinate their actions. However, an optimal coordination strategy is often non-trivial to be manually defined due to the complex nature of patrolling environments. In this paper, we consider a patrolling problem with environmental factors, agent limitations, and three typical cooperation problems -- collision avoidance, congestion avoidance, and patrolling target negotiation. We propose a multi-agent reinforcement learning solution based on a reinforced inter-agent learning (RIAL) method. With this approach, agents are trained to develop their own communication protocol to cooperate during patrolling where faults can and do occur. The solution is validated through simulation experiments and is compared with several state-of-the-art patrolling solutions from different perspectives, including the overall patrol performance, the collision avoidance performance, the efficiency of battery recharging strategies, and the overall fault tolerance.
<div id='section'>Paperid: <span id='pid'>1317, <a href='https://arxiv.org/pdf/2402.08184.pdf' target='_blank'>https://arxiv.org/pdf/2402.08184.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ayesha Siddika Nipu, Siming Liu, Anthony Harris
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.08184">Enabling Multi-Agent Transfer Reinforcement Learning via Scenario Independent Representation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-Agent Reinforcement Learning (MARL) algorithms are widely adopted in tackling complex tasks that require collaboration and competition among agents in dynamic Multi-Agent Systems (MAS). However, learning such tasks from scratch is arduous and may not always be feasible, particularly for MASs with a large number of interactive agents due to the extensive sample complexity. Therefore, reusing knowledge gained from past experiences or other agents could efficiently accelerate the learning process and upscale MARL algorithms. In this study, we introduce a novel framework that enables transfer learning for MARL through unifying various state spaces into fixed-size inputs that allow one unified deep-learning policy viable in different scenarios within a MAS. We evaluated our approach in a range of scenarios within the StarCraft Multi-Agent Challenge (SMAC) environment, and the findings show significant enhancements in multi-agent learning performance using maneuvering skills learned from other scenarios compared to agents learning from scratch. Furthermore, we adopted Curriculum Transfer Learning (CTL), enabling our deep learning policy to progressively acquire knowledge and skills across pre-designed homogeneous learning scenarios organized by difficulty levels. This process promotes inter- and intra-agent knowledge transfer, leading to high multi-agent learning performance in more complicated heterogeneous scenarios.
<div id='section'>Paperid: <span id='pid'>1318, <a href='https://arxiv.org/pdf/2402.05906.pdf' target='_blank'>https://arxiv.org/pdf/2402.05906.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hafez Ghaemi, Hamed Kebriaei, Alireza Ramezani Moghaddam, Majid Nili Ahamdabadi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.05906">Risk-Sensitive Multi-Agent Reinforcement Learning in Network Aggregative Markov Games</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Classical multi-agent reinforcement learning (MARL) assumes risk neutrality and complete objectivity for agents. However, in settings where agents need to consider or model human economic or social preferences, a notion of risk must be incorporated into the RL optimization problem. This will be of greater importance in MARL where other human or non-human agents are involved, possibly with their own risk-sensitive policies. In this work, we consider risk-sensitive and non-cooperative MARL with cumulative prospect theory (CPT), a non-convex risk measure and a generalization of coherent measures of risk. CPT is capable of explaining loss aversion in humans and their tendency to overestimate/underestimate small/large probabilities. We propose a distributed sampling-based actor-critic (AC) algorithm with CPT risk for network aggregative Markov games (NAMGs), which we call Distributed Nested CPT-AC. Under a set of assumptions, we prove the convergence of the algorithm to a subjective notion of Markov perfect Nash equilibrium in NAMGs. The experimental results show that subjective CPT policies obtained by our algorithm can be different from the risk-neutral ones, and agents with a higher loss aversion are more inclined to socially isolate themselves in an NAMG.
<div id='section'>Paperid: <span id='pid'>1319, <a href='https://arxiv.org/pdf/2402.02957.pdf' target='_blank'>https://arxiv.org/pdf/2402.02957.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Abhishek Mondal, Deepak Mishra, Ganesh Prasad, George C. Alexandropoulos, Azzam Alnahari, Riku Jantti
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.02957">Multi-Agent Reinforcement Learning for Offloading Cellular Communications with Cooperating UAVs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Effective solutions for intelligent data collection in terrestrial cellular networks are crucial, especially in the context of Internet of Things applications. The limited spectrum and coverage area of terrestrial base stations pose challenges in meeting the escalating data rate demands of network users. Unmanned aerial vehicles, known for their high agility, mobility, and flexibility, present an alternative means to offload data traffic from terrestrial BSs, serving as additional access points. This paper introduces a novel approach to efficiently maximize the utilization of multiple UAVs for data traffic offloading from terrestrial BSs. Specifically, the focus is on maximizing user association with UAVs by jointly optimizing UAV trajectories and users association indicators under quality of service constraints. Since, the formulated UAVs control problem is nonconvex and combinatorial, this study leverages the multi agent reinforcement learning framework. In this framework, each UAV acts as an independent agent, aiming to maintain inter UAV cooperative behavior. The proposed approach utilizes the finite state Markov decision process to account for UAVs velocity constraints and the relationship between their trajectories and state space. A low complexity distributed state action reward state action algorithm is presented to determine UAVs optimal sequential decision making policies over training episodes. The extensive simulation results validate the proposed analysis and offer valuable insights into the optimal UAV trajectories. The derived trajectories demonstrate superior average UAV association performance compared to benchmark techniques such as Q learning and particle swarm optimization.
<div id='section'>Paperid: <span id='pid'>1320, <a href='https://arxiv.org/pdf/2401.13947.pdf' target='_blank'>https://arxiv.org/pdf/2401.13947.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chen Feng, Andrew L. Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.13947">Peer-to-Peer Energy Trading of Solar and Energy Storage: A Networked Multiagent Reinforcement Learning Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Utilizing distributed renewable and energy storage resources in local distribution networks via peer-to-peer (P2P) energy trading has long been touted as a solution to improve energy systems' resilience and sustainability. Consumers and prosumers (those who have energy generation resources), however, do not have the expertise to engage in repeated P2P trading, and the zero-marginal costs of renewables present challenges in determining fair market prices. To address these issues, we propose multi-agent reinforcement learning (MARL) frameworks to help automate consumers' bidding and management of their solar PV and energy storage resources, under a specific P2P clearing mechanism that utilizes the so-called supply-demand ratio. In addition, we show how the MARL frameworks can integrate physical network constraints to realize voltage control, hence ensuring physical feasibility of the P2P energy trading and paving way for real-world implementations.
<div id='section'>Paperid: <span id='pid'>1321, <a href='https://arxiv.org/pdf/2401.10149.pdf' target='_blank'>https://arxiv.org/pdf/2401.10149.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alec Wilson, Ryan Menzies, Neela Morarji, David Foster, Marco Casassa Mont, Esin Turkbeyler, Lisa Gralewski
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.10149">Multi-Agent Reinforcement Learning for Maritime Operational Technology Cyber Security</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper demonstrates the potential for autonomous cyber defence to be applied on industrial control systems and provides a baseline environment to further explore Multi-Agent Reinforcement Learning's (MARL) application to this problem domain. It introduces a simulation environment, IPMSRL, of a generic Integrated Platform Management System (IPMS) and explores the use of MARL for autonomous cyber defence decision-making on generic maritime based IPMS Operational Technology (OT). OT cyber defensive actions are less mature than they are for Enterprise IT. This is due to the relatively brittle nature of OT infrastructure originating from the use of legacy systems, design-time engineering assumptions, and lack of full-scale modern security controls. There are many obstacles to be tackled across the cyber landscape due to continually increasing cyber-attack sophistication and the limitations of traditional IT-centric cyber defence solutions. Traditional IT controls are rarely deployed on OT infrastructure, and where they are, some threats aren't fully addressed. In our experiments, a shared critic implementation of Multi Agent Proximal Policy Optimisation (MAPPO) outperformed Independent Proximal Policy Optimisation (IPPO). MAPPO reached an optimal policy (episode outcome mean of 1) after 800K timesteps, whereas IPPO was only able to reach an episode outcome mean of 0.966 after one million timesteps. Hyperparameter tuning greatly improved training performance. Across one million timesteps the tuned hyperparameters reached an optimal policy whereas the default hyperparameters only managed to win sporadically, with most simulations resulting in a draw. We tested a real-world constraint, attack detection alert success, and found that when alert success probability is reduced to 0.75 or 0.9, the MARL defenders were still able to win in over 97.5% or 99.5% of episodes, respectively.
<div id='section'>Paperid: <span id='pid'>1322, <a href='https://arxiv.org/pdf/2401.08850.pdf' target='_blank'>https://arxiv.org/pdf/2401.08850.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>David Ireland, Giovanni Montana
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.08850">REValueD: Regularised Ensemble Value-Decomposition for Factorisable Markov Decision Processes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Discrete-action reinforcement learning algorithms often falter in tasks with high-dimensional discrete action spaces due to the vast number of possible actions. A recent advancement leverages value-decomposition, a concept from multi-agent reinforcement learning, to tackle this challenge. This study delves deep into the effects of this value-decomposition, revealing that whilst it curtails the over-estimation bias inherent to Q-learning algorithms, it amplifies target variance. To counteract this, we present an ensemble of critics to mitigate target variance. Moreover, we introduce a regularisation loss that helps to mitigate the effects that exploratory actions in one dimension can have on the value of optimal actions in other dimensions. Our novel algorithm, REValueD, tested on discretised versions of the DeepMind Control Suite tasks, showcases superior performance, especially in the challenging humanoid and dog tasks. We further dissect the factors influencing REValueD's performance, evaluating the significance of the regularisation loss and the scalability of REValueD with increasing sub-actions per dimension.
<div id='section'>Paperid: <span id='pid'>1323, <a href='https://arxiv.org/pdf/2401.01481.pdf' target='_blank'>https://arxiv.org/pdf/2401.01481.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shamyo Brotee, Farhan Kabir, Md. Abdur Razzaque, Palash Roy, Md. Mamun-Or-Rashid, Md. Rafiul Hassan, Mohammad Mehedi Hassan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.01481">Optimizing UAV-UGV Coalition Operations: A Hybrid Clustering and Multi-Agent Reinforcement Learning Approach for Path Planning in Obstructed Environment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>One of the most critical applications undertaken by coalitions of Unmanned Aerial Vehicles (UAVs) and Unmanned Ground Vehicles (UGVs) is reaching predefined targets by following the most time-efficient routes while avoiding collisions. Unfortunately, UAVs are hampered by limited battery life, and UGVs face challenges in reachability due to obstacles and elevation variations. Existing literature primarily focuses on one-to-one coalitions, which constrains the efficiency of reaching targets. In this work, we introduce a novel approach for a UAV-UGV coalition with a variable number of vehicles, employing a modified mean-shift clustering algorithm to segment targets into multiple zones. Each vehicle utilizes Multi-agent Deep Deterministic Policy Gradient (MADDPG) and Multi-agent Proximal Policy Optimization (MAPPO), two advanced reinforcement learning algorithms, to form an effective coalition for navigating obstructed environments without collisions. This approach of assigning targets to various circular zones, based on density and range, significantly reduces the time required to reach these targets. Moreover, introducing variability in the number of UAVs and UGVs in a coalition enhances task efficiency by enabling simultaneous multi-target engagement. The results of our experimental evaluation demonstrate that our proposed method substantially surpasses current state-of-the-art techniques, nearly doubling efficiency in terms of target navigation time and task completion rate.
<div id='section'>Paperid: <span id='pid'>1324, <a href='https://arxiv.org/pdf/2312.15674.pdf' target='_blank'>https://arxiv.org/pdf/2312.15674.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiawei Wang, Jian Zhao, Zhengtao Cao, Ruili Feng, Rongjun Qin, Yang Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.15674">Multi-Task Multi-Agent Shared Layers are Universal Cognition of Multi-Agent Coordination</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent reinforcement learning shines as the pinnacle of multi-agent systems, conquering intricate real-world challenges, fostering collaboration and coordination among agents, and unleashing the potential for intelligent decision-making across domains. However, training a multi-agent reinforcement learning network is a formidable endeavor, demanding substantial computational resources to interact with diverse environmental variables, extract state representations, and acquire decision-making knowledge. The recent breakthroughs in large-scale pre-trained models ignite our curiosity: Can we uncover shared knowledge in multi-agent reinforcement learning and leverage pre-trained models to expedite training for future tasks? Addressing this issue, we present an innovative multi-task learning approach that aims to extract and harness common decision-making knowledge, like cooperation and competition, across different tasks. Our approach involves concurrent training of multiple multi-agent tasks, with each task employing independent front-end perception layers while sharing back-end decision-making layers. This effective decoupling of state representation extraction from decision-making allows for more efficient training and better transferability. To evaluate the efficacy of our proposed approach, we conduct comprehensive experiments in two distinct environments: the StarCraft Multi-agent Challenge (SMAC) and the Google Research Football (GRF) environments. The experimental results unequivocally demonstrate the smooth transferability of the shared decision-making network to other tasks, thereby significantly reducing training costs and improving final performance. Furthermore, visualizations authenticate the presence of general multi-agent decision-making knowledge within the shared network layers, further validating the effectiveness of our approach.
<div id='section'>Paperid: <span id='pid'>1325, <a href='https://arxiv.org/pdf/2312.14625.pdf' target='_blank'>https://arxiv.org/pdf/2312.14625.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Taha Eghtesad, Sirui Li, Yevgeniy Vorobeychik, Aron Laszka
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.14625">Multi-Agent Reinforcement Learning for Assessing False-Data Injection Attacks on Transportation Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The increasing reliance of drivers on navigation applications has made transportation networks more susceptible to data-manipulation attacks by malicious actors. Adversaries may exploit vulnerabilities in the data collection or processing of navigation services to inject false information, and to thus interfere with the drivers' route selection. Such attacks can significantly increase traffic congestions, resulting in substantial waste of time and resources, and may even disrupt essential services that rely on road networks. To assess the threat posed by such attacks, we introduce a computational framework to find worst-case data-injection attacks against transportation networks. First, we devise an adversarial model with a threat actor who can manipulate drivers by increasing the travel times that they perceive on certain roads. Then, we employ hierarchical multi-agent reinforcement learning to find an approximate optimal adversarial strategy for data manipulation. We demonstrate the applicability of our approach through simulating attacks on the Sioux Falls, ND network topology.
<div id='section'>Paperid: <span id='pid'>1326, <a href='https://arxiv.org/pdf/2311.14390.pdf' target='_blank'>https://arxiv.org/pdf/2311.14390.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhuoying Chen, Huiping Li, Zhaoxu Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.14390">Directly Attention Loss Adjusted Prioritized Experience Replay</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Prioritized Experience Replay (PER) enables the model to learn more about relatively important samples by artificially changing their accessed frequencies. However, this non-uniform sampling method shifts the state-action distribution that is originally used to estimate Q-value functions, which brings about the estimation deviation. In this article, an novel off policy reinforcement learning training framework called Directly Attention Loss Adjusted Prioritized Experience Replay (DALAP) is proposed, which can directly quantify the changed extent of the shifted distribution through Parallel Self-Attention network, so as to accurately compensate the error. In addition, a Priority-Encouragement mechanism is designed simultaneously to optimize the sample screening criterion, and further improve the training efficiency. In order to verify the effectiveness and generality of DALAP, we integrate it with the value-function based, the policy-gradient based and multi-agent reinforcement learning algorithm, respectively. The multiple groups of comparative experiments show that DALAP has the significant advantages of both improving the convergence rate and reducing the training variance.
<div id='section'>Paperid: <span id='pid'>1327, <a href='https://arxiv.org/pdf/2311.12854.pdf' target='_blank'>https://arxiv.org/pdf/2311.12854.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ghadi Nehme, Ishan Sabane, Tejas Y. Deo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.12854">Enhancing Robotic Manipulation: Harnessing the Power of Multi-Task Reinforcement Learning and Single Life Reinforcement Learning in Meta-World</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>At present, robots typically require extensive training to successfully accomplish a single task. However, to truly enhance their usefulness in real-world scenarios, robots should possess the capability to perform multiple tasks effectively. To address this need, various multi-task reinforcement learning (RL) algorithms have been developed, including multi-task proximal policy optimization (PPO), multi-task trust region policy optimization (TRPO), and multi-task soft-actor critic (SAC). Nevertheless, these algorithms demonstrate optimal performance only when operating within an environment or observation space that exhibits a similar distribution. In reality, such conditions are often not the norm, as robots may encounter scenarios or observations that differ from those on which they were trained. Addressing this challenge, algorithms like Q-Weighted Adversarial Learning (QWALE) attempt to tackle the issue by training the base algorithm (generating prior data) solely for a particular task, rendering it unsuitable for generalization across tasks. So, the aim of this research project is to enable a robotic arm to successfully execute seven distinct tasks within the Meta World environment. To achieve this, a multi-task soft actor-critic (MT-SAC) is employed to train the robotic arm. Subsequently, the trained model will serve as a source of prior data for the single-life RL algorithm. The effectiveness of this MT-QWALE algorithm will be assessed by conducting tests on various target positions (novel positions). In the end, a comparison is provided between the trained MT-SAC and the MT-QWALE algorithm where the MT-QWALE performs better. An ablation study demonstrates that MT-QWALE successfully completes tasks with a slightly larger number of steps even after hiding the final goal position.
<div id='section'>Paperid: <span id='pid'>1328, <a href='https://arxiv.org/pdf/2311.00356.pdf' target='_blank'>https://arxiv.org/pdf/2311.00356.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rizhong Wang, Huiping Li, Di Cui, Demin Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.00356">QFree: A Universal Value Function Factorization for Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Centralized training is widely utilized in the field of multi-agent reinforcement learning (MARL) to assure the stability of training process. Once a joint policy is obtained, it is critical to design a value function factorization method to extract optimal decentralized policies for the agents, which needs to satisfy the individual-global-max (IGM) principle. While imposing additional limitations on the IGM function class can help to meet the requirement, it comes at the cost of restricting its application to more complex multi-agent environments. In this paper, we propose QFree, a universal value function factorization method for MARL. We start by developing mathematical equivalent conditions of the IGM principle based on the advantage function, which ensures that the principle holds without any compromise, removing the conservatism of conventional methods. We then establish a more expressive mixing network architecture that can fulfill the equivalent factorization. In particular, the novel loss function is developed by considering the equivalent conditions as regularization term during policy evaluation in the MARL algorithm. Finally, the effectiveness of the proposed method is verified in a nonmonotonic matrix game scenario. Moreover, we show that QFree achieves the state-of-the-art performance in a general-purpose complex MARL benchmark environment, Starcraft Multi-Agent Challenge (SMAC).
<div id='section'>Paperid: <span id='pid'>1329, <a href='https://arxiv.org/pdf/2310.14348.pdf' target='_blank'>https://arxiv.org/pdf/2310.14348.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Raheeb Hassan, K. M. Shadman Wadith, Md. Mamun or Rashid, Md. Mosaddek Khan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.14348">DePAint: A Decentralized Safe Multi-Agent Reinforcement Learning Algorithm considering Peak and Average Constraints</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The domain of safe multi-agent reinforcement learning (MARL), despite its potential applications in areas ranging from drone delivery and vehicle automation to the development of zero-energy communities, remains relatively unexplored. The primary challenge involves training agents to learn optimal policies that maximize rewards while adhering to stringent safety constraints, all without the oversight of a central controller. These constraints are critical in a wide array of applications. Moreover, ensuring the privacy of sensitive information in decentralized settings introduces an additional layer of complexity, necessitating innovative solutions that uphold privacy while achieving the system's safety and efficiency goals. In this paper, we address the problem of multi-agent policy optimization in a decentralized setting, where agents communicate with their neighbors to maximize the sum of their cumulative rewards while also satisfying each agent's safety constraints. We consider both peak and average constraints. In this scenario, there is no central controller coordinating the agents and both the rewards and constraints are only known to each agent locally/privately. We formulate the problem as a decentralized constrained multi-agent Markov Decision Problem and propose a momentum-based decentralized policy gradient method, DePAint, to solve it. To the best of our knowledge, this is the first privacy-preserving fully decentralized multi-agent reinforcement learning algorithm that considers both peak and average constraints. We then provide theoretical analysis and empirical evaluation of our algorithm in a number of scenarios and compare its performance to centralized algorithms that consider similar constraints.
<div id='section'>Paperid: <span id='pid'>1330, <a href='https://arxiv.org/pdf/2310.05939.pdf' target='_blank'>https://arxiv.org/pdf/2310.05939.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jacob Wiebe, Ranwa Al Mallah, Li Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.05939">Learning Cyber Defence Tactics from Scratch with Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in deep learning techniques have opened new possibilities for designing solutions for autonomous cyber defence. Teams of intelligent agents in computer network defence roles may reveal promising avenues to safeguard cyber and kinetic assets. In a simulated game environment, agents are evaluated on their ability to jointly mitigate attacker activity in host-based defence scenarios. Defender systems are evaluated against heuristic attackers with the goals of compromising network confidentiality, integrity, and availability. Value-based Independent Learning and Centralized Training Decentralized Execution (CTDE) cooperative Multi-Agent Reinforcement Learning (MARL) methods are compared revealing that both approaches outperform a simple multi-agent heuristic defender. This work demonstrates the ability of cooperative MARL to learn effective cyber defence tactics against varied threats.
<div id='section'>Paperid: <span id='pid'>1331, <a href='https://arxiv.org/pdf/2310.02605.pdf' target='_blank'>https://arxiv.org/pdf/2310.02605.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Erica van der Sar, Alessandro Zocca, Sandjai Bhulai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.02605">Multi-Agent Reinforcement Learning for Power Grid Topology Optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent challenges in operating power networks arise from increasing energy demands and unpredictable renewable sources like wind and solar. While reinforcement learning (RL) shows promise in managing these networks, through topological actions like bus and line switching, efficiently handling large action spaces as networks grow is crucial. This paper presents a hierarchical multi-agent reinforcement learning (MARL) framework tailored for these expansive action spaces, leveraging the power grid's inherent hierarchical nature. Experimental results indicate the MARL framework's competitive performance with single-agent RL methods. We also compare different RL algorithms for lower-level agents alongside different policies for higher-order agents.
<div id='section'>Paperid: <span id='pid'>1332, <a href='https://arxiv.org/pdf/2309.14727.pdf' target='_blank'>https://arxiv.org/pdf/2309.14727.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chenyang Miao, Yunduan Cui, Huiyun Li, Xinyu Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.14727">Effective Multi-Agent Deep Reinforcement Learning Control with Relative Entropy Regularization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, a novel Multi-agent Reinforcement Learning (MARL) approach, Multi-Agent Continuous Dynamic Policy Gradient (MACDPP) was proposed to tackle the issues of limited capability and sample efficiency in various scenarios controlled by multiple agents. It alleviates the inconsistency of multiple agents' policy updates by introducing the relative entropy regularization to the Centralized Training with Decentralized Execution (CTDE) framework with the Actor-Critic (AC) structure. Evaluated by multi-agent cooperation and competition tasks and traditional control tasks including OpenAI benchmarks and robot arm manipulation, MACDPP demonstrates significant superiority in learning capability and sample efficiency compared with both related multi-agent and widely implemented signal-agent baselines and therefore expands the potential of MARL in effectively learning challenging control scenarios.
<div id='section'>Paperid: <span id='pid'>1333, <a href='https://arxiv.org/pdf/2309.08776.pdf' target='_blank'>https://arxiv.org/pdf/2309.08776.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Josselin Somerville Roberts, Julia Di
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.08776">Projected Task-Specific Layers for Multi-Task Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-task reinforcement learning could enable robots to scale across a wide variety of manipulation tasks in homes and workplaces. However, generalizing from one task to another and mitigating negative task interference still remains a challenge. Addressing this challenge by successfully sharing information across tasks will depend on how well the structure underlying the tasks is captured. In this work, we introduce our new architecture, Projected Task-Specific Layers (PTSL), that leverages a common policy with dense task-specific corrections through task-specific layers to better express shared and variable task information. We then show that our model outperforms the state of the art on the MT10 and MT50 benchmarks of Meta-World consisting of 10 and 50 goal-conditioned tasks for a Sawyer arm.
<div id='section'>Paperid: <span id='pid'>1334, <a href='https://arxiv.org/pdf/2309.06684.pdf' target='_blank'>https://arxiv.org/pdf/2309.06684.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhuoying Chen, Huiping Li, Rizhong Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.06684">Attention Loss Adjusted Prioritized Experience Replay</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Prioritized Experience Replay (PER) is a technical means of deep reinforcement learning by selecting experience samples with more knowledge quantity to improve the training rate of neural network. However, the non-uniform sampling used in PER inevitably shifts the state-action space distribution and brings the estimation error of Q-value function. In this paper, an Attention Loss Adjusted Prioritized (ALAP) Experience Replay algorithm is proposed, which integrates the improved Self-Attention network with Double-Sampling mechanism to fit the hyperparameter that can regulate the importance sampling weights to eliminate the estimation error caused by PER. In order to verify the effectiveness and generality of the algorithm, the ALAP is tested with value-function based, policy-gradient based and multi-agent reinforcement learning algorithms in OPENAI gym, and comparison studies verify the advantage and efficiency of the proposed training framework.
<div id='section'>Paperid: <span id='pid'>1335, <a href='https://arxiv.org/pdf/2308.14971.pdf' target='_blank'>https://arxiv.org/pdf/2308.14971.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jigang Kim, Dohyun Jang, H. Jin Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.14971">Distributed multi-agent target search and tracking with Gaussian process and reinforcement learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deploying multiple robots for target search and tracking has many practical applications, yet the challenge of planning over unknown or partially known targets remains difficult to address. With recent advances in deep learning, intelligent control techniques such as reinforcement learning have enabled agents to learn autonomously from environment interactions with little to no prior knowledge. Such methods can address the exploration-exploitation tradeoff of planning over unknown targets in a data-driven manner, eliminating the reliance on heuristics typical of traditional approaches and streamlining the decision-making pipeline with end-to-end training. In this paper, we propose a multi-agent reinforcement learning technique with target map building based on distributed Gaussian process. We leverage the distributed Gaussian process to encode belief over the target locations and efficiently plan over unknown targets. We evaluate the performance and transferability of the trained policy in simulation and demonstrate the method on a swarm of micro unmanned aerial vehicles with hardware experiments.
<div id='section'>Paperid: <span id='pid'>1336, <a href='https://arxiv.org/pdf/2307.14266.pdf' target='_blank'>https://arxiv.org/pdf/2307.14266.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jobst Heitzig, JÃ¶rg Oechssler, Christoph PrÃ¶schel, Niranjana Ragavan, Richie YatLong Lo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.14266">Improving International Climate Policy via Mutually Conditional Binding Commitments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper proposes enhancements to the RICE-N simulation and multi-agent reinforcement learning framework to improve the realism of international climate policy negotiations. Acknowledging the framework's value, we highlight the necessity of significant enhancements to address the diverse array of factors in modeling climate negotiations. Building upon our previous work on the "Conditional Commitments Mechanism" (CCF mechanism) we discuss ways to bridge the gap between simulation and reality. We suggest the inclusion of a recommender or planner agent to enhance coordination, address the Real2Sim gap by incorporating social factors and non-party stakeholder sub-agents, and propose enhancements to the underlying Reinforcement Learning solution algorithm. These proposed improvements aim to advance the evaluation and formulation of negotiation protocols for more effective international climate policy decision-making in Rice-N. However, further experimentation and testing are required to determine the implications and effectiveness of these suggestions.
<div id='section'>Paperid: <span id='pid'>1337, <a href='https://arxiv.org/pdf/2307.11432.pdf' target='_blank'>https://arxiv.org/pdf/2307.11432.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Marwan Mousa, Damien van de Berg, Niki Kotecha, Ehecatl Antonio del Rio-Chanona, Max Mowbray
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.11432">An Analysis of Multi-Agent Reinforcement Learning for Decentralized Inventory Control Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Most solutions to the inventory management problem assume a centralization of information that is incompatible with organisational constraints in real supply chain networks. The inventory management problem is a well-known planning problem in operations research, concerned with finding the optimal re-order policy for nodes in a supply chain. While many centralized solutions to the problem exist, they are not applicable to real-world supply chains made up of independent entities. The problem can however be naturally decomposed into sub-problems, each associated with an independent entity, turning it into a multi-agent system. Therefore, a decentralized data-driven solution to inventory management problems using multi-agent reinforcement learning is proposed where each entity is controlled by an agent. Three multi-agent variations of the proximal policy optimization algorithm are investigated through simulations of different supply chain networks and levels of uncertainty. The centralized training decentralized execution framework is deployed, which relies on offline centralization during simulation-based policy identification, but enables decentralization when the policies are deployed online to the real system. Results show that using multi-agent proximal policy optimization with a centralized critic leads to performance very close to that of a centralized data-driven solution and outperforms a distributed model-based solution in most cases while respecting the information constraints of the system.
<div id='section'>Paperid: <span id='pid'>1338, <a href='https://arxiv.org/pdf/2307.08794.pdf' target='_blank'>https://arxiv.org/pdf/2307.08794.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Patrick Emami, Xiangyu Zhang, David Biagioni, Ahmed S. Zamzam
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.08794">Non-Stationary Policy Learning for Multi-Timescale Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In multi-timescale multi-agent reinforcement learning (MARL), agents interact across different timescales. In general, policies for time-dependent behaviors, such as those induced by multiple timescales, are non-stationary. Learning non-stationary policies is challenging and typically requires sophisticated or inefficient algorithms. Motivated by the prevalence of this control problem in real-world complex systems, we introduce a simple framework for learning non-stationary policies for multi-timescale MARL. Our approach uses available information about agent timescales to define a periodic time encoding. In detail, we theoretically demonstrate that the effects of non-stationarity introduced by multiple timescales can be learned by a periodic multi-agent policy. To learn such policies, we propose a policy gradient algorithm that parameterizes the actor and critic with phase-functioned neural networks, which provide an inductive bias for periodicity. The framework's ability to effectively learn multi-timescale policies is validated on a gridworld and building energy management environment.
<div id='section'>Paperid: <span id='pid'>1339, <a href='https://arxiv.org/pdf/2307.05834.pdf' target='_blank'>https://arxiv.org/pdf/2307.05834.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sanae Amani, Khushbu Pahwa, Vladimir Braverman, Lin F. Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.05834">Scaling Distributed Multi-task Reinforcement Learning with Experience Sharing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, DARPA launched the ShELL program, which aims to explore how experience sharing can benefit distributed lifelong learning agents in adapting to new challenges. In this paper, we address this issue by conducting both theoretical and empirical research on distributed multi-task reinforcement learning (RL), where a group of $N$ agents collaboratively solves $M$ tasks without prior knowledge of their identities. We approach the problem by formulating it as linearly parameterized contextual Markov decision processes (MDPs), where each task is represented by a context that specifies the transition dynamics and rewards. To tackle this problem, we propose an algorithm called DistMT-LSVI. First, the agents identify the tasks, and then they exchange information through a central server to derive $Îµ$-optimal policies for the tasks. Our research demonstrates that to achieve $Îµ$-optimal policies for all $M$ tasks, a single agent using DistMT-LSVI needs to run a total number of episodes that is at most $\tilde{\mathcal{O}}({d^3H^6(Îµ^{-2}+c_{\rm sep}^{-2})}\cdot M/N)$, where $c_{\rm sep}>0$ is a constant representing task separability, $H$ is the horizon of each episode, and $d$ is the feature dimension of the dynamics and rewards. Notably, DistMT-LSVI improves the sample complexity of non-distributed settings by a factor of $1/N$, as each agent independently learns $Îµ$-optimal policies for all $M$ tasks using $\tilde{\mathcal{O}}(d^3H^6MÎµ^{-2})$ episodes. Additionally, we provide numerical experiments conducted on OpenAI Gym Atari environments that validate our theoretical findings.
<div id='section'>Paperid: <span id='pid'>1340, <a href='https://arxiv.org/pdf/2307.02691.pdf' target='_blank'>https://arxiv.org/pdf/2307.02691.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qiushi Lin, Hang Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.02691">SACHA: Soft Actor-Critic with Heuristic-Based Attention for Partially Observable Multi-Agent Path Finding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-Agent Path Finding (MAPF) is a crucial component for many large-scale robotic systems, where agents must plan their collision-free paths to their given goal positions. Recently, multi-agent reinforcement learning has been introduced to solve the partially observable variant of MAPF by learning a decentralized single-agent policy in a centralized fashion based on each agent's partial observation. However, existing learning-based methods are ineffective in achieving complex multi-agent cooperation, especially in congested environments, due to the non-stationarity of this setting. To tackle this challenge, we propose a multi-agent actor-critic method called Soft Actor-Critic with Heuristic-Based Attention (SACHA), which employs novel heuristic-based attention mechanisms for both the actors and critics to encourage cooperation among agents. SACHA learns a neural network for each agent to selectively pay attention to the shortest path heuristic guidance from multiple agents within its field of view, thereby allowing for more scalable learning of cooperation. SACHA also extends the existing multi-agent actor-critic framework by introducing a novel critic centered on each agent to approximate $Q$-values. Compared to existing methods that use a fully observable critic, our agent-centered multi-agent actor-critic method results in more impartial credit assignment and better generalizability of the learned policy to MAPF instances with varying numbers of agents and types of environments. We also implement SACHA(C), which embeds a communication module in the agent's policy network to enable information exchange among agents. We evaluate both SACHA and SACHA(C) on a variety of MAPF instances and demonstrate decent improvements over several state-of-the-art learning-based MAPF methods with respect to success rate and solution quality.
<div id='section'>Paperid: <span id='pid'>1341, <a href='https://arxiv.org/pdf/2306.10985.pdf' target='_blank'>https://arxiv.org/pdf/2306.10985.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Julien Perez, Denys Proux, Claude Roux, Michael Niemaz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.10985">LARG, Language-based Automatic Reward and Goal Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Goal-conditioned and Multi-Task Reinforcement Learning (GCRL and MTRL) address numerous problems related to robot learning, including locomotion, navigation, and manipulation scenarios. Recent works focusing on language-defined robotic manipulation tasks have led to the tedious production of massive human annotations to create dataset of textual descriptions associated with trajectories. To leverage reinforcement learning with text-based task descriptions, we need to produce reward functions associated with individual tasks in a scalable manner. In this paper, we leverage recent capabilities of Large Language Models (LLMs) and introduce \larg, Language-based Automatic Reward and Goal Generation, an approach that converts a text-based task description into its corresponding reward and goal-generation functions We evaluate our approach for robotic manipulation and demonstrate its ability to train and execute policies in a scalable manner, without the need for handcrafted reward functions.
<div id='section'>Paperid: <span id='pid'>1342, <a href='https://arxiv.org/pdf/2306.08900.pdf' target='_blank'>https://arxiv.org/pdf/2306.08900.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiangsen Wang, Xianyuan Zhan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.08900">Offline Multi-Agent Reinforcement Learning with Coupled Value Factorization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Offline reinforcement learning (RL) that learns policies from offline datasets without environment interaction has received considerable attention in recent years. Compared with the rich literature in the single-agent case, offline multi-agent RL is still a relatively underexplored area. Most existing methods directly apply offline RL ingredients in the multi-agent setting without fully leveraging the decomposable problem structure, leading to less satisfactory performance in complex tasks. We present OMAC, a new offline multi-agent RL algorithm with coupled value factorization. OMAC adopts a coupled value factorization scheme that decomposes the global value function into local and shared components, and also maintains the credit assignment consistency between the state-value and Q-value functions. Moreover, OMAC performs in-sample learning on the decomposed local state-value functions, which implicitly conducts max-Q operation at the local level while avoiding distributional shift caused by evaluating out-of-distribution actions. Based on the comprehensive evaluations of the offline multi-agent StarCraft II micro-management tasks, we demonstrate the superior performance of OMAC over the state-of-the-art offline multi-agent RL methods.
<div id='section'>Paperid: <span id='pid'>1343, <a href='https://arxiv.org/pdf/2306.08785.pdf' target='_blank'>https://arxiv.org/pdf/2306.08785.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Babatunji Omoniwa, Boris Galkin, Ivana Dusparic
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.08785">Density-Aware Reinforcement Learning to Optimise Energy Efficiency in UAV-Assisted Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Unmanned aerial vehicles (UAVs) serving as aerial base stations can be deployed to provide wireless connectivity to mobile users, such as vehicles. However, the density of vehicles on roads often varies spatially and temporally primarily due to mobility and traffic situations in a geographical area, making it difficult to provide ubiquitous service. Moreover, as energy-constrained UAVs hover in the sky while serving mobile users, they may be faced with interference from nearby UAV cells or other access points sharing the same frequency band, thereby impacting the system's energy efficiency (EE). Recent multi-agent reinforcement learning (MARL) approaches applied to optimise the users' coverage worked well in reasonably even densities but might not perform as well in uneven users' distribution, i.e., in urban road networks with uneven concentration of vehicles. In this work, we propose a density-aware communication-enabled multi-agent decentralised double deep Q-network (DACEMAD-DDQN) approach that maximises the total system's EE by jointly optimising the trajectory of each UAV, the number of connected users, and the UAVs' energy consumption while keeping track of dense and uneven users' distribution. Our result outperforms state-of-the-art MARL approaches in terms of EE by as much as 65% - 85%.
<div id='section'>Paperid: <span id='pid'>1344, <a href='https://arxiv.org/pdf/2306.05016.pdf' target='_blank'>https://arxiv.org/pdf/2306.05016.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinhang Li, Yiying Yang, Zheng Yuan, Zhe Wang, Qinwen Wang, Chen Xu, Lei Li, Jianhua He, Lin Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.05016">Progression Cognition Reinforcement Learning with Prioritized Experience for Multi-Vehicle Pursuit</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-vehicle pursuit (MVP) such as autonomous police vehicles pursuing suspects is important but very challenging due to its mission and safety critical nature. While multi-agent reinforcement learning (MARL) algorithms have been proposed for MVP problem in structured grid-pattern roads, the existing algorithms use randomly training samples in centralized learning, which leads to homogeneous agents showing low collaboration performance. For the more challenging problem of pursuing multiple evading vehicles, these algorithms typically select a fixed target evading vehicle for pursuing vehicles without considering dynamic traffic situation, which significantly reduces pursuing success rate. To address the above problems, this paper proposes a Progression Cognition Reinforcement Learning with Prioritized Experience for MVP (PEPCRL-MVP) in urban multi-intersection dynamic traffic scenes. PEPCRL-MVP uses a prioritization network to assess the transitions in the global experience replay buffer according to the parameters of each MARL agent. With the personalized and prioritized experience set selected via the prioritization network, diversity is introduced to the learning process of MARL, which can improve collaboration and task related performance. Furthermore, PEPCRL-MVP employs an attention module to extract critical features from complex urban traffic environments. These features are used to develop progression cognition method to adaptively group pursuing vehicles. Each group efficiently target one evading vehicle in dynamic driving environments. Extensive experiments conducted with a simulator over unstructured roads of an urban area show that PEPCRL-MVP is superior to other state-of-the-art methods. Specifically, PEPCRL-MVP improves pursuing efficiency by 3.95% over TD3-DMAP and its success rate is 34.78% higher than that of MADDPG. Codes are open sourced.
<div id='section'>Paperid: <span id='pid'>1345, <a href='https://arxiv.org/pdf/2305.18875.pdf' target='_blank'>https://arxiv.org/pdf/2305.18875.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Flora Charbonnier, Bei Peng, Thomas Morstyn, Malcolm McCulloch
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.18875">Centralised rehearsal of decentralised cooperation: Multi-agent reinforcement learning for the scalable coordination of residential energy flexibility</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper investigates how deep multi-agent reinforcement learning can enable the scalable and privacy-preserving coordination of residential energy flexibility. The coordination of distributed resources such as electric vehicles and heating will be critical to the successful integration of large shares of renewable energy in our electricity grid and, thus, to help mitigate climate change. The pre-learning of individual reinforcement learning policies can enable distributed control with no sharing of personal data required during execution. However, previous approaches for multi-agent reinforcement learning-based distributed energy resources coordination impose an ever greater training computational burden as the size of the system increases. We therefore adopt a deep multi-agent actor-critic method which uses a \emph{centralised but factored critic} to rehearse coordination ahead of execution. Results show that coordination is achieved at scale, with minimal information and communication infrastructure requirements, no interference with daily activities, and privacy protection. Significant savings are obtained for energy users, the distribution network and greenhouse gas emissions. Moreover, training times are nearly 40 times shorter than with a previous state-of-the-art reinforcement learning approach without the factored critic for 30 homes.
<div id='section'>Paperid: <span id='pid'>1346, <a href='https://arxiv.org/pdf/2305.14586.pdf' target='_blank'>https://arxiv.org/pdf/2305.14586.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kangkang Duan, Christine Wun Ki Suen, Zhengbo Zou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.14586">MARC: A multi-agent robots control framework for enhancing reinforcement learning in construction tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Letting robots emulate human behavior has always posed a challenge, particularly in scenarios involving multiple robots. In this paper, we presented a framework aimed at achieving multi-agent reinforcement learning for robot control in construction tasks. The construction industry often necessitates complex interactions and coordination among multiple robots, demanding a solution that enables effective collaboration and efficient task execution. Our proposed framework leverages the principles of proximal policy optimization and developed a multi-agent version to enable the robots to acquire sophisticated control policies. We evaluated the effectiveness of our framework by learning four different collaborative tasks in the construction environments. The results demonstrated the capability of our approach in enabling multiple robots to learn and adapt their behaviors in complex construction tasks while effectively preventing collisions. Results also revealed the potential of combining and exploring the advantages of reinforcement learning algorithms and inverse kinematics. The findings from this research contributed to the advancement of multi-agent reinforcement learning in the domain of construction robotics. By enabling robots to behave like human counterparts and collaborate effectively, we pave the way for more efficient, flexible, and intelligent construction processes.
<div id='section'>Paperid: <span id='pid'>1347, <a href='https://arxiv.org/pdf/2305.10548.pdf' target='_blank'>https://arxiv.org/pdf/2305.10548.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Daniel Waelchli, Pascal Weber, Petros Koumoutsakos
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.10548">Discovering Individual Rewards in Collective Behavior through Inverse Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The discovery of individual objectives in collective behavior of complex dynamical systems such as fish schools and bacteria colonies is a long-standing challenge. Inverse reinforcement learning is a potent approach for addressing this challenge but its applicability to dynamical systems, involving continuous state-action spaces and multiple interacting agents, has been limited. In this study, we tackle this challenge by introducing an off-policy inverse multi-agent reinforcement learning algorithm (IMARL). Our approach combines the ReF-ER techniques with guided cost learning. By leveraging demonstrations, our algorithm automatically uncovers the reward function and learns an effective policy for the agents. Through extensive experimentation, we demonstrate that the proposed policy captures the behavior observed in the provided data, and achieves promising results across problem domains including single agent models in the OpenAI gym and multi-agent models of schooling behavior. The present study shows that the proposed IMARL algorithm is a significant step towards understanding collective dynamics from the perspective of its constituents, and showcases its value as a tool for studying complex physical systems exhibiting collective behaviour.
<div id='section'>Paperid: <span id='pid'>1348, <a href='https://arxiv.org/pdf/2305.10378.pdf' target='_blank'>https://arxiv.org/pdf/2305.10378.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kayla Boggess, Sarit Kraus, Lu Feng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.10378">Explainable Multi-Agent Reinforcement Learning for Temporal Queries</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As multi-agent reinforcement learning (MARL) systems are increasingly deployed throughout society, it is imperative yet challenging for users to understand the emergent behaviors of MARL agents in complex environments. This work presents an approach for generating policy-level contrastive explanations for MARL to answer a temporal user query, which specifies a sequence of tasks completed by agents with possible cooperation. The proposed approach encodes the temporal query as a PCTL logic formula and checks if the query is feasible under a given MARL policy via probabilistic model checking. Such explanations can help reconcile discrepancies between the actual and anticipated multi-agent behaviors. The proposed approach also generates correct and complete explanations to pinpoint reasons that make a user query infeasible. We have successfully applied the proposed approach to four benchmark MARL domains (up to 9 agents in one domain). Moreover, the results of a user study show that the generated explanations significantly improve user performance and satisfaction.
<div id='section'>Paperid: <span id='pid'>1349, <a href='https://arxiv.org/pdf/2304.12280.pdf' target='_blank'>https://arxiv.org/pdf/2304.12280.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ram Rachum, Yonatan Nakar, Reuth Mirsky
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.12280">Stubborn: An Environment for Evaluating Stubbornness between Agents with Aligned Incentives</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent research in multi-agent reinforcement learning (MARL) has shown success in learning social behavior and cooperation. Social dilemmas between agents in mixed-sum settings have been studied extensively, but there is little research into social dilemmas in fullycooperative settings, where agents have no prospect of gaining reward at another agent's expense.
  While fully-aligned interests are conducive to cooperation between agents, they do not guarantee it. We propose a measure of "stubbornness" between agents that aims to capture the human social behavior from which it takes its name: a disagreement that is gradually escalating and potentially disastrous. We would like to promote research into the tendency of agents to be stubborn, the reactions of counterpart agents, and the resulting social dynamics.
  In this paper we present Stubborn, an environment for evaluating stubbornness between agents with fully-aligned incentives. In our preliminary results, the agents learn to use their partner's stubbornness as a signal for improving the choices that they make in the environment.
<div id='section'>Paperid: <span id='pid'>1350, <a href='https://arxiv.org/pdf/2304.10124.pdf' target='_blank'>https://arxiv.org/pdf/2304.10124.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chenglu Sun, Yichi Zhang, Yu Zhang, Ziling Lu, Jingbin Liu, Sijia Xu, Weidong Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.10124">Mastering Asymmetrical Multiplayer Game with Multi-Agent Asymmetric-Evolution Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Asymmetrical multiplayer (AMP) game is a popular game genre which involves multiple types of agents competing or collaborating with each other in the game. It is difficult to train powerful agents that can defeat top human players in AMP games by typical self-play training method because of unbalancing characteristics in their asymmetrical environments. We propose asymmetric-evolution training (AET), a novel multi-agent reinforcement learning framework that can train multiple kinds of agents simultaneously in AMP game. We designed adaptive data adjustment (ADA) and environment randomization (ER) to optimize the AET process. We tested our method in a complex AMP game named Tom \& Jerry, and our AIs trained without using any human data can achieve a win rate of 98.5% against top human players over 65 matches. The ablation experiments indicated that the proposed modules are beneficial to the framework.
<div id='section'>Paperid: <span id='pid'>1351, <a href='https://arxiv.org/pdf/2304.06281.pdf' target='_blank'>https://arxiv.org/pdf/2304.06281.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenli Xiao, Yiwei Lyu, John Dolan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.06281">Model-based Dynamic Shielding for Safe and Efficient Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-Agent Reinforcement Learning (MARL) discovers policies that maximize reward but do not have safety guarantees during the learning and deployment phases. Although shielding with Linear Temporal Logic (LTL) is a promising formal method to ensure safety in single-agent Reinforcement Learning (RL), it results in conservative behaviors when scaling to multi-agent scenarios. Additionally, it poses computational challenges for synthesizing shields in complex multi-agent environments. This work introduces Model-based Dynamic Shielding (MBDS) to support MARL algorithm design. Our algorithm synthesizes distributive shields, which are reactive systems running in parallel with each MARL agent, to monitor and rectify unsafe behaviors. The shields can dynamically split, merge, and recompute based on agents' states. This design enables efficient synthesis of shields to monitor agents in complex environments without coordination overheads. We also propose an algorithm to synthesize shields without prior knowledge of the dynamics model. The proposed algorithm obtains an approximate world model by interacting with the environment during the early stage of exploration, making our MBDS enjoy formal safety guarantees with high probability. We demonstrate in simulations that our framework can surpass existing baselines in terms of safety guarantees and learning performance.
<div id='section'>Paperid: <span id='pid'>1352, <a href='https://arxiv.org/pdf/2303.08680.pdf' target='_blank'>https://arxiv.org/pdf/2303.08680.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mouhamed Naby Ndiaye, El Houcine Bergou, Hajar El Hammouti
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.08680">Muti-Agent Proximal Policy Optimization For Data Freshness in UAV-assisted Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Unmanned aerial vehicles (UAVs) are seen as a promising technology to perform a wide range of tasks in wireless communication networks. In this work, we consider the deployment of a group of UAVs to collect the data generated by IoT devices. Specifically, we focus on the case where the collected data is time-sensitive, and it is critical to maintain its timeliness. Our objective is to optimally design the UAVs' trajectories and the subsets of visited IoT devices such as the global Age-of-Updates (AoU) is minimized. To this end, we formulate the studied problem as a mixed-integer nonlinear programming (MINLP) under time and quality of service constraints. To efficiently solve the resulting optimization problem, we investigate the cooperative Multi-Agent Reinforcement Learning (MARL) framework and propose an RL approach based on the popular on-policy Reinforcement Learning (RL) algorithm: Policy Proximal Optimization (PPO). Our approach leverages the centralized training decentralized execution (CTDE) framework where the UAVs learn their optimal policies while training a centralized value function. Our simulation results show that the proposed MAPPO approach reduces the global AoU by at least a factor of 1/2 compared to conventional off-policy reinforcement learning approaches.
<div id='section'>Paperid: <span id='pid'>1353, <a href='https://arxiv.org/pdf/2303.07850.pdf' target='_blank'>https://arxiv.org/pdf/2303.07850.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziyi Liu, Yongchun Fang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.07850">Learning Adaptable Risk-Sensitive Policies to Coordinate in Multi-Agent General-Sum Games</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In general-sum games, the interaction of self-interested learning agents commonly leads to socially worse outcomes, such as defect-defect in the iterated stag hunt (ISH). Previous works address this challenge by sharing rewards or shaping their opponents' learning process, which require too strong assumptions. In this paper, we demonstrate that agents trained to optimize expected returns are more likely to choose a safe action that leads to guaranteed but lower rewards. However, there typically exists a risky action that leads to higher rewards in the long run only if agents cooperate, e.g., cooperate-cooperate in ISH. To overcome this, we propose using action value distribution to characterize the decision's risk and corresponding potential payoffs. Specifically, we present Adaptable Risk-Sensitive Policy (ARSP). ARSP learns the distributions over agent's return and estimates a dynamic risk-seeking bonus to discover risky coordination strategies. Furthermore, to avoid overfitting training opponents, ARSP learns an auxiliary opponent modeling task to infer opponents' types and dynamically alter corresponding strategies during execution. Empirically, agents trained via ARSP can achieve stable coordination during training without accessing opponent's rewards or learning process, and can adapt to non-cooperative opponents during execution. To the best of our knowledge, it is the first method to learn coordination strategies between agents both in iterated prisoner's dilemma (IPD) and iterated stag hunt (ISH) without shaping opponents or rewards, and can adapt to opponents with distinct strategies during execution. Furthermore, we show that ARSP can be scaled to high-dimensional settings.
<div id='section'>Paperid: <span id='pid'>1354, <a href='https://arxiv.org/pdf/2302.14765.pdf' target='_blank'>https://arxiv.org/pdf/2302.14765.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Luciano Miuccio, Salvatore Riolo, Mehdi Bennis, Daniela Panno
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.14765">On Learning Intrinsic Rewards for Faster Multi-Agent Reinforcement Learning based MAC Protocol Design in 6G Wireless Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we propose a novel framework for designing a fast convergent multi-agent reinforcement learning (MARL)-based medium access control (MAC) protocol operating in a single cell scenario. The user equipments (UEs) are cast as learning agents that need to learn a proper signaling policy to coordinate the transmission of protocol data units (PDUs) to the base station (BS) over shared radio resources. In many MARL tasks, the conventional centralized training with decentralized execution (CTDE) is adopted, where each agent receives the same global extrinsic reward from the environment. However, this approach involves a long training time. To overcome this drawback, we adopt the concept of learning a per-agent intrinsic reward, in which each agent learns a different intrinsic reward signal based solely on its individual behavior. Moreover, in order to provide an intrinsic reward function that takes into account the long-term training history, we represent it as a long shortterm memory (LSTM) network. As a result, each agent updates its policy network considering both the extrinsic reward, which characterizes the cooperative task, and the intrinsic reward that reflects local dynamics. The proposed learning framework yields a faster convergence and higher transmission performance compared to the baselines. Simulation results show that the proposed learning solution yields 75% improvement in convergence speed compared to the most performing baseline.
<div id='section'>Paperid: <span id='pid'>1355, <a href='https://arxiv.org/pdf/2302.09531.pdf' target='_blank'>https://arxiv.org/pdf/2302.09531.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wei Li, Weiyan Liu, Shitong Shao, Shiyi Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.09531">AIIR-MIX: Multi-Agent Reinforcement Learning Meets Attention Individual Intrinsic Reward Mixing Network</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deducing the contribution of each agent and assigning the corresponding reward to them is a crucial problem in cooperative Multi-Agent Reinforcement Learning (MARL). Previous studies try to resolve the issue through designing an intrinsic reward function, but the intrinsic reward is simply combined with the environment reward by summation in these studies, which makes the performance of their MARL framework unsatisfactory. We propose a novel method named Attention Individual Intrinsic Reward Mixing Network (AIIR-MIX) in MARL, and the contributions of AIIR-MIX are listed as follows:(a) we construct a novel intrinsic reward network based on the attention mechanism to make teamwork more effective. (b) we propose a Mixing network that is able to combine intrinsic and extrinsic rewards non-linearly and dynamically in response to changing conditions of the environment. We compare AIIR-MIX with many State-Of-The-Art (SOTA) MARL methods on battle games in StarCraft II. And the results demonstrate that AIIR-MIX performs admirably and can defeat the current advanced methods on average test win rate. To validate the effectiveness of AIIR-MIX, we conduct additional ablation studies. The results show that AIIR-MIX can dynamically assign each agent a real-time intrinsic reward in accordance with their actual contribution.
<div id='section'>Paperid: <span id='pid'>1356, <a href='https://arxiv.org/pdf/2302.07176.pdf' target='_blank'>https://arxiv.org/pdf/2302.07176.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nancirose Piazza, Vahid Behzadan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.07176">A Theory of Mind Approach as Test-Time Mitigation Against Emergent Adversarial Communication</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-Agent Systems (MAS) is the study of multi-agent interactions in a shared environment. Communication for cooperation is a fundamental construct for sharing information in partially observable environments. Cooperative Multi-Agent Reinforcement Learning (CoMARL) is a learning framework where we learn agent policies either with cooperative mechanisms or policies that exhibit cooperative behavior. Explicitly, there are works on learning to communicate messages from CoMARL agents; however, non-cooperative agents, when capable of access a cooperative team's communication channel, have been shown to learn adversarial communication messages, sabotaging the cooperative team's performance particularly when objectives depend on finite resources. To address this issue, we propose a technique which leverages local formulations of Theory-of-Mind (ToM) to distinguish exhibited cooperative behavior from non-cooperative behavior before accepting messages from any agent. We demonstrate the efficacy and feasibility of the proposed technique in empirical evaluations in a centralized training, decentralized execution (CTDE) CoMARL benchmark. Furthermore, while we propose our explicit ToM defense for test-time, we emphasize that ToM is a construct for designing a cognitive defense rather than be the objective of the defense.
<div id='section'>Paperid: <span id='pid'>1357, <a href='https://arxiv.org/pdf/2301.10993.pdf' target='_blank'>https://arxiv.org/pdf/2301.10993.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Prashant Trivedi, Nandyala Hemachandra
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.10993">Multi-Agent Congestion Cost Minimization With Linear Function Approximations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work considers multiple agents traversing a network from a source node to the goal node. The cost to an agent for traveling a link has a private as well as a congestion component. The agent's objective is to find a path to the goal node with minimum overall cost in a decentralized way. We model this as a fully decentralized multi-agent reinforcement learning problem and propose a novel multi-agent congestion cost minimization (MACCM) algorithm. Our MACCM algorithm uses linear function approximations of transition probabilities and the global cost function. In the absence of a central controller and to preserve privacy, agents communicate the cost function parameters to their neighbors via a time-varying communication network. Moreover, each agent maintains its estimate of the global state-action value, which is updated via a multi-agent extended value iteration (MAEVI) sub-routine. We show that our MACCM algorithm achieves a sub-linear regret. The proof requires the convergence of cost function parameters, the MAEVI algorithm, and analysis of the regret bounds induced by the MAEVI triggering condition for each agent. We implement our algorithm on a two node network with multiple links to validate it. We first identify the optimal policy, the optimal number of agents going to the goal node in each period. We observe that the average regret is close to zero for 2 and 3 agents. The optimal policy captures the trade-off between the minimum cost of staying at a node and the congestion cost of going to the goal node. Our work is a generalization of learning the stochastic shortest path problem.
<div id='section'>Paperid: <span id='pid'>1358, <a href='https://arxiv.org/pdf/2301.02593.pdf' target='_blank'>https://arxiv.org/pdf/2301.02593.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Vincent Mai, Philippe Maisonneuve, Tianyu Zhang, Hadi Nekoei, Liam Paull, Antoine Lesage-Landry
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.02593">Multi-Agent Reinforcement Learning for Fast-Timescale Demand Response of Residential Loads</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To integrate high amounts of renewable energy resources, electrical power grids must be able to cope with high amplitude, fast timescale variations in power generation. Frequency regulation through demand response has the potential to coordinate temporally flexible loads, such as air conditioners, to counteract these variations. Existing approaches for discrete control with dynamic constraints struggle to provide satisfactory performance for fast timescale action selection with hundreds of agents. We propose a decentralized agent trained with multi-agent proximal policy optimization with localized communication. We explore two communication frameworks: hand-engineered, or learned through targeted multi-agent communication. The resulting policies perform well and robustly for frequency regulation, and scale seamlessly to arbitrary numbers of houses for constant processing times.
<div id='section'>Paperid: <span id='pid'>1359, <a href='https://arxiv.org/pdf/2212.08230.pdf' target='_blank'>https://arxiv.org/pdf/2212.08230.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chenhao Tong, Aaron Harwood, Maria A. Rodriguez, Richard O. Sinnott
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2212.08230">An Energy-aware and Fault-tolerant Deep Reinforcement Learning based approach for Multi-agent Patrolling Problems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autonomous vehicles are suited for continuous area patrolling problems. However, finding an optimal patrolling strategy can be challenging for many reasons. Firstly, patrolling environments are often complex and can include unknown environmental factors, such as wind or landscape. Secondly, autonomous vehicles can have failures or hardware constraints, such as limited battery life. Importantly, patrolling large areas often requires multiple agents that need to collectively coordinate their actions. In this work, we consider these limitations and propose an approach based on model-free, deep multi-agent reinforcement learning. In this approach, the agents are trained to patrol an environment with various unknown dynamics and factors. They can automatically recharge themselves to support continuous collective patrolling. A distributed homogeneous multi-agent architecture is proposed, where all patrolling agents execute identical policies locally based on their local observations and shared location information. This architecture provides a patrolling system that can tolerate agent failures and allow supplementary agents to be added to replace failed agents or to increase the overall patrol performance. The solution is validated through simulation experiments from multiple perspectives, including the overall patrol performance, the efficiency of battery recharging strategies, the overall fault tolerance, and the ability to cooperate with supplementary agents.
<div id='section'>Paperid: <span id='pid'>1360, <a href='https://arxiv.org/pdf/2212.05331.pdf' target='_blank'>https://arxiv.org/pdf/2212.05331.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kinal Mehta, Anuj Mahajan, Pawan Kumar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2212.05331">Effects of Spectral Normalization in Multi-agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A reliable critic is central to on-policy actor-critic learning. But it becomes challenging to learn a reliable critic in a multi-agent sparse reward scenario due to two factors: 1) The joint action space grows exponentially with the number of agents 2) This, combined with the reward sparseness and environment noise, leads to large sample requirements for accurate learning. We show that regularising the critic with spectral normalization (SN) enables it to learn more robustly, even in multi-agent on-policy sparse reward scenarios. Our experiments show that the regularised critic is quickly able to learn from the sparse rewarding experience in the complex SMAC and RWARE domains. These findings highlight the importance of regularisation in the critic for stable learning.
<div id='section'>Paperid: <span id='pid'>1361, <a href='https://arxiv.org/pdf/2210.11942.pdf' target='_blank'>https://arxiv.org/pdf/2210.11942.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Matthias Gerstgrasser, David C. Parkes
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2210.11942">Oracles & Followers: Stackelberg Equilibria in Deep Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Stackelberg equilibria arise naturally in a range of popular learning problems, such as in security games or indirect mechanism design, and have received increasing attention in the reinforcement learning literature. We present a general framework for implementing Stackelberg equilibria search as a multi-agent RL problem, allowing a wide range of algorithmic design choices. We discuss how previous approaches can be seen as specific instantiations of this framework. As a key insight, we note that the design space allows for approaches not previously seen in the literature, for instance by leveraging multitask and meta-RL techniques for follower convergence. We propose one such approach using contextual policies, and evaluate it experimentally on both standard and novel benchmark domains, showing greatly improved sample efficiency compared to previous approaches. Finally, we explore the effect of adopting algorithm designs outside the borders of our framework.
<div id='section'>Paperid: <span id='pid'>1362, <a href='https://arxiv.org/pdf/2210.00849.pdf' target='_blank'>https://arxiv.org/pdf/2210.00849.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Oren Neumann, Claudius Gros
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2210.00849">Scaling Laws for a Multi-Agent Reinforcement Learning Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The recent observation of neural power-law scaling relations has made a significant impact in the field of deep learning. A substantial amount of attention has been dedicated as a consequence to the description of scaling laws, although mostly for supervised learning and only to a reduced extent for reinforcement learning frameworks. In this paper we present an extensive study of performance scaling for a cornerstone reinforcement learning algorithm, AlphaZero. On the basis of a relationship between Elo rating, playing strength and power-law scaling, we train AlphaZero agents on the games Connect Four and Pentago and analyze their performance. We find that player strength scales as a power law in neural network parameter count when not bottlenecked by available compute, and as a power of compute when training optimally sized agents. We observe nearly identical scaling exponents for both games. Combining the two observed scaling laws we obtain a power law relating optimal size to compute similar to the ones observed for language models. We find that the predicted scaling of optimal neural network size fits our data for both games. This scaling law implies that previously published state-of-the-art game-playing models are significantly smaller than their optimal size, given the respective compute budgets. We also show that large AlphaZero models are more sample efficient, performing better than smaller models with the same amount of training data.
<div id='section'>Paperid: <span id='pid'>1363, <a href='https://arxiv.org/pdf/2208.10469.pdf' target='_blank'>https://arxiv.org/pdf/2208.10469.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Andreas A. Haupt, Phillip J. K. Christoffersen, Mehul Damani, Dylan Hadfield-Menell
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2208.10469">Formal Contracts Mitigate Social Dilemmas in Multi-Agent RL</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent Reinforcement Learning (MARL) is a powerful tool for training autonomous agents acting independently in a common environment. However, it can lead to sub-optimal behavior when individual incentives and group incentives diverge. Humans are remarkably capable at solving these social dilemmas. It is an open problem in MARL to replicate such cooperative behaviors in selfish agents. In this work, we draw upon the idea of formal contracting from economics to overcome diverging incentives between agents in MARL. We propose an augmentation to a Markov game where agents voluntarily agree to binding transfers of reward, under pre-specified conditions. Our contributions are theoretical and empirical. First, we show that this augmentation makes all subgame-perfect equilibria of all Fully Observable Markov Games exhibit socially optimal behavior, given a sufficiently rich space of contracts. Next, we show that for general contract spaces, and even under partial observability, richer contract spaces lead to higher welfare. Hence, contract space design solves an exploration-exploitation tradeoff, sidestepping incentive issues. We complement our theoretical analysis with experiments. Issues of exploration in the contracting augmentation are mitigated using a training methodology inspired by multi-objective reinforcement learning: Multi-Objective Contract Augmentation Learning (MOCA). We test our methodology in static, single-move games, as well as dynamic domains that simulate traffic, pollution management and common pool resource management.
<div id='section'>Paperid: <span id='pid'>1364, <a href='https://arxiv.org/pdf/2202.05135.pdf' target='_blank'>https://arxiv.org/pdf/2202.05135.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kaiyue Wu, Xiao-Jun Zeng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2202.05135">Group-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>It can largely benefit the reinforcement learning (RL) process of each agent if multiple geographically distributed agents perform their separate RL tasks cooperatively. Different from multi-agent reinforcement learning (MARL) where multiple agents are in a common environment and should learn to cooperate or compete with each other, in this case each agent has its separate environment and only communicates with others to share knowledge without any cooperative or competitive behaviour as a learning outcome. In fact, this scenario exists widely in real life whose concept can be utilised in many applications, but is not well understood yet and not well formulated. As the first effort, we propose group-agent system for RL as a formulation of this scenario and the third type of RL system with respect to single-agent and multi-agent systems. We then propose a distributed RL framework called DDAL (Decentralised Distributed Asynchronous Learning) designed for group-agent reinforcement learning (GARL). We show through experiments that DDAL achieved desirable performance with very stable training and has good scalability.
<div id='section'>Paperid: <span id='pid'>1365, <a href='https://arxiv.org/pdf/2111.06614.pdf' target='_blank'>https://arxiv.org/pdf/2111.06614.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ilai Shraga, Guy Azran, Matthias Gerstgrasser, Ofir Abu, Jeffrey S. Rosenschein, Sarah Keren
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2111.06614">Collaboration Promotes Group Resilience in Multi-Agent RL</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To effectively operate in various dynamic scenarios, RL agents must be resilient to unexpected changes in their environment. Previous work on this form of resilience has focused on single-agent settings. In this work, we introduce and formalize a multi-agent variant of resilience, which we term group resilience. We further hypothesize that collaboration with other agents is key to achieving group resilience; collaborating agents adapt better to environmental perturbations in multi-agent reinforcement learning (MARL) settings. We test our hypothesis empirically by evaluating different collaboration protocols and examining their effect on group resilience. Our experiments show that all the examined collaborative approaches achieve higher group resilience than their non-collaborative counterparts.
<div id='section'>Paperid: <span id='pid'>1366, <a href='https://arxiv.org/pdf/2108.12988.pdf' target='_blank'>https://arxiv.org/pdf/2108.12988.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shenao Zhang, Li Shen, Lei Han, Li Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2108.12988">Learning Meta Representations for Agents in Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In multi-agent reinforcement learning, the behaviors that agents learn in a single Markov Game (MG) are typically confined to the given agent number. Every single MG induced by varying the population may possess distinct optimal joint strategies and game-specific knowledge, which are modeled independently in modern multi-agent reinforcement learning algorithms. In this work, our focus is on creating agents that can generalize across population-varying MGs. Instead of learning a unimodal policy, each agent learns a policy set comprising effective strategies across a variety of games. To achieve this, we propose Meta Representations for Agents (MRA) that explicitly models the game-common and game-specific strategic knowledge. By representing the policy sets with multi-modal latent policies, the game-common strategic knowledge and diverse strategic modes are discovered through an iterative optimization procedure. We prove that by approximately maximizing the resulting constrained mutual information objective, the policies can reach Nash Equilibrium in every evaluation MG when the latent space is sufficiently large. When deploying MRA in practical settings with limited latent space sizes, fast adaptation can be achieved by leveraging the first-order gradient information. Extensive experiments demonstrate the effectiveness of MRA in improving training performance and generalization ability in challenging evaluation games.
<div id='section'>Paperid: <span id='pid'>1367, <a href='https://arxiv.org/pdf/2107.14316.pdf' target='_blank'>https://arxiv.org/pdf/2107.14316.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Piyush K. Sharma, Rolando Fernandez, Erin Zaroukian, Michael Dorothy, Anjon Basak, Derrik E. Asher
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2107.14316">Survey of Recent Multi-Agent Reinforcement Learning Algorithms Utilizing Centralized Training</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Much work has been dedicated to the exploration of Multi-Agent Reinforcement Learning (MARL) paradigms implementing a centralized learning with decentralized execution (CLDE) approach to achieve human-like collaboration in cooperative tasks. Here, we discuss variations of centralized training and describe a recent survey of algorithmic approaches. The goal is to explore how different implementations of information sharing mechanism in centralized learning may give rise to distinct group coordinated behaviors in multi-agent systems performing cooperative tasks.
<div id='section'>Paperid: <span id='pid'>1368, <a href='https://arxiv.org/pdf/2006.10822.pdf' target='_blank'>https://arxiv.org/pdf/2006.10822.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yan Zhang, Michael M. Zavlanos
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2006.10822">Cooperative Multi-Agent Reinforcement Learning with Partial Observations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we propose a distributed zeroth-order policy optimization method for Multi-Agent Reinforcement Learning (MARL). Existing MARL algorithms often assume that every agent can observe the states and actions of all the other agents in the network. This can be impractical in large-scale problems, where sharing the state and action information with multi-hop neighbors may incur significant communication overhead. The advantage of the proposed zeroth-order policy optimization method is that it allows the agents to compute the local policy gradients needed to update their local policy functions using local estimates of the global accumulated rewards that depend on partial state and action information only and can be obtained using consensus. Specifically, to calculate the local policy gradients, we develop a new distributed zeroth-order policy gradient estimator that relies on one-point residual-feedback which, compared to existing zeroth-order estimators that also rely on one-point feedback, significantly reduces the variance of the policy gradient estimates improving, in this way, the learning performance. We show that the proposed distributed zeroth-order policy optimization method with constant stepsize converges to the neighborhood of a policy that is a stationary point of the global objective function. The size of this neighborhood depends on the agents' learning rates, the exploration parameters, and the number of consensus steps used to calculate the local estimates of the global accumulated rewards. Moreover, we provide numerical experiments that demonstrate that our new zeroth-order policy gradient estimator is more sample-efficient compared to other existing one-point estimators.
<div id='section'>Paperid: <span id='pid'>1369, <a href='https://arxiv.org/pdf/1902.06239.pdf' target='_blank'>https://arxiv.org/pdf/1902.06239.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Babak Badnava, Mona Esmaeili, Nasser Mozayani, Payman Zarkesh-Ha
</span></div><div id="title">Title: <a href="https://arxiv.org/html/1902.06239">A new Potential-Based Reward Shaping for Reinforcement Learning Agent</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Potential-based reward shaping (PBRS) is a particular category of machine learning methods which aims to improve the learning speed of a reinforcement learning agent by extracting and utilizing extra knowledge while performing a task. There are two steps in the process of transfer learning: extracting knowledge from previously learned tasks and transferring that knowledge to use it in a target task. The latter step is well discussed in the literature with various methods being proposed for it, while the former has been explored less. With this in mind, the type of knowledge that is transmitted is very important and can lead to considerable improvement. Among the literature of both the transfer learning and the potential-based reward shaping, a subject that has never been addressed is the knowledge gathered during the learning process itself. In this paper, we presented a novel potential-based reward shaping method that attempted to extract knowledge from the learning process. The proposed method extracts knowledge from episodes' cumulative rewards. The proposed method has been evaluated in the Arcade learning environment and the results indicate an improvement in the learning process in both the single-task and the multi-task reinforcement learner agents.
<div id='section'>Paperid: <span id='pid'>1370, <a href='https://arxiv.org/pdf/2510.11410.pdf' target='_blank'>https://arxiv.org/pdf/2510.11410.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Anastasia Psarou, Łukasz Gorczyca, Dominik Gaweł, Rafał Kucharski
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.11410">Autonomous vehicles need social awareness to find optima in multi-agent reinforcement learning routing games</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Previous work has shown that when multiple selfish Autonomous Vehicles (AVs) are introduced to future cities and start learning optimal routing strategies using Multi-Agent Reinforcement Learning (MARL), they may destabilize traffic systems, as they would require a significant amount of time to converge to the optimal solution, equivalent to years of real-world commuting. We demonstrate that moving beyond the selfish component in the reward significantly relieves this issue. If each AV, apart from minimizing its own travel time, aims to reduce its impact on the system, this will be beneficial not only for the system-wide performance but also for each individual player in this routing game. By introducing an intrinsic reward signal based on the marginal cost matrix, we significantly reduce training time and achieve convergence more reliably. Marginal cost quantifies the impact of each individual action (route-choice) on the system (total travel time). Including it as one of the components of the reward can reduce the degree of non-stationarity by aligning agents' objectives. Notably, the proposed counterfactual formulation preserves the system's equilibria and avoids oscillations. Our experiments show that training MARL algorithms with our novel reward formulation enables the agents to converge to the optimal solution, whereas the baseline algorithms fail to do so. We show these effects in both a toy network and the real-world network of Saint-Arnoult. Our results optimistically indicate that social awareness (i.e., including marginal costs in routing decisions) improves both the system-wide and individual performance of future urban systems with AVs.
<div id='section'>Paperid: <span id='pid'>1371, <a href='https://arxiv.org/pdf/2508.20315.pdf' target='_blank'>https://arxiv.org/pdf/2508.20315.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>RexCharles Donatus, Kumater Ter, Ore-Ofe Ajayi, Daniel Udekwe
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.20315">Multi-Agent Reinforcement Learning in Intelligent Transportation Systems: A Comprehensive Survey</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The growing complexity of urban mobility and the demand for efficient, sustainable, and adaptive solutions have positioned Intelligent Transportation Systems (ITS) at the forefront of modern infrastructure innovation. At the core of ITS lies the challenge of autonomous decision-making across dynamic, large scale, and uncertain environments where multiple agents traffic signals, autonomous vehicles, or fleet units must coordinate effectively. Multi Agent Reinforcement Learning (MARL) offers a promising paradigm for addressing these challenges by enabling distributed agents to jointly learn optimal strategies that balance individual objectives with system wide efficiency. This paper presents a comprehensive survey of MARL applications in ITS. We introduce a structured taxonomy that categorizes MARL approaches according to coordination models and learning algorithms, spanning value based, policy based, actor critic, and communication enhanced frameworks. Applications are reviewed across key ITS domains, including traffic signal control, connected and autonomous vehicle coordination, logistics optimization, and mobility on demand systems. Furthermore, we highlight widely used simulation platforms such as SUMO, CARLA, and CityFlow that support MARL experimentation, along with emerging benchmarks. The survey also identifies core challenges, including scalability, non stationarity, credit assignment, communication constraints, and the sim to real transfer gap, which continue to hinder real world deployment.
<div id='section'>Paperid: <span id='pid'>1372, <a href='https://arxiv.org/pdf/2508.14131.pdf' target='_blank'>https://arxiv.org/pdf/2508.14131.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junjie Qi, Siqi Mao, Tianyi Tan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.14131">An Improved Multi-Agent Algorithm for Cooperative and Competitive Environments by Identifying and Encouraging Cooperation among Agents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose an improved algorithm by identifying and encouraging cooperative behavior in multi-agent environments. First, we analyze the shortcomings of existing algorithms in addressing multi-agent reinforcement learning problems. Then, based on the existing algorithm MADDPG, we introduce a new parameter to increase the reward that an agent can obtain when cooperative behavior among agents is identified. Finally, we compare our improved algorithm with MADDPG in environments from PettingZoo. The results show that the new algorithm helps agents achieve both higher team rewards and individual rewards.
<div id='section'>Paperid: <span id='pid'>1373, <a href='https://arxiv.org/pdf/2508.12296.pdf' target='_blank'>https://arxiv.org/pdf/2508.12296.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bin Wang, Jiwen Zhang, Song Wang, Dan Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.12296">A robust and compliant robotic assembly control strategy for batch precision assembly task with uncertain fit types and fit amounts</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In some high-precision industrial applications, robots are deployed to perform precision assembly tasks on mass batches of manufactured pegs and holes. If the peg and hole are designed with transition fit, machining errors may lead to either a clearance or an interference fit for a specific pair of components, with uncertain fit amounts. This paper focuses on the robotic batch precision assembly task involving components with uncertain fit types and fit amounts, and proposes an efficient methodology to construct the robust and compliant assembly control strategy. Specifically, the batch precision assembly task is decomposed into multiple deterministic subtasks, and a force-vision fusion controller-driven reinforcement learning method and a multi-task reinforcement learning training method (FVFC-MTRL) are proposed to jointly learn multiple compliance control strategies for these subtasks. Subsequently, the multi-teacher policy distillation approach is designed to integrate multiple trained strategies into a unified student network, thereby establishing a robust control strategy. Real-world experiments demonstrate that the proposed method successfully constructs the robust control strategy for high-precision assembly task with different fit types and fit amounts. Moreover, the MTRL framework significantly improves training efficiency, and the final developed control strategy achieves superior force compliance and higher success rate compared with many existing methods.
<div id='section'>Paperid: <span id='pid'>1374, <a href='https://arxiv.org/pdf/2508.11618.pdf' target='_blank'>https://arxiv.org/pdf/2508.11618.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jungang Chen, Seyyed A. Hosseini
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.11618">Optimal CO2 storage management considering safety constraints in multi-stakeholder multi-site CCS projects: a game theoretic perspective</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Carbon capture and storage (CCS) projects typically involve a diverse array of stakeholders or players from public, private, and regulatory sectors, each with different objectives and responsibilities. Given the complexity, scale, and long-term nature of CCS operations, determining whether individual stakeholders can independently maximize their interests or whether collaborative coalition agreements are needed remains a central question for effective CCS project planning and management. CCS projects are often implemented in geologically connected sites, where shared geological features such as pressure space and reservoir pore capacity can lead to competitive behavior among stakeholders. Furthermore, CO2 storage sites are often located in geologically mature basins that previously served as sites for hydrocarbon extraction or wastewater disposal in order to leverage existing infrastructures, which makes unilateral optimization even more complicated and unrealistic.
  In this work, we propose a paradigm based on Markov games to quantitatively investigate how different coalition structures affect the goals of stakeholders. We frame this multi-stakeholder multi-site problem as a multi-agent reinforcement learning problem with safety constraints. Our approach enables agents to learn optimal strategies while compliant with safety regulations. We present an example where multiple operators are injecting CO2 into their respective project areas in a geologically connected basin. To address the high computational cost of repeated simulations of high-fidelity models, a previously developed surrogate model based on the Embed-to-Control (E2C) framework is employed. Our results demonstrate the effectiveness of the proposed framework in addressing optimal management of CO2 storage when multiple stakeholders with various objectives and goals are involved.
<div id='section'>Paperid: <span id='pid'>1375, <a href='https://arxiv.org/pdf/2508.09275.pdf' target='_blank'>https://arxiv.org/pdf/2508.09275.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Amine Andam, Jamal Bentahar, Mustapha Hedabou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.09275">Constrained Black-Box Attacks Against Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Collaborative multi-agent reinforcement learning (c-MARL) has rapidly evolved, offering state-of-the-art algorithms for real-world applications, including sensitive domains. However, a key challenge to its widespread adoption is the lack of a thorough investigation into its vulnerabilities to adversarial attacks. Existing work predominantly focuses on training-time attacks or unrealistic scenarios, such as access to policy weights or the ability to train surrogate policies. In this paper, we investigate new vulnerabilities under more realistic and constrained conditions, assuming an adversary can only collect and perturb the observations of deployed agents. We also consider scenarios where the adversary has no access at all. We propose simple yet highly effective algorithms for generating adversarial perturbations designed to misalign how victim agents perceive their environment. Our approach is empirically validated on three benchmarks and 22 environments, demonstrating its effectiveness across diverse algorithms and environments. Furthermore, we show that our algorithm is sample-efficient, requiring only 1,000 samples compared to the millions needed by previous methods.
<div id='section'>Paperid: <span id='pid'>1376, <a href='https://arxiv.org/pdf/2508.08325.pdf' target='_blank'>https://arxiv.org/pdf/2508.08325.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hangcheng Zhao, Ron Berman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.08325">Algorithmic Collusion of Pricing and Advertising on E-commerce Platforms</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Online sellers have been adopting AI learning algorithms to automatically make product pricing and advertising decisions on e-commerce platforms. When sellers compete using such algorithms, one concern is that of tacit collusion - the algorithms learn to coordinate on higher than competitive. We empirically investigate whether these concerns are valid when sellers make pricing and advertising decisions together, i.e., two-dimensional decisions. Our empirical strategy is to analyze competition with multi-agent reinforcement learning, which we calibrate to a large-scale dataset collected from Amazon.com products. Our first contribution is to find conditions under which learning algorithms can facilitate win-win-win outcomes that are beneficial for consumers, sellers, and even the platform, when consumers have high search costs. In these cases the algorithms learn to coordinate on prices that are lower than competitive prices. The intuition is that the algorithms learn to coordinate on lower advertising bids, which lower advertising costs, leading to lower prices. Our second contribution is an analysis of a large-scale, high-frequency keyword-product dataset for more than 2 million products on Amazon.com. Our estimates of consumer search costs show a wide range of costs for different product keywords. We generate an algorithm usage and find a negative interaction between the estimated consumer search costs and the algorithm usage index, providing empirical evidence of beneficial collusion. Finally, we analyze the platform's strategic response. We find that reserve price adjustments will not increase profits for the platform, but commission adjustments will. Our analyses help alleviate some worries about the potentially harmful effects of competing learning algorithms, and can help sellers, platforms and policymakers to decide on whether to adopt or regulate such algorithms.
<div id='section'>Paperid: <span id='pid'>1377, <a href='https://arxiv.org/pdf/2508.06836.pdf' target='_blank'>https://arxiv.org/pdf/2508.06836.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xutong Zhao, Yaqi Xie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.06836">Multi-level Advantage Credit Assignment for Cooperative Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cooperative multi-agent reinforcement learning (MARL) aims to coordinate multiple agents to achieve a common goal. A key challenge in MARL is credit assignment, which involves assessing each agent's contribution to the shared reward. Given the diversity of tasks, agents may perform different types of coordination, with rewards attributed to diverse and often overlapping agent subsets. In this work, we formalize the credit assignment level as the number of agents cooperating to obtain a reward, and address scenarios with multiple coexisting levels. We introduce a multi-level advantage formulation that performs explicit counterfactual reasoning to infer credits across distinct levels. Our method, Multi-level Advantage Credit Assignment (MACA), captures agent contributions at multiple levels by integrating advantage functions that reason about individual, joint, and correlated actions. Utilizing an attention-based framework, MACA identifies correlated agent relationships and constructs multi-level advantages to guide policy learning. Comprehensive experiments on challenging Starcraft v1\&v2 tasks demonstrate MACA's superior performance, underscoring its efficacy in complex credit assignment scenarios.
<div id='section'>Paperid: <span id='pid'>1378, <a href='https://arxiv.org/pdf/2508.06767.pdf' target='_blank'>https://arxiv.org/pdf/2508.06767.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Arman Dogru, R. Irem Bor-Yaliniz, Nimal Gamini Senarath
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.06767">PANAMA: A Network-Aware MARL Framework for Multi-Agent Path Finding in Digital Twin Ecosystems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Digital Twins (DTs) are transforming industries through advanced data processing and analysis, positioning the world of DTs, Digital World, as a cornerstone of nextgeneration technologies including embodied AI. As robotics and automated systems scale, efficient data-sharing frameworks and robust algorithms become critical. We explore the pivotal role of data handling in next-gen networks, focusing on dynamics between application and network providers (AP/NP) in DT ecosystems. We introduce PANAMA, a novel algorithm with Priority Asymmetry for Network Aware Multi-agent Reinforcement Learning (MARL) based multi-agent path finding (MAPF). By adopting a Centralized Training with Decentralized Execution (CTDE) framework and asynchronous actor-learner architectures, PANAMA accelerates training while enabling autonomous task execution by embodied AI. Our approach demonstrates superior pathfinding performance in accuracy, speed, and scalability compared to existing benchmarks. Through simulations, we highlight optimized data-sharing strategies for scalable, automated systems, ensuring resilience in complex, real-world environments. PANAMA bridges the gap between network-aware decision-making and robust multi-agent coordination, advancing the synergy between DTs, wireless networks, and AI-driven automation.
<div id='section'>Paperid: <span id='pid'>1379, <a href='https://arxiv.org/pdf/2508.02421.pdf' target='_blank'>https://arxiv.org/pdf/2508.02421.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Akshay Dodwadmath, Setareh Maghsudi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.02421">Emergence of Fair Leaders via Mediators in Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Stackelberg games and their resulting equilibria have received increasing attention in the multi-agent reinforcement learning literature. Each stage of a traditional Stackelberg game involves a leader(s) acting first, followed by the followers. In situations where the roles of leader(s) and followers can be interchanged, the designated role can have considerable advantages, for example, in first-mover advantage settings. Then the question arises: Who should be the leader and when? A bias in the leader selection process can lead to unfair outcomes. This problem is aggravated if the agents are self-interested and care only about their goals and rewards. We formally define this leader selection problem and show its relation to fairness in agents' returns. Furthermore, we propose a multi-agent reinforcement learning framework that maximizes fairness by integrating mediators. Mediators have previously been used in the simultaneous action setting with varying levels of control, such as directly performing agents' actions or just recommending them. Our framework integrates mediators in the Stackelberg setting with minimal control (leader selection). We show that the presence of mediators leads to self-interested agents taking fair actions, resulting in higher overall fairness in agents' returns.
<div id='section'>Paperid: <span id='pid'>1380, <a href='https://arxiv.org/pdf/2508.00159.pdf' target='_blank'>https://arxiv.org/pdf/2508.00159.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jobst Heitzig, Ram Potham
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.00159">Model-Based Soft Maximization of Suitable Metrics of Long-Term Human Power</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Power is a key concept in AI safety: power-seeking as an instrumental goal, sudden or gradual disempowerment of humans, power balance in human-AI interaction and international AI governance. At the same time, power as the ability to pursue diverse goals is essential for wellbeing.
  This paper explores the idea of promoting both safety and wellbeing by forcing AI agents explicitly to empower humans and to manage the power balance between humans and AI agents in a desirable way. Using a principled, partially axiomatic approach, we design a parametrizable and decomposable objective function that represents an inequality- and risk-averse long-term aggregate of human power. It takes into account humans' bounded rationality and social norms, and, crucially, considers a wide variety of possible human goals.
  We derive algorithms for computing that metric by backward induction or approximating it via a form of multi-agent reinforcement learning from a given world model. We exemplify the consequences of (softly) maximizing this metric in a variety of paradigmatic situations and describe what instrumental sub-goals it will likely imply. Our cautious assessment is that softly maximizing suitable aggregate metrics of human power might constitute a beneficial objective for agentic AI systems that is safer than direct utility-based objectives.
<div id='section'>Paperid: <span id='pid'>1381, <a href='https://arxiv.org/pdf/2507.20377.pdf' target='_blank'>https://arxiv.org/pdf/2507.20377.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Farshid Nooshi, Suining He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.20377">Multi-Agent Reinforcement Learning for Dynamic Mobility Resource Allocation with Hierarchical Adaptive Grouping</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Allocating mobility resources (e.g., shared bikes/e-scooters, ride-sharing vehicles) is crucial for rebalancing the mobility demand and supply in the urban environments. We propose in this work a novel multi-agent reinforcement learning named Hierarchical Adaptive Grouping-based Parameter Sharing (HAG-PS) for dynamic mobility resource allocation. HAG-PS aims to address two important research challenges regarding multi-agent reinforcement learning for mobility resource allocation: (1) how to dynamically and adaptively share the mobility resource allocation policy (i.e., how to distribute mobility resources) across agents (i.e., representing the regional coordinators of mobility resources); and (2) how to achieve memory-efficient parameter sharing in an urban-scale setting. To address the above challenges, we have provided following novel designs within HAG-PS. To enable dynamic and adaptive parameter sharing, we have designed a hierarchical approach that consists of global and local information of the mobility resource states (e.g., distribution of mobility resources). We have developed an adaptive agent grouping approach in order to split or merge the groups of agents based on their relative closeness of encoded trajectories (i.e., states, actions, and rewards). We have designed a learnable identity (ID) embeddings to enable agent specialization beyond simple parameter copy. We have performed extensive experimental studies based on real-world NYC bike sharing data (a total of more than 1.2 million trips), and demonstrated the superior performance (e.g., improved bike availability) of HAG-PS compared with other baseline approaches.
<div id='section'>Paperid: <span id='pid'>1382, <a href='https://arxiv.org/pdf/2507.16479.pdf' target='_blank'>https://arxiv.org/pdf/2507.16479.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoyang Zhang, Mina Montazeri, Philipp Heer, Koen Kok, Nikolaos G. Paterakis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.16479">Arbitrage Tactics in the Local Markets via Hierarchical Multi-agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Strategic bidding tactics employed by prosumers in local markets, including the Local Electricity Market (LEM) and Local Flexibility Market (LFM), have attracted significant attention due to their potential to enhance economic benefits for market participants through optimized energy management and bidding. While existing research has explored strategic bidding in a single market with multi-agent reinforcement learning (MARL) algorithms, arbitrage opportunities across local markets remain unexplored. This paper introduces a hierarchical MARL (HMARL) algorithm designed to enable aggregator arbitrage across multiple local markets. The strategic behavior of these aggregators in local markets is modeled as a two-stage Markov game: the first stage involves the LEM, while the second stage encompasses both the LFM and the balancing market. To solve this two-stage Markov game, the HMARL framework assigns two sub-agents to each aggregator, a primary sub-agent and a secondary sub-agent. Without the arbitrage strategy, these sub-agents operate in silos, with the primary sub-agent focusing on first-stage profits and the secondary sub-agent on second-stage profits, each employing independent MARLs. On the contrary, when implementing the arbitrage strategy with the proposed HMARL, the sub-agents communicate and coordinate to perform arbitrage across multiple local markets, enhancing overall efficiency. The case study, conducted under a scenario where all aggregators employ the arbitrage strategy, shows that despite higher initial costs in the LEM, this strategy generates substantial savings in the LFM and the balancing market, resulting in a total profit increase of $40.6\%$ on average. This highlights the capability of the proposed HMARL to address the two-stage Markov game and facilitate arbitrage across local markets, thereby enhancing profitability for participants.
<div id='section'>Paperid: <span id='pid'>1383, <a href='https://arxiv.org/pdf/2507.16382.pdf' target='_blank'>https://arxiv.org/pdf/2507.16382.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chenhao Yao, Zike Yuan, Xiaoxu Liu, Chi Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.16382">Application of LLM Guided Reinforcement Learning in Formation Control with Collision Avoidance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-Agent Systems (MAS) excel at accomplishing complex objectives through the collaborative efforts of individual agents. Among the methodologies employed in MAS, Multi-Agent Reinforcement Learning (MARL) stands out as one of the most efficacious algorithms. However, when confronted with the complex objective of Formation Control with Collision Avoidance (FCCA): designing an effective reward function that facilitates swift convergence of the policy network to an optimal solution. In this paper, we introduce a novel framework that aims to overcome this challenge. By giving large language models (LLMs) on the prioritization of tasks and the observable information available to each agent, our framework generates reward functions that can be dynamically adjusted online based on evaluation outcomes by employing more advanced evaluation metrics rather than the rewards themselves. This mechanism enables the MAS to simultaneously achieve formation control and obstacle avoidance in dynamic environments with enhanced efficiency, requiring fewer iterations to reach superior performance levels. Our empirical studies, conducted in both simulation and real-world settings, validate the practicality and effectiveness of our proposed approach.
<div id='section'>Paperid: <span id='pid'>1384, <a href='https://arxiv.org/pdf/2507.15455.pdf' target='_blank'>https://arxiv.org/pdf/2507.15455.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hee Jun Yang, Minjung Gim, Yeoneung Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.15455">Solving nonconvex Hamilton--Jacobi--Isaacs equations with PINN-based policy iteration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a mesh-free policy iteration framework that combines classical dynamic programming with physics-informed neural networks (PINNs) to solve high-dimensional, nonconvex Hamilton--Jacobi--Isaacs (HJI) equations arising in stochastic differential games and robust control. The method alternates between solving linear second-order PDEs under fixed feedback policies and updating the controls via pointwise minimax optimization using automatic differentiation. Under standard Lipschitz and uniform ellipticity assumptions, we prove that the value function iterates converge locally uniformly to the unique viscosity solution of the HJI equation. The analysis establishes equi-Lipschitz regularity of the iterates, enabling provable stability and convergence without requiring convexity of the Hamiltonian. Numerical experiments demonstrate the accuracy and scalability of the method. In a two-dimensional stochastic path-planning game with a moving obstacle, our method matches finite-difference benchmarks with relative $L^2$-errors below %10^{-2}%. In five- and ten-dimensional publisher-subscriber differential games with anisotropic noise, the proposed approach consistently outperforms direct PINN solvers, yielding smoother value functions and lower residuals. Our results suggest that integrating PINNs with policy iteration is a practical and theoretically grounded method for solving high-dimensional, nonconvex HJI equations, with potential applications in robotics, finance, and multi-agent reinforcement learning.
<div id='section'>Paperid: <span id='pid'>1385, <a href='https://arxiv.org/pdf/2507.14995.pdf' target='_blank'>https://arxiv.org/pdf/2507.14995.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chengwei Lou, Zekai Jin, Wei Tang, Guangfei Geng, Jin Yang, Lu Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.14995">LLM-Enhanced Multi-Agent Reinforcement Learning with Expert Workflow for Real-Time P2P Energy Trading</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Real-time peer-to-peer (P2P) electricity markets dynamically adapt to fluctuations in renewable energy and variations in demand, maximizing economic benefits through instantaneous price responses while enhancing grid flexibility. However, scaling expert guidance for massive personalized prosumers poses critical challenges, including diverse decision-making demands and lack of customized modeling frameworks. This paper proposed an integrated large language model-multi-agent reinforcement learning (LLM-MARL) framework for real-time P2P energy trading to address challenges such as the limited technical capability of prosumers, the lack of expert experience, and security issues of distribution networks. LLMs are introduced as experts to generate personalized strategy, guiding MARL under the centralized training with decentralized execution (CTDE) paradigm through imitation learning. A differential attention-based critic network is designed to enhance convergence performance. Experimental results demonstrate that LLM generated strategies effectively substitute human experts. The proposed multi-agent imitation learning algorithms achieve significantly lower economic costs and voltage violation rates on test sets compared to baselines algorithms, while maintaining robust stability. This work provides an effective solution for real-time P2P electricity market decision-making by bridging expert knowledge with agent learning.
<div id='section'>Paperid: <span id='pid'>1386, <a href='https://arxiv.org/pdf/2507.07074.pdf' target='_blank'>https://arxiv.org/pdf/2507.07074.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Farhaan Ebadulla, Dharini Hindlatti, Srinivaasan NS, Apoorva VH, Ayman Aftab
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.07074">Graph-Based Complexity Metrics for Multi-Agent Curriculum Learning: A Validated Approach to Task Ordering in Cooperative Coordination Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent reinforcement learning (MARL) faces significant challenges in task sequencing and curriculum design, particularly for cooperative coordination scenarios. While curriculum learning has demonstrated success in single-agent domains, principled approaches for multi-agent coordination remain limited due to the absence of validated task complexity metrics. This approach presents a graph-based coordination complexity metric that integrates agent dependency entropy, spatial interference patterns, and goal overlap analysis to predict task difficulty in multi-agent environments. The complexity metric achieves strong empirical validation with rho = 0.952 correlation (p < 0.001) between predicted complexity and empirical difficulty determined by random agent performance evaluation. This approach evaluates the curriculum learning framework using MADDPG across two distinct coordination environments: achieving 56x performance improvement in tight coordination tasks (MultiWalker) and demonstrating systematic task progression in cooperative navigation (Simple Spread). Through systematic analysis, coordination tightness emerges as a predictor of curriculum learning effectiveness, where environments requiring strict agent interdependence benefit substantially from structured progression. This approach provides a validated complexity metric for multi-agent curriculum design and establishes empirical guidelines for multi-robot coordination applications.
<div id='section'>Paperid: <span id='pid'>1387, <a href='https://arxiv.org/pdf/2507.06278.pdf' target='_blank'>https://arxiv.org/pdf/2507.06278.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kemboi Cheruiyot, Nickson Kiprotich, Vyacheslav Kungurtsev, Kennedy Mugo, Vivian Mwirigi, Marvin Ngesa
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.06278">A Survey of Multi Agent Reinforcement Learning: Federated Learning and Cooperative and Noncooperative Decentralized Regimes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The increasing interest in research and innovation towards the development of autonomous agents presents a number of complex yet important scenarios of multiple AI Agents interacting with each other in an environment. The particular setting can be understood as exhibiting three possibly topologies of interaction - centrally coordinated cooperation, ad-hoc interaction and cooperation, and settings with noncooperative incentive structures. This article presents a comprehensive survey of all three domains, defined under the formalism of Federal Reinforcement Learning (RL), Decentralized RL, and Noncooperative RL, respectively. Highlighting the structural similarities and distinctions, we review the state of the art in these subjects, primarily explored and developed only recently in the literature. We include the formulations as well as known theoretical guarantees and highlights and limitations of numerical performance.
<div id='section'>Paperid: <span id='pid'>1388, <a href='https://arxiv.org/pdf/2506.20039.pdf' target='_blank'>https://arxiv.org/pdf/2506.20039.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Koorosh Moslemi, Chi-Guhn Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.20039">Learning Bilateral Team Formation in Cooperative Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Team formation and the dynamics of team-based learning have drawn significant interest in the context of Multi-Agent Reinforcement Learning (MARL). However, existing studies primarily focus on unilateral groupings, predefined teams, or fixed-population settings, leaving the effects of algorithmic bilateral grouping choices in dynamic populations underexplored. To address this gap, we introduce a framework for learning two-sided team formation in dynamic multi-agent systems. Through this study, we gain insight into what algorithmic properties in bilateral team formation influence policy performance and generalization. We validate our approach using widely adopted multi-agent scenarios, demonstrating competitive performance and improved generalization in most scenarios.
<div id='section'>Paperid: <span id='pid'>1389, <a href='https://arxiv.org/pdf/2506.18537.pdf' target='_blank'>https://arxiv.org/pdf/2506.18537.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Azad Deihim, Eduardo Alonso, Dimitra Apostolopoulou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.18537">Transformer World Model for Sample Efficient Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present the Multi-Agent Transformer World Model (MATWM), a novel transformer-based world model designed for multi-agent reinforcement learning in both vector- and image-based environments. MATWM combines a decentralized imagination framework with a semi-centralized critic and a teammate prediction module, enabling agents to model and anticipate the behavior of others under partial observability. To address non-stationarity, we incorporate a prioritized replay mechanism that trains the world model on recent experiences, allowing it to adapt to agents' evolving policies. We evaluated MATWM on a broad suite of benchmarks, including the StarCraft Multi-Agent Challenge, PettingZoo, and MeltingPot. MATWM achieves state-of-the-art performance, outperforming both model-free and prior world model approaches, while demonstrating strong sample efficiency, achieving near-optimal performance in as few as 50K environment interactions. Ablation studies confirm the impact of each component, with substantial gains in coordination-heavy tasks.
<div id='section'>Paperid: <span id='pid'>1390, <a href='https://arxiv.org/pdf/2506.18126.pdf' target='_blank'>https://arxiv.org/pdf/2506.18126.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiang Yuming, Li Sizhao, Li Rongpeng, Zhao Zhifeng, Zhang Honggang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.18126">Decentralized Consensus Inference-based Hierarchical Reinforcement Learning for Multi-Constrained UAV Pursuit-Evasion Game</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multiple quadrotor unmanned aerial vehicle (UAV) systems have garnered widespread research interest and fostered tremendous interesting applications, especially in multi-constrained pursuit-evasion games (MC-PEG). The Cooperative Evasion and Formation Coverage (CEFC) task, where the UAV swarm aims to maximize formation coverage across multiple target zones while collaboratively evading predators, belongs to one of the most challenging issues in MC-PEG, especially under communication-limited constraints. This multifaceted problem, which intertwines responses to obstacles, adversaries, target zones, and formation dynamics, brings up significant high-dimensional complications in locating a solution. In this paper, we propose a novel two-level framework (i.e., Consensus Inference-based Hierarchical Reinforcement Learning (CI-HRL)), which delegates target localization to a high-level policy, while adopting a low-level policy to manage obstacle avoidance, navigation, and formation. Specifically, in the high-level policy, we develop a novel multi-agent reinforcement learning module, Consensus-oriented Multi-Agent Communication (ConsMAC), to enable agents to perceive global information and establish consensus from local states by effectively aggregating neighbor messages. Meanwhile, we leverage an Alternative Training-based Multi-agent proximal policy optimization (AT-M) and policy distillation to accomplish the low-level control. The experimental results, including the high-fidelity software-in-the-loop (SITL) simulations, validate that CI-HRL provides a superior solution with enhanced swarm's collaborative evasion and task completion capabilities.
<div id='section'>Paperid: <span id='pid'>1391, <a href='https://arxiv.org/pdf/2506.15587.pdf' target='_blank'>https://arxiv.org/pdf/2506.15587.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Martino Brambati, Antonio Celani, Marco Gherardi, Francesco Ginelli
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.15587">Learning to flock in open space by avoiding collisions and staying together</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We investigate the emergence of cohesive flocking in open, boundless space using a multi-agent reinforcement learning framework. Agents integrate positional and orientational information from their closest topological neighbours and learn to balance alignment and attractive interactions by optimizing a local cost function that penalizes both excessive separation and close-range crowding. The resulting Vicsek-like dynamics is robust to algorithmic implementation details and yields cohesive collective motion with high polar order. The optimal policy is dominated by strong aligning interactions when agents are sufficiently close to their neighbours, and a flexible combination of alignment and attraction at larger separations. We further characterize the internal structure and dynamics of the resulting groups using liquid-state metrics and neighbour exchange rates, finding qualitative agreement with empirical observations in starling flocks. These results suggest that flocking may emerge in groups of moving agents as an adaptive response to the biological imperatives of staying together while avoiding collisions.
<div id='section'>Paperid: <span id='pid'>1392, <a href='https://arxiv.org/pdf/2506.05894.pdf' target='_blank'>https://arxiv.org/pdf/2506.05894.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Philipp Plank, Yufei Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.05894">Policy Optimization for Continuous-time Linear-Quadratic Graphon Mean Field Games</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent reinforcement learning, despite its popularity and empirical success, faces significant scalability challenges in large-population dynamic games. Graphon mean field games (GMFGs) offer a principled framework for approximating such games while capturing heterogeneity among players. In this paper, we propose and analyze a policy optimization framework for continuous-time, finite-horizon linear-quadratic GMFGs. Exploiting the structural properties of GMFGs, we design an efficient policy parameterization in which each player's policy is represented as an affine function of their private state, with a shared slope function and player-specific intercepts. We develop a bilevel optimization algorithm that alternates between policy gradient updates for best-response computation under a fixed population distribution, and distribution updates using the resulting policies. We prove linear convergence of the policy gradient steps to best-response policies and establish global convergence of the overall algorithm to the Nash equilibrium. The analysis relies on novel landscape characterizations over infinite-dimensional policy spaces. Numerical experiments demonstrate the convergence and robustness of the proposed algorithm under varying graphon structures, noise levels, and action frequencies.
<div id='section'>Paperid: <span id='pid'>1393, <a href='https://arxiv.org/pdf/2506.02841.pdf' target='_blank'>https://arxiv.org/pdf/2506.02841.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tom Danino, Nahum Shimkin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.02841">Ensemble-MIX: Enhancing Sample Efficiency in Multi-Agent RL Using Ensemble Methods</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent reinforcement learning (MARL) methods have achieved state-of-the-art results on a range of multi-agent tasks. Yet, MARL algorithms typically require significantly more environment interactions than their single-agent counterparts to converge, a problem exacerbated by the difficulty in exploring over a large joint action space and the high variance intrinsic to MARL environments. To tackle these issues, we propose a novel algorithm that combines a decomposed centralized critic with decentralized ensemble learning, incorporating several key contributions. The main component in our scheme is a selective exploration method that leverages ensemble kurtosis. We extend the global decomposed critic with a diversity-regularized ensemble of individual critics and utilize its excess kurtosis to guide exploration toward high-uncertainty states and actions. To improve sample efficiency, we train the centralized critic with a novel truncated variation of the TD($Î»$) algorithm, enabling efficient off-policy learning with reduced variance. On the actor side, our suggested algorithm adapts the mixed samples approach to MARL, mixing on-policy and off-policy loss functions for training the actors. This approach balances between stability and efficiency and outperforms purely off-policy learning. The evaluation shows our method outperforms state-of-the-art baselines on standard MARL benchmarks, including a variety of SMAC II maps.
<div id='section'>Paperid: <span id='pid'>1394, <a href='https://arxiv.org/pdf/2506.00797.pdf' target='_blank'>https://arxiv.org/pdf/2506.00797.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianglin Ding, Jingcheng Tang, Gangshan Jing
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.00797">Action Dependency Graphs for Globally Optimal Coordinated Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Action-dependent individual policies, which incorporate both environmental states and the actions of other agents in decision-making, have emerged as a promising paradigm for achieving global optimality in multi-agent reinforcement learning (MARL). However, the existing literature often adopts auto-regressive action-dependent policies, where each agent's policy depends on the actions of all preceding agents. This formulation incurs substantial computational complexity as the number of agents increases, thereby limiting scalability. In this work, we consider a more generalized class of action-dependent policies, which do not necessarily follow the auto-regressive form. We propose to use the `action dependency graph (ADG)' to model the inter-agent action dependencies. Within the context of MARL problems structured by coordination graphs, we prove that an action-dependent policy with a sparse ADG can achieve global optimality, provided the ADG satisfies specific conditions specified by the coordination graph. Building on this theoretical foundation, we develop a tabular policy iteration algorithm with guaranteed global optimality. Furthermore, we integrate our framework into several SOTA algorithms and conduct experiments in complex environments. The empirical results affirm the robustness and applicability of our approach in more general scenarios, underscoring its potential for broader MARL challenges.
<div id='section'>Paperid: <span id='pid'>1395, <a href='https://arxiv.org/pdf/2505.21985.pdf' target='_blank'>https://arxiv.org/pdf/2505.21985.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Naoto Yoshida, Tadahiro Taniguchi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.21985">Reward-Independent Messaging for Decentralized Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In multi-agent reinforcement learning (MARL), effective communication improves agent performance, particularly under partial observability. We propose MARL-CPC, a framework that enables communication among fully decentralized, independent agents without parameter sharing. MARL-CPC incorporates a message learning model based on collective predictive coding (CPC) from emergent communication research. Unlike conventional methods that treat messages as part of the action space and assume cooperation, MARL-CPC links messages to state inference, supporting communication in non-cooperative, reward-independent settings. We introduce two algorithms -Bandit-CPC and IPPO-CPC- and evaluate them in non-cooperative MARL tasks. Benchmarks show that both outperform standard message-as-action approaches, establishing effective communication even when messages offer no direct benefit to the sender. These results highlight MARL-CPC's potential for enabling coordination in complex, decentralized environments.
<div id='section'>Paperid: <span id='pid'>1396, <a href='https://arxiv.org/pdf/2505.19637.pdf' target='_blank'>https://arxiv.org/pdf/2505.19637.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Byunghyun Yoo, Younghwan Shin, Hyunwoo Kim, Euisok Chung, Jeongmin Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.19637">Adaptive Episode Length Adjustment for Multi-agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In standard reinforcement learning, an episode is defined as a sequence of interactions between agents and the environment, which terminates upon reaching a terminal state or a pre-defined episode length. Setting a shorter episode length enables the generation of multiple episodes with the same number of data samples, thereby facilitating an exploration of diverse states. While shorter episodes may limit the collection of long-term interactions, they may offer significant advantages when properly managed. For example, trajectory truncation in single-agent reinforcement learning has shown how the benefits of shorter episodes can be leveraged despite the trade-off of reduced long-term interaction experiences. However, this approach remains underexplored in MARL. This paper proposes a novel MARL approach, Adaptive Episode Length Adjustment (AELA), where the episode length is initially limited and gradually increased based on an entropy-based assessment of learning progress. By starting with shorter episodes, agents can focus on learning effective strategies for initial states and minimize time spent in dead-end states. The use of entropy as an assessment metric prevents premature convergence to suboptimal policies and ensures balanced training over varying episode lengths. We validate our approach using the StarCraft Multi-agent Challenge (SMAC) and a modified predator-prey environment, demonstrating significant improvements in both convergence speed and overall performance compared to existing methods. To the best of our knowledge, this is the first study to adaptively adjust episode length in MARL based on learning progress.
<div id='section'>Paperid: <span id='pid'>1397, <a href='https://arxiv.org/pdf/2505.11461.pdf' target='_blank'>https://arxiv.org/pdf/2505.11461.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wesley A Suttle, Vipul K Sharma, Brian M Sadler
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.11461">Signal attenuation enables scalable decentralized multi-agent reinforcement learning over networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent reinforcement learning (MARL) methods typically require that agents enjoy global state observability, preventing development of decentralized algorithms and limiting scalability. Recent work has shown that, under assumptions on decaying inter-agent influence, global observability can be replaced by local neighborhood observability at each agent, enabling decentralization and scalability. Real-world applications enjoying such decay properties remain underexplored, however, despite the fact that signal power decay, or signal attenuation, due to path loss is an intrinsic feature of many problems in wireless communications and radar networks. In this paper, we show that signal attenuation enables decentralization in MARL by considering the illustrative special case of performing power allocation for target detection in a radar network. To achieve this, we propose two new constrained multi-agent Markov decision process formulations of this power allocation problem, derive local neighborhood approximations for global value function and policy gradient estimates and establish corresponding error bounds, and develop decentralized saddle point policy gradient algorithms for solving the proposed problems. Our approach, though oriented towards the specific radar network problem we consider, provides a useful model for extensions to additional problems in wireless communications and radar networks.
<div id='section'>Paperid: <span id='pid'>1398, <a href='https://arxiv.org/pdf/2505.07207.pdf' target='_blank'>https://arxiv.org/pdf/2505.07207.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chiqiang Liu, Dazi Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.07207">HYGMA: Hypergraph Coordination Networks with Dynamic Grouping for Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cooperative multi-agent reinforcement learning faces significant challenges in effectively organizing agent relationships and facilitating information exchange, particularly when agents need to adapt their coordination patterns dynamically. This paper presents a novel framework that integrates dynamic spectral clustering with hypergraph neural networks to enable adaptive group formation and efficient information processing in multi-agent systems. The proposed framework dynamically constructs and updates hypergraph structures through spectral clustering on agents' state histories, enabling higher-order relationships to emerge naturally from agent interactions. The hypergraph structure is enhanced with attention mechanisms for selective information processing, providing an expressive and efficient way to model complex agent relationships. This architecture can be implemented in both value-based and policy-based paradigms through a unified objective combining task performance with structural regularization. Extensive experiments on challenging cooperative tasks demonstrate that our method significantly outperforms state-of-the-art approaches in both sample efficiency and final performance.
<div id='section'>Paperid: <span id='pid'>1399, <a href='https://arxiv.org/pdf/2505.03949.pdf' target='_blank'>https://arxiv.org/pdf/2505.03949.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>John Christopher Tidwell, John Storm Tidwell
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.03949">Deep Q-Network (DQN) multi-agent reinforcement learning (MARL) for Stock Trading</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This project addresses the challenge of automated stock trading, where traditional methods and direct reinforcement learning (RL) struggle with market noise, complexity, and generalization. Our proposed solution is an integrated deep learning framework combining a Convolutional Neural Network (CNN) to identify patterns in technical indicators formatted as images, a Long Short-Term Memory (LSTM) network to capture temporal dependencies across both price history and technical indicators, and a Deep Q-Network (DQN) agent which learns the optimal trading policy (buy, sell, hold) based on the features extracted by the CNN and LSTM.
<div id='section'>Paperid: <span id='pid'>1400, <a href='https://arxiv.org/pdf/2505.01453.pdf' target='_blank'>https://arxiv.org/pdf/2505.01453.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bharathkumar Hegde, Melanie Bouroche
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.01453">Safe and Efficient CAV Lane Changing using Decentralised Safety Shields</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Lane changing is a complex decision-making problem for Connected and Autonomous Vehicles (CAVs) as it requires balancing traffic efficiency with safety. Although traffic efficiency can be improved by using vehicular communication for training lane change controllers using Multi-Agent Reinforcement Learning (MARL), ensuring safety is difficult. To address this issue, we propose a decentralised Hybrid Safety Shield (HSS) that combines optimisation and a rule-based approach to guarantee safety. Our method applies control barrier functions to constrain longitudinal and lateral control inputs of a CAV to ensure safe manoeuvres. Additionally, we present an architecture to integrate HSS with MARL, called MARL-HSS, to improve traffic efficiency while ensuring safety. We evaluate MARL-HSS using a gym-like environment that simulates an on-ramp merging scenario with two levels of traffic densities, such as light and moderate densities. The results show that HSS provides a safety guarantee by strictly enforcing a dynamic safety constraint defined on a time headway, even in moderate traffic density that offers challenging lane change scenarios. Moreover, the proposed method learns stable policies compared to the baseline, a state-of-the-art MARL lane change controller without a safety shield. Further policy evaluation shows that our method achieves a balance between safety and traffic efficiency with zero crashes and comparable average speeds in light and moderate traffic densities.
<div id='section'>Paperid: <span id='pid'>1401, <a href='https://arxiv.org/pdf/2505.00540.pdf' target='_blank'>https://arxiv.org/pdf/2505.00540.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ian O'Flynn, Harun Å iljak
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.00540">Emergence of Roles in Robotic Teams with Model Sharing and Limited Communication</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a reinforcement learning strategy for use in multi-agent foraging systems in which the learning is centralised to a single agent and its model is periodically disseminated among the population of non-learning agents. In a domain where multi-agent reinforcement learning (MARL) is the common approach, this approach aims to significantly reduce the computational and energy demands compared to approaches such as MARL and centralised learning models. By developing high performing foraging agents, these approaches can be translated into real-world applications such as logistics, environmental monitoring, and autonomous exploration. A reward function was incorporated into this approach that promotes role development among agents, without explicit directives. This led to the differentiation of behaviours among the agents. The implicit encouragement of role differentiation allows for dynamic actions in which agents can alter roles dependent on their interactions with the environment without the need for explicit communication between agents.
<div id='section'>Paperid: <span id='pid'>1402, <a href='https://arxiv.org/pdf/2504.20529.pdf' target='_blank'>https://arxiv.org/pdf/2504.20529.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Costas Mylonas, Emmanouel Varvarigos, Georgios Tsaousoglou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.20529">Safe Bottom-Up Flexibility Provision from Distributed Energy Resources</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Modern renewables-based power systems need to tap on the flexibility of Distributed Energy Resources (DERs) connected to distribution networks. It is important, however, that DER owners/users remain in control of their assets, decisions, and objectives. At the same time, the dynamic landscape of DER-penetrated distribution networks calls for agile, data-driven flexibility management frameworks. In the face of these developments, the Multi-Agent Reinforcement Learning (MARL) paradigm is gaining significant attention, as a distributed and data-driven decision-making policy. This paper addresses the need for bottom-up DER management decisions to account for the distribution network's safety-related constraints. While the related literature on safe MARL typically assumes that network characteristics are available and incorporated into the policy's safety layer, which implies active DSO engagement, this paper ensures that self-organized DER communities are enabled to provide distribution-network-safe flexibility services without relying on the aspirational and problematic requirement of bringing the DSO in the decision-making loop.
<div id='section'>Paperid: <span id='pid'>1403, <a href='https://arxiv.org/pdf/2504.08417.pdf' target='_blank'>https://arxiv.org/pdf/2504.08417.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Paul J. Pritz, Kin K. Leung
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.08417">Belief States for Cooperative Multi-Agent Reinforcement Learning under Partial Observability</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reinforcement learning in partially observable environments is typically challenging, as it requires agents to learn an estimate of the underlying system state. These challenges are exacerbated in multi-agent settings, where agents learn simultaneously and influence the underlying state as well as each others' observations. We propose the use of learned beliefs on the underlying state of the system to overcome these challenges and enable reinforcement learning with fully decentralized training and execution. Our approach leverages state information to pre-train a probabilistic belief model in a self-supervised fashion. The resulting belief states, which capture both inferred state information as well as uncertainty over this information, are then used in a state-based reinforcement learning algorithm to create an end-to-end model for cooperative multi-agent reinforcement learning under partial observability. By separating the belief and reinforcement learning tasks, we are able to significantly simplify the policy and value function learning tasks and improve both the convergence speed and the final performance. We evaluate our proposed method on diverse partially observable multi-agent tasks designed to exhibit different variants of partial observability.
<div id='section'>Paperid: <span id='pid'>1404, <a href='https://arxiv.org/pdf/2504.06684.pdf' target='_blank'>https://arxiv.org/pdf/2504.06684.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Delin Zhao, Yanbo Shan, Chang Liu, Shenghang Lin, Yingxin Shou, Bin Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.06684">SDHN: Skewness-Driven Hypergraph Networks for Enhanced Localized Multi-Robot Coordination</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-Agent Reinforcement Learning is widely used for multi-robot coordination, where simple graphs typically model pairwise interactions. However, such representations fail to capture higher-order collaborations, limiting effectiveness in complex tasks. While hypergraph-based approaches enhance cooperation, existing methods often generate arbitrary hypergraph structures and lack adaptability to environmental uncertainties. To address these challenges, we propose the Skewness-Driven Hypergraph Network (SDHN), which employs stochastic Bernoulli hyperedges to explicitly model higher-order multi-robot interactions. By introducing a skewness loss, SDHN promotes an efficient structure with Small-Hyperedge Dominant Hypergraph, allowing robots to prioritize localized synchronization while still adhering to the overall information, similar to human coordination. Extensive experiments on Moving Agents in Formation and Robotic Warehouse tasks validate SDHN's effectiveness, demonstrating superior performance over state-of-the-art baselines.
<div id='section'>Paperid: <span id='pid'>1405, <a href='https://arxiv.org/pdf/2503.24296.pdf' target='_blank'>https://arxiv.org/pdf/2503.24296.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yubo Zhang, Pedro Botelho, Trevor Gordon, Gil Zussman, Igor Kadota
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.24296">Fair Dynamic Spectrum Access via Fully Decentralized Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We consider a decentralized wireless network with several source-destination pairs sharing a limited number of orthogonal frequency bands. Sources learn to adapt their transmissions (specifically, their band selection strategy) over time, in a decentralized manner, without sharing information with each other. Sources can only observe the outcome of their own transmissions (i.e., success or collision), having no prior knowledge of the network size or of the transmission strategy of other sources. The goal of each source is to maximize their own throughput while striving for network-wide fairness. We propose a novel fully decentralized Reinforcement Learning (RL)-based solution that achieves fairness without coordination. The proposed Fair Share RL (FSRL) solution combines: (i) state augmentation with a semi-adaptive time reference; (ii) an architecture that leverages risk control and time difference likelihood; and (iii) a fairness-driven reward structure. We evaluate FSRL in more than 50 network settings with different number of agents, different amounts of available spectrum, in the presence of jammers, and in an ad-hoc setting. Simulation results suggest that, when we compare FSRL with a common baseline RL algorithm from the literature, FSRL can be up to 89.0% fairer (as measured by Jain's fairness index) in stringent settings with several sources and a single frequency band, and 48.1% fairer on average.
<div id='section'>Paperid: <span id='pid'>1406, <a href='https://arxiv.org/pdf/2503.22779.pdf' target='_blank'>https://arxiv.org/pdf/2503.22779.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junkai Hu, Li Xia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.22779">Policy Optimization and Multi-agent Reinforcement Learning for Mean-variance Team Stochastic Games</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We study a long-run mean-variance team stochastic game (MV-TSG), where each agent shares a common mean-variance objective for the system and takes actions independently to maximize it. MV-TSG has two main challenges. First, the variance metric is neither additive nor Markovian in a dynamic setting. Second, simultaneous policy updates of all agents lead to a non-stationary environment for each individual agent. Both challenges make dynamic programming inapplicable. In this paper, we study MV-TSGs from the perspective of sensitivity-based optimization. The performance difference and performance derivative formulas for joint policies are derived, which provide optimization information for MV-TSGs. We prove the existence of a deterministic Nash policy for this problem. Subsequently, we propose a Mean-Variance Multi-Agent Policy Iteration (MV-MAPI) algorithm with a sequential update scheme, where individual agent policies are updated one by one in a given order. We prove that the MV-MAPI algorithm converges to a first-order stationary point of the objective function. By analyzing the local geometry of stationary points, we derive specific conditions for stationary points to be (local) Nash equilibria, and further, strict local optima. To solve large-scale MV-TSGs in scenarios with unknown environmental parameters, we extend the idea of trust region methods to MV-MAPI and develop a multi-agent reinforcement learning algorithm named Mean-Variance Multi-Agent Trust Region Policy Optimization (MV-MATRPO). We derive a performance lower bound for each update of joint policies. Finally, numerical experiments on energy management in multiple microgrid systems are conducted.
<div id='section'>Paperid: <span id='pid'>1407, <a href='https://arxiv.org/pdf/2503.18201.pdf' target='_blank'>https://arxiv.org/pdf/2503.18201.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Georg Ziegner, Michael Choi, Hung Mac Chan Le, Sahil Sakhuja, Arash Sarmadi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.18201">Iterative Multi-Agent Reinforcement Learning: A Novel Approach Toward Real-World Multi-Echelon Inventory Optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-echelon inventory optimization (MEIO) is critical for effective supply chain management, but its inherent complexity can pose significant challenges. Heuristics are commonly used to address this complexity, yet they often face limitations in scope and scalability. Recent research has found deep reinforcement learning (DRL) to be a promising alternative to traditional heuristics, offering greater versatility by utilizing dynamic decision-making capabilities. However, since DRL is known to struggle with the curse of dimensionality, its relevance to complex real-life supply chain scenarios is still to be determined. This thesis investigates DRL's applicability to MEIO problems of increasing complexity. A state-of-the-art DRL model was replicated, enhanced, and tested across 13 supply chain scenarios, combining diverse network structures and parameters. To address DRL's challenges with dimensionality, additional models leveraging graph neural networks (GNNs) and multi-agent reinforcement learning (MARL) were developed, culminating in the novel iterative multi-agent reinforcement learning (IMARL) approach. IMARL demonstrated superior scalability, effectiveness, and reliability in optimizing inventory policies, consistently outperforming benchmarks. These findings confirm the potential of DRL, particularly IMARL, to address real-world supply chain challenges and call for additional research to further expand its applicability.
<div id='section'>Paperid: <span id='pid'>1408, <a href='https://arxiv.org/pdf/2503.13553.pdf' target='_blank'>https://arxiv.org/pdf/2503.13553.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Philipp D. Siedler, Ian Gemp
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.13553">LLM-Mediated Guidance of MARL Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In complex multi-agent environments, achieving efficient learning and desirable behaviours is a significant challenge for Multi-Agent Reinforcement Learning (MARL) systems. This work explores the potential of combining MARL with Large Language Model (LLM)-mediated interventions to guide agents toward more desirable behaviours. Specifically, we investigate how LLMs can be used to interpret and facilitate interventions that shape the learning trajectories of multiple agents. We experimented with two types of interventions, referred to as controllers: a Natural Language (NL) Controller and a Rule-Based (RB) Controller. The NL Controller, which uses an LLM to simulate human-like interventions, showed a stronger impact than the RB Controller. Our findings indicate that agents particularly benefit from early interventions, leading to more efficient training and higher performance. Both intervention types outperform the baseline without interventions, highlighting the potential of LLM-mediated guidance to accelerate training and enhance MARL performance in challenging environments.
<div id='section'>Paperid: <span id='pid'>1409, <a href='https://arxiv.org/pdf/2503.09755.pdf' target='_blank'>https://arxiv.org/pdf/2503.09755.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Guangyi Liu, Suzan Iloglu, Michael Caldara, Joseph W. Durham, Michael M. Zavlanos
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.09755">Distributionally Robust Multi-Agent Reinforcement Learning for Dynamic Chute Mapping</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In Amazon robotic warehouses, the destination-to-chute mapping problem is crucial for efficient package sorting. Often, however, this problem is complicated by uncertain and dynamic package induction rates, which can lead to increased package recirculation. To tackle this challenge, we introduce a Distributionally Robust Multi-Agent Reinforcement Learning (DRMARL) framework that learns a destination-to-chute mapping policy that is resilient to adversarial variations in induction rates. Specifically, DRMARL relies on group distributionally robust optimization (DRO) to learn a policy that performs well not only on average but also on each individual subpopulation of induction rates within the group that capture, for example, different seasonality or operation modes of the system. This approach is then combined with a novel contextual bandit-based predictor of the worst-case induction distribution for each state-action pair, significantly reducing the cost of exploration and thereby increasing the learning efficiency and scalability of our framework. Extensive simulations demonstrate that DRMARL achieves robust chute mapping in the presence of varying induction distributions, reducing package recirculation by an average of 80\% in the simulation scenario.
<div id='section'>Paperid: <span id='pid'>1410, <a href='https://arxiv.org/pdf/2503.08493.pdf' target='_blank'>https://arxiv.org/pdf/2503.08493.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>F. GiarrÃ¨, I. A. Meer, M. Masoudi, M. Ozger, C. Cavdar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.08493">Hierarchical Multi Agent DRL for Soft Handovers Between Edge Clouds in Open RAN</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-connectivity (MC) for aerial users via a set of ground access points offers the potential for highly reliable communication. Within an open radio access network (O-RAN) architecture, edge clouds (ECs) enable MC with low latency for users within their coverage area. However, ensuring seamless service continuity for transitional users-those moving between the coverage areas of neighboring ECs-poses challenges due to centralized processing demands. To address this, we formulate a problem facilitating soft handovers between ECs, ensuring seamless transitions while maintaining service continuity for all users. We propose a hierarchical multi-agent reinforcement learning (HMARL) algorithm to dynamically determine the optimal functional split configuration for transitional and non-transitional users. Simulation results show that the proposed approach outperforms the conventional functional split in terms of the percentage of users maintaining service continuity, with at most 4% optimality gap. Additionally, HMARL achieves better scalability compared to the static baselines.
<div id='section'>Paperid: <span id='pid'>1411, <a href='https://arxiv.org/pdf/2503.07397.pdf' target='_blank'>https://arxiv.org/pdf/2503.07397.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kha Vo, Chin-Teng Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.07397">Q-MARL: A quantum-inspired algorithm using neural message passing for large-scale multi-agent reinforcement learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Inspired by a graph-based technique for predicting molecular properties in quantum chemistry -- atoms' position within molecules in three-dimensional space -- we present Q-MARL, a completely decentralised learning architecture that supports very large-scale multi-agent reinforcement learning scenarios without the need for strong assumptions like common rewards or agent order. The key is to treat each agent as relative to its surrounding agents in an environment that is presumed to change dynamically. Hence, in each time step, an agent is the centre of its own neighbourhood and also a neighbour to many other agents. Each role is formulated as a sub-graph, and each sub-graph is used as a training sample. A message-passing neural network supports full-scale vertex and edge interaction within a local neighbourhood, while a parameter governing the depth of the sub-graphs eases the training burden. During testing, an agent's actions are locally ensembled across all the sub-graphs that contain it, resulting in robust decisions. Where other approaches struggle to manage 50 agents, Q-MARL can easily marshal thousands. A detailed theoretical analysis proves improvement and convergence, and simulations with the typical collaborative and competitive scenarios show dramatically faster training speeds and reduced training losses.
<div id='section'>Paperid: <span id='pid'>1412, <a href='https://arxiv.org/pdf/2503.06747.pdf' target='_blank'>https://arxiv.org/pdf/2503.06747.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Diego Bolliger, Lorenz Zauter, Robert Ziegler
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.06747">Fully-Decentralized MADDPG with Networked Agents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we devise three actor-critic algorithms with decentralized training for multi-agent reinforcement learning in cooperative, adversarial, and mixed settings with continuous action spaces. To this goal, we adapt the MADDPG algorithm by applying a networked communication approach between agents. We introduce surrogate policies in order to decentralize the training while allowing for local communication during training. The decentralized algorithms achieve comparable results to the original MADDPG in empirical tests, while reducing computational cost. This is more pronounced with larger numbers of agents.
<div id='section'>Paperid: <span id='pid'>1413, <a href='https://arxiv.org/pdf/2503.03889.pdf' target='_blank'>https://arxiv.org/pdf/2503.03889.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Muhammad Waseem, Kshitij Bhatta, Chen Li, Qing Chang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.03889">Pretrained LLMs as Real-Time Controllers for Robot Operated Serial Production Line</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The manufacturing industry is undergoing a transformative shift, driven by cutting-edge technologies like 5G, AI, and cloud computing. Despite these advancements, effective system control, which is crucial for optimizing production efficiency, remains a complex challenge due to the intricate, knowledge-dependent nature of manufacturing processes and the reliance on domain-specific expertise. Conventional control methods often demand heavy customization, considerable computational resources, and lack transparency in decision-making. In this work, we investigate the feasibility of using Large Language Models (LLMs), particularly GPT-4, as a straightforward, adaptable solution for controlling manufacturing systems, specifically, mobile robot scheduling. We introduce an LLM-based control framework to assign mobile robots to different machines in robot assisted serial production lines, evaluating its performance in terms of system throughput. Our proposed framework outperforms traditional scheduling approaches such as First-Come-First-Served (FCFS), Shortest Processing Time (SPT), and Longest Processing Time (LPT). While it achieves performance that is on par with state-of-the-art methods like Multi-Agent Reinforcement Learning (MARL), it offers a distinct advantage by delivering comparable throughput without the need for extensive retraining. These results suggest that the proposed LLM-based solution is well-suited for scenarios where technical expertise, computational resources, and financial investment are limited, while decision transparency and system scalability are critical concerns.
<div id='section'>Paperid: <span id='pid'>1414, <a href='https://arxiv.org/pdf/2503.01941.pdf' target='_blank'>https://arxiv.org/pdf/2503.01941.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Marc Speckmann, Theresa Eimer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.01941">Task Scheduling & Forgetting in Multi-Task Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reinforcement learning (RL) agents can forget tasks they have previously been trained on. There is a rich body of work on such forgetting effects in humans. Therefore we look for commonalities in the forgetting behavior of humans and RL agents across tasks and test the viability of forgetting prevention measures from learning theory in RL. We find that in many cases, RL agents exhibit forgetting curves similar to those of humans. Methods like Leitner or SuperMemo have been shown to be effective at counteracting human forgetting, but we demonstrate they do not transfer as well to RL. We identify a likely cause: asymmetrical learning and retention patterns between tasks that cannot be captured by retention-based or performance-based curriculum strategies.
<div id='section'>Paperid: <span id='pid'>1415, <a href='https://arxiv.org/pdf/2503.00684.pdf' target='_blank'>https://arxiv.org/pdf/2503.00684.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Maria Ana Cardei, Afsaneh Doryab
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.00684">Factorized Deep Q-Network for Cooperative Multi-Agent Reinforcement Learning in Victim Tagging</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Mass casualty incidents (MCIs) are a growing concern, characterized by complexity and uncertainty that demand adaptive decision-making strategies. The victim tagging step in the emergency medical response must be completed quickly and is crucial for providing information to guide subsequent time-constrained response actions. In this paper, we present a mathematical formulation of multi-agent victim tagging to minimize the time it takes for responders to tag all victims. Five distributed heuristics are formulated and evaluated with simulation experiments. The heuristics considered are on-the go, practical solutions that represent varying levels of situational uncertainty in the form of global or local communication capabilities, showcasing practical constraints. We further investigate the performance of a multi-agent reinforcement learning (MARL) strategy, factorized deep Q-network (FDQN), to minimize victim tagging time as compared to baseline heuristics. Extensive simulations demonstrate that between the heuristics, methods with local communication are more efficient for adaptive victim tagging, specifically choosing the nearest victim with the option to replan. Analyzing all experiments, we find that our FDQN approach outperforms heuristics in smaller-scale scenarios, while heuristics excel in more complex scenarios. Our experiments contain diverse complexities that explore the upper limits of MARL capabilities for real-world applications and reveal key insights.
<div id='section'>Paperid: <span id='pid'>1416, <a href='https://arxiv.org/pdf/2501.15735.pdf' target='_blank'>https://arxiv.org/pdf/2501.15735.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Madan Dahal, Mojtaba Vaezi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.15735">Selective Experience Sharing in Reinforcement Learning Enhances Interference Management</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a novel multi-agent reinforcement learning (RL) approach for inter-cell interference mitigation, in which agents selectively share their experiences with other agents. Each base station is equipped with an agent, which receives signal-to-interference-plus-noise ratio from its own associated users. This information is used to evaluate and selectively share experiences with neighboring agents. The idea is that even a few pertinent experiences from other agents can lead to effective learning. This approach enables fully decentralized training and execution, minimizes information sharing between agents and significantly reduces communication overhead, which is typically the burden of interference management. The proposed method outperforms state-of-the-art multi-agent RL techniques where training is done in a decentralized manner. Furthermore, with a 75% reduction in experience sharing, the proposed algorithm achieves 98% of the spectral efficiency obtained by algorithms sharing all experiences.
<div id='section'>Paperid: <span id='pid'>1417, <a href='https://arxiv.org/pdf/2501.13727.pdf' target='_blank'>https://arxiv.org/pdf/2501.13727.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haikuo Du, Fandi Gou, Yunze Cai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.13727">Scalable Safe Multi-Agent Reinforcement Learning for Multi-Agent System</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Safety and scalability are two critical challenges faced by practical Multi-Agent Systems (MAS). However, existing Multi-Agent Reinforcement Learning (MARL) algorithms that rely solely on reward shaping are ineffective in ensuring safety, and their scalability is rather limited due to the fixed-size network output. To address these issues, we propose a novel framework, Scalable Safe MARL (SS-MARL), to enhance the safety and scalability of MARL methods. Leveraging the inherent graph structure of MAS, we design a multi-layer message passing network to aggregate local observations and communications of varying sizes. Furthermore, we develop a constrained joint policy optimization method in the setting of local observation to improve safety. Simulation experiments demonstrate that SS-MARL achieves a better trade-off between optimality and safety compared to baselines, and its scalability significantly outperforms the latest methods in scenarios with a large number of agents.
<div id='section'>Paperid: <span id='pid'>1418, <a href='https://arxiv.org/pdf/2501.12199.pdf' target='_blank'>https://arxiv.org/pdf/2501.12199.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tuo Zhang, Leonardo Stella, Julian Barreiro-Gomez
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.12199">Experience-replay Innovative Dynamics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite its groundbreaking success, multi-agent reinforcement learning (MARL) still suffers from instability and nonstationarity. Replicator dynamics, the most well-known model from evolutionary game theory (EGT), provide a theoretical framework for the convergence of the trajectories to Nash equilibria and, as a result, have been used to ensure formal guarantees for MARL algorithms in stable game settings. However, they exhibit the opposite behavior in other settings, which poses the problem of finding alternatives to ensure convergence. In contrast, innovative dynamics, such as the Brown-von Neumann-Nash (BNN) or Smith, result in periodic trajectories with the potential to approximate Nash equilibria. Yet, no MARL algorithms based on these dynamics have been proposed. In response to this challenge, we develop a novel experience replay-based MARL algorithm that incorporates revision protocols as tunable hyperparameters. We demonstrate, by appropriately adjusting the revision protocols, that the behavior of our algorithm mirrors the trajectories resulting from these dynamics. Importantly, our contribution provides a framework capable of extending the theoretical guarantees of MARL algorithms beyond replicator dynamics. Finally, we corroborate our theoretical findings with empirical results.
<div id='section'>Paperid: <span id='pid'>1419, <a href='https://arxiv.org/pdf/2501.10367.pdf' target='_blank'>https://arxiv.org/pdf/2501.10367.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mengxian Li, Qi Wang, Yongjun Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.10367">GTDE: Grouped Training with Decentralized Execution for Multi-agent Actor-Critic</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid advancement of multi-agent reinforcement learning (MARL) has given rise to diverse training paradigms to learn the policies of each agent in the multi-agent system. The paradigms of decentralized training and execution (DTDE) and centralized training with decentralized execution (CTDE) have been proposed and widely applied. However, as the number of agents increases, the inherent limitations of these frameworks significantly degrade the performance metrics, such as win rate, total reward, etc. To reduce the influence of the increasing number of agents on the performance metrics, we propose a novel training paradigm of grouped training decentralized execution (GTDE). This framework eliminates the need for a centralized module and relies solely on local information, effectively meeting the training requirements of large-scale multi-agent systems. Specifically, we first introduce an adaptive grouping module, which divides each agent into different groups based on their observation history. To implement end-to-end training, GTDE uses Gumbel-Sigmoid for efficient point-to-point sampling on the grouping distribution while ensuring gradient backpropagation. To adapt to the uncertainty in the number of members in a group, two methods are used to implement a group information aggregation module that merges member information within the group. Empirical results show that in a cooperative environment with 495 agents, GTDE increased the total reward by an average of 382\% compared to the baseline. In a competitive environment with 64 agents, GTDE achieved a 100\% win rate against the baseline.
<div id='section'>Paperid: <span id='pid'>1420, <a href='https://arxiv.org/pdf/2501.00160.pdf' target='_blank'>https://arxiv.org/pdf/2501.00160.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>David Goll, Jobst Heitzig, Wolfram Barfuss
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.00160">Deterministic Model of Incremental Multi-Agent Boltzmann Q-Learning: Transient Cooperation, Metastability, and Oscillations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-Agent Reinforcement Learning involves agents that learn together in a shared environment, leading to emergent dynamics sensitive to initial conditions and parameter variations. A Dynamical Systems approach, which studies the evolution of multi-component systems over time, has uncovered some of the underlying dynamics by constructing deterministic approximation models of stochastic algorithms. In this work, we demonstrate that even in the simplest case of independent Q-learning with a Boltzmann exploration policy, significant discrepancies arise between the actual algorithm and previous approximations. We elaborate why these models actually approximate interesting variants rather than the original incremental algorithm. To explain the discrepancies, we introduce a new discrete-time approximation model that explicitly accounts for agents' update frequencies within the learning process and show that its dynamics fundamentally differ from the simplified dynamics of prior models. We illustrate the usefulness of our approach by applying it to the question of spontaneous cooperation in social dilemmas, specifically the Prisoner's Dilemma as the simplest case study. We identify conditions under which the learning behaviour appears as long-term stable cooperation from an external perspective. However, our model shows that this behaviour is merely a metastable transient phase and not a true equilibrium, making it exploitable. We further exemplify how specific parameter settings can significantly exacerbate the moving target problem in independent learning. Through a systematic analysis of our model, we show that increasing the discount factor induces oscillations, preventing convergence to a joint policy. These oscillations arise from a supercritical Neimark-Sacker bifurcation, which transforms the unique stable fixed point into an unstable focus surrounded by a stable limit cycle.
<div id='section'>Paperid: <span id='pid'>1421, <a href='https://arxiv.org/pdf/2412.20523.pdf' target='_blank'>https://arxiv.org/pdf/2412.20523.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Neil De La Fuente, Miquel Noguer i Alonso, Guim CasadellÃ
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.20523">Game Theory and Multi-Agent Reinforcement Learning : From Nash Equilibria to Evolutionary Dynamics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper explores advanced topics in complex multi-agent systems building upon our previous work. We examine four fundamental challenges in Multi-Agent Reinforcement Learning (MARL): non-stationarity, partial observability, scalability with large agent populations, and decentralized learning. The paper provides mathematical formulations and analysis of recent algorithmic advancements designed to address these challenges, with a particular focus on their integration with game-theoretic concepts. We investigate how Nash equilibria, evolutionary game theory, correlated equilibrium, and adversarial dynamics can be effectively incorporated into MARL algorithms to improve learning outcomes. Through this comprehensive analysis, we demonstrate how the synthesis of game theory and MARL can enhance the robustness and effectiveness of multi-agent systems in complex, dynamic environments.
<div id='section'>Paperid: <span id='pid'>1422, <a href='https://arxiv.org/pdf/2412.19064.pdf' target='_blank'>https://arxiv.org/pdf/2412.19064.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shenghong He, Chao Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.19064">Hierarchical Multi-agent Meta-Reinforcement Learning for Cross-channel Bidding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Real-time bidding (RTB) plays a pivotal role in online advertising ecosystems. Advertisers employ strategic bidding to optimize their advertising impact while adhering to various financial constraints, such as the return-on-investment (ROI) and cost-per-click (CPC). Primarily focusing on bidding with fixed budget constraints, traditional approaches cannot effectively manage the dynamic budget allocation problem where the goal is to achieve global optimization of bidding performance across multiple channels with a shared budget. In this paper, we propose a hierarchical multi-agent reinforcement learning framework for multi-channel bidding optimization. In this framework, the top-level strategy applies a CPC constrained diffusion model to dynamically allocate budgets among the channels according to their distinct features and complex interdependencies, while the bottom-level strategy adopts a state-action decoupled actor-critic method to address the problem of extrapolation errors in offline learning caused by out-of-distribution actions and a context-based meta-channel knowledge learning method to improve the state representation capability of the policy based on the shared knowledge among different channels. Comprehensive experiments conducted on a large scale real-world industrial dataset from the Meituan ad bidding platform demonstrate that our method achieves a state-of-the-art performance.
<div id='section'>Paperid: <span id='pid'>1423, <a href='https://arxiv.org/pdf/2412.15573.pdf' target='_blank'>https://arxiv.org/pdf/2412.15573.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Joshua Holder, Natasha Jaques, Mehran Mesbahi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.15573">Multi Agent Reinforcement Learning for Sequential Satellite Assignment Problems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Assignment problems are a classic combinatorial optimization problem in which a group of agents must be assigned to a group of tasks such that maximum utility is achieved while satisfying assignment constraints. Given the utility of each agent completing each task, polynomial-time algorithms exist to solve a single assignment problem in its simplest form. However, in many modern-day applications such as satellite constellations, power grids, and mobile robot scheduling, assignment problems unfold over time, with the utility for a given assignment depending heavily on the state of the system. We apply multi-agent reinforcement learning to this problem, learning the value of assignments by bootstrapping from a known polynomial-time greedy solver and then learning from further experience. We then choose assignments using a distributed optimal assignment mechanism rather than by selecting them directly. We demonstrate that this algorithm is theoretically justified and avoids pitfalls experienced by other RL algorithms in this setting. Finally, we show that our algorithm significantly outperforms other methods in the literature, even while scaling to realistic scenarios with hundreds of agents and tasks.
<div id='section'>Paperid: <span id='pid'>1424, <a href='https://arxiv.org/pdf/2412.06333.pdf' target='_blank'>https://arxiv.org/pdf/2412.06333.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>F. Bredell, H. A. Engelbrecht, J. C. Schoeman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.06333">Augmenting the action space with conventions to improve multi-agent cooperation in Hanabi</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The card game Hanabi is considered a strong medium for the testing and development of multi-agent reinforcement learning (MARL) algorithms, due to its cooperative nature, partial observability, limited communication and remarkable complexity. Previous research efforts have explored the capabilities of MARL algorithms within Hanabi, focusing largely on advanced architecture design and algorithmic manipulations to achieve state-of-the-art performance for various number of cooperators. However, this often leads to complex solution strategies with high computational cost and requiring large amounts of training data. For humans to solve the Hanabi game effectively, they require the use of conventions, which often allows for a means to implicitly convey ideas or knowledge based on a predefined, and mutually agreed upon, set of "rules" or principles. Multi-agent problems containing partial observability, especially when limited communication is present, can benefit greatly from the use of implicit knowledge sharing. In this paper, we propose a novel approach to augmenting an agent's action space using conventions, which act as a sequence of special cooperative actions that span over and include multiple time steps and multiple agents, requiring agents to actively opt in for it to reach fruition. These conventions are based on existing human conventions, and result in a significant improvement on the performance of existing techniques for self-play and cross-play for various number of cooperators within Hanabi.
<div id='section'>Paperid: <span id='pid'>1425, <a href='https://arxiv.org/pdf/2412.04369.pdf' target='_blank'>https://arxiv.org/pdf/2412.04369.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoran Su, Joseph Y. J. Chow
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.04369">Intersection-Aware Assessment of EMS Accessibility in NYC: A Data-Driven Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Emergency response times are critical in densely populated urban environments like New York City (NYC), where traffic congestion significantly impedes emergency vehicle (EMV) mobility. This study introduces an intersection-aware emergency medical service (EMS) accessibility model to evaluate and improve EMV travel times across NYC. Integrating intersection density metrics, road network characteristics, and demographic data, the model identifies vulnerable regions with inadequate EMS coverage. The analysis reveals that densely interconnected areas, such as parts of Staten Island, Queens, and Manhattan, experience significant accessibility deficits due to intersection delays and sparse medical infrastructure. To address these challenges, this study explores the adoption of EMVLight, a multi-agent reinforcement learning framework, which demonstrates the potential to reduce intersection delays by 50\%, increasing EMS accessibility to 95\% of NYC residents within the critical benchmark of 4 minutes. Results indicate that advanced traffic signal control (TSC) systems can alleviate congestion-induced delays while improving equity in emergency response. The findings provide actionable insights for urban planning and policy interventions to enhance EMS accessibility and ensure timely care for underserved populations.
<div id='section'>Paperid: <span id='pid'>1426, <a href='https://arxiv.org/pdf/2412.04341.pdf' target='_blank'>https://arxiv.org/pdf/2412.04341.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ke Sun, Huan Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.04341">Reinforcement Learning for Freeway Lane-Change Regulation via Connected Vehicles</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Lane change decision-making is a complex task due to intricate vehicle-vehicle and vehicle-infrastructure interactions. Existing algorithms for lane-change control often depend on vehicles with a certain level of autonomy (e.g., autonomous or connected autonomous vehicles). To address the challenges posed by low penetration rates of autonomous vehicles and the high costs of precise data collection, this study proposes a dynamic lane change regulation design based on multi-agent reinforcement learning (MARL) to enhance freeway traffic efficiency. The proposed framework leverages multi-lane macroscopic traffic models that describe spatial-temporal dynamics of the density and speed for each lane. Lateral traffic flow between adjacent lanes, resulting from aggregated lane-changing behaviors, is modeled as source terms exchanged between the partial differential equations (PDEs). We propose a lane change regulation strategy using MARL, where one agent is placed at each discretized lane grid. The state of each agent is represented by aggregated vehicle attributes within its grid, generated from the SUMO microscopic simulation environment. The agent's actions are lane-change regulations for vehicles in its grid. Specifically, lane-change regulation signals are computed at a centralized traffic management center and then broadcast to connected vehicles in the corresponding lane grids. Compared to vehicle-level maneuver control, this approach achieves a higher regulation rate by leveraging vehicle connectivity while introducing no critical safety concerns, and accommodating varying levels of connectivity and autonomy within the traffic system. The proposed model is simulated and evaluated in varied traffic scenarios and demand conditions. Experimental results demonstrate that the method improves overall traffic efficiency with minimal additional energy consumption while maintaining driving safety.
<div id='section'>Paperid: <span id='pid'>1427, <a href='https://arxiv.org/pdf/2412.02057.pdf' target='_blank'>https://arxiv.org/pdf/2412.02057.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Anubha Mahajan, Shreya Hegde, Ethan Shay, Daniel Wu, Aviva Prins
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.02057">Comparative Analysis of Multi-Agent Reinforcement Learning Policies for Crop Planning Decision Support</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In India, the majority of farmers are classified as small or marginal, making their livelihoods particularly vulnerable to economic losses due to market saturation and climate risks. Effective crop planning can significantly impact their expected income, yet existing decision support systems (DSS) often provide generic recommendations that fail to account for real-time market dynamics and the interactions among multiple farmers. In this paper, we evaluate the viability of three multi-agent reinforcement learning (MARL) approaches for optimizing total farmer income and promoting fairness in crop planning: Independent Q-Learning (IQL), where each farmer acts independently without coordination, Agent-by-Agent (ABA), which sequentially optimizes each farmer's policy in relation to the others, and the Multi-agent Rollout Policy, which jointly optimizes all farmers' actions for global reward maximization. Our results demonstrate that while IQL offers computational efficiency with linear runtime, it struggles with coordination among agents, leading to lower total rewards and an unequal distribution of income. Conversely, the Multi-agent Rollout policy achieves the highest total rewards and promotes equitable income distribution among farmers but requires significantly more computational resources, making it less practical for large numbers of agents. ABA strikes a balance between runtime efficiency and reward optimization, offering reasonable total rewards with acceptable fairness and scalability. These findings highlight the importance of selecting appropriate MARL approaches in DSS to provide personalized and equitable crop planning recommendations, advancing the development of more adaptive and farmer-centric agricultural decision-making systems.
<div id='section'>Paperid: <span id='pid'>1428, <a href='https://arxiv.org/pdf/2411.15036.pdf' target='_blank'>https://arxiv.org/pdf/2411.15036.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zeyang Li, Navid Azizan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.15036">Safe Multi-Agent Reinforcement Learning with Convergence to Generalized Nash Equilibrium</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent reinforcement learning (MARL) has achieved notable success in cooperative tasks, demonstrating impressive performance and scalability. However, deploying MARL agents in real-world applications presents critical safety challenges. Current safe MARL algorithms are largely based on the constrained Markov decision process (CMDP) framework, which enforces constraints only on discounted cumulative costs and lacks an all-time safety assurance. Moreover, these methods often overlook the feasibility issue (the system will inevitably violate state constraints within certain regions of the constraint set), resulting in either suboptimal performance or increased constraint violations. To address these challenges, we propose a novel theoretical framework for safe MARL with $\textit{state-wise}$ constraints, where safety requirements are enforced at every state the agents visit. To resolve the feasibility issue, we leverage a control-theoretic notion of the feasible region, the controlled invariant set (CIS), characterized by the safety value function. We develop a multi-agent method for identifying CISs, ensuring convergence to a Nash equilibrium on the safety value function. By incorporating CIS identification into the learning process, we introduce a multi-agent dual policy iteration algorithm that guarantees convergence to a generalized Nash equilibrium in state-wise constrained cooperative Markov games, achieving an optimal balance between feasibility and performance. Furthermore, for practical deployment in complex high-dimensional systems, we propose $\textit{Multi-Agent Dual Actor-Critic}$ (MADAC), a safe MARL algorithm that approximates the proposed iteration scheme within the deep RL paradigm. Empirical evaluations on safe MARL benchmarks demonstrate that MADAC consistently outperforms existing methods, delivering much higher rewards while reducing constraint violations.
<div id='section'>Paperid: <span id='pid'>1429, <a href='https://arxiv.org/pdf/2411.12246.pdf' target='_blank'>https://arxiv.org/pdf/2411.12246.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>David Ge, Hao Ji
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.12246">Efficient Training in Multi-Agent Reinforcement Learning: A Communication-Free Framework for the Box-Pushing Problem</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Self-organizing systems consist of autonomous agents that can perform complex tasks and adapt to dynamic environments without a central controller. Prior research often relies on reinforcement learning to enable agents to gain the skills needed for task completion, such as in the box-pushing environment. However, when agents push from opposing directions during exploration, they tend to exert equal and opposite forces on the box, resulting in minimal displacement and inefficient training. This paper proposes a model called Shared Pool of Information (SPI), which enables information to be accessible to all agents and facilitates coordination, reducing force conflicts among agents and enhancing exploration efficiency. Through computer simulations, we demonstrate that SPI not only expedites the training process but also requires fewer steps per episode, significantly improving the agents' collaborative effectiveness.
<div id='section'>Paperid: <span id='pid'>1430, <a href='https://arxiv.org/pdf/2411.10459.pdf' target='_blank'>https://arxiv.org/pdf/2411.10459.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Brian Mintz, Feng Fu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.10459">Evolutionary Multi-agent Reinforcement Learning in Group Social Dilemmas</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reinforcement learning (RL) is a powerful machine learning technique that has been successfully applied to a wide variety of problems. However, it can be unpredictable and produce suboptimal results in complicated learning environments. This is especially true when multiple agents learn simultaneously, which creates a complex system that is often analytically intractable. Our work considers the fundamental framework of Q-learning in Public Goods Games, where RL individuals must work together to achieve a common goal. This setting allows us to study the tragedy of the commons and free rider effects in AI cooperation, an emerging field with potential to resolve challenging obstacles to the wider application of artificial intelligence. While this social dilemma has been mainly investigated through traditional and evolutionary game theory, our approach bridges the gap between these two by studying agents with an intermediate level of intelligence. Specifically, we consider the influence of learning parameters on cooperation levels in simulations and a limiting system of differential equations, as well as the effect of evolutionary pressures on exploration rate in both of these models. We find selection for higher and lower levels of exploration, as well as attracting values, and a condition that separates these in a restricted class of games. Our work enhances the theoretical understanding of evolutionary Q-learning, and extends our knowledge of the evolution of machine behavior in social dilemmas.
<div id='section'>Paperid: <span id='pid'>1431, <a href='https://arxiv.org/pdf/2411.09856.pdf' target='_blank'>https://arxiv.org/pdf/2411.09856.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaoxuan Hou, Jiayi Yuan, Joel Z. Leibo, Natasha Jaques
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.09856">InvestESG: A multi-agent reinforcement learning benchmark for studying climate investment as a social dilemma</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>InvestESG is a novel multi-agent reinforcement learning (MARL) benchmark designed to study the impact of Environmental, Social, and Governance (ESG) disclosure mandates on corporate climate investments. The benchmark models an intertemporal social dilemma where companies balance short-term profit losses from climate mitigation efforts and long-term benefits from reducing climate risk, while ESG-conscious investors attempt to influence corporate behavior through their investment decisions. Companies allocate capital across mitigation, greenwashing, and resilience, with varying strategies influencing climate outcomes and investor preferences. We are releasing open-source versions of InvestESG in both PyTorch and JAX, which enable scalable and hardware-accelerated simulations for investigating competing incentives in mitigate climate change. Our experiments show that without ESG-conscious investors with sufficient capital, corporate mitigation efforts remain limited under the disclosure mandate. However, when a critical mass of investors prioritizes ESG, corporate cooperation increases, which in turn reduces climate risks and enhances long-term financial stability. Additionally, providing more information about global climate risks encourages companies to invest more in mitigation, even without investor involvement. Our findings align with empirical research using real-world data, highlighting MARL's potential to inform policy by providing insights into large-scale socio-economic challenges through efficient testing of alternative policy and market designs.
<div id='section'>Paperid: <span id='pid'>1432, <a href='https://arxiv.org/pdf/2411.08896.pdf' target='_blank'>https://arxiv.org/pdf/2411.08896.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruili Zhao, Jun Cai, Jiangtao Luo, Junpeng Gao, Yongyi Ran
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.08896">Demand-Aware Beam Hopping and Power Allocation for Load Balancing in Digital Twin empowered LEO Satellite Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Low-Earth orbit (LEO) satellites utilizing beam hopping (BH) technology offer extensive coverage, low latency, high bandwidth, and significant flexibility. However, the uneven geographical distribution and temporal variability of ground traffic demands, combined with the high mobility of LEO satellites, present significant challenges for efficient beam resource utilization. Traditional BH methods based on GEO satellites fail to address issues such as satellite interference, overlapping coverage, and mobility. This paper explores a Digital Twin (DT)-based collaborative resource allocation network for multiple LEO satellites with overlapping coverage areas. A two-tier optimization problem, focusing on load balancing and cell service fairness, is proposed to maximize throughput and minimize inter-cell service delay. The DT layer optimizes the allocation of overlapping coverage cells by designing BH patterns for each satellite, while the LEO layer optimizes power allocation for each selected service cell. At the DT layer, an Actor-Critic network is deployed on each agent, with a global critic network in the cloud center. The A3C algorithm is employed to optimize the DT layer. Concurrently, the LEO layer optimization is performed using a Multi-Agent Reinforcement Learning algorithm, where each beam functions as an independent agent. The simulation results show that this method reduces satellite load disparity by about 72.5% and decreases the average delay to 12ms. Additionally, our approach outperforms other benchmarks in terms of throughput, ensuring a better alignment between offered and requested data.
<div id='section'>Paperid: <span id='pid'>1433, <a href='https://arxiv.org/pdf/2411.04867.pdf' target='_blank'>https://arxiv.org/pdf/2411.04867.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Satchit Chatterji, Erman Acar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.04867">Think Smart, Act SMARL! Analyzing Probabilistic Logic Shields for Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Safe reinforcement learning (RL) is crucial for real-world applications, and multi-agent interactions introduce additional safety challenges. While Probabilistic Logic Shields (PLS) has been a powerful proposal to enforce safety in single-agent RL, their generalizability to multi-agent settings remains unexplored. In this paper, we address this gap by conducting extensive analyses of PLS within decentralized, multi-agent environments, and in doing so, propose $\textbf{Shielded Multi-Agent Reinforcement Learning (SMARL)}$ as a general framework for steering MARL towards norm-compliant outcomes. Our key contributions are: (1) a novel Probabilistic Logic Temporal Difference (PLTD) update for shielded, independent Q-learning, which incorporates probabilistic constraints directly into the value update process; (2) a probabilistic logic policy gradient method for shielded PPO with formal safety guarantees for MARL; and (3) comprehensive evaluation across symmetric and asymmetrically shielded $n$-player game-theoretic benchmarks, demonstrating fewer constraint violations and significantly better cooperation under normative constraints. These results position SMARL as an effective mechanism for equilibrium selection, paving the way toward safer, socially aligned multi-agent systems.
<div id='section'>Paperid: <span id='pid'>1434, <a href='https://arxiv.org/pdf/2410.22578.pdf' target='_blank'>https://arxiv.org/pdf/2410.22578.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ying Li, Changling Li, Jiyao Chen, Christine Roinou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.22578">Energy-Aware Multi-Agent Reinforcement Learning for Collaborative Execution in Mission-Oriented Drone Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Mission-oriented drone networks have been widely used for structural inspection, disaster monitoring, border surveillance, etc. Due to the limited battery capacity of drones, mission execution strategy impacts network performance and mission completion. However, collaborative execution is a challenging problem for drones in such a dynamic environment as it also involves efficient trajectory design. We leverage multi-agent reinforcement learning (MARL) to manage the challenge in this study, letting each drone learn to collaboratively execute tasks and plan trajectories based on its current status and environment. Simulation results show that the proposed collaborative execution model can successfully complete the mission at least 80% of the time, regardless of task locations and lengths, and can even achieve a 100% success rate when the task density is not way too sparse. To the best of our knowledge, our work is one of the pioneer studies on leveraging MARL on collaborative execution for mission-oriented drone networks; the unique value of this work lies in drone battery level driving our model design.
<div id='section'>Paperid: <span id='pid'>1435, <a href='https://arxiv.org/pdf/2410.21290.pdf' target='_blank'>https://arxiv.org/pdf/2410.21290.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Y. Wang, Y. Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.21290">Multiple Ships Cooperative Navigation and Collision Avoidance using Multi-agent Reinforcement Learning with Communication</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the real world, unmanned surface vehicles (USV) often need to coordinate with each other to accomplish specific tasks. However, achieving cooperative control in multi-agent systems is challenging due to issues such as non-stationarity and partial observability. Recent advancements in Multi-Agent Reinforcement Learning (MARL) provide new perspectives to address these challenges. Therefore, we propose using the multi-agent deep deterministic policy gradient (MADDPG) algorithm with communication to address multiple ships' cooperation problems under partial observability. We developed two tasks based on OpenAI's gym environment: cooperative navigation and cooperative collision avoidance. In these tasks, ships must not only learn effective control strategies but also establish communication protocols with other agents. We analyze the impact of external noise on communication, the effect of inter-agent communication on performance, and the communication patterns learned by the agents. The results demonstrate that our proposed framework effectively addresses cooperative navigation and collision avoidance among multiple vessels, significantly outperforming traditional single-agent algorithms. Agents establish a consistent communication protocol, enabling them to compensate for missing information through shared observations and achieve better coordination.
<div id='section'>Paperid: <span id='pid'>1436, <a href='https://arxiv.org/pdf/2410.18631.pdf' target='_blank'>https://arxiv.org/pdf/2410.18631.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Niki Kotecha, Antonio del Rio Chanona
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.18631">Leveraging Graph Neural Networks and Multi-Agent Reinforcement Learning for Inventory Control in Supply Chains</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Inventory control in modern supply chains has attracted significant attention due to the increasing number of disruptive shocks and the challenges posed by complex dynamics, uncertainties, and limited collaboration. Traditional methods, which often rely on static parameters, struggle to adapt to changing environments. This paper proposes a Multi-Agent Reinforcement Learning (MARL) framework with Graph Neural Networks (GNNs) for state representation to address these limitations.
  Our approach redefines the action space by parameterizing heuristic inventory control policies, making it adaptive as the parameters dynamically adjust based on system conditions. By leveraging the inherent graph structure of supply chains, our framework enables agents to learn the system's topology, and we employ a centralized learning, decentralized execution scheme that allows agents to learn collaboratively while overcoming information-sharing constraints. Additionally, we incorporate global mean pooling and regularization techniques to enhance performance.
  We test the capabilities of our proposed approach on four different supply chain configurations and conduct a sensitivity analysis. This work paves the way for utilizing MARL-GNN frameworks to improve inventory management in complex, decentralized supply chain environments.
<div id='section'>Paperid: <span id='pid'>1437, <a href='https://arxiv.org/pdf/2410.18621.pdf' target='_blank'>https://arxiv.org/pdf/2410.18621.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wonhyung Choi, Inkyung Ahn
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.18621">Evolutionary Dispersal of Ecological Species via Multi-Agent Deep Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding species dynamics in heterogeneous environments is essential for ecosystem studies. Traditional models assumed homogeneous habitats, but recent approaches include spatial and temporal variability, highlighting species migration. We adopt starvation-driven diffusion (SDD) models as nonlinear diffusion to describe species dispersal based on local resource conditions, showing advantages for species survival. However, accurate prediction remains challenging due to model simplifications. This study uses multi-agent reinforcement learning (MARL) with deep Q-networks (DQN) to simulate single species and predator-prey interactions, incorporating SDD-type rewards. Our simulations reveal evolutionary dispersal strategies, providing insights into species dispersal mechanisms and validating traditional mathematical models.
<div id='section'>Paperid: <span id='pid'>1438, <a href='https://arxiv.org/pdf/2410.17068.pdf' target='_blank'>https://arxiv.org/pdf/2410.17068.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianan Bai, Zheng Chen, Erik. G. Larsson
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.17068">Delay-Constrained Grant-Free Random Access in MIMO Systems: Distributed Pilot Allocation and Power Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We study a delay-constrained grant-free random access system with a multi-antenna base station. The users randomly generate data packets with expiration deadlines, which are then transmitted from data queues on a first-in first-out basis. To deliver a packet, a user needs to succeed in both random access phase (sending a pilot without collision) and data transmission phase (achieving a required data rate with imperfect channel information) before the packet expires. We develop a distributed, cross-layer policy that allows the users to dynamically and independently choose their pilots and transmit powers to achieve a high effective sum throughput with fairness consideration. Our policy design involves three key components: 1) a proxy of the instantaneous data rate that depends only on macroscopic environment variables and transmission decisions, considering pilot collisions and imperfect channel estimation; 2) a quantitative, instantaneous measure of fairness within each communication round; and 3) a deep learning-based, multi-agent control framework with centralized training and distributed execution. The proposed framework benefits from an accurate, differentiable objective function for training, thereby achieving a higher sample efficiency compared with a conventional application of model-free, multi-agent reinforcement learning algorithms. The performance of the proposed approach is verified by simulations under highly dynamic and heterogeneous scenarios.
<div id='section'>Paperid: <span id='pid'>1439, <a href='https://arxiv.org/pdf/2410.15335.pdf' target='_blank'>https://arxiv.org/pdf/2410.15335.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ali Kahe, Hamed Kebriaei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.15335">A Distributed Primal-Dual Method for Constrained Multi-agent Reinforcement Learning with General Parameterization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper proposes a novel distributed approach for solving a cooperative Constrained Multi-agent Reinforcement Learning (CMARL) problem, where agents seek to minimize a global objective function subject to shared constraints. Unlike existing methods that rely on centralized training or coordination, our approach enables fully decentralized online learning, with each agent maintaining local estimates of both primal and dual variables. Specifically, we develop a distributed primal-dual algorithm based on actor-critic methods, leveraging local information to estimate Lagrangian multipliers. We establish consensus among the Lagrangian multipliers across agents and prove the convergence of our algorithm to an equilibrium point, analyzing the sub-optimality of this equilibrium compared to the exact solution of the unparameterized problem. Furthermore, we introduce a constrained cooperative Cournot game with stochastic dynamics as a test environment to evaluate the algorithm's performance in complex, real-world scenarios.
<div id='section'>Paperid: <span id='pid'>1440, <a href='https://arxiv.org/pdf/2410.07976.pdf' target='_blank'>https://arxiv.org/pdf/2410.07976.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Baraah A. M. Sidahmed, Tatjana Chavdarova
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.07976">Addressing Rotational Learning Dynamics in Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent reinforcement learning (MARL) has emerged as a powerful paradigm for solving complex problems through agents' cooperation and competition, finding widespread applications across domains. Despite its success, MARL faces a reproducibility crisis. We show that, in part, this issue is related to the rotational optimization dynamics arising from competing agents' objectives, and require methods beyond standard optimization algorithms. We reframe MARL approaches using Variational Inequalities (VIs), offering a unified framework to address such issues. Leveraging optimization techniques designed for VIs, we propose a general approach for integrating gradient-based VI methods capable of handling rotational dynamics into existing MARL algorithms. Empirical results demonstrate significant performance improvements across benchmarks. In zero-sum games, Rock--paper--scissors and Matching pennies, VI methods achieve better convergence to equilibrium strategies, and in the Multi-Agent Particle Environment: Predator-prey, they also enhance team coordination. These results underscore the transformative potential of advanced optimization techniques in MARL.
<div id='section'>Paperid: <span id='pid'>1441, <a href='https://arxiv.org/pdf/2410.04631.pdf' target='_blank'>https://arxiv.org/pdf/2410.04631.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mathias Jackermeier, Alessandro Abate
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.04631">DeepLTL: Learning to Efficiently Satisfy Complex LTL Specifications for Multi-Task RL</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Linear temporal logic (LTL) has recently been adopted as a powerful formalism for specifying complex, temporally extended tasks in multi-task reinforcement learning (RL). However, learning policies that efficiently satisfy arbitrary specifications not observed during training remains a challenging problem. Existing approaches suffer from several shortcomings: they are often only applicable to finite-horizon fragments of LTL, are restricted to suboptimal solutions, and do not adequately handle safety constraints. In this work, we propose a novel learning approach to address these concerns. Our method leverages the structure of BÃ¼chi automata, which explicitly represent the semantics of LTL specifications, to learn policies conditioned on sequences of truth assignments that lead to satisfying the desired formulae. Experiments in a variety of discrete and continuous domains demonstrate that our approach is able to zero-shot satisfy a wide range of finite- and infinite-horizon specifications, and outperforms existing methods in terms of both satisfaction probability and efficiency. Code available at: https://deep-ltl.github.io/
<div id='section'>Paperid: <span id='pid'>1442, <a href='https://arxiv.org/pdf/2409.09509.pdf' target='_blank'>https://arxiv.org/pdf/2409.09509.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shatayu Kulkarni, Sabine Brunswicker
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.09509">Learning Nudges for Conditional Cooperation: A Multi-Agent Reinforcement Learning Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The public goods game describes a social dilemma in which a large proportion of agents act as conditional cooperators (CC): they only act cooperatively if they see others acting cooperatively because they satisfice with the social norm to be in line with what others are doing instead of optimizing cooperation. CCs are guided by aspiration-based reinforcement learning guided by past experiences of interactions with others and satisficing aspirations. In many real-world settings, reinforcing social norms do not emerge. In this paper, we propose that an optimizing reinforcement agent can facilitate cooperation through nudges, i.e. indirect mechanisms for cooperation to happen. The agent's goal is to motivate CCs into cooperation through its own actions to create social norms that signal that others are cooperating. We introduce a multi-agent reinforcement learning model for public goods games, with 3 CC learning agents using aspirational reinforcement learning and 1 nudging agent using deep reinforcement learning to learn nudges that optimize cooperation. For our nudging agent, we model two distinct reward functions, one maximizing the total game return (sum DRL) and one maximizing the number of cooperative contributions contributions higher than a proportional threshold (prop DRL). Our results show that our aspiration-based RL model for CC agents is consistent with empirically observed CC behavior. Games combining 3 CC RL agents and one nudging RL agent outperform the baseline consisting of 4 CC RL agents only. The sum DRL nudging agent increases the total sum of contributions by 8.22% and the total proportion of cooperative contributions by 12.42%, while the prop nudging DRL increases the total sum of contributions by 8.85% and the total proportion of cooperative contributions by 14.87%. Our findings advance the literature on public goods games and reinforcement learning.
<div id='section'>Paperid: <span id='pid'>1443, <a href='https://arxiv.org/pdf/2409.03875.pdf' target='_blank'>https://arxiv.org/pdf/2409.03875.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Benjamin Heymann, Marc Lanctot
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.03875">Learning in Games with Progressive Hiding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>When learning to play an imperfect information game, it is often easier to first start with the basic mechanics of the game rules. For example, one can play several example rounds with private cards revealed to all players to better understand the basic actions and their effects. Building on this intuition, this paper introduces {\it progressive hiding}, an algorithm that balances learning the basic mechanics of an imperfect information game and satisfying the information constraints. Progressive hiding is inspired by methods from stochastic multistage optimization, such as scenario decomposition and progressive hedging. We prove that it enables the adaptation of counterfactual regret minimization to games where perfect recall is not satisfied. Numerical experiments illustrate that progressive hiding produces notable improvements in several settings.
<div id='section'>Paperid: <span id='pid'>1444, <a href='https://arxiv.org/pdf/2409.00754.pdf' target='_blank'>https://arxiv.org/pdf/2409.00754.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiaming Yin, Weixiong Rao, Yu Xiao, Keshuang Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.00754">Cooperative Path Planning with Asynchronous Multiagent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we study the shortest path problem (SPP) with multiple source-destination pairs (MSD), namely MSD-SPP, to minimize average travel time of all shortest paths. The inherent traffic capacity limits within a road network contributes to the competition among vehicles. Multi-agent reinforcement learning (MARL) model cannot offer effective and efficient path planning cooperation due to the asynchronous decision making setting in MSD-SPP, where vehicles (a.k.a agents) cannot simultaneously complete routing actions in the previous time step. To tackle the efficiency issue, we propose to divide an entire road network into multiple sub-graphs and subsequently execute a two-stage process of inter-region and intra-region route planning. To address the asynchronous issue, in the proposed asyn-MARL framework, we first design a global state, which exploits a low-dimensional vector to implicitly represent the joint observations and actions of multi-agents. Then we develop a novel trajectory collection mechanism to decrease the redundancy in training trajectories. Additionally, we design a novel actor network to facilitate the cooperation among vehicles towards the same or close destinations and a reachability graph aimed at preventing infinite loops in routing paths. On both synthetic and real road networks, our evaluation result demonstrates that our approach outperforms state-of-the-art planning approaches.
<div id='section'>Paperid: <span id='pid'>1445, <a href='https://arxiv.org/pdf/2408.12067.pdf' target='_blank'>https://arxiv.org/pdf/2408.12067.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shaozhuang Bai, Zhenzhen Gao, Xuewen Liao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.12067">Distributed Noncoherent Joint Transmission Based on Multi-Agent Reinforcement Learning for Dense Small Cell MISO Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We consider a dense small cell (DSC) network where multi-antenna small cell base stations (SBSs) transmit data to single-antenna users over a shared frequency band. To enhance capacity, a state-of-the-art technique known as noncoherent joint transmission (JT) is applied, enabling users to receive data from multiple coordinated SBSs. However, the sum rate maximization problem with noncoherent JT is inherently nonconvex and NP-hard. While existing optimization-based noncoherent JT algorithms can provide near-optimal performance, they require global channel state information (CSI) and multiple iterations, which makes them difficult to be implemeted in DSC networks.To overcome these challenges, we first prove that the optimal beamforming structure is the same for both the power minimization problem and the sum rate maximization problem, and then mathematically derive the optimal beamforming structure for both problems by solving the power minimization problem.The optimal beamforming structure can effectively reduces the variable dimensions.By exploiting the optimal beamforming structure, we propose a deep deterministic policy gradient-based distributed noncoherent JT scheme to maximize the system sum rate.In the proposed scheme, each SBS utilizes global information for training and uses local CSI to determine beamforming vectors. Simulation results demonstrate that the proposed scheme achieves comparable performance with considerably lower computational complexity and information overhead compared to centralized iterative optimization-based techniques, making it more attractive for practical deployment.
<div id='section'>Paperid: <span id='pid'>1446, <a href='https://arxiv.org/pdf/2408.06503.pdf' target='_blank'>https://arxiv.org/pdf/2408.06503.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jahir Sadik Monon, Deeparghya Dutta Barua, Md. Mosaddek Khan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.06503">Enhancing Heterogeneous Multi-Agent Cooperation in Decentralized MARL via GNN-driven Intrinsic Rewards</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent Reinforcement Learning (MARL) is emerging as a key framework for various sequential decision-making and control tasks. Unlike their single-agent counterparts, multi-agent systems necessitate successful cooperation among the agents. The deployment of these systems in real-world scenarios often requires decentralized training, a diverse set of agents, and learning from infrequent environmental reward signals. These challenges become more pronounced under partial observability and the lack of prior knowledge about agent heterogeneity. While notable studies use intrinsic motivation (IM) to address reward sparsity or cooperation in decentralized settings, those dealing with heterogeneity typically assume centralized training, parameter sharing, and agent indexing. To overcome these limitations, we propose the CoHet algorithm, which utilizes a novel Graph Neural Network (GNN) based intrinsic motivation to facilitate the learning of heterogeneous agent policies in decentralized settings, under the challenges of partial observability and reward sparsity. Evaluation of CoHet in the Multi-agent Particle Environment (MPE) and Vectorized Multi-Agent Simulator (VMAS) benchmarks demonstrates superior performance compared to the state-of-the-art in a range of cooperative multi-agent scenarios. Our research is supplemented by an analysis of the impact of the agent dynamics model on the intrinsic motivation module, insights into the performance of different CoHet variants, and its robustness to an increasing number of heterogeneous agents.
<div id='section'>Paperid: <span id='pid'>1447, <a href='https://arxiv.org/pdf/2407.16034.pdf' target='_blank'>https://arxiv.org/pdf/2407.16034.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mukul Chodhary, Kevin Octavian, SooJean Han
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.16034">Efficient Replay Memory Architectures in Multi-Agent Reinforcement Learning for Traffic Congestion Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Episodic control, inspired by the role of episodic memory in the human brain, has been shown to improve the sample inefficiency of model-free reinforcement learning by reusing high-return past experiences. However, the memory growth of episodic control is undesirable in large-scale multi-agent problems such as vehicle traffic management. This paper proposes a novel replay memory architecture called Dual-Memory Integrated Learning, to augment to multi-agent reinforcement learning methods for congestion control via adaptive light signal scheduling. Our dual-memory architecture mimics two core capabilities of human decision-making. First, it relies on diverse types of memory--semantic and episodic, short-term and long-term--in order to remember high-return states that occur often in the network and filter out states that don't. Second, it employs equivalence classes to group together similar state-action pairs and that can be controlled using the same action (i.e., light signal sequence). Theoretical analyses establish memory growth bounds, and simulation experiments on several intersection networks showcase improved congestion performance (e.g., vehicle throughput) from our method.
<div id='section'>Paperid: <span id='pid'>1448, <a href='https://arxiv.org/pdf/2407.12539.pdf' target='_blank'>https://arxiv.org/pdf/2407.12539.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ichrak Mokhtari, Walid Bechkit, Mohamed Sami Assenine, HervÃ© Rivano
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.12539">Navigating the Smog: A Cooperative Multi-Agent RL for Accurate Air Pollution Mapping through Data Assimilation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid rise of air pollution events necessitates accurate, real-time monitoring for informed mitigation strategies. Data Assimilation (DA) methods provide promising solutions, but their effectiveness hinges heavily on optimal measurement locations. This paper presents a novel approach for air quality mapping where autonomous drones, guided by a collaborative multi-agent reinforcement learning (MARL) framework, act as airborne detectives. Ditching the limitations of static sensor networks, the drones engage in a synergistic interaction, adapting their flight paths in real time to gather optimal data for Data Assimilation (DA). Our approach employs a tailored reward function with dynamic credit assignment, enabling drones to prioritize informative measurements without requiring unavailable ground truth data, making it practical for real-world deployments. Extensive experiments using a real-world dataset demonstrate that our solution achieves significantly improved pollution estimates, even with limited drone resources or limited prior knowledge of the pollution plume. Beyond air quality, this solution unlocks possibilities for tackling diverse environmental challenges like wildfire detection and management through scalable and autonomous drone cooperation.
<div id='section'>Paperid: <span id='pid'>1449, <a href='https://arxiv.org/pdf/2407.10987.pdf' target='_blank'>https://arxiv.org/pdf/2407.10987.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Daniel Ayepah-Mensah, Guolin Sun, Yu Pang, Wei Jiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.10987">Adaptive Digital Twin and Communication-Efficient Federated Learning Network Slicing for 5G-enabled Internet of Things</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Network slicing enables industrial Internet of Things (IIoT) networks with multiservice and differentiated resource requirements to meet increasing demands through efficient use and management of network resources. Typically, the network slice orchestrator relies on demand forecasts for each slice to make informed decisions and maximize resource utilization. The new generation of Industry 4.0 has introduced digital twins to map physical systems to digital models for accurate decision-making. In our approach, we first use graph-attention networks to build a digital twin environment for network slices, enabling real-time traffic analysis, monitoring, and demand forecasting. Based on these predictions, we formulate the resource allocation problem as a federated multi-agent reinforcement learning problem and employ a deep deterministic policy gradient to determine the resource allocation policy while preserving the privacy of the slices. Our results demonstrate that the proposed approaches can improve the accuracy of demand prediction for network slices and reduce the communication overhead of dynamic network slicing.
<div id='section'>Paperid: <span id='pid'>1450, <a href='https://arxiv.org/pdf/2407.03521.pdf' target='_blank'>https://arxiv.org/pdf/2407.03521.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Igor Sadoune, Marcelin Joanis, Andrea Lodi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.03521">Algorithmic Collusion And The Minimum Price Markov Game</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces the Minimum Price Markov Game (MPMG), a theoretical model that reasonably approximates real-world first-price markets following the minimum price rule, such as public auctions. The goal is to provide researchers and practitioners with a framework to study market fairness and regulation in both digitized and non-digitized public procurement processes, amid growing concerns about algorithmic collusion in online markets. Using multi-agent reinforcement learning-driven artificial agents, we demonstrate that (i) the MPMG is a reliable model for first-price market dynamics, (ii) the minimum price rule is generally resilient to non-engineered tacit coordination among rational actors, and (iii) when tacit coordination occurs, it relies heavily on self-reinforcing trends. These findings contribute to the ongoing debate about algorithmic pricing and its implications.
<div id='section'>Paperid: <span id='pid'>1451, <a href='https://arxiv.org/pdf/2406.11496.pdf' target='_blank'>https://arxiv.org/pdf/2406.11496.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianhao Bu, Hang Li, Guojie Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.11496">Decentralized Collaborative Pricing and Shunting for Multiple EV Charging Stations Based on Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The extraordinary electric vehicle (EV) popularization in the recent years has facilitated research studies in alleviating EV energy charging demand. Previous studies primarily focused on the optimizations over charging stations (CS) profit and EV users cost savings through charge/discharge scheduling events. In this work, the random behaviors of EVs are considered, with EV users preferences over multi-CS characteristics modelled to imitate the potential CS selection disequilibrium. A price scheduling strategy under decentralized collaborative framework is proposed to achieve EV shunting in a multi-CS environment, while minimizing the charging cost through multi agent reinforcement learning. The proposed problem is formulated as a Markov Decision Process (MDP) with uncertain transition probability.
<div id='section'>Paperid: <span id='pid'>1452, <a href='https://arxiv.org/pdf/2406.11240.pdf' target='_blank'>https://arxiv.org/pdf/2406.11240.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Michelle Li, Michael Dennis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.11240">The Benefits of Power Regularization in Cooperative Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cooperative Multi-Agent Reinforcement Learning (MARL) algorithms, trained only to optimize task reward, can lead to a concentration of power where the failure or adversarial intent of a single agent could decimate the reward of every agent in the system. In the context of teams of people, it is often useful to explicitly consider how power is distributed to ensure no person becomes a single point of failure. Here, we argue that explicitly regularizing the concentration of power in cooperative RL systems can result in systems which are more robust to single agent failure, adversarial attacks, and incentive changes of co-players. To this end, we define a practical pairwise measure of power that captures the ability of any co-player to influence the ego agent's reward, and then propose a power-regularized objective which balances task reward and power concentration. Given this new objective, we show that there always exists an equilibrium where every agent is playing a power-regularized best-response balancing power and task reward. Moreover, we present two algorithms for training agents towards this power-regularized objective: Sample Based Power Regularization (SBPR), which injects adversarial data during training; and Power Regularization via Intrinsic Motivation (PRIM), which adds an intrinsic motivation to regulate power to the training objective. Our experiments demonstrate that both algorithms successfully balance task reward and power, leading to lower power behavior than the baseline of task-only reward and avoid catastrophic events in case an agent in the system goes off-policy.
<div id='section'>Paperid: <span id='pid'>1453, <a href='https://arxiv.org/pdf/2406.07848.pdf' target='_blank'>https://arxiv.org/pdf/2406.07848.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhenglong Luo, Zhiyong Chen, James Welsh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.07848">Multi-agent Reinforcement Learning with Deep Networks for Diverse Q-Vectors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent reinforcement learning (MARL) has become a significant research topic due to its ability to facilitate learning in complex environments. In multi-agent tasks, the state-action value, commonly referred to as the Q-value, can vary among agents because of their individual rewards, resulting in a Q-vector. Determining an optimal policy is challenging, as it involves more than just maximizing a single Q-value. Various optimal policies, such as a Nash equilibrium, have been studied in this context. Algorithms like Nash Q-learning and Nash Actor-Critic have shown effectiveness in these scenarios. This paper extends this research by proposing a deep Q-networks (DQN) algorithm capable of learning various Q-vectors using Max, Nash, and Maximin strategies. The effectiveness of this approach is demonstrated in an environment where dual robotic arms collaborate to lift a pot.
<div id='section'>Paperid: <span id='pid'>1454, <a href='https://arxiv.org/pdf/2406.06500.pdf' target='_blank'>https://arxiv.org/pdf/2406.06500.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mohidul Haque Mridul, Mohammad Foysal Khan, Redwan Ahmed Rizvee, Md Mosaddek Khan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.06500">Adaptive Opponent Policy Detection in Multi-Agent MDPs: Real-Time Strategy Switch Identification Using Running Error Estimation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In Multi-agent Reinforcement Learning (MARL), accurately perceiving opponents' strategies is essential for both cooperative and adversarial contexts, particularly within dynamic environments. While Proximal Policy Optimization (PPO) and related algorithms such as Actor-Critic with Experience Replay (ACER), Trust Region Policy Optimization (TRPO), and Deep Deterministic Policy Gradient (DDPG) perform well in single-agent, stationary environments, they suffer from high variance in MARL due to non-stationary and hidden policies of opponents, leading to diminished reward performance. Additionally, existing methods in MARL face significant challenges, including the need for inter-agent communication, reliance on explicit reward information, high computational demands, and sampling inefficiencies. These issues render them less effective in continuous environments where opponents may abruptly change their policies without prior notice. Against this background, we present OPS-DeMo (Online Policy Switch-Detection Model), an online algorithm that employs dynamic error decay to detect changes in opponents' policies. OPS-DeMo continuously updates its beliefs using an Assumed Opponent Policy (AOP) Bank and selects corresponding responses from a pre-trained Response Policy Bank. Each response policy is trained against consistently strategizing opponents, reducing training uncertainty and enabling the effective use of algorithms like PPO in multi-agent environments. Comparative assessments show that our approach outperforms PPO-trained models in dynamic scenarios like the Predator-Prey setting, providing greater robustness to sudden policy shifts and enabling more informed decision-making through precise opponent policy insights.
<div id='section'>Paperid: <span id='pid'>1455, <a href='https://arxiv.org/pdf/2405.18703.pdf' target='_blank'>https://arxiv.org/pdf/2405.18703.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tyler Becker, Zachary Sunberg
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.18703">Bridging the Gap between Partially Observable Stochastic Games and Sparse POMDP Methods</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Many real-world decision problems involve the interaction of multiple self-interested agents with limited sensing ability. The partially observable stochastic game (POSG) provides a mathematical framework for modeling these problems, however solving a POSG requires difficult reasoning over two critical factors: (1) information revealed by partial observations and (2) decisions other agents make. In the single agent case, partially observable Markov decision process (POMDP) planning can efficiently address partial observability with particle filtering. In the multi-agent case, extensive form game solution methods account for other agent's decisions, but preclude belief approximation. We propose a unifying framework that combines POMDP-inspired state distribution approximation and game-theoretic equilibrium search on information sets. This paper lays a theoretical foundation for the approach by bounding errors due to belief approximation, and empirically demonstrates effectiveness with a numerical example. The new approach enables planning in POSGs with very large state spaces, paving the way for reliable autonomous interaction in real-world physical environments and complementing multi-agent reinforcement learning.
<div id='section'>Paperid: <span id='pid'>1456, <a href='https://arxiv.org/pdf/2405.18190.pdf' target='_blank'>https://arxiv.org/pdf/2405.18190.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Johann Bauer, Sheldon West, Eduardo Alonso, Mark Broom
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.18190">Mutation-Bias Learning in Games</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present two variants of a multi-agent reinforcement learning algorithm based on evolutionary game theoretic considerations. The intentional simplicity of one variant enables us to prove results on its relationship to a system of ordinary differential equations of replicator-mutator dynamics type, allowing us to present proofs on the algorithm's convergence conditions in various settings via its ODE counterpart. The more complicated variant enables comparisons to Q-learning based algorithms. We compare both variants experimentally to WoLF-PHC and frequency-adjusted Q-learning on a range of settings, illustrating cases of increasing dimensionality where our variants preserve convergence in contrast to more complicated algorithms. The availability of analytic results provides a degree of transferability of results as compared to purely empirical case studies, illustrating the general utility of a dynamical systems perspective on multi-agent reinforcement learning when addressing questions of convergence and reliable generalisation.
<div id='section'>Paperid: <span id='pid'>1457, <a href='https://arxiv.org/pdf/2405.17486.pdf' target='_blank'>https://arxiv.org/pdf/2405.17486.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alexander DeRieux, Walid Saad
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.17486">eQMARL: Entangled Quantum Multi-Agent Reinforcement Learning for Distributed Cooperation over Quantum Channels</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Collaboration is a key challenge in distributed multi-agent reinforcement learning (MARL) environments. Learning frameworks for these decentralized systems must weigh the benefits of explicit player coordination against the communication overhead and computational cost of sharing local observations and environmental data. Quantum computing has sparked a potential synergy between quantum entanglement and cooperation in multi-agent environments, which could enable more efficient distributed collaboration with minimal information sharing. This relationship is largely unexplored, however, as current state-of-the-art quantum MARL (QMARL) implementations rely on classical information sharing rather than entanglement over a quantum channel as a coordination medium. In contrast, in this paper, a novel framework dubbed entangled QMARL (eQMARL) is proposed. The proposed eQMARL is a distributed actor-critic framework that facilitates cooperation over a quantum channel and eliminates local observation sharing via a quantum entangled split critic. Introducing a quantum critic uniquely spread across the agents allows coupling of local observation encoders through entangled input qubits over a quantum channel, which requires no explicit sharing of local observations and reduces classical communication overhead. Further, agent policies are tuned through joint observation-value function estimation via joint quantum measurements, thereby reducing the centralized computational burden. Experimental results show that eQMARL with $Î¨^{+}$ entanglement converges to a cooperative strategy up to $17.8\%$ faster and with a higher overall score compared to split classical and fully centralized classical and quantum baselines. The results also show that eQMARL achieves this performance with a constant factor of $25$-times fewer centralized parameters compared to the split classical baseline.
<div id='section'>Paperid: <span id='pid'>1458, <a href='https://arxiv.org/pdf/2405.02724.pdf' target='_blank'>https://arxiv.org/pdf/2405.02724.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yingjie Fei, Ruitu Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.02724">Taming Equilibrium Bias in Risk-Sensitive Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We study risk-sensitive multi-agent reinforcement learning under general-sum Markov games, where agents optimize the entropic risk measure of rewards with possibly diverse risk preferences. We show that using the regret naively adapted from existing literature as a performance metric could induce policies with equilibrium bias that favor the most risk-sensitive agents and overlook the other agents. To address such deficiency of the naive regret, we propose a novel notion of regret, which we call risk-balanced regret, and show through a lower bound that it overcomes the issue of equilibrium bias. Furthermore, we develop a self-play algorithm for learning Nash, correlated, and coarse correlated equilibria in risk-sensitive Markov games. We prove that the proposed algorithm attains near-optimal regret guarantees with respect to the risk-balanced regret.
<div id='section'>Paperid: <span id='pid'>1459, <a href='https://arxiv.org/pdf/2404.13096.pdf' target='_blank'>https://arxiv.org/pdf/2404.13096.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yidong Bai, Toshiharu Sugawara
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.13096">Reducing Redundant Computation in Multi-Agent Coordination through Locally Centralized Execution</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In multi-agent reinforcement learning, decentralized execution is a common approach, yet it suffers from the redundant computation problem. This occurs when multiple agents redundantly perform the same or similar computation due to overlapping observations. To address this issue, this study introduces a novel method referred to as locally centralized team transformer (LCTT). LCTT establishes a locally centralized execution framework where selected agents serve as leaders, issuing instructions, while the rest agents, designated as workers, act as these instructions without activating their policy networks. For LCTT, we proposed the team-transformer (T-Trans) architecture that allows leaders to provide specific instructions to each worker, and the leadership shift mechanism that allows agents autonomously decide their roles as leaders or workers. Our experimental results demonstrate that the proposed method effectively reduces redundant computation, does not decrease reward levels, and leads to faster learning convergence.
<div id='section'>Paperid: <span id='pid'>1460, <a href='https://arxiv.org/pdf/2404.11831.pdf' target='_blank'>https://arxiv.org/pdf/2404.11831.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chenxing Liu, Guizhong Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.11831">JointPPO: Diving Deeper into the Effectiveness of PPO in Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While Centralized Training with Decentralized Execution (CTDE) has become the prevailing paradigm in Multi-Agent Reinforcement Learning (MARL), it may not be suitable for scenarios in which agents can fully communicate and share observations with each other. Fully centralized methods, also know as Centralized Training with Centralized Execution (CTCE) methods, can fully utilize observations of all the agents by treating the entire system as a single agent. However, traditional CTCE methods suffer from scalability issues due to the exponential growth of the joint action space. To address these challenges, in this paper we propose JointPPO, a CTCE method that uses Proximal Policy Optimization (PPO) to directly optimize the joint policy of the multi-agent system. JointPPO decomposes the joint policy into conditional probabilities, transforming the decision-making process into a sequence generation task. A Transformer-based joint policy network is constructed, trained with a PPO loss tailored for the joint policy. JointPPO effectively handles a large joint action space and extends PPO to multi-agent setting in a clear and concise manner. Extensive experiments on the StarCraft Multi-Agent Challenge (SMAC) testbed demonstrate the superiority of JointPPO over strong baselines. Ablation experiments and analyses are conducted to explores the factors influencing JointPPO's performance.
<div id='section'>Paperid: <span id='pid'>1461, <a href='https://arxiv.org/pdf/2404.09001.pdf' target='_blank'>https://arxiv.org/pdf/2404.09001.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhihao Cao, Zidong Wang, Siwen Xie, Anji Liu, Lifeng Fan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.09001">Smart Help: Strategic Opponent Modeling for Proactive and Adaptive Robot Assistance in Households</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite the significant demand for assistive technology among vulnerable groups (e.g., the elderly, children, and the disabled) in daily tasks, research into advanced AI-driven assistive solutions that genuinely accommodate their diverse needs remains sparse. Traditional human-machine interaction tasks often require machines to simply help without nuanced consideration of human abilities and feelings, such as their opportunity for practice and learning, sense of self-improvement, and self-esteem. Addressing this gap, we define a pivotal and novel challenge Smart Help, which aims to provide proactive yet adaptive support to human agents with diverse disabilities and dynamic goals in various tasks and environments. To establish this challenge, we leverage AI2-THOR to build a new interactive 3D realistic household environment for the Smart Help task. We introduce an innovative opponent modeling module that provides a nuanced understanding of the main agent's capabilities and goals, in order to optimize the assisting agent's helping policy. Rigorous experiments validate the efficacy of our model components and show the superiority of our holistic approach against established baselines. Our findings illustrate the potential of AI-imbued assistive robots in improving the well-being of vulnerable groups.
<div id='section'>Paperid: <span id='pid'>1462, <a href='https://arxiv.org/pdf/2404.05840.pdf' target='_blank'>https://arxiv.org/pdf/2404.05840.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Andre R Kuroswiski, Annie S Wu, Angelo Passaro
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.05840">Attention-Driven Multi-Agent Reinforcement Learning: Enhancing Decisions with Expertise-Informed Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we introduce an alternative approach to enhancing Multi-Agent Reinforcement Learning (MARL) through the integration of domain knowledge and attention-based policy mechanisms. Our methodology focuses on the incorporation of domain-specific expertise into the learning process, which simplifies the development of collaborative behaviors. This approach aims to reduce the complexity and learning overhead typically associated with MARL by enabling agents to concentrate on essential aspects of complex tasks, thus optimizing the learning curve. The utilization of attention mechanisms plays a key role in our model. It allows for the effective processing of dynamic context data and nuanced agent interactions, leading to more refined decision-making. Applied in standard MARL scenarios, such as the Stanford Intelligent Systems Laboratory (SISL) Pursuit and Multi-Particle Environments (MPE) Simple Spread, our method has been shown to improve both learning efficiency and the effectiveness of collaborative behaviors. The results indicate that our attention-based approach can be a viable approach for improving the efficiency of MARL training process, integrating domain-specific knowledge at the action level.
<div id='section'>Paperid: <span id='pid'>1463, <a href='https://arxiv.org/pdf/2404.01131.pdf' target='_blank'>https://arxiv.org/pdf/2404.01131.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ashish Rana, Michael Oesterle, Jannik Brinkmann
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.01131">GOV-REK: Governed Reward Engineering Kernels for Designing Robust Multi-Agent Reinforcement Learning Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>For multi-agent reinforcement learning systems (MARLS), the problem formulation generally involves investing massive reward engineering effort specific to a given problem. However, this effort often cannot be translated to other problems; worse, it gets wasted when system dynamics change drastically. This problem is further exacerbated in sparse reward scenarios, where a meaningful heuristic can assist in the policy convergence task. We propose GOVerned Reward Engineering Kernels (GOV-REK), which dynamically assign reward distributions to agents in MARLS during its learning stage. We also introduce governance kernels, which exploit the underlying structure in either state or joint action space for assigning meaningful agent reward distributions. During the agent learning stage, it iteratively explores different reward distribution configurations with a Hyperband-like algorithm to learn ideal agent reward models in a problem-agnostic manner. Our experiments demonstrate that our meaningful reward priors robustly jumpstart the learning process for effectively learning different MARL problems.
<div id='section'>Paperid: <span id='pid'>1464, <a href='https://arxiv.org/pdf/2403.08879.pdf' target='_blank'>https://arxiv.org/pdf/2403.08879.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jing Tan, Ramin Khalili, Holger Karl
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.08879">Multi-Objective Optimization Using Adaptive Distributed Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The Intelligent Transportation System (ITS) environment is known to be dynamic and distributed, where participants (vehicle users, operators, etc.) have multiple, changing and possibly conflicting objectives. Although Reinforcement Learning (RL) algorithms are commonly applied to optimize ITS applications such as resource management and offloading, most RL algorithms focus on single objectives. In many situations, converting a multi-objective problem into a single-objective one is impossible, intractable or insufficient, making such RL algorithms inapplicable. We propose a multi-objective, multi-agent reinforcement learning (MARL) algorithm with high learning efficiency and low computational requirements, which automatically triggers adaptive few-shot learning in a dynamic, distributed and noisy environment with sparse and delayed reward. We test our algorithm in an ITS environment with edge cloud computing. Empirical results show that the algorithm is quick to adapt to new environments and performs better in all individual and system metrics compared to the state-of-the-art benchmark. Our algorithm also addresses various practical concerns with its modularized and asynchronous online training method. In addition to the cloud simulation, we test our algorithm on a single-board computer and show that it can make inference in 6 milliseconds.
<div id='section'>Paperid: <span id='pid'>1465, <a href='https://arxiv.org/pdf/2403.07005.pdf' target='_blank'>https://arxiv.org/pdf/2403.07005.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuejing Zheng, Chao Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.07005">Multi-Agent Reinforcement Learning with a Hierarchy of Reward Machines</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we study the cooperative Multi-Agent Reinforcement Learning (MARL) problems using Reward Machines (RMs) to specify the reward functions such that the prior knowledge of high-level events in a task can be leveraged to facilitate the learning efficiency. Unlike the existing work that RMs have been incorporated into MARL for task decomposition and policy learning in relatively simple domains or with an assumption of independencies among the agents, we present Multi-Agent Reinforcement Learning with a Hierarchy of RMs (MAHRM) that is capable of dealing with more complex scenarios when the events among agents can occur concurrently and the agents are highly interdependent.
  MAHRM exploits the relationship of high-level events to decompose a task into a hierarchy of simpler subtasks that are assigned to a small group of agents, so as to reduce the overall computational complexity.
  Experimental results in three cooperative MARL domains show that MAHRM outperforms other MARL methods using the same prior knowledge of high-level events.
<div id='section'>Paperid: <span id='pid'>1466, <a href='https://arxiv.org/pdf/2402.13481.pdf' target='_blank'>https://arxiv.org/pdf/2402.13481.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Liu Weiwei, Hu Wenxuan, Jing Wei, Lei Lanxin, Gao Lingping, Liu Yong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.13481">Learning to Model Diverse Driving Behaviors in Highly Interactive Autonomous Driving Scenarios with Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autonomous vehicles trained through Multi-Agent Reinforcement Learning (MARL) have shown impressive results in many driving scenarios. However, the performance of these trained policies can be impacted when faced with diverse driving styles and personalities, particularly in highly interactive situations. This is because conventional MARL algorithms usually operate under the assumption of fully cooperative behavior among all agents and focus on maximizing team rewards during training. To address this issue, we introduce the Personality Modeling Network (PeMN), which includes a cooperation value function and personality parameters to model the varied interactions in high-interactive scenarios. The PeMN also enables the training of a background traffic flow with diverse behaviors, thereby improving the performance and generalization of the ego vehicle. Our extensive experimental studies, which incorporate different personality parameters in high-interactive driving scenarios, demonstrate that the personality parameters effectively model diverse driving styles and that policies trained with PeMN demonstrate better generalization compared to traditional MARL methods.
<div id='section'>Paperid: <span id='pid'>1467, <a href='https://arxiv.org/pdf/2402.07588.pdf' target='_blank'>https://arxiv.org/pdf/2402.07588.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tinashe Handina, Eric Mazumdar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.07588">Understanding Model Selection For Learning In Strategic Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The deployment of ever-larger machine learning models reflects a growing consensus that the more expressive the model class one optimizes over$\unicode{x2013}$and the more data one has access to$\unicode{x2013}$the more one can improve performance. As models get deployed in a variety of real-world scenarios, they inevitably face strategic environments. In this work, we consider the natural question of how the interplay of models and strategic interactions affects the relationship between performance at equilibrium and the expressivity of model classes. We find that strategic interactions can break the conventional view$\unicode{x2013}$meaning that performance does not necessarily monotonically improve as model classes get larger or more expressive (even with infinite data). We show the implications of this result in several contexts including strategic regression, strategic classification, and multi-agent reinforcement learning. In particular, we show that each of these settings admits a Braess' paradox-like phenomenon in which optimizing over less expressive model classes allows one to achieve strictly better equilibrium outcomes. Motivated by these examples, we then propose a new paradigm for model selection in games wherein an agent seeks to choose amongst different model classes to use as their action set in a game.
<div id='section'>Paperid: <span id='pid'>1468, <a href='https://arxiv.org/pdf/2401.17880.pdf' target='_blank'>https://arxiv.org/pdf/2401.17880.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zikai Feng, Di Wu, Mengxing Huang, Chau Yuen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.17880">Graph Attention-based Reinforcement Learning for Trajectory Design and Resource Assignment in Multi-UAV Assisted Communication</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the multiple unmanned aerial vehicle (UAV)- assisted downlink communication, it is challenging for UAV base stations (UAV BSs) to realize trajectory design and resource assignment in unknown environments. The cooperation and competition between UAV BSs in the communication network leads to a Markov game problem. Multi-agent reinforcement learning is a significant solution for the above decision-making. However, there are still many common issues, such as the instability of the system and low utilization of historical data, that limit its application. In this paper, a novel graph-attention multi-agent trust region (GA-MATR) reinforcement learning framework is proposed to solve the multi-UAV assisted communication problem. Graph recurrent network is introduced to process and analyze complex topology of the communication network, so as to extract useful information and patterns from observational information. The attention mechanism provides additional weighting for conveyed information, so that the critic network can accurately evaluate the value of behavior for UAV BSs. This provides more reliable feedback signals and helps the actor network update the strategy more effectively. Ablation simulations indicate that the proposed approach attains improved convergence over the baselines. UAV BSs learn the optimal communication strategies to achieve their maximum cumulative rewards. Additionally, multi-agent trust region method with monotonic convergence provides an estimated Nash equilibrium for the multi-UAV assisted communication Markov game.
<div id='section'>Paperid: <span id='pid'>1469, <a href='https://arxiv.org/pdf/2401.11378.pdf' target='_blank'>https://arxiv.org/pdf/2401.11378.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zheng Fang, Tianhao Chen, Dong Jiang, Zheng Zhang, Guangliang Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.11378">Multi-Agent Generative Adversarial Interactive Self-Imitation Learning for AUV Formation Control and Obstacle Avoidance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multiple autonomous underwater vehicles (multi-AUV) can cooperatively accomplish tasks that a single AUV cannot complete. Recently, multi-agent reinforcement learning has been introduced to control of multi-AUV. However, designing efficient reward functions for various tasks of multi-AUV control is difficult or even impractical. Multi-agent generative adversarial imitation learning (MAGAIL) allows multi-AUV to learn from expert demonstration instead of pre-defined reward functions, but suffers from the deficiency of requiring optimal demonstrations and not surpassing provided expert demonstrations. This paper builds upon the MAGAIL algorithm by proposing multi-agent generative adversarial interactive self-imitation learning (MAGAISIL), which can facilitate AUVs to learn policies by gradually replacing the provided sub-optimal demonstrations with self-generated good trajectories selected by a human trainer. Our experimental results in a multi-AUV formation control and obstacle avoidance task on the Gazebo platform with AUV simulator of our lab show that AUVs trained via MAGAISIL can surpass the provided sub-optimal expert demonstrations and reach a performance close to or even better than MAGAIL with optimal demonstrations. Further results indicate that AUVs' policies trained via MAGAISIL can adapt to complex and different tasks as well as MAGAIL learning from optimal demonstrations.
<div id='section'>Paperid: <span id='pid'>1470, <a href='https://arxiv.org/pdf/2401.06168.pdf' target='_blank'>https://arxiv.org/pdf/2401.06168.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Prathamesh Sonawane, Arav Chheda
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.06168">A Survey on Game Theory Optimal Poker</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Poker is in the family of imperfect information games unlike other games such as chess, connect four, etc which are perfect information game instead. While many perfect information games have been solved, no non-trivial imperfect information game has been solved to date. This makes poker a great test bed for Artificial Intelligence research. In this paper we firstly compare Game theory optimal poker to Exploitative poker. Secondly, we discuss the intricacies of abstraction techniques, betting models, and specific strategies employed by successful poker bots like Tartanian[1] and Pluribus[6]. Thirdly, we also explore 2-player vs multi-player games and the limitations that come when playing with more players. Finally, this paper discusses the role of machine learning and theoretical approaches in developing winning strategies and suggests future directions for this rapidly evolving field.
<div id='section'>Paperid: <span id='pid'>1471, <a href='https://arxiv.org/pdf/2312.05746.pdf' target='_blank'>https://arxiv.org/pdf/2312.05746.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiming Zhang, Dongning Guo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.05746">Multi-Agent Reinforcement Learning for Multi-Cell Spectrum and Power Allocation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces a novel approach to radio resource allocation in multi-cell wireless networks using a fully scalable multi-agent reinforcement learning (MARL) framework. A distributed method is developed where agents control individual cells and determine spectrum and power allocation based on limited local information, yet achieve quality of service (QoS) performance comparable to centralized methods using global information. The objective is to minimize packet delays across devices under stochastic arrivals and applies to both conflict graph abstractions and cellular network configurations. This is formulated as a distributed learning problem, implementing a multi-agent proximal policy optimization (MAPPO) algorithm with recurrent neural networks and queueing dynamics. This traffic-driven MARL-based solution enables decentralized training and execution, ensuring scalability to large networks. Extensive simulations demonstrate that the proposed methods achieve comparable QoS performance to genie-aided centralized algorithms with significantly less execution time. The trained policies also exhibit scalability and robustness across various network sizes and traffic conditions.
<div id='section'>Paperid: <span id='pid'>1472, <a href='https://arxiv.org/pdf/2311.17190.pdf' target='_blank'>https://arxiv.org/pdf/2311.17190.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Daniel Bairamian, Philippe Marcotte, Joshua Romoff, Gabriel Robert, Derek Nowrouzezahrai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.17190">Minimax Exploiter: A Data Efficient Approach for Competitive Self-Play</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in Competitive Self-Play (CSP) have achieved, or even surpassed, human level performance in complex game environments such as Dota 2 and StarCraft II using Distributed Multi-Agent Reinforcement Learning (MARL). One core component of these methods relies on creating a pool of learning agents -- consisting of the Main Agent, past versions of this agent, and Exploiter Agents -- where Exploiter Agents learn counter-strategies to the Main Agents. A key drawback of these approaches is the large computational cost and physical time that is required to train the system, making them impractical to deploy in highly iterative real-life settings such as video game productions. In this paper, we propose the Minimax Exploiter, a game theoretic approach to exploiting Main Agents that leverages knowledge of its opponents, leading to significant increases in data efficiency. We validate our approach in a diversity of settings, including simple turn based games, the arcade learning environment, and For Honor, a modern video game. The Minimax Exploiter consistently outperforms strong baselines, demonstrating improved stability and data efficiency, leading to a robust CSP-MARL method that is both flexible and easy to deploy.
<div id='section'>Paperid: <span id='pid'>1473, <a href='https://arxiv.org/pdf/2311.12839.pdf' target='_blank'>https://arxiv.org/pdf/2311.12839.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Amjad Yousef Majid, Eduard Marin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.12839">A Review of Deep Reinforcement Learning in Serverless Computing: Function Scheduling and Resource Auto-Scaling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the rapidly evolving field of serverless computing, efficient function scheduling and resource scaling are critical for optimizing performance and cost. This paper presents a comprehensive review of the application of Deep Reinforcement Learning (DRL) techniques in these areas. We begin by providing an overview of serverless computing, highlighting its benefits and challenges, with a particular focus on function scheduling and resource scaling. We then delve into the principles of deep reinforcement learning (DRL) and its potential for addressing these challenges. A systematic review of recent studies applying DRL to serverless computing is presented, covering various algorithms, models, and performances. Our analysis reveals that DRL, with its ability to learn and adapt from an environment, shows promising results in improving the efficiency of function scheduling and resource scaling in serverless computing. However, several challenges remain, including the need for more realistic simulation environments, handling of cold starts, and the trade-off between learning time and scheduling performance. We conclude by discussing potential future directions for this research area, emphasizing the need for more robust DRL models, better benchmarking methods, and the exploration of multi-agent reinforcement learning for more complex serverless architectures. This review serves as a valuable resource for researchers and practitioners aiming to understand and advance the application of DRL in serverless computing.
<div id='section'>Paperid: <span id='pid'>1474, <a href='https://arxiv.org/pdf/2311.00855.pdf' target='_blank'>https://arxiv.org/pdf/2311.00855.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dinesh Sharma, Ankit Shah, Chaitra Gopalappa
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.00855">A Multi-Agent Reinforcement Learning Framework for Evaluating the U.S. Ending the HIV Epidemic Plan</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human immunodeficiency virus (HIV) is a major public health concern in the United States, with about 1.2 million people living with HIV and 35,000 newly infected each year. There are considerable geographical disparities in HIV burden and care access across the U.S. The 2019 Ending the HIV Epidemic (EHE) initiative aims to reduce new infections by 90% by 2030, by improving coverage of diagnoses, treatment, and prevention interventions and prioritizing jurisdictions with high HIV prevalence. Identifying optimal scale-up of intervention combinations will help inform resource allocation. Existing HIV decision analytic models either evaluate specific cities or the overall national population, thus overlooking jurisdictional interactions or differences. In this paper, we propose a multi-agent reinforcement learning (MARL) model, that enables jurisdiction-specific decision analyses but in an environment with cross-jurisdictional epidemiological interactions. In experimental analyses, conducted on jurisdictions within California and Florida, optimal policies from MARL were significantly different than those generated from single-agent RL, highlighting the influence of jurisdictional variations and interactions. By using comprehensive modeling of HIV and formulations of state space, action space, and reward functions, this work helps demonstrate the strengths and applicability of MARL for informing public health policies, and provides a framework for expanding to the national-level to inform the EHE.
<div id='section'>Paperid: <span id='pid'>1475, <a href='https://arxiv.org/pdf/2310.14685.pdf' target='_blank'>https://arxiv.org/pdf/2310.14685.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Anna M. Maddux, Maryam Kamgarpour
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.14685">Multi-Agent Learning in Contextual Games under Unknown Constraints</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We consider the problem of learning to play a repeated contextual game with unknown reward and unknown constraints functions. Such games arise in applications where each agent's action needs to belong to a feasible set, but the feasible set is a priori unknown. For example, in constrained multi-agent reinforcement learning, the constraints on the agents' policies are a function of the unknown dynamics and hence, are themselves unknown. Under kernel-based regularity assumptions on the unknown functions, we develop a no-regret, no-violation approach which exploits similarities among different reward and constraint outcomes. The no-violation property ensures that the time-averaged sum of constraint violations converges to zero as the game is repeated. We show that our algorithm, referred to as c.z.AdaNormalGP, obtains kernel-dependent regret bounds and that the cumulative constraint violations have sublinear kernel-dependent upper bounds. In addition we introduce the notion of constrained contextual coarse correlated equilibria (c.z.CCE) and show that $Îµ$-c.z.CCEs can be approached whenever players' follow a no-regret no-violation strategy. Finally, we experimentally demonstrate the effectiveness of c.z.AdaNormalGP on an instance of multi-agent reinforcement learning.
<div id='section'>Paperid: <span id='pid'>1476, <a href='https://arxiv.org/pdf/2310.12290.pdf' target='_blank'>https://arxiv.org/pdf/2310.12290.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Baofu Fang, Caiming Zheng, Hao Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.12290">Fact-based Agent modeling for Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In multi-agent systems, agents need to interact and collaborate with other agents in environments. Agent modeling is crucial to facilitate agent interactions and make adaptive cooperation strategies. However, it is challenging for agents to model the beliefs, behaviors, and intentions of other agents in non-stationary environment where all agent policies are learned simultaneously. In addition, the existing methods realize agent modeling through behavior cloning which assume that the local information of other agents can be accessed during execution or training. However, this assumption is infeasible in unknown scenarios characterized by unknown agents, such as competition teams, unreliable communication and federated learning due to privacy concerns. To eliminate this assumption and achieve agent modeling in unknown scenarios, Fact-based Agent modeling (FAM) method is proposed in which fact-based belief inference (FBI) network models other agents in partially observable environment only based on its local information. The reward and observation obtained by agents after taking actions are called facts, and FAM uses facts as reconstruction target to learn the policy representation of other agents through a variational autoencoder. We evaluate FAM on various Multiagent Particle Environment (MPE) and compare the results with several state-of-the-art MARL algorithms. Experimental results show that compared with baseline methods, FAM can effectively improve the efficiency of agent policy learning by making adaptive cooperation strategies in multi-agent reinforcement learning tasks, while achieving higher returns in complex competitive-cooperative mixed scenarios.
<div id='section'>Paperid: <span id='pid'>1477, <a href='https://arxiv.org/pdf/2310.05430.pdf' target='_blank'>https://arxiv.org/pdf/2310.05430.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haider Kamal, Muaz A. Niazi, Hammad Afzal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.05430">Replication of Multi-agent Reinforcement Learning for the "Hide and Seek" Problem</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reinforcement learning generates policies based on reward functions and hyperparameters. Slight changes in these can significantly affect results. The lack of documentation and reproducibility in Reinforcement learning research makes it difficult to replicate once-deduced strategies. While previous research has identified strategies using grounded maneuvers, there is limited work in more complex environments. The agents in this study are simulated similarly to Open Al's hider and seek agents, in addition to a flying mechanism, enhancing their mobility, and expanding their range of possible actions and strategies. This added functionality improves the Hider agents to develop a chasing strategy from approximately 2 million steps to 1.6 million steps and hiders
<div id='section'>Paperid: <span id='pid'>1478, <a href='https://arxiv.org/pdf/2310.04623.pdf' target='_blank'>https://arxiv.org/pdf/2310.04623.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Atsushi Ueshima, Shayegan Omidshafiei, Hirokazu Shirado
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.04623">Deconstructing Cooperation and Ostracism via Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cooperation is challenging in biological systems, human societies, and multi-agent systems in general. While a group can benefit when everyone cooperates, it is tempting for each agent to act selfishly instead. Prior human studies show that people can overcome such social dilemmas while choosing interaction partners, i.e., strategic network rewiring. However, little is known about how agents, including humans, can learn about cooperation from strategic rewiring and vice versa. Here, we perform multi-agent reinforcement learning simulations in which two agents play the Prisoner's Dilemma game iteratively. Each agent has two policies: one controls whether to cooperate or defect; the other controls whether to rewire connections with another agent. This setting enables us to disentangle complex causal dynamics between cooperation and network rewiring. We find that network rewiring facilitates mutual cooperation even when one agent always offers cooperation, which is vulnerable to free-riding. We then confirm that the network-rewiring effect is exerted through agents' learning of ostracism, that is, connecting to cooperators and disconnecting from defectors. However, we also find that ostracism alone is not sufficient to make cooperation emerge. Instead, ostracism emerges from the learning of cooperation, and existing cooperation is subsequently reinforced due to the presence of ostracism. Our findings provide insights into the conditions and mechanisms necessary for the emergence of cooperation with network rewiring.
<div id='section'>Paperid: <span id='pid'>1479, <a href='https://arxiv.org/pdf/2309.16263.pdf' target='_blank'>https://arxiv.org/pdf/2309.16263.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Vaigarai Sathi, Sabahat Shaik, Jaswanth Nidamanuri
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.16263">Cooperation Dynamics in Multi-Agent Systems: Exploring Game-Theoretic Scenarios with Mean-Field Equilibria</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cooperation is fundamental in Multi-Agent Systems (MAS) and Multi-Agent Reinforcement Learning (MARL), often requiring agents to balance individual gains with collective rewards. In this regard, this paper aims to investigate strategies to invoke cooperation in game-theoretic scenarios, namely the Iterated Prisoner's Dilemma, where agents must optimize both individual and group outcomes. Existing cooperative strategies are analyzed for their effectiveness in promoting group-oriented behavior in repeated games. Modifications are proposed where encouraging group rewards will also result in a higher individual gain, addressing real-world dilemmas seen in distributed systems. The study extends to scenarios with exponentially growing agent populations ($N \longrightarrow +\infty$), where traditional computation and equilibrium determination are challenging. Leveraging mean-field game theory, equilibrium solutions and reward structures are established for infinitely large agent sets in repeated games. Finally, practical insights are offered through simulations using the Multi Agent-Posthumous Credit Assignment trainer, and the paper explores adapting simulation algorithms to create scenarios favoring cooperation for group rewards. These practical implementations bridge theoretical concepts with real-world applications.
<div id='section'>Paperid: <span id='pid'>1480, <a href='https://arxiv.org/pdf/2309.08477.pdf' target='_blank'>https://arxiv.org/pdf/2309.08477.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hadar Szostak, Kobi Cohen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.08477">Deep Multi-Agent Reinforcement Learning for Decentralized Active Hypothesis Testing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We consider a decentralized formulation of the active hypothesis testing (AHT) problem, where multiple agents gather noisy observations from the environment with the purpose of identifying the correct hypothesis. At each time step, agents have the option to select a sampling action. These different actions result in observations drawn from various distributions, each associated with a specific hypothesis. The agents collaborate to accomplish the task, where message exchanges between agents are allowed over a rate-limited communications channel. The objective is to devise a multi-agent policy that minimizes the Bayes risk. This risk comprises both the cost of sampling and the joint terminal cost incurred by the agents upon making a hypothesis declaration. Deriving optimal structured policies for AHT problems is generally mathematically intractable, even in the context of a single agent. As a result, recent efforts have turned to deep learning methodologies to address these problems, which have exhibited significant success in single-agent learning scenarios. In this paper, we tackle the multi-agent AHT formulation by introducing a novel algorithm rooted in the framework of deep multi-agent reinforcement learning. This algorithm, named Multi-Agent Reinforcement Learning for AHT (MARLA), operates at each time step by having each agent map its state to an action (sampling rule or stopping rule) using a trained deep neural network with the goal of minimizing the Bayes risk. We present a comprehensive set of experimental results that effectively showcase the agents' ability to learn collaborative strategies and enhance performance using MARLA. Furthermore, we demonstrate the superiority of MARLA over single-agent learning approaches. Finally, we provide an open-source implementation of the MARLA framework, for the benefit of researchers and developers in related domains.
<div id='section'>Paperid: <span id='pid'>1481, <a href='https://arxiv.org/pdf/2309.06940.pdf' target='_blank'>https://arxiv.org/pdf/2309.06940.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Daniel Bayer, Marco Pruckner
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.06940">Enhancing the Performance of Multi-Agent Reinforcement Learning for Controlling HVAC Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Systems for heating, ventilation and air-conditioning (HVAC) of buildings are traditionally controlled by a rule-based approach. In order to reduce the energy consumption and the environmental impact of HVAC systems more advanced control methods such as reinforcement learning are promising. Reinforcement learning (RL) strategies offer a good alternative, as user feedback can be integrated more easily and presence can also be incorporated. Moreover, multi-agent RL approaches scale well and can be generalized. In this paper, we propose a multi-agent RL framework based on existing work that learns reducing on one hand energy consumption by optimizing HVAC control and on the other hand user feedback by occupants about uncomfortable room temperatures. Second, we show how to reduce training time required for proper RL-agent-training by using parameter sharing between the multiple agents and apply different pretraining techniques. Results show that our framework is capable of reducing the energy by around 6% when controlling a complete building or 8% for a single room zone. The occupants complaints are acceptable or even better compared to a rule-based baseline. Additionally, our performance analysis show that the training time can be drastically reduced by using parameter sharing.
<div id='section'>Paperid: <span id='pid'>1482, <a href='https://arxiv.org/pdf/2308.15394.pdf' target='_blank'>https://arxiv.org/pdf/2308.15394.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zheng Xiong, Biao Luo, Bing-Chuan Wang, Xiaodong Xu, Xiaodong Liu, Tingwen Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.15394">Decentralized Multi-agent Reinforcement Learning based State-of-Charge Balancing Strategy for Distributed Energy Storage System</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper develops a Decentralized Multi-Agent Reinforcement Learning (Dec-MARL) method to solve the SoC balancing problem in the distributed energy storage system (DESS). First, the SoC balancing problem is formulated into a finite Markov decision process with action constraints derived from demand balance, which can be solved by Dec-MARL. Specifically, the first-order average consensus algorithm is utilized to expand the observations of the DESS state in a fully-decentralized way, and the initial actions (i.e., output power) are decided by the agents (i.e., energy storage units) according to these observations. In order to get the final actions in the allowable range, a counterfactual demand balance algorithm is proposed to balance the total demand and the initial actions. Next, the agents execute the final actions and get local rewards from the environment, and the DESS steps into the next state. Finally, through the first-order average consensus algorithm, the agents get the average reward and the expended observation of the next state for later training. By the above procedure, Dec-MARL reveals outstanding performance in a fully-decentralized system without any expert experience or constructing any complicated model. Besides, it is flexible and can be extended to other decentralized multi-agent systems straightforwardly. Extensive simulations have validated the effectiveness and efficiency of Dec-MARL.
<div id='section'>Paperid: <span id='pid'>1483, <a href='https://arxiv.org/pdf/2308.14308.pdf' target='_blank'>https://arxiv.org/pdf/2308.14308.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mingxi Tan, Andong Tian, Ludovic Denoyer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.14308">Policy Diversity for Cooperative Agents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Standard cooperative multi-agent reinforcement learning (MARL) methods aim to find the optimal team cooperative policy to complete a task. However there may exist multiple different ways of cooperating, which usually are very needed by domain experts. Therefore, identifying a set of significantly different policies can alleviate the task complexity for them. Unfortunately, there is a general lack of effective policy diversity approaches specifically designed for the multi-agent domain. In this work, we propose a method called Moment-Matching Policy Diversity to alleviate this problem. This method can generate different team policies to varying degrees by formalizing the difference between team policies as the difference in actions of selected agents in different policies. Theoretically, we show that our method is a simple way to implement a constrained optimization problem that regularizes the difference between two trajectory distributions by using the maximum mean discrepancy. The effectiveness of our approach is demonstrated on a challenging team-based shooter.
<div id='section'>Paperid: <span id='pid'>1484, <a href='https://arxiv.org/pdf/2308.10721.pdf' target='_blank'>https://arxiv.org/pdf/2308.10721.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Giovanni Minelli, Mirco Musolesi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.10721">CoMIX: A Multi-agent Reinforcement Learning Training Architecture for Efficient Decentralized Coordination and Independent Decision-Making</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robust coordination skills enable agents to operate cohesively in shared environments, together towards a common goal and, ideally, individually without hindering each other's progress. To this end, this paper presents Coordinated QMIX (CoMIX), a novel training framework for decentralized agents that enables emergent coordination through flexible policies, allowing at the same time independent decision-making at individual level. CoMIX models selfish and collaborative behavior as incremental steps in each agent's decision process. This allows agents to dynamically adapt their behavior to different situations balancing independence and collaboration. Experiments using a variety of simulation environments demonstrate that CoMIX outperforms baselines on collaborative tasks. The results validate our incremental approach as effective technique for improving coordination in multi-agent systems.
<div id='section'>Paperid: <span id='pid'>1485, <a href='https://arxiv.org/pdf/2308.06741.pdf' target='_blank'>https://arxiv.org/pdf/2308.06741.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mohammad Mehdi Nasiri, Mansoor Rezghi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.06741">Heterogeneous Multi-Agent Reinforcement Learning via Mirror Descent Policy Optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents an extension of the Mirror Descent method to overcome challenges in cooperative Multi-Agent Reinforcement Learning (MARL) settings, where agents have varying abilities and individual policies. The proposed Heterogeneous-Agent Mirror Descent Policy Optimization (HAMDPO) algorithm utilizes the multi-agent advantage decomposition lemma to enable efficient policy updates for each agent while ensuring overall performance improvements. By iteratively updating agent policies through an approximate solution of the trust-region problem, HAMDPO guarantees stability and improves performance. Moreover, the HAMDPO algorithm is capable of handling both continuous and discrete action spaces for heterogeneous agents in various MARL problems. We evaluate HAMDPO on Multi-Agent MuJoCo and StarCraftII tasks, demonstrating its superiority over state-of-the-art algorithms such as HATRPO and HAPPO. These results suggest that HAMDPO is a promising approach for solving cooperative MARL problems and could potentially be extended to address other challenging problems in the field of MARL.
<div id='section'>Paperid: <span id='pid'>1486, <a href='https://arxiv.org/pdf/2308.06036.pdf' target='_blank'>https://arxiv.org/pdf/2308.06036.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hiroshi Yoshitake, Pieter Abbeel
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.06036">The Impact of Overall Optimization on Warehouse Automation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this study, we propose a novel approach for investigating optimization performance by flexible robot coordination in automated warehouses with multi-agent reinforcement learning (MARL)-based control. Automated systems using robots are expected to achieve efficient operations compared with manual systems in terms of overall optimization performance. However, the impact of overall optimization on performance remains unclear in most automated systems due to a lack of suitable control methods. Thus, we proposed a centralized training-and-decentralized execution MARL framework as a practical overall optimization control method. In the proposed framework, we also proposed a single shared critic, trained with global states and rewards, applicable to a case in which heterogeneous agents make decisions asynchronously. Our proposed MARL framework was applied to the task selection of material handling equipment through automated order picking simulation, and its performance was evaluated to determine how far overall optimization outperforms partial optimization by comparing it with other MARL frameworks and rule-based control methods.
<div id='section'>Paperid: <span id='pid'>1487, <a href='https://arxiv.org/pdf/2308.01649.pdf' target='_blank'>https://arxiv.org/pdf/2308.01649.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>RÃ©mi Leluc, Elie Kadoche, Antoine Bertoncello, SÃ©bastien GourvÃ©nec
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.01649">MARLIM: Multi-Agent Reinforcement Learning for Inventory Management</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Maintaining a balance between the supply and demand of products by optimizing replenishment decisions is one of the most important challenges in the supply chain industry. This paper presents a novel reinforcement learning framework called MARLIM, to address the inventory management problem for a single-echelon multi-products supply chain with stochastic demands and lead-times. Within this context, controllers are developed through single or multiple agents in a cooperative setting. Numerical experiments on real data demonstrate the benefits of reinforcement learning methods over traditional baselines.
<div id='section'>Paperid: <span id='pid'>1488, <a href='https://arxiv.org/pdf/2307.09051.pdf' target='_blank'>https://arxiv.org/pdf/2307.09051.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiufeng Huang, Sheng Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.09051">QMNet: Importance-Aware Message Exchange for Decentralized Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To improve the performance of multi-agent reinforcement learning under the constraint of wireless resources, we propose a message importance metric and design an importance-aware scheduling policy to effectively exchange messages. The key insight is spending the precious communication resources on important messages. The message importance depends not only on the messages themselves, but also on the needs of agents who receive them. Accordingly, we propose a query-message-based architecture, called QMNet. Agents generate queries and messages with the environment observation. Sharing queries can help calculate message importance. Exchanging messages can help agents cooperate better. Besides, we exploit the message importance to deal with random access collisions in decentralized systems. Furthermore, a message prediction mechanism is proposed to compensate for messages that are not transmitted. Finally, we evaluate the proposed schemes in a traffic junction environment, where only a fraction of agents can send messages due to limited wireless resources. Results show that QMNet can extract valuable information to guarantee the system performance even when only $30\%$ of agents can share messages. By exploiting message prediction, the system can further save $40\%$ of wireless resources. The importance-aware decentralized multi-access mechanism can effectively avoid collisions, achieving almost the same performance as centralized scheduling.
<div id='section'>Paperid: <span id='pid'>1489, <a href='https://arxiv.org/pdf/2306.12926.pdf' target='_blank'>https://arxiv.org/pdf/2306.12926.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Joshua Bloom, Pranjal Paliwal, Apratim Mukherjee, Carlo Pinciroli
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.12926">Decentralized Multi-Agent Reinforcement Learning with Global State Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep reinforcement learning (DRL) has seen remarkable success in the control of single robots. However, applying DRL to robot swarms presents significant challenges. A critical challenge is non-stationarity, which occurs when two or more robots update individual or shared policies concurrently, thereby engaging in an interdependent training process with no guarantees of convergence. Circumventing non-stationarity typically involves training the robots with global information about other agents' states and/or actions. In contrast, in this paper we explore how to remove the need for global information. We pose our problem as a Partially Observable Markov Decision Process, due to the absence of global knowledge on other agents. Using collective transport as a testbed scenario, we study two approaches to multi-agent training. In the first, the robots exchange no messages, and are trained to rely on implicit communication through push-and-pull on the object to transport. In the second approach, we introduce Global State Prediction (GSP), a network trained to forma a belief over the swarm as a whole and predict its future states. We provide a comprehensive study over four well-known deep reinforcement learning algorithms in environments with obstacles, measuring performance as the successful transport of the object to the goal within a desired time-frame. Through an ablation study, we show that including GSP boosts performance and increases robustness when compared with methods that use global knowledge.
<div id='section'>Paperid: <span id='pid'>1490, <a href='https://arxiv.org/pdf/2306.11551.pdf' target='_blank'>https://arxiv.org/pdf/2306.11551.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pascal Leroy, Pablo G. Morato, Jonathan Pisane, Athanasios Kolios, Damien Ernst
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.11551">IMP-MARL: a Suite of Environments for Large-scale Infrastructure Management Planning via MARL</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce IMP-MARL, an open-source suite of multi-agent reinforcement learning (MARL) environments for large-scale Infrastructure Management Planning (IMP), offering a platform for benchmarking the scalability of cooperative MARL methods in real-world engineering applications. In IMP, a multi-component engineering system is subject to a risk of failure due to its components' damage condition. Specifically, each agent plans inspections and repairs for a specific system component, aiming to minimise maintenance costs while cooperating to minimise system failure risk. With IMP-MARL, we release several environments including one related to offshore wind structural systems, in an effort to meet today's needs to improve management strategies to support sustainable and reliable energy systems. Supported by IMP practical engineering environments featuring up to 100 agents, we conduct a benchmark campaign, where the scalability and performance of state-of-the-art cooperative MARL methods are compared against expert-based heuristic policies. The results reveal that centralised training with decentralised execution methods scale better with the number of agents than fully centralised or decentralised RL approaches, while also outperforming expert-based heuristic policies in most IMP environments. Based on our findings, we additionally outline remaining cooperation and scalability challenges that future MARL methods should still address. Through IMP-MARL, we encourage the implementation of new environments and the further development of MARL methods.
<div id='section'>Paperid: <span id='pid'>1491, <a href='https://arxiv.org/pdf/2306.08419.pdf' target='_blank'>https://arxiv.org/pdf/2306.08419.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dmitry Ivanov, Ilya Zisman, Kirill Chernyshev
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.08419">Mediated Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The majority of Multi-Agent Reinforcement Learning (MARL) literature equates the cooperation of self-interested agents in mixed environments to the problem of social welfare maximization, allowing agents to arbitrarily share rewards and private information. This results in agents that forgo their individual goals in favour of social good, which can potentially be exploited by selfish defectors. We argue that cooperation also requires agents' identities and boundaries to be respected by making sure that the emergent behaviour is an equilibrium, i.e., a convention that no agent can deviate from and receive higher individual payoffs. Inspired by advances in mechanism design, we propose to solve the problem of cooperation, defined as finding socially beneficial equilibrium, by using mediators. A mediator is a benevolent entity that may act on behalf of agents, but only for the agents that agree to it. We show how a mediator can be trained alongside agents with policy gradient to maximize social welfare subject to constraints that encourage agents to cooperate through the mediator. Our experiments in matrix and iterative games highlight the potential power of applying mediators in MARL.
<div id='section'>Paperid: <span id='pid'>1492, <a href='https://arxiv.org/pdf/2306.03830.pdf' target='_blank'>https://arxiv.org/pdf/2306.03830.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>John Isak Fjellvang Villanger, Troels Arnfred Bojesen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.03830">Inductive Bias for Emergent Communication in a Continuous Setting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We study emergent communication in a multi-agent reinforcement learning setting, where the agents solve cooperative tasks and have access to a communication channel. The communication channel may consist of either discrete symbols or continuous variables. We introduce an inductive bias to aid with the emergence of good communication protocols for continuous messages, and we look at the effect this type of inductive bias has for continuous and discrete messages in itself or when used in combination with reinforcement learning. We demonstrate that this type of inductive bias has a beneficial effect on the communication protocols learnt in two toy environments, Negotiation and Sequence Guess.
<div id='section'>Paperid: <span id='pid'>1493, <a href='https://arxiv.org/pdf/2306.01925.pdf' target='_blank'>https://arxiv.org/pdf/2306.01925.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianyu Shi, Francois-Xavier Devailly, Denis Larocque, Laurent Charlin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.01925">Improving the generalizability and robustness of large-scale traffic signal control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A number of deep reinforcement-learning (RL) approaches propose to control traffic signals. In this work, we study the robustness of such methods along two axes. First, sensor failures and GPS occlusions create missing-data challenges and we show that recent methods remain brittle in the face of these missing data. Second, we provide a more systematic study of the generalization ability of RL methods to new networks with different traffic regimes. Again, we identify the limitations of recent approaches. We then propose using a combination of distributional and vanilla reinforcement learning through a policy ensemble. Building upon the state-of-the-art previous model which uses a decentralized approach for large-scale traffic signal control with graph convolutional networks (GCNs), we first learn models using a distributional reinforcement learning (DisRL) approach. In particular, we use implicit quantile networks (IQN) to model the state-action return distribution with quantile regression. For traffic signal control problems, an ensemble of standard RL and DisRL yields superior performance across different scenarios, including different levels of missing sensor data and traffic flow patterns. Furthermore, the learning scheme of the resulting model can improve zero-shot transferability to different road network structures, including both synthetic networks and real-world networks (e.g., Luxembourg, Manhattan). We conduct extensive experiments to compare our approach to multi-agent reinforcement learning and traditional transportation approaches. Results show that the proposed method improves robustness and generalizability in the face of missing data, varying road networks, and traffic flows.
<div id='section'>Paperid: <span id='pid'>1494, <a href='https://arxiv.org/pdf/2305.17198.pdf' target='_blank'>https://arxiv.org/pdf/2305.17198.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Paul Barde, Jakob Foerster, Derek Nowrouzezahrai, Amy Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.17198">A Model-Based Solution to the Offline Multi-Agent Reinforcement Learning Coordination Problem</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Training multiple agents to coordinate is an essential problem with applications in robotics, game theory, economics, and social sciences. However, most existing Multi-Agent Reinforcement Learning (MARL) methods are online and thus impractical for real-world applications in which collecting new interactions is costly or dangerous. While these algorithms should leverage offline data when available, doing so gives rise to what we call the offline coordination problem. Specifically, we identify and formalize the strategy agreement (SA) and the strategy fine-tuning (SFT) coordination challenges, two issues at which current offline MARL algorithms fail. Concretely, we reveal that the prevalent model-free methods are severely deficient and cannot handle coordination-intensive offline multi-agent tasks in either toy or MuJoCo domains. To address this setback, we emphasize the importance of inter-agent interactions and propose the very first model-based offline MARL method. Our resulting algorithm, Model-based Offline Multi-Agent Proximal Policy Optimization (MOMA-PPO) generates synthetic interaction data and enables agents to converge on a strategy while fine-tuning their policies accordingly. This simple model-based solution solves the coordination-intensive offline tasks, significantly outperforming the prevalent model-free methods even under severe partial observability and with learned world models.
<div id='section'>Paperid: <span id='pid'>1495, <a href='https://arxiv.org/pdf/2305.16170.pdf' target='_blank'>https://arxiv.org/pdf/2305.16170.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shahaf Yamin, Haim Permuter
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.16170">Multi-Agent Reinforcement Learning for Network Routing in Integrated Access Backhaul Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We investigate the problem of wireless routing in integrated access backhaul (IAB) networks consisting of fiber-connected and wireless base stations and multiple users. The physical constraints of these networks prevent the use of a central controller, and base stations have limited access to real-time network conditions. We aim to maximize packet arrival ratio while minimizing their latency, for this purpose, we formulate the problem as a multi-agent partially observed Markov decision process (POMDP). To solve this problem, we develop a Relational Advantage Actor Critic (Relational A2C) algorithm that uses Multi-Agent Reinforcement Learning (MARL) and information about similar destinations to derive a joint routing policy on a distributed basis. We present three training paradigms for this algorithm and demonstrate its ability to achieve near-centralized performance. Our results show that Relational A2C outperforms other reinforcement learning algorithms, leading to increased network efficiency and reduced selfish agent behavior. To the best of our knowledge, this work is the first to optimize routing strategy for IAB networks.
<div id='section'>Paperid: <span id='pid'>1496, <a href='https://arxiv.org/pdf/2305.03735.pdf' target='_blank'>https://arxiv.org/pdf/2305.03735.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Boling Yang, Liyuan Zheng, Lillian J. Ratliff, Byron Boots, Joshua R. Smith
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.03735">Stackelberg Games for Learning Emergent Behaviors During Competitive Autocurricula</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autocurricular training is an important sub-area of multi-agent reinforcement learning~(MARL) that allows multiple agents to learn emergent skills in an unsupervised co-evolving scheme. The robotics community has experimented autocurricular training with physically grounded problems, such as robust control and interactive manipulation tasks. However, the asymmetric nature of these tasks makes the generation of sophisticated policies challenging. Indeed, the asymmetry in the environment may implicitly or explicitly provide an advantage to a subset of agents which could, in turn, lead to a low-quality equilibrium. This paper proposes a novel game-theoretic algorithm, Stackelberg Multi-Agent Deep Deterministic Policy Gradient (ST-MADDPG), which formulates a two-player MARL problem as a Stackelberg game with one player as the `leader' and the other as the `follower' in a hierarchical interaction structure wherein the leader has an advantage. We first demonstrate that the leader's advantage from ST-MADDPG can be used to alleviate the inherent asymmetry in the environment. By exploiting the leader's advantage, ST-MADDPG improves the quality of a co-evolution process and results in more sophisticated and complex strategies that work well even against an unseen strong opponent.
<div id='section'>Paperid: <span id='pid'>1497, <a href='https://arxiv.org/pdf/2304.10375.pdf' target='_blank'>https://arxiv.org/pdf/2304.10375.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yoshinari Motokawa, Toshiharu Sugawara
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.10375">Interpretability for Conditional Coordinated Behavior in Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a model-free reinforcement learning architecture, called distributed attentional actor architecture after conditional attention (DA6-X), to provide better interpretability of conditional coordinated behaviors. The underlying principle involves reusing the saliency vector, which represents the conditional states of the environment, such as the global position of agents. Hence, agents with DA6-X flexibility built into their policy exhibit superior performance by considering the additional information in the conditional states during the decision-making process. The effectiveness of the proposed method was experimentally evaluated by comparing it with conventional methods in an objects collection game. By visualizing the attention weights from DA6-X, we confirmed that agents successfully learn situation-dependent coordinated behaviors by correctly identifying various conditional states, leading to improved interpretability of agents along with superior performance.
<div id='section'>Paperid: <span id='pid'>1498, <a href='https://arxiv.org/pdf/2304.08769.pdf' target='_blank'>https://arxiv.org/pdf/2304.08769.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Madhav Khirwar, Karthik S. Gurumoorthy, Ankit Ajit Jain, Shantala Manchenahally
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.08769">Cooperative Multi-Agent Reinforcement Learning for Inventory Management</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With Reinforcement Learning (RL) for inventory management (IM) being a nascent field of research, approaches tend to be limited to simple, linear environments with implementations that are minor modifications of off-the-shelf RL algorithms. Scaling these simplistic environments to a real-world supply chain comes with a few challenges such as: minimizing the computational requirements of the environment, specifying agent configurations that are representative of dynamics at real world stores and warehouses, and specifying a reward framework that encourages desirable behavior across the whole supply chain. In this work, we present a system with a custom GPU-parallelized environment that consists of one warehouse and multiple stores, a novel architecture for agent-environment dynamics incorporating enhanced state and action spaces, and a shared reward specification that seeks to optimize for a large retailer's supply chain needs. Each vertex in the supply chain graph is an independent agent that, based on its own inventory, able to place replenishment orders to the vertex upstream. The warehouse agent, aside from placing orders from the supplier, has the special property of also being able to constrain replenishment to stores downstream, which results in it learning an additional allocation sub-policy. We achieve a system that outperforms standard inventory control policies such as a base-stock policy and other RL-based specifications for 1 product, and lay out a future direction of work for multiple products.
<div id='section'>Paperid: <span id='pid'>1499, <a href='https://arxiv.org/pdf/2304.08189.pdf' target='_blank'>https://arxiv.org/pdf/2304.08189.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shrudhi R S, Sreyash Mohanty, Susan Elias
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.08189">Control and Coordination of a SWARM of Unmanned Surface Vehicles using Deep Reinforcement Learning in ROS</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>An unmanned surface vehicle (USV) can perform complex missions by continuously observing the state of its surroundings and taking action toward a goal. A SWARM of USVs working together can complete missions faster, and more effectively than a single USV alone. In this paper, we propose an autonomous communication model for a swarm of USVs. The goal of this system is to implement a software system using Robot Operating System (ROS) and Gazebo. With the main objective of coordinated task completion, the Markov decision process (MDP) provides a base to formulate a task decision problem to achieve efficient localization and tracking in a highly dynamic water environment. To coordinate multiple USVs performing real-time target tracking, we propose an enhanced multi-agent reinforcement learning approach. Our proposed scheme uses MA-DDPG, or Multi-Agent Deep Deterministic Policy Gradient, an extension of the Deep Deterministic Policy Gradients (DDPG) algorithm that allows for decentralized control of multiple agents in a cooperative environment. MA-DDPG's decentralised control allows each and every agent to make decisions based on its own observations and objectives, which can lead to superior gross performance and improved stability. Additionally, it provides communication and coordination among agents through the use of collective readings and rewards.
<div id='section'>Paperid: <span id='pid'>1500, <a href='https://arxiv.org/pdf/2304.04751.pdf' target='_blank'>https://arxiv.org/pdf/2304.04751.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Eloghosa Ikponmwoba, Ope Owoyele
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.04751">DeepHive: A multi-agent reinforcement learning approach for automated discovery of swarm-based optimization policies</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present an approach for designing swarm-based optimizers for the global optimization of expensive black-box functions. In the proposed approach, the problem of finding efficient optimizers is framed as a reinforcement learning problem, where the goal is to find optimization policies that require a few function evaluations to converge to the global optimum. The state of each agent within the swarm is defined as its current position and function value within a design space and the agents learn to take favorable actions that maximize reward, which is based on the final value of the objective function. The proposed approach is tested on various benchmark optimization functions and compared to the performance of other global optimization strategies. Furthermore, the effect of changing the number of agents, as well as the generalization capabilities of the trained agents are investigated. The results show superior performance compared to the other optimizers, desired scaling when the number of agents is varied, and acceptable performance even when applied to unseen functions. On a broader scale, the results show promise for the rapid development of domain-specific optimizers.
<div id='section'>Paperid: <span id='pid'>1501, <a href='https://arxiv.org/pdf/2304.04086.pdf' target='_blank'>https://arxiv.org/pdf/2304.04086.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jasper van Tilburg, Luciano C. Siebert, Jochen L. Cremer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.04086">MARL-iDR: Multi-Agent Reinforcement Learning for Incentive-based Residential Demand Response</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a decentralized Multi-Agent Reinforcement Learning (MARL) approach to an incentive-based Demand Response (DR) program, which aims to maintain the capacity limits of the electricity grid and prevent grid congestion by financially incentivizing residential consumers to reduce their energy consumption. The proposed approach addresses the key challenge of coordinating heterogeneous preferences and requirements from multiple participants while preserving their privacy and minimizing financial costs for the aggregator. The participant agents use a novel Disjunctively Constrained Knapsack Problem optimization to curtail or shift the requested household appliances based on the selected demand reduction. Through case studies with electricity data from $25$ households, the proposed approach effectively reduced energy consumption's Peak-to-Average ratio (PAR) by $14.48$% compared to the original PAR while fully preserving participant privacy. This approach has the potential to significantly improve the efficiency and reliability of the electricity grid, making it an important contribution to the management of renewable energy resources and the growing electricity demand.
<div id='section'>Paperid: <span id='pid'>1502, <a href='https://arxiv.org/pdf/2304.00009.pdf' target='_blank'>https://arxiv.org/pdf/2304.00009.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Siddarth Singh, Benjamin Rosman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.00009">The challenge of redundancy on multi-agent value factorisation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the field of cooperative multi-agent reinforcement learning (MARL), the standard paradigm is the use of centralised training and decentralised execution where a central critic conditions the policies of the cooperative agents based on a central state. It has been shown, that in cases with large numbers of redundant agents these methods become less effective. In a more general case, there is likely to be a larger number of agents in an environment than is required to solve the task. These redundant agents reduce performance by enlarging the dimensionality of both the state space and and increasing the size of the joint policy used to solve the environment. We propose leveraging layerwise relevance propagation (LRP) to instead separate the learning of the joint value function and generation of local reward signals and create a new MARL algorithm: relevance decomposition network (RDN). We find that although the performance of both baselines VDN and Qmix degrades with the number of redundant agents, RDN is unaffected.
<div id='section'>Paperid: <span id='pid'>1503, <a href='https://arxiv.org/pdf/2303.08447.pdf' target='_blank'>https://arxiv.org/pdf/2303.08447.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nicolas Cuadrado, Roberto Gutierrez, Yongli Zhu, Martin Takac
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.08447">MAHTM: A Multi-Agent Framework for Hierarchical Transactive Microgrids</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Integrating variable renewable energy into the grid has posed challenges to system operators in achieving optimal trade-offs among energy availability, cost affordability, and pollution controllability. This paper proposes a multi-agent reinforcement learning framework for managing energy transactions in microgrids. The framework addresses the challenges above: it seeks to optimize the usage of available resources by minimizing the carbon footprint while benefiting all stakeholders. The proposed architecture consists of three layers of agents, each pursuing different objectives. The first layer, comprised of prosumers and consumers, minimizes the total energy cost. The other two layers control the energy price to decrease the carbon impact while balancing the consumption and production of both renewable and conventional energy. This framework also takes into account fluctuations in energy demand and supply.
<div id='section'>Paperid: <span id='pid'>1504, <a href='https://arxiv.org/pdf/2303.01799.pdf' target='_blank'>https://arxiv.org/pdf/2303.01799.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Maryam Kouzeghar, Youngbin Song, Malika Meghjani, Roland Bouffanais
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.01799">Multi-Target Pursuit by a Decentralized Heterogeneous UAV Swarm using Deep Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent pursuit-evasion tasks involving intelligent targets are notoriously challenging coordination problems. In this paper, we investigate new ways to learn such coordinated behaviors of unmanned aerial vehicles (UAVs) aimed at keeping track of multiple evasive targets. Within a Multi-Agent Reinforcement Learning (MARL) framework, we specifically propose a variant of the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) method. Our approach addresses multi-target pursuit-evasion scenarios within non-stationary and unknown environments with random obstacles. In addition, given the critical role played by collective exploration in terms of detecting possible targets, we implement heterogeneous roles for the pursuers for enhanced exploratory actions balanced by exploitation (i.e. tracking) of previously identified targets. Our proposed role-based MADDPG algorithm is not only able to track multiple targets, but also is able to explore for possible targets by means of the proposed Voronoi-based rewarding policy. We implemented, tested and validated our approach in a simulation environment prior to deploying a real-world multi-robot system comprising of Crazyflie drones. Our results demonstrate that a multi-agent pursuit team has the ability to learn highly efficient coordinated control policies in terms of target tracking and exploration even when confronted with multiple fast evasive targets in complex environments.
<div id='section'>Paperid: <span id='pid'>1505, <a href='https://arxiv.org/pdf/2303.01170.pdf' target='_blank'>https://arxiv.org/pdf/2303.01170.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alberto Castagna, Ivana Dusparic
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.01170">Expert-Free Online Transfer Learning in Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Transfer learning in Reinforcement Learning (RL) has been widely studied to overcome training issues of Deep-RL, i.e., exploration cost, data availability and convergence time, by introducing a way to enhance training phase with external knowledge. Generally, knowledge is transferred from expert-agents to novices. While this fixes the issue for a novice agent, a good understanding of the task on expert agent is required for such transfer to be effective. As an alternative, in this paper we propose Expert-Free Online Transfer Learning (EF-OnTL), an algorithm that enables expert-free real-time dynamic transfer learning in multi-agent system. No dedicated expert exists, and transfer source agent and knowledge to be transferred are dynamically selected at each transfer step based on agents' performance and uncertainty. To improve uncertainty estimation, we also propose State Action Reward Next-State Random Network Distillation (sars-RND), an extension of RND that estimates uncertainty from RL agent-environment interaction. We demonstrate EF-OnTL effectiveness against a no-transfer scenario and advice-based baselines, with and without expert agents, in three benchmark tasks: Cart-Pole, a grid-based Multi-Team Predator-Prey (mt-pp) and Half Field Offense (HFO). Our results show that EF-OnTL achieve overall comparable performance when compared against advice-based baselines while not requiring any external input nor threshold tuning. EF-OnTL outperforms no-transfer with an improvement related to the complexity of the task addressed.
<div id='section'>Paperid: <span id='pid'>1506, <a href='https://arxiv.org/pdf/2303.00460.pdf' target='_blank'>https://arxiv.org/pdf/2303.00460.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tao Li, Feng Xie, Ya Xiong, Qingchun Feng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.00460">Multi-Arm Robot Task Planning for Fruit Harvesting Using Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The emergence of harvesting robotics offers a promising solution to the issue of limited agricultural labor resources and the increasing demand for fruits. Despite notable advancements in the field of harvesting robotics, the utilization of such technology in orchards is still limited. The key challenge is to improve operational efficiency. Taking into account inner-arm conflicts, couplings of DoFs, and dynamic tasks, we propose a task planning strategy for a harvesting robot with four arms in this paper. The proposed method employs a Markov game framework to formulate the four-arm robotic harvesting task, which avoids the computational complexity of solving an NP-hard scheduling problem. Furthermore, a multi-agent reinforcement learning (MARL) structure with a fully centralized collaboration protocol is used to train a MARL-based task planning network. Several simulations and orchard experiments are conducted to validate the effectiveness of the proposed method for a multi-arm harvesting robot in comparison with the existing method.
<div id='section'>Paperid: <span id='pid'>1507, <a href='https://arxiv.org/pdf/2302.14604.pdf' target='_blank'>https://arxiv.org/pdf/2302.14604.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bengisu Guresti, Abdullah Vanlioglu, Nazim Kemal Ure
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.14604">IQ-Flow: Mechanism Design for Inducing Cooperative Behavior to Self-Interested Agents in Sequential Social Dilemmas</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Achieving and maintaining cooperation between agents to accomplish a common objective is one of the central goals of Multi-Agent Reinforcement Learning (MARL). Nevertheless in many real-world scenarios, separately trained and specialized agents are deployed into a shared environment, or the environment requires multiple objectives to be achieved by different coexisting parties. These variations among specialties and objectives are likely to cause mixed motives that eventually result in a social dilemma where all the parties are at a loss. In order to resolve this issue, we propose the Incentive Q-Flow (IQ-Flow) algorithm, which modifies the system's reward setup with an incentive regulator agent such that the cooperative policy also corresponds to the self-interested policy for the agents. Unlike the existing methods that learn to incentivize self-interested agents, IQ-Flow does not make any assumptions about agents' policies or learning algorithms, which enables the generalization of the developed framework to a wider array of applications. IQ-Flow performs an offline evaluation of the optimality of the learned policies using the data provided by other agents to determine cooperative and self-interested policies. Next, IQ-Flow uses meta-gradient learning to estimate how policy evaluation changes according to given incentives and modifies the incentive such that the greedy policy for cooperative objective and self-interested objective yield the same actions. We present the operational characteristics of IQ-Flow in Iterated Matrix Games. We demonstrate that IQ-Flow outperforms the state-of-the-art incentive design algorithm in Escape Room and 2-Player Cleanup environments. We further demonstrate that the pretrained IQ-Flow mechanism significantly outperforms the performance of the shared reward setup in the 2-Player Cleanup environment.
<div id='section'>Paperid: <span id='pid'>1508, <a href='https://arxiv.org/pdf/2302.12899.pdf' target='_blank'>https://arxiv.org/pdf/2302.12899.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Adriano Mendo, Jose Outes-Carnero, Yak Ng-Molina, Juan Ramiro-Moreno
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.12899">Multi-Agent Reinforcement Learning with Common Policy for Antenna Tilt Optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a method for optimizing wireless networks by adjusting cell parameters that affect both the performance of the cell being optimized and the surrounding cells. The method uses multiple reinforcement learning agents that share a common policy and take into account information from neighboring cells to determine the state and reward. In order to avoid impairing network performance during the initial stages of learning, agents are pre-trained in an earlier phase of offline learning. During this phase, an initial policy is obtained using feedback from a static network simulator and considering a wide variety of scenarios. Finally, agents can intelligently tune the cell parameters of a test network by suggesting small incremental changes, slowly guiding the network toward an optimal configuration. The agents propose optimal changes using the experience gained with the simulator in the pre-training phase, but they can also continue to learn from current network readings after each change. The results show how the proposed approach significantly improves the performance gains already provided by expert system-based methods when applied to remote antenna tilt optimization. The significant gains of this approach have truly been observed when compared with a similar method in which the state and reward do not incorporate information from neighboring cells.
<div id='section'>Paperid: <span id='pid'>1509, <a href='https://arxiv.org/pdf/2302.10825.pdf' target='_blank'>https://arxiv.org/pdf/2302.10825.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiong Li, Pratik Gajane
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.10825">Curiosity-driven Exploration in Sparse-reward Multi-agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Sparsity of rewards while applying a deep reinforcement learning method negatively affects its sample-efficiency. A viable solution to deal with the sparsity of rewards is to learn via intrinsic motivation which advocates for adding an intrinsic reward to the reward function to encourage the agent to explore the environment and expand the sample space. Though intrinsic motivation methods are widely used to improve data-efficient learning in the reinforcement learning model, they also suffer from the so-called detachment problem. In this article, we discuss the limitations of intrinsic curiosity module in sparse-reward multi-agent reinforcement learning and propose a method called I-Go-Explore that combines the intrinsic curiosity module with the Go-Explore framework to alleviate the detachment problem.
<div id='section'>Paperid: <span id='pid'>1510, <a href='https://arxiv.org/pdf/2302.07837.pdf' target='_blank'>https://arxiv.org/pdf/2302.07837.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Muhammad Awais Jadoon, Adriano Pastore, Monica Navarro, Alvaro Valcarce
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.07837">Learning Random Access Schemes for Massive Machine-Type Communication with MARL</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we explore various multi-agent reinforcement learning (MARL) techniques to design grant-free random access (RA) schemes for low-complexity, low-power battery operated devices in massive machine-type communication (mMTC) wireless networks. We use value decomposition networks (VDN) and QMIX algorithms with parameter sharing (PS) with centralized training and decentralized execution (CTDE) while maintaining scalability. We then compare the policies learned by VDN, QMIX, and deep recurrent Q-network (DRQN) and explore the impact of including the agent identifiers in the observation vector. We show that the MARL-based RA schemes can achieve a better throughput-fairness trade-off between agents without having to condition on the agent identifiers. We also present a novel correlated traffic model, which is more descriptive of mMTC scenarios, and show that the proposed algorithm can easily adapt to traffic non-stationarities
<div id='section'>Paperid: <span id='pid'>1511, <a href='https://arxiv.org/pdf/2302.07573.pdf' target='_blank'>https://arxiv.org/pdf/2302.07573.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mohamed Sana, Benoit Miscopein
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.07573">Learning Hierarchical Resource Allocation and Multi-agent Coordination of 5G mobile IAB Nodes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We consider a dynamic millimeter-wave network with integrated access and backhaul, where mobile relay nodes move to auto-reconfigure the wireless backhaul. Specifically, we focus on in-band relaying networks, which conduct access and backhaul links on the same frequency band with severe constraints on co-channel interference. In this context, we jointly study the complex problem of dynamic relay node positioning, user association, and backhaul capacity allocation. To address this problem, with limited complexity, we adopt a hierarchical multi-agent reinforcement with a two-level structure. A high-level policy dynamically coordinates mobile relay nodes, defining the backhaul configuration for a low-level policy, which jointly assigns user equipment to each relay and allocates the backhaul capacity accordingly. The resulting solution automatically adapts the access and backhaul network to changes in the number of users, the traffic distribution, and the variations of the channels. Numerical results show the effectiveness of our proposed solution in terms of convergence of the hierarchical learning procedure. It also provides a significant backhaul capacity and network sum-rate increase (up to 3.5x) compared to baseline approaches.
<div id='section'>Paperid: <span id='pid'>1512, <a href='https://arxiv.org/pdf/2302.07337.pdf' target='_blank'>https://arxiv.org/pdf/2302.07337.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Malintha Fernando, Ransalu Senanayake, Heeyoul Choi, Martin Swany
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.07337">Graph Attention Multi-Agent Fleet Autonomy for Advanced Air Mobility</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autonomous mobility is emerging as a new disruptive mode of urban transportation for moving cargo and passengers. However, designing scalable autonomous fleet coordination schemes to accommodate fast-growing mobility systems is challenging primarily due to the increasing heterogeneity of the fleets, time-varying demand patterns, service area expansions, and communication limitations. We introduce the concept of partially observable advanced air mobility games to coordinate a fleet of aerial vehicles by accounting for the heterogeneity of the interacting agents and the self-interested nature inherent to commercial mobility fleets. To model the complex interactions among the agents and the observation uncertainty in the mobility networks, we propose a novel heterogeneous graph attention encoder-decoder (HetGAT Enc-Dec) neural network-based stochastic policy. We train the policy by leveraging deep multi-agent reinforcement learning, allowing decentralized decision-making for the agents using their local observations. Through extensive experimentation, we show that the learned policy generalizes to various fleet compositions, demand patterns, and observation topologies. Further, fleets operating under the HetGAT Enc-Dec policy outperform other state-of-the-art graph neural network policies by achieving the highest fleet reward and fulfillment ratios in on-demand mobility networks.
<div id='section'>Paperid: <span id='pid'>1513, <a href='https://arxiv.org/pdf/2302.06083.pdf' target='_blank'>https://arxiv.org/pdf/2302.06083.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Samuel Allen Alexander, David Quarel, Len Du, Marcus Hutter
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.06083">Universal Agent Mixtures and the Geometry of Intelligence</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Inspired by recent progress in multi-agent Reinforcement Learning (RL), in this work we examine the collective intelligent behaviour of theoretical universal agents by introducing a weighted mixture operation. Given a weighted set of agents, their weighted mixture is a new agent whose expected total reward in any environment is the corresponding weighted average of the original agents' expected total rewards in that environment. Thus, if RL agent intelligence is quantified in terms of performance across environments, the weighted mixture's intelligence is the weighted average of the original agents' intelligences. This operation enables various interesting new theorems that shed light on the geometry of RL agent intelligence, namely: results about symmetries, convex agent-sets, and local extrema. We also show that any RL agent intelligence measure based on average performance across environments, subject to certain weak technical conditions, is identical (up to a constant factor) to performance within a single environment dependent on said intelligence measure.
<div id='section'>Paperid: <span id='pid'>1514, <a href='https://arxiv.org/pdf/2302.01199.pdf' target='_blank'>https://arxiv.org/pdf/2302.01199.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Maxime Bouton, Jaeseong Jeong, Jose Outes, Adriano Mendo, Alexandros Nikou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.01199">Multi-agent Reinforcement Learning with Graph Q-Networks for Antenna Tuning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Future generations of mobile networks are expected to contain more and more antennas with growing complexity and more parameters. Optimizing these parameters is necessary for ensuring the good performance of the network. The scale of mobile networks makes it challenging to optimize antenna parameters using manual intervention or hand-engineered strategies. Reinforcement learning is a promising technique to address this challenge but existing methods often use local optimizations to scale to large network deployments. We propose a new multi-agent reinforcement learning algorithm to optimize mobile network configurations globally. By using a value decomposition approach, our algorithm can be trained from a global reward function instead of relying on an ad-hoc decomposition of the network performance across the different cells. The algorithm uses a graph neural network architecture which generalizes to different network topologies and learns coordination behaviors. We empirically demonstrate the performance of the algorithm on an antenna tilt tuning problem and a joint tilt and power control problem in a simulated environment.
<div id='section'>Paperid: <span id='pid'>1515, <a href='https://arxiv.org/pdf/2301.08278.pdf' target='_blank'>https://arxiv.org/pdf/2301.08278.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nayana Dasgupta, Mirco Musolesi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.08278">Investigating the Impact of Direct Punishment on the Emergence of Cooperation in Multi-Agent Reinforcement Learning Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Solving the problem of cooperation is fundamentally important for the creation and maintenance of functional societies. Problems of cooperation are omnipresent within human society, with examples ranging from navigating busy road junctions to negotiating treaties. As the use of AI becomes more pervasive throughout society, the need for socially intelligent agents capable of navigating these complex cooperative dilemmas is becoming increasingly evident. Direct punishment is a ubiquitous social mechanism that has been shown to foster the emergence of cooperation in both humans and non-humans. In the natural world, direct punishment is often strongly coupled with partner selection and reputation and used in conjunction with third-party punishment. The interactions between these mechanisms could potentially enhance the emergence of cooperation within populations. However, no previous work has evaluated the learning dynamics and outcomes emerging from Multi-Agent Reinforcement Learning (MARL) populations that combine these mechanisms. This paper addresses this gap. It presents a comprehensive analysis and evaluation of the behaviors and learning dynamics associated with direct punishment, third-party punishment, partner selection, and reputation. Finally, we discuss the implications of using these mechanisms on the design of cooperative AI systems.
<div id='section'>Paperid: <span id='pid'>1516, <a href='https://arxiv.org/pdf/2301.04299.pdf' target='_blank'>https://arxiv.org/pdf/2301.04299.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Maxwell Standen, Junae Kim, Claudia Szabo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.04299">SoK: Adversarial Machine Learning Attacks and Defences in Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-Agent Reinforcement Learning (MARL) is vulnerable to Adversarial Machine Learning (AML) attacks and needs adequate defences before it can be used in real world applications. We have conducted a survey into the use of execution-time AML attacks against MARL and the defences against those attacks. We surveyed related work in the application of AML in Deep Reinforcement Learning (DRL) and Multi-Agent Learning (MAL) to inform our analysis of AML for MARL. We propose a novel perspective to understand the manner of perpetrating an AML attack, by defining Attack Vectors. We develop two new frameworks to address a gap in current modelling frameworks, focusing on the means and tempo of an AML attack against MARL, and identify knowledge gaps and future avenues of research.
<div id='section'>Paperid: <span id='pid'>1517, <a href='https://arxiv.org/pdf/2301.03145.pdf' target='_blank'>https://arxiv.org/pdf/2301.03145.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mohammad Farzanullah, Tho Le-Ngoc
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.03145">Platoon Leader Selection, User Association and Resource Allocation on a C-V2X based highway: A Reinforcement Learning Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We consider the problem of dynamic platoon leader selection, user association, channel assignment, and power allocation on a cellular vehicle-to-everything (C-V2X) based highway, where multiple vehicle-to-vehicle (V2V) and vehicle-to-infrastructure (V2I) links share the frequency resources. There are multiple roadside units (RSUs) on a highway, and vehicles can form platoons, which has been identified as an advanced use case to increase road efficiency. The traditional optimization methods, requiring global channel information at a central controller, are not viable for high-mobility vehicular networks. To deal with this challenge, we propose a distributed multi-agent reinforcement learning (MARL) for resource allocation (RA). Each platoon leader, acting as an agent, can collaborate with other agents for joint sub-band selection and power allocation for its V2V links, and joint user association and power control for its V2I links. Moreover, each platoon can dynamically select the vehicle most suitable to be the platoon leader. We aim to maximize the V2V and V2I packet delivery probability in the desired latency using the deep Q-learning algorithm. Simulation results indicate that our proposed MARL outperforms the centralized hill-climbing algorithm, and platoon leader selection helps to improve both V2V and V2I performance.
<div id='section'>Paperid: <span id='pid'>1518, <a href='https://arxiv.org/pdf/2301.00896.pdf' target='_blank'>https://arxiv.org/pdf/2301.00896.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wei Xingxing, Wang Songping, Yan Huanqian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.00896">Efficient Robustness Assessment via Adversarial Spatial-Temporal Focus on Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Adversarial robustness assessment for video recognition models has raised concerns owing to their wide applications on safety-critical tasks. Compared with images, videos have much high dimension, which brings huge computational costs when generating adversarial videos. This is especially serious for the query-based black-box attacks where gradient estimation for the threat models is usually utilized, and high dimensions will lead to a large number of queries. To mitigate this issue, we propose to simultaneously eliminate the temporal and spatial redundancy within the video to achieve an effective and efficient gradient estimation on the reduced searching space, and thus query number could decrease. To implement this idea, we design the novel Adversarial spatial-temporal Focus (AstFocus) attack on videos, which performs attacks on the simultaneously focused key frames and key regions from the inter-frames and intra-frames in the video. AstFocus attack is based on the cooperative Multi-Agent Reinforcement Learning (MARL) framework. One agent is responsible for selecting key frames, and another agent is responsible for selecting key regions. These two agents are jointly trained by the common rewards received from the black-box threat models to perform a cooperative prediction. By continuously querying, the reduced searching space composed of key frames and key regions is becoming precise, and the whole query number becomes less than that on the original video. Extensive experiments on four mainstream video recognition models and three widely used action recognition datasets demonstrate that the proposed AstFocus attack outperforms the SOTA methods, which is prevenient in fooling rate, query number, time, and perturbation magnitude at the same.
<div id='section'>Paperid: <span id='pid'>1519, <a href='https://arxiv.org/pdf/2301.00637.pdf' target='_blank'>https://arxiv.org/pdf/2301.00637.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuli. Zhang, Shangbo. Wang, Ruiyuan. Jiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.00637">Large-Scale Traffic Signal Control by a Nash Deep Q-network Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reinforcement Learning (RL) is currently one of the most commonly used techniques for traffic signal control (TSC), which can adaptively adjusted traffic signal phase and duration according to real-time traffic data. However, a fully centralized RL approach is beset with difficulties in a multi-network scenario because of exponential growth in state-action space with increasing intersections. Multi-agent reinforcement learning (MARL) can overcome the high-dimension problem by employing the global control of each local RL agent, but it also brings new challenges, such as the failure of convergence caused by the non-stationary Markov Decision Process (MDP). In this paper, we introduce an off-policy nash deep Q-Network (OPNDQN) algorithm, which mitigates the weakness of both fully centralized and MARL approaches. The OPNDQN algorithm solves the problem that traditional algorithms cannot be used in large state-action space traffic models by utilizing a fictitious game approach at each iteration to find the nash equilibrium among neighboring intersections, from which no intersection has incentive to unilaterally deviate. One of main advantages of OPNDQN is to mitigate the non-stationarity of multi-agent Markov process because it considers the mutual influence among neighboring intersections by sharing their actions. On the other hand, for training a large traffic network, the convergence rate of OPNDQN is higher than that of existing MARL approaches because it does not incorporate all state information of each agent. We conduct an extensive experiments by using Simulation of Urban MObility simulator (SUMO), and show the dominant superiority of OPNDQN over several existing MARL approaches in terms of average queue length, episode training reward and average waiting time.
<div id='section'>Paperid: <span id='pid'>1520, <a href='https://arxiv.org/pdf/2212.14156.pdf' target='_blank'>https://arxiv.org/pdf/2212.14156.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chen Feng, Andrew L. Lu, Yihsu Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2212.14156">Decentralized Voltage Control with Peer-to-peer Energy Trading in a Distribution Network</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Utilizing distributed renewable and energy storage resources via peer-to-peer (P2P) energy trading has long been touted as a solution to improve energy system's resilience and sustainability. Consumers and prosumers (those who have energy generation resources), however, do not have expertise to engage in repeated P2P trading, and the zero-marginal costs of renewables present challenges in determining fair market prices. To address these issues, we propose a multi-agent reinforcement learning (MARL) framework to help automate consumers' bidding and management of their solar PV and energy storage resources, under a specific P2P clearing mechanism that utilizes the so-called supply-demand ratio. In addition, we show how the MARL framework can integrate physical network constraints to realize decentralized voltage control, hence ensuring physical feasibility of the P2P energy trading and paving ways for real-world implementations.
<div id='section'>Paperid: <span id='pid'>1521, <a href='https://arxiv.org/pdf/2212.06027.pdf' target='_blank'>https://arxiv.org/pdf/2212.06027.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sam Ganzfried, Kevin A. Wang, Max Chiswick
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2212.06027">Opponent Modeling in Multiplayer Imperfect-Information Games</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In many real-world settings agents engage in strategic interactions with multiple opposing agents who can employ a wide variety of strategies. The standard approach for designing agents for such settings is to compute or approximate a relevant game-theoretic solution concept such as Nash equilibrium and then follow the prescribed strategy. However, such a strategy ignores any observations of opponents' play, which may indicate shortcomings that can be exploited. We present an approach for opponent modeling in multiplayer imperfect-information games where we collect observations of opponents' play through repeated interactions. We run experiments against a wide variety of real opponents and exact Nash equilibrium strategies in three-player Kuhn poker and show that our algorithm significantly outperforms all of the agents, including the exact Nash equilibrium strategies.
<div id='section'>Paperid: <span id='pid'>1522, <a href='https://arxiv.org/pdf/2212.02733.pdf' target='_blank'>https://arxiv.org/pdf/2212.02733.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lin Shi, Qiyuan Liu, Bei Peng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2212.02733">CURO: Curriculum Learning for Relative Overgeneralization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Relative overgeneralization (RO) is a pathology that can arise in cooperative multi-agent tasks when the optimal joint action's utility falls below that of a sub-optimal joint action. RO can cause the agents to get stuck into local optima or fail to solve cooperative tasks requiring significant coordination between agents within a given timestep. In this work, we empirically find that, in multi-agent reinforcement learning (MARL), both value-based and policy gradient MARL algorithms can suffer from RO and fail to learn effective coordination policies. To better overcome RO, we propose a novel approach called curriculum learning for relative overgeneralization (CURO). To solve a target task that exhibits strong RO, in CURO, we first fine-tune the reward function of the target task to generate source tasks to train the agent. Then, to effectively transfer the knowledge acquired in one task to the next, we use a transfer learning method that combines value function transfer with buffer transfer, which enables more efficient exploration in the target task. CURO is general and can be applied to both value-based and policy gradient MARL methods. We demonstrate that, when applied to QMIX, HAPPO, and HATRPO, CURO can successfully overcome severe RO, achieve improved performance, and outperform baseline methods in a variety of challenging cooperative multi-agent tasks.
<div id='section'>Paperid: <span id='pid'>1523, <a href='https://arxiv.org/pdf/2211.05952.pdf' target='_blank'>https://arxiv.org/pdf/2211.05952.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinyu Zhao, Razvan C. Fetecau, Mo Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2211.05952">Efficient Domain Coverage for Vehicles with Second-Order Dynamics via Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Collaborative autonomous multi-agent systems covering a specified area have many potential applications, such as UAV search and rescue, forest fire fighting, and real-time high-resolution monitoring. Traditional approaches for such coverage problems involve designing a model-based control policy based on sensor data. However, designing model-based controllers is challenging, and the state-of-the-art classical control policy still exhibits a large degree of sub-optimality. In this paper, we present a reinforcement learning (RL) approach for the multi-agent efficient domain coverage problem involving agents with second-order dynamics. Our approach is based on the Multi-Agent Proximal Policy Optimization Algorithm (MAPPO). Our proposed network architecture includes the incorporation of LSTM and self-attention, which allows the trained policy to adapt to a variable number of agents. Our trained policy significantly outperforms the state-of-the-art classical control policy. We demonstrate our proposed method in a variety of simulated experiments.
<div id='section'>Paperid: <span id='pid'>1524, <a href='https://arxiv.org/pdf/2205.06011.pdf' target='_blank'>https://arxiv.org/pdf/2205.06011.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bibo Zhang, Ilario Filippini
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2205.06011">Mobility-Aware Resource Allocation for mmWave IAB Networks: A Multi-Agent Reinforcement Learning Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>MmWaves have been envisioned as a promising direction to provide Gbps wireless access. However, they are susceptible to high path losses and blockages, which directional antennas can only partially mitigate. That makes mmWave networks coverage-limited, thus requiring dense deployments. Integrated access and backhaul (IAB) architectures have emerged as a cost-effective solution for network densification. Resource allocation in mmWave IAB networks must face big challenges to cope with heavy temporal dynamics, such as intermittent links caused by user mobility and blockages from moving obstacles. This makes it extremely difficult to find optimal and adaptive solutions. In this article, exploiting the distributed structure of the problem, we propose a Multi-Agent Reinforcement Learning (MARL) framework to optimize user throughput via flow routing and link scheduling in mmWave IAB networks characterized by user mobility and link outages generated by moving obstacles. The proposed approach implicitly captures the environment dynamics, coordinates the interference, and manages the buffer levels of IAB relay nodes. We design different MARL components, considering full-duplex and half-duplex IAB-nodes. In addition, we provide a communication and coordination scheme for RL agents in an online training framework, addressing the feasibility issues of practical systems. Numerical results show the effectiveness of the proposed approach.
<div id='section'>Paperid: <span id='pid'>1525, <a href='https://arxiv.org/pdf/2202.09019.pdf' target='_blank'>https://arxiv.org/pdf/2202.09019.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Baoqian Wang, Junfei Xie, Nikolay Atanasov
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2202.09019">Distributed Multi-Agent Reinforcement Learning with One-hop Neighbors and Compute Straggler Mitigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Most multi-agent reinforcement learning (MARL) methods are limited in the scale of problems they can handle. With increasing numbers of agents, the number of training iterations required to find the optimal behaviors increases exponentially due to the exponentially growing joint state and action spaces. This paper tackles this limitation by introducing a scalable MARL method called Distributed multi-Agent Reinforcement Learning with One-hop Neighbors (DARL1N). DARL1N is an off-policy actor-critic method that addresses the curse of dimensionality by restricting information exchanges among the agents to one-hop neighbors when representing value and policy functions. Each agent optimizes its value and policy functions over a one-hop neighborhood, significantly reducing the learning complexity, yet maintaining expressiveness by training with varying neighbor numbers and states. This structure allows us to formulate a distributed learning framework to further speed up the training procedure. Distributed computing systems, however, contain straggler compute nodes, which are slow or unresponsive due to communication bottlenecks, software or hardware problems. To mitigate the detrimental straggler effect, we introduce a novel coded distributed learning architecture, which leverages coding theory to improve the resilience of the learning system to stragglers. Comprehensive experiments show that DARL1N significantly reduces training time without sacrificing policy quality and is scalable as the number of agents increases. Moreover, the coded distributed learning architecture improves training efficiency in the presence of stragglers.
<div id='section'>Paperid: <span id='pid'>1526, <a href='https://arxiv.org/pdf/2201.09057.pdf' target='_blank'>https://arxiv.org/pdf/2201.09057.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fitsum Debebe Tilahun, Ameha Tsegaye Abebe, Chung G. Kang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2201.09057">Multi-Agent Reinforcement Learning for Distributed Joint Communication and Computing Resource Allocation over Cell-Free Massive MIMO-enabled Mobile Edge Computing Network</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To support the newly introduced multimedia services with ultra-low latency and extensive computation requirements, resource-constrained end user devices should utilize the ubiquitous computing resources available at network edge for augmenting on-board (local) processing with edge computing. In this regard, the capability of cell-free massive MIMO to provide reliable access links by guaranteeing uniform quality of service without cell edge can be exploited for seamless parallel processing. Taking this into account, we consider a cell-free massive MIMO-enabled mobile edge network to meet the stringent requirements of the advanced services. For the considered mobile edge network, we formulate a joint communication and computing resource allocation (JCCRA) problem with the objective of minimizing energy consumption of the users while meeting the tight delay constraints. We then propose a fully distributed cooperative solution approach based on multiagent deep deterministic policy gradient (MADDPG) algorithm. The simulation results demonstrate that the performance of the proposed distributed approach has converged to that of a centralized deep deterministic policy gradient (DDPG)-based target benchmark, while alleviating the large overhead associated with the latter. Furthermore, it has been shown that our approach significantly outperforms heuristic baselines in terms of energy efficiency, roughly up to 5 times less total energy consumption.
<div id='section'>Paperid: <span id='pid'>1527, <a href='https://arxiv.org/pdf/2111.02827.pdf' target='_blank'>https://arxiv.org/pdf/2111.02827.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kevin Eloff, Okko RÃ¤sÃ¤nen, Herman A. Engelbrecht, Arnu Pretorius, Herman Kamper
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2111.02827">Towards Learning to Speak and Hear Through Multi-Agent Communication over a Continuous Acoustic Channel</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent reinforcement learning has been used as an effective means to study emergent communication between agents, yet little focus has been given to continuous acoustic communication. This would be more akin to human language acquisition; human infants acquire language in large part through continuous signalling with their caregivers. We therefore ask: Are we able to observe emergent language between agents with a continuous communication channel? Our goal is to provide a platform to begin bridging the gap between human and agent communication, allowing us to analyse continuous signals, how they emerge, their characteristics, and how they relate to human language acquisition. We propose a messaging environment where a Speaker agent needs to convey a set of attributes to a Listener over a noisy acoustic channel. Using DQN to train our agents, we show that: (1) unlike the discrete case, the acoustic Speaker learns redundancy to improve Listener coherency, (2) the acoustic Speaker develops more compositional communication protocols which implicitly compensates for transmission errors over a noisy channel, and (3) DQN has significant performance gains and increased compositionality when compared to previous methods optimised using REINFORCE.
<div id='section'>Paperid: <span id='pid'>1528, <a href='https://arxiv.org/pdf/2111.01946.pdf' target='_blank'>https://arxiv.org/pdf/2111.01946.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiawei Wang, Lijun Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2111.01946">Robust Dynamic Bus Control: A Distributional Multi-agent Reinforcement Learning Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Bus system is a critical component of sustainable urban transportation. However, the operation of a bus fleet is unstable in nature, and bus bunching has become a common phenomenon that undermines the efficiency and reliability of bus systems. Recently research has demonstrated the promising application of multi-agent reinforcement learning (MARL) to achieve efficient vehicle holding control to avoid bus bunching. However, existing studies essentially overlook the robustness issue resulting from various events, perturbations and anomalies in a transit system, which is of utmost importance when transferring the models for real-world deployment/application. In this study, we integrate implicit quantile network and meta-learning to develop a distributional MARL framework -- IQNC-M -- to learn continuous control. The proposed IQNC-M framework achieves efficient and reliable control decisions through better handling various uncertainties/events in real-time transit operations. Specifically, we introduce an interpretable meta-learning module to incorporate global information into the distributional MARL framework, which is an effective solution to circumvent the credit assignment issue in the transit system. In addition, we design a specific learning procedure to train each agent within the framework to pursue a robust control policy. We develop simulation environments based on real-world bus services and passenger demand data and evaluate the proposed framework against both traditional holding control models and state-of-the-art MARL models. Our results show that the proposed IQNC-M framework can effectively handle the various extreme events, such as traffic state perturbations, service interruptions, and demand surges, thus improving both efficiency and reliability of the system.
<div id='section'>Paperid: <span id='pid'>1529, <a href='https://arxiv.org/pdf/2107.05949.pdf' target='_blank'>https://arxiv.org/pdf/2107.05949.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hamed Rahimi, Iago Felipe Trentin, Fano Ramparany, Olivier Boissier
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2107.05949">Q-SMASH: Q-Learning-based Self-Adaptation of Human-Centered Internet of Things</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As the number of Human-Centered Internet of Things (HCIoT) applications increases, the self-adaptation of its services and devices is becoming a fundamental requirement for addressing the uncertainties of the environment in decision-making processes. Self-adaptation of HCIoT aims to manage run-time changes in a dynamic environment and to adjust the functionality of IoT objects in order to achieve desired goals during execution. SMASH is a semantic-enabled multi-agent system for self-adaptation of HCIoT that autonomously adapts IoT objects to uncertainties of their environment. SMASH addresses the self-adaptation of IoT applications only according to the human values of users, while the behavior of users is not addressed. This article presents Q-SMASH: a multi-agent reinforcement learning-based approach for self-adaptation of IoT objects in human-centered environments. Q-SMASH aims to learn the behaviors of users along with respecting human values. The learning ability of Q-SMASH allows it to adapt itself to the behavioral change of users and make more accurate decisions in different states and situations.
<div id='section'>Paperid: <span id='pid'>1530, <a href='https://arxiv.org/pdf/2107.01347.pdf' target='_blank'>https://arxiv.org/pdf/2107.01347.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Paolo Fazzini, Isaac Wheeler, Francesco Petracchini
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2107.01347">Traffic Signal Control with Communicative Deep Reinforcement Learning Agents: a Case Study</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this work we analyze Multi-Agent Advantage Actor-Critic (MA2C) a recently proposed multi-agent reinforcement learning algorithm that can be applied to adaptive traffic signal control (ATSC) problems. To evaluate its potential we compare MA2C with Independent Advantage Actor-Critic (IA2C) and other Reinforcement Learning or heuristic based algorithms. Specifically, we analyze MA2C theoretically with the framework provided by non-Markov decision processes, which allows a deeper insight of the algorithm, and we critically examine the effectiveness and the robustness of the method by testing it in two traffic areas located in Bologna (Italy) simulated in SUMO, a software modeling tool for ATSC problems. Our results indicate that MA2C, trained with pseudo-random vehicle flows, is a promising technique able to outperform the alternative methods.
<div id='section'>Paperid: <span id='pid'>1531, <a href='https://arxiv.org/pdf/2012.15472.pdf' target='_blank'>https://arxiv.org/pdf/2012.15472.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yoav Alon, Huiyu Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2012.15472">Multi-Agent Reinforcement Learning for Unmanned Aerial Vehicle Coordination by Multi-Critic Policy Gradient Optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent technological progress in the development of Unmanned Aerial Vehicles (UAVs) together with decreasing acquisition costs make the application of drone fleets attractive for a wide variety of tasks. In agriculture, disaster management, search and rescue operations, commercial and military applications, the advantage of applying a fleet of drones originates from their ability to cooperate autonomously. Multi-Agent Reinforcement Learning approaches that aim to optimize a neural network based control policy, such as the best performing actor-critic policy gradient algorithms, struggle to effectively back-propagate errors of distinct rewards signal sources and tend to favor lucrative signals while neglecting coordination and exploitation of previously learned similarities. We propose a Multi-Critic Policy Optimization architecture with multiple value estimating networks and a novel advantage function that optimizes a stochastic actor policy network to achieve optimal coordination of agents. Consequently, we apply the algorithm to several tasks that require the collaboration of multiple drones in a physics-based reinforcement learning environment. Our approach achieves a stable policy network update and similarity in reward signal development for an increasing number of agents. The resulting policy achieves optimal coordination and compliance with constraints such as collision avoidance.
<div id='section'>Paperid: <span id='pid'>1532, <a href='https://arxiv.org/pdf/2010.07916.pdf' target='_blank'>https://arxiv.org/pdf/2010.07916.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hepeng Li, Haibo He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2010.07916">Multi-Agent Trust Region Policy Optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We extend trust region policy optimization (TRPO) to multi-agent reinforcement learning (MARL) problems. We show that the policy update of TRPO can be transformed into a distributed consensus optimization problem for multi-agent cases. By making a series of approximations to the consensus optimization model, we propose a decentralized MARL algorithm, which we call multi-agent TRPO (MATRPO). This algorithm can optimize distributed policies based on local observations and private rewards. The agents do not need to know observations, rewards, policies or value/action-value functions of other agents. The agents only share a likelihood ratio with their neighbors during the training process. The algorithm is fully decentralized and privacy-preserving. Our experiments on two cooperative games demonstrate its robust performance on complicated MARL tasks.
<div id='section'>Paperid: <span id='pid'>1533, <a href='https://arxiv.org/pdf/2007.03151.pdf' target='_blank'>https://arxiv.org/pdf/2007.03151.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Adel Nabli, Margarida Carvalho
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2007.03151">Curriculum learning for multilevel budgeted combinatorial problems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Learning heuristics for combinatorial optimization problems through graph neural networks have recently shown promising results on some classic NP-hard problems. These are single-level optimization problems with only one player. Multilevel combinatorial optimization problems are their generalization, encompassing situations with multiple players taking decisions sequentially. By framing them in a multi-agent reinforcement learning setting, we devise a value-based method to learn to solve multilevel budgeted combinatorial problems involving two players in a zero-sum game over a graph. Our framework is based on a simple curriculum: if an agent knows how to estimate the value of instances with budgets up to $B$, then solving instances with budget $B+1$ can be done in polynomial time regardless of the direction of the optimization by checking the value of every possible afterstate. Thus, in a bottom-up approach, we generate datasets of heuristically solved instances with increasingly larger budgets to train our agent. We report results close to optimality on graphs up to $100$ nodes and a $185 \times$ speedup on average compared to the quickest exact solver known for the Multilevel Critical Node problem, a max-min-max trilevel problem that has been shown to be at least $Î£_2^p$-hard.
<div id='section'>Paperid: <span id='pid'>1534, <a href='https://arxiv.org/pdf/1807.07024.pdf' target='_blank'>https://arxiv.org/pdf/1807.07024.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Stefano Balietti, Brennan Klein, Christoph Riedl
</span></div><div id="title">Title: <a href="https://arxiv.org/html/1807.07024">Optimal design of experiments to identify latent behavioral types</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Bayesian optimal experiments that maximize the information gained from collected data are critical to efficiently identify behavioral models. We extend a seminal method for designing Bayesian optimal experiments by introducing two computational improvements that make the procedure tractable: (1) a search algorithm from artificial intelligence that efficiently explores the space of possible design parameters, and (2) a sampling procedure which evaluates each design parameter combination more efficiently. We apply our procedure to a game of imperfect information to evaluate and quantify the computational improvements. We then collect data across five different experimental designs to compare the ability of the optimal experimental design to discriminate among competing behavioral models against the experimental designs chosen by a "wisdom of experts" prediction experiment. We find that data from the experiment suggested by the optimal design approach requires significantly less data to distinguish behavioral models (i.e., test hypotheses) than data from the experiment suggested by experts. Substantively, we find that reinforcement learning best explains human decision-making in the imperfect information game and that behavior is not adequately described by the Bayesian Nash equilibrium. Our procedure is general and computationally efficient and can be applied to dynamically optimize online experiments.
<div id='section'>Paperid: <span id='pid'>1535, <a href='https://arxiv.org/pdf/1708.06233.pdf' target='_blank'>https://arxiv.org/pdf/1708.06233.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Christoph Aymanns, Jakob Foerster, Co-Pierre Georg, Matthias Weber
</span></div><div id="title">Title: <a href="https://arxiv.org/html/1708.06233">Fake News in Social Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose multi-agent reinforcement learning as a new method for modeling fake news in social networks. This method allows us to model human behavior in social networks both in unaccustomed populations and in populations that have adapted to the presence of fake news. In particular the latter is challenging for existing methods. We find that a fake-news attack is more effective if it targets highly connected people and people with weaker private information. Attacks are more effective when the disinformation is spread across several agents than when the disinformation is concentrated with more intensity on fewer agents. Furthermore, fake news spread less well in balanced networks than in clustered networks. We test a part of our findings in a human-subject experiment. The experimental evidence provides support for the predictions from the model, suggesting that the model is suitable to analyze the spread of fake news in social networks.
<div id='section'>Paperid: <span id='pid'>1536, <a href='https://arxiv.org/pdf/2510.06138.pdf' target='_blank'>https://arxiv.org/pdf/2510.06138.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rushiv Arora
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.06138">Multi-Task Reinforcement Learning with Language-Encoded Gated Policy Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-task reinforcement learning often relies on task metadata -- such as brief natural-language descriptions -- to guide behavior across diverse objectives. We present Lexical Policy Networks (LEXPOL), a language-conditioned mixture-of-policies architecture for multi-task RL. LEXPOL encodes task metadata with a text encoder and uses a learned gating module to select or blend among multiple sub-policies, enabling end-to-end training across tasks. On MetaWorld benchmarks, LEXPOL matches or exceeds strong multi-task baselines in success rate and sample efficiency, without task-specific retraining. To analyze the mechanism, we further study settings with fixed expert policies obtained independently of the gate and show that the learned language gate composes these experts to produce behaviors appropriate to novel task descriptions and unseen task combinations. These results indicate that natural-language metadata can effectively index and recombine reusable skills within a single policy.
<div id='section'>Paperid: <span id='pid'>1537, <a href='https://arxiv.org/pdf/2510.06138.pdf' target='_blank'>https://arxiv.org/pdf/2510.06138.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rushiv Arora
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.06138">Multi-Task Reinforcement Learning with Language-Encoded Gated Policy Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-task reinforcement learning often relies on task metadata -- such as brief natural-language descriptions -- to guide behavior across diverse objectives. We present Lexical Policy Networks (LEXPOL), a language-conditioned mixture-of-policies architecture for multi-task RL. LEXPOL encodes task metadata with a text encoder and uses a learned gating module to select or blend among multiple sub-policies, enabling end-to-end training across tasks. On MetaWorld benchmarks, LEXPOL matches or exceeds strong multi-task baselines in success rate and sample efficiency, without task-specific retraining. To analyze the mechanism, we further study settings with fixed expert policies obtained independently of the gate and show that the learned language gate composes these experts to produce behaviors appropriate to novel task descriptions and unseen task combinations. These results indicate that natural-language metadata can effectively index and recombine reusable skills within a single policy.
<div id='section'>Paperid: <span id='pid'>1538, <a href='https://arxiv.org/pdf/2510.00022.pdf' target='_blank'>https://arxiv.org/pdf/2510.00022.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ansh Kamthan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.00022">Learning to Lead Themselves: Agentic AI in MAS using MARL</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As autonomous systems move from prototypes to real deployments, the ability of multiple agents to make decentralized, cooperative decisions becomes a core requirement. This paper examines how agentic artificial intelligence, agents that act independently, adaptively and proactively can improve task allocation and coordination in multi-agent systems, with primary emphasis on drone delivery and secondary relevance to warehouse automation. We formulate the problem in a cooperative multi-agent reinforcement learning setting and implement a lightweight multi-agent Proximal Policy Optimization, called IPPO, approach in PyTorch under a centralized-training, decentralized-execution paradigm. Experiments are conducted in PettingZoo environment, where multiple homogeneous drones or agents must self-organize to cover distinct targets without explicit communication.
<div id='section'>Paperid: <span id='pid'>1539, <a href='https://arxiv.org/pdf/2510.00022.pdf' target='_blank'>https://arxiv.org/pdf/2510.00022.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ansh Kamthan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.00022">Learning to Lead Themselves: Agentic AI in MAS using MARL</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As autonomous systems move from prototypes to real deployments, the ability of multiple agents to make decentralized, cooperative decisions becomes a core requirement. This paper examines how agentic artificial intelligence, agents that act independently, adaptively and proactively can improve task allocation and coordination in multi-agent systems, with primary emphasis on drone delivery and secondary relevance to warehouse automation. We formulate the problem in a cooperative multi-agent reinforcement learning setting and implement a lightweight multi-agent Proximal Policy Optimization, called IPPO, approach in PyTorch under a centralized-training, decentralized-execution paradigm. Experiments are conducted in PettingZoo environment, where multiple homogeneous drones or agents must self-organize to cover distinct targets without explicit communication.
<div id='section'>Paperid: <span id='pid'>1540, <a href='https://arxiv.org/pdf/2509.23026.pdf' target='_blank'>https://arxiv.org/pdf/2509.23026.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yue Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.23026">Game-Theoretic Understandings of Multi-Agent Systems with Multiple Objectives</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In practical multi-agent systems, agents often have diverse objectives, which makes the system more complex, as each agent's performance across multiple criteria depends on the joint actions of all agents, creating intricate strategic trade-offs. To address this, we introduce the Multi-Objective Markov Game (MOMG), a framework for multi-agent reinforcement learning with multiple objectives. We propose the Pareto-Nash Equilibrium (PNE) as the primary solution concept, where no agent can unilaterally improve one objective without sacrificing performance on another. We prove existence of PNE, and establish an equivalence between the PNE and the set of Nash Equilibria of MOMG's corresponding linearly scalarized games, enabling solutions of MOMG by transferring to a standard single-objective Markov game. However, we note that computing a PNE is theoretically and computationally challenging, thus we propose and study weaker but more tractable solution concepts. Building on these foundations, we develop online learning algorithm that identify a single solution to MOMGs. Furthermore, we propose a two-phase, preference-free algorithm that decouples exploration from planning. Our algorithm enables computation of a PNE for any given preference profile without collecting new samples, providing an efficient methodological characterization of the entire Pareto-Nash front.
<div id='section'>Paperid: <span id='pid'>1541, <a href='https://arxiv.org/pdf/2509.23026.pdf' target='_blank'>https://arxiv.org/pdf/2509.23026.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yue Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.23026">Game-Theoretic Understandings of Multi-Agent Systems with Multiple Objectives</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In practical multi-agent systems, agents often have diverse objectives, which makes the system more complex, as each agent's performance across multiple criteria depends on the joint actions of all agents, creating intricate strategic trade-offs. To address this, we introduce the Multi-Objective Markov Game (MOMG), a framework for multi-agent reinforcement learning with multiple objectives. We propose the Pareto-Nash Equilibrium (PNE) as the primary solution concept, where no agent can unilaterally improve one objective without sacrificing performance on another. We prove existence of PNE, and establish an equivalence between the PNE and the set of Nash Equilibria of MOMG's corresponding linearly scalarized games, enabling solutions of MOMG by transferring to a standard single-objective Markov game. However, we note that computing a PNE is theoretically and computationally challenging, thus we propose and study weaker but more tractable solution concepts. Building on these foundations, we develop online learning algorithm that identify a single solution to MOMGs. Furthermore, we propose a two-phase, preference-free algorithm that decouples exploration from planning. Our algorithm enables computation of a PNE for any given preference profile without collecting new samples, providing an efficient methodological characterization of the entire Pareto-Nash front.
<div id='section'>Paperid: <span id='pid'>1542, <a href='https://arxiv.org/pdf/2509.04731.pdf' target='_blank'>https://arxiv.org/pdf/2509.04731.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Brennen Hill
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.04731">Hierarchical Task Environments as the Next Frontier for Embodied World Models in Robot Soccer</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in agent development have focused on scaling model size and raw interaction data, mirroring the successes seen in large language models. However, for complex, long-horizon multi-agent tasks such as robotic soccer, this end-to-end approach often fails due to intractable exploration spaces and sparse rewards. This position paper argues that the next frontier in developing embodied world models is not merely increasing the fidelity or size of environments, but scaling their structural complexity through explicit hierarchical scaffolding. We posit that an effective world model for decision-making must model not only the world's physics but also its task semantics. Drawing from a systematic review of 2024 research in low-resource multi-agent soccer, we identify a clear trend towards integrating symbolic and hierarchical methods, such as Hierarchical Task Networks (HTNs) and Bayesian Strategy Networks (BSNs), with multi-agent reinforcement learning (MARL). These methods decompose complex goals into manageable subgoals, creating an intrinsic curriculum that shapes agent learning. We propose that such structured environments are essential for bridging the gap between simple, reactive behaviors and sophisticated, strategic team play. We further extend this principle, proposing that this scaffolding can be generalized to other complex domains and dynamically generated by Large Language Models (LLMs), which act as generative world models of tasks. By building environments with explicit, composable task layers, we can guide agent exploration more efficiently, generate meaningful learning signals, and ultimately train more capable and general-purpose agents with fewer resources than purely end-to-end approaches.
<div id='section'>Paperid: <span id='pid'>1543, <a href='https://arxiv.org/pdf/2509.04731.pdf' target='_blank'>https://arxiv.org/pdf/2509.04731.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Brennen Hill
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.04731">Hierarchical Task Environments as the Next Frontier for Embodied World Models in Robot Soccer</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in agent development have focused on scaling model size and raw interaction data, mirroring the successes seen in large language models. However, for complex, long-horizon multi-agent tasks such as robotic soccer, this end-to-end approach often fails due to intractable exploration spaces and sparse rewards. This position paper argues that the next frontier in developing embodied world models is not merely increasing the fidelity or size of environments, but scaling their structural complexity through explicit hierarchical scaffolding. We posit that an effective world model for decision-making must model not only the world's physics but also its task semantics. Drawing from a systematic review of 2024 research in low-resource multi-agent soccer, we identify a clear trend towards integrating symbolic and hierarchical methods, such as Hierarchical Task Networks (HTNs) and Bayesian Strategy Networks (BSNs), with multi-agent reinforcement learning (MARL). These methods decompose complex goals into manageable subgoals, creating an intrinsic curriculum that shapes agent learning. We propose that such structured environments are essential for bridging the gap between simple, reactive behaviors and sophisticated, strategic team play. We further extend this principle, proposing that this scaffolding can be generalized to other complex domains and dynamically generated by Large Language Models (LLMs), which act as generative world models of tasks. By building environments with explicit, composable task layers, we can guide agent exploration more efficiently, generate meaningful learning signals, and ultimately train more capable and general-purpose agents with fewer resources than purely end-to-end approaches.
<div id='section'>Paperid: <span id='pid'>1544, <a href='https://arxiv.org/pdf/2508.20784.pdf' target='_blank'>https://arxiv.org/pdf/2508.20784.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yifan Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.20784">Single Agent Robust Deep Reinforcement Learning for Bus Fleet Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Bus bunching remains a challenge for urban transit due to stochastic traffic and passenger demand. Traditional solutions rely on multi-agent reinforcement learning (MARL) in loop-line settings, which overlook realistic operations characterized by heterogeneous routes, timetables, fluctuating demand, and varying fleet sizes. We propose a novel single-agent reinforcement learning (RL) framework for bus holding control that avoids the data imbalance and convergence issues of MARL under near-realistic simulation. A bidirectional timetabled network with dynamic passenger demand is constructed. The key innovation is reformulating the multi-agent problem into a single-agent one by augmenting the state space with categorical identifiers (vehicle ID, station ID, time period) in addition to numerical features (headway, occupancy, velocity). This high-dimensional encoding enables single-agent policies to capture inter-agent dependencies, analogous to projecting non-separable inputs into a higher-dimensional space. We further design a structured reward function aligned with operational goals: instead of exponential penalties on headway deviations, a ridge-shaped reward balances uniform headways and schedule adherence. Experiments show that our modified soft actor-critic (SAC) achieves more stable and superior performance than benchmarks, including MADDPG (e.g., -430k vs. -530k under stochastic conditions). These results demonstrate that single-agent deep RL, when enhanced with categorical structuring and schedule-aware rewards, can effectively manage bus holding in non-loop, real-world contexts. This paradigm offers a robust, scalable alternative to MARL frameworks, particularly where agent-specific experiences are imbalanced.
<div id='section'>Paperid: <span id='pid'>1545, <a href='https://arxiv.org/pdf/2508.17671.pdf' target='_blank'>https://arxiv.org/pdf/2508.17671.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sam Ganzfried
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.17671">Consistent Opponent Modeling of Static Opponents in Imperfect-Information Games</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The goal of agents in multi-agent environments is to maximize total reward against the opposing agents that are encountered. Following a game-theoretic solution concept, such as Nash equilibrium, may obtain a strong performance in some settings; however, such approaches fail to capitalize on historical and observed data from repeated interactions against our opponents. Opponent modeling algorithms integrate machine learning techniques to exploit suboptimal opponents utilizing available data; however, the effectiveness of such approaches in imperfect-information games to date is quite limited. We show that existing opponent modeling approaches fail to satisfy a simple desirable property even against static opponents drawn from a known prior distribution; namely, they do not guarantee that the model approaches the opponent's true strategy even in the limit as the number of game iterations approaches infinity. We develop a new algorithm that is able to achieve this property and runs efficiently by solving a convex minimization problem based on the sequence-form game representation using projected gradient descent. The algorithm is guaranteed to efficiently converge to the opponent's true strategy given observations from gameplay and possibly additional historical data if it is available.
<div id='section'>Paperid: <span id='pid'>1546, <a href='https://arxiv.org/pdf/2508.17671.pdf' target='_blank'>https://arxiv.org/pdf/2508.17671.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sam Ganzfried
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.17671">Consistent Opponent Modeling of Static Opponents in Imperfect-Information Games</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The goal of agents in multi-agent environments is to maximize total reward against the opposing agents that are encountered. Following a game-theoretic solution concept, such as Nash equilibrium, may obtain a strong performance in some settings; however, such approaches fail to capitalize on historical and observed data from repeated interactions against our opponents. Opponent modeling algorithms integrate machine learning techniques to exploit suboptimal opponents utilizing available data; however, the effectiveness of such approaches in imperfect-information games to date is quite limited. We show that existing opponent modeling approaches fail to satisfy a simple desirable property even against static opponents drawn from a known prior distribution; namely, they do not guarantee that the model approaches the opponent's true strategy even in the limit as the number of game iterations approaches infinity. We develop a new algorithm that is able to achieve this property and runs efficiently by solving a convex minimization problem based on the sequence-form game representation using projected gradient descent. The algorithm is guaranteed to efficiently converge to the opponent's true strategy given observations from gameplay and possibly additional historical data if it is available.
<div id='section'>Paperid: <span id='pid'>1547, <a href='https://arxiv.org/pdf/2508.17671.pdf' target='_blank'>https://arxiv.org/pdf/2508.17671.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sam Ganzfried
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.17671">Consistent Opponent Modeling of Static Opponents in Imperfect-Information Games</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The goal of agents in multi-agent environments is to maximize total reward against the opposing agents that are encountered. Following a game-theoretic solution concept, such as Nash equilibrium, may obtain a strong performance in some settings; however, such approaches fail to capitalize on historical and observed data from repeated interactions against our opponents. Opponent modeling algorithms integrate machine learning techniques to exploit suboptimal opponents utilizing available data; however, the effectiveness of such approaches in imperfect-information games to date is quite limited. We show that existing opponent modeling approaches fail to satisfy a simple desirable property even against static opponents drawn from a known prior distribution; namely, they do not guarantee that the model approaches the opponent's true strategy even in the limit as the number of game iterations approaches infinity. We develop a new algorithm that is able to achieve this property and runs efficiently by solving a convex minimization problem based on the sequence-form game representation using projected gradient descent. The algorithm is guaranteed to efficiently converge to the opponent's true strategy given observations from gameplay and possibly additional historical data if it is available.
<div id='section'>Paperid: <span id='pid'>1548, <a href='https://arxiv.org/pdf/2507.10566.pdf' target='_blank'>https://arxiv.org/pdf/2507.10566.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hung Ming Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.10566">AI Mother Tongue: Self-Emergent Communication in MARL via Endogenous Symbol Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In Decentralized Multi-Agent Reinforcement Learning (MARL), the development of Emergent Communication has long been constrained by the ``Joint Exploration Dilemma'', leading agents to fall into a ``Communication Vacuum Equilibrium'' . Traditional methods address this by introducing inductive biases to facilitate communication emergence . This study fundamentally questions whether such artificial inductive biases are, in fact, over-engineering. Through experiments with the ``AI Mother Tongue'' (AIM) framework, based on a Vector Quantized Variational Autoencoder (VQ-VAE), we demonstrate that when agents possess an endogenous symbol system, their neural representations naturally exhibit spontaneous semantic compression and Nash equilibrium-driven semantic convergence, achieving effective symbolic communication without external inductive biases. This aligns with recent neuroscience findings suggesting that the human brain does not directly use human language for internal thought , and resonates with research on ``soft thinking'' capabilities in Large Language Models (LLMs) . Compared to traditional explicit communication methods, AIM demonstrates stronger generality and efficiency. The interpretable analysis toolkit developed in this study confirms that symbol usage exhibits a significant power-law distribution, leading to three major theoretical insights: the ``Neural Communication Hypothesis'', the ``Tool-First Principle'', and the ``Semantic Interpretability Paradigm''. Future research will explore the integration of Hierarchical Quantized Variational Autoencoders (HQ-VAE) to enhance AIM's complex expressive capabilities and investigate the potential for ``Reinforcement Learning (RL) Low-Level Pre-training''. This discovery offers new avenues for bridging symbolism and connectionism.
<div id='section'>Paperid: <span id='pid'>1549, <a href='https://arxiv.org/pdf/2507.07302.pdf' target='_blank'>https://arxiv.org/pdf/2507.07302.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ashish Kumar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.07302">Application of LLMs to Multi-Robot Path Planning and Task Allocation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Efficient exploration is a well known problem in deep reinforcement learning and this problem is exacerbated in multi-agent reinforcement learning due the intrinsic complexities of such algorithms. There are several approaches to efficiently explore an environment to learn to solve tasks by multi-agent operating in that environment, of which, the idea of expert exploration is investigated in this work. More specifically, this work investigates the application of large-language models as expert planners for efficient exploration in planning based tasks for multiple agents.
<div id='section'>Paperid: <span id='pid'>1550, <a href='https://arxiv.org/pdf/2506.22445.pdf' target='_blank'>https://arxiv.org/pdf/2506.22445.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Saad Alqithami
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.22445">Hierarchical Adversarially-Resilient Multi-Agent Reinforcement Learning for Cyber-Physical Systems Security</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cyber-Physical Systems play a critical role in the infrastructure of various sectors, including manufacturing, energy distribution, and autonomous transportation systems. However, their increasing connectivity renders them highly vulnerable to sophisticated cyber threats, such as adaptive and zero-day attacks, against which traditional security methods like rule-based intrusion detection and single-agent reinforcement learning prove insufficient. To overcome these challenges, this paper introduces a novel Hierarchical Adversarially-Resilient Multi-Agent Reinforcement Learning (HAMARL) framework. HAMARL employs a hierarchical structure consisting of local agents dedicated to subsystem security and a global coordinator that oversees and optimizes comprehensive, system-wide defense strategies. Furthermore, the framework incorporates an adversarial training loop designed to simulate and anticipate evolving cyber threats, enabling proactive defense adaptation. Extensive experimental evaluations conducted on a simulated industrial IoT testbed indicate that HAMARL substantially outperforms traditional multi-agent reinforcement learning approaches, significantly improving attack detection accuracy, reducing response times, and ensuring operational continuity. The results underscore the effectiveness of combining hierarchical multi-agent coordination with adversarially-aware training to enhance the resilience and security of next-generation CPS.
<div id='section'>Paperid: <span id='pid'>1551, <a href='https://arxiv.org/pdf/2506.14164.pdf' target='_blank'>https://arxiv.org/pdf/2506.14164.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hanzhong Cao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.14164">Light Aircraft Game : Basic Implementation and training results analysis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper investigates multi-agent reinforcement learning (MARL) in a partially observable, cooperative-competitive combat environment known as LAG. We describe the environment's setup, including agent actions, hierarchical controls, and reward design across different combat modes such as No Weapon and ShootMissile. Two representative algorithms are evaluated: HAPPO, an on-policy hierarchical variant of PPO, and HASAC, an off-policy method based on soft actor-critic. We analyze their training stability, reward progression, and inter-agent coordination capabilities. Experimental results show that HASAC performs well in simpler coordination tasks without weapons, while HAPPO demonstrates stronger adaptability in more dynamic and expressive scenarios involving missile combat. These findings provide insights into the trade-offs between on-policy and off-policy methods in multi-agent settings.
<div id='section'>Paperid: <span id='pid'>1552, <a href='https://arxiv.org/pdf/2506.12497.pdf' target='_blank'>https://arxiv.org/pdf/2506.12497.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ali Baheri
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.12497">Wasserstein-Barycenter Consensus for Cooperative Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cooperative multi-agent reinforcement learning (MARL) demands principled mechanisms to align heterogeneous policies while preserving the capacity for specialized behavior. We introduce a novel consensus framework that defines the team strategy as the entropic-regularized $p$-Wasserstein barycenter of agents' joint state--action visitation measures. By augmenting each agent's policy objective with a soft penalty proportional to its Sinkhorn divergence from this barycenter, the proposed approach encourages coherent group behavior without enforcing rigid parameter sharing. We derive an algorithm that alternates between Sinkhorn-barycenter computation and policy-gradient updates, and we prove that, under standard Lipschitz and compactness assumptions, the maximal pairwise policy discrepancy contracts at a geometric rate. Empirical evaluation on a cooperative navigation case study demonstrates that our OT-barycenter consensus outperforms an independent learners baseline in convergence speed and final coordination success.
<div id='section'>Paperid: <span id='pid'>1553, <a href='https://arxiv.org/pdf/2506.09331.pdf' target='_blank'>https://arxiv.org/pdf/2506.09331.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Arjun Vaithilingam Sudhakar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.09331">Multi-Agent Language Models: Advancing Cooperation, Coordination, and Adaptation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Modern Large Language Models (LLMs) exhibit impressive zero-shot and few-shot generalization capabilities across complex natural language tasks, enabling their widespread use as virtual assistants for diverse applications such as translation and summarization. Despite being trained solely on large corpora of text without explicit supervision on author intent, LLMs appear to infer the underlying meaning of textual interactions. This raises a fundamental question: can LLMs model and reason about the intentions of others, i.e., do they possess a form of theory of mind? Understanding other's intentions is crucial for effective collaboration, which underpins human societal success and is essential for cooperative interactions among multiple agents, including humans and autonomous systems. In this work, we investigate the theory of mind in LLMs through the lens of cooperative multi-agent reinforcement learning (MARL), where agents learn to collaborate via repeated interactions, mirroring human social reasoning. Our approach aims to enhance artificial agent's ability to adapt and cooperate with both artificial and human partners. By leveraging LLM-based agents capable of natural language interaction, we move towards creating hybrid human-AI systems that can foster seamless collaboration, with broad implications for the future of human-artificial interaction.
<div id='section'>Paperid: <span id='pid'>1554, <a href='https://arxiv.org/pdf/2506.09215.pdf' target='_blank'>https://arxiv.org/pdf/2506.09215.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Greyson Brothers
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.09215">Robust Noise Attenuation via Adaptive Pooling of Transformer Outputs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We investigate the design of pooling methods used to summarize the outputs of transformer embedding models, primarily motivated by reinforcement learning and vision applications. This work considers problems where a subset of the input vectors contains requisite information for a downstream task (signal) while the rest are distractors (noise). By framing pooling as vector quantization with the goal of minimizing signal loss, we demonstrate that the standard methods used to aggregate transformer outputs, AvgPool, MaxPool, and ClsToken, are vulnerable to performance collapse as the signal-to-noise ratio (SNR) of inputs fluctuates. We then show that an attention-based adaptive pooling method can approximate the signal-optimal vector quantizer within derived error bounds for any SNR. Our theoretical results are first validated by supervised experiments on a synthetic dataset designed to isolate the SNR problem, then generalized to standard relational reasoning, multi-agent reinforcement learning, and vision benchmarks with noisy observations, where transformers with adaptive pooling display superior robustness across tasks.
<div id='section'>Paperid: <span id='pid'>1555, <a href='https://arxiv.org/pdf/2506.04251.pdf' target='_blank'>https://arxiv.org/pdf/2506.04251.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhengyang Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.04251">Language-Guided Multi-Agent Learning in Simulations: A Unified Framework and Evaluation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces LLM-MARL, a unified framework that incorporates large language models (LLMs) into multi-agent reinforcement learning (MARL) to enhance coordination, communication, and generalization in simulated game environments. The framework features three modular components of Coordinator, Communicator, and Memory, which dynamically generate subgoals, facilitate symbolic inter-agent messaging, and support episodic recall. Training combines PPO with a language-conditioned loss and LLM query gating. LLM-MARL is evaluated in Google Research Football, MAgent Battle, and StarCraft II. Results show consistent improvements over MAPPO and QMIX in win rate, coordination score, and zero-shot generalization. Ablation studies demonstrate that subgoal generation and language-based messaging each contribute significantly to performance gains. Qualitative analysis reveals emergent behaviors such as role specialization and communication-driven tactics. By bridging language modeling and policy learning, this work contributes to the design of intelligent, cooperative agents in interactive simulations. It offers a path forward for leveraging LLMs in multi-agent systems used for training, games, and human-AI collaboration.
<div id='section'>Paperid: <span id='pid'>1556, <a href='https://arxiv.org/pdf/2506.04251.pdf' target='_blank'>https://arxiv.org/pdf/2506.04251.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhengyang Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.04251">Language-Guided Multi-Agent Learning in Simulations: A Unified Framework and Evaluation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces LLM-MARL, a unified framework that incorporates large language models (LLMs) into multi-agent reinforcement learning (MARL) to enhance coordination, communication, and generalization in simulated game environments. The framework features three modular components of Coordinator, Communicator, and Memory, which dynamically generate subgoals, facilitate symbolic inter-agent messaging, and support episodic recall. Training combines PPO with a language-conditioned loss and LLM query gating. LLM-MARL is evaluated in Google Research Football, MAgent Battle, and StarCraft II. Results show consistent improvements over MAPPO and QMIX in win rate, coordination score, and zero-shot generalization. Ablation studies demonstrate that subgoal generation and language-based messaging each contribute significantly to performance gains. Qualitative analysis reveals emergent behaviors such as role specialization and communication-driven tactics. By bridging language modeling and policy learning, this work contributes to the design of intelligent, cooperative agents in interactive simulations. It offers a path forward for leveraging LLMs in multi-agent systems used for training, games, and human-AI collaboration.
<div id='section'>Paperid: <span id='pid'>1557, <a href='https://arxiv.org/pdf/2505.23960.pdf' target='_blank'>https://arxiv.org/pdf/2505.23960.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Henry Conklin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.23960">Information Structure in Mappings: An Approach to Learning, Representation, and Generalisation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite the remarkable success of large large-scale neural networks, we still lack unified notation for thinking about and describing their representational spaces. We lack methods to reliably describe how their representations are structured, how that structure emerges over training, and what kinds of structures are desirable. This thesis introduces quantitative methods for identifying systematic structure in a mapping between spaces, and leverages them to understand how deep-learning models learn to represent information, what representational structures drive generalisation, and how design decisions condition the structures that emerge. To do this I identify structural primitives present in a mapping, along with information theoretic quantifications of each. These allow us to analyse learning, structure, and generalisation across multi-agent reinforcement learning models, sequence-to-sequence models trained on a single task, and Large Language Models. I also introduce a novel, performant, approach to estimating the entropy of vector space, that allows this analysis to be applied to models ranging in size from 1 million to 12 billion parameters.
  The experiments here work to shed light on how large-scale distributed models of cognition learn, while allowing us to draw parallels between those systems and their human analogs. They show how the structures of language and the constraints that give rise to them in many ways parallel the kinds of structures that drive performance of contemporary neural networks.
<div id='section'>Paperid: <span id='pid'>1558, <a href='https://arxiv.org/pdf/2505.18457.pdf' target='_blank'>https://arxiv.org/pdf/2505.18457.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Abir Ray
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.18457">EdgeAgentX: A Novel Framework for Agentic AI at the Edge in Military Communication Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces EdgeAgentX, a novel framework integrating federated learning (FL), multi-agent reinforcement learning (MARL), and adversarial defense mechanisms, tailored for military communication networks. EdgeAgentX significantly improves autonomous decision-making, reduces latency, enhances throughput, and robustly withstands adversarial disruptions, as evidenced by comprehensive simulations.
<div id='section'>Paperid: <span id='pid'>1559, <a href='https://arxiv.org/pdf/2505.14544.pdf' target='_blank'>https://arxiv.org/pdf/2505.14544.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Saahil Mahato
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.14544">Smart Traffic Signals: Comparing MARL and Fixed-Time Strategies</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Urban traffic congestion, particularly at intersections, significantly impacts travel time, fuel consumption, and emissions. Traditional fixed-time signal control systems often lack the adaptability to manage dynamic traffic patterns effectively. This study explores the application of multi-agent reinforcement learning (MARL) to optimize traffic signal coordination across multiple intersections within a simulated environment. Utilizing Pygame, a simulation was developed to model a network of interconnected intersections with randomly generated vehicle flows to reflect realistic traffic variability. A decentralized MARL controller was implemented, in which each traffic signal operates as an autonomous agent, making decisions based on local observations and information from neighboring agents. Performance was evaluated against a baseline fixed-time controller using metrics such as average vehicle wait time and overall throughput. The MARL approach demonstrated statistically significant improvements, including reduced average waiting times and improved throughput. These findings suggest that MARL-based dynamic control strategies hold substantial promise for improving urban traffic management efficiency. More research is recommended to address scalability and real-world implementation challenges.
<div id='section'>Paperid: <span id='pid'>1560, <a href='https://arxiv.org/pdf/2505.09756.pdf' target='_blank'>https://arxiv.org/pdf/2505.09756.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhaoyang Shi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.09756">Community-based Multi-Agent Reinforcement Learning with Transfer and Active Exploration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a new framework for multi-agent reinforcement learning (MARL), where the agents cooperate in a time-evolving network with latent community structures and mixed memberships. Unlike traditional neighbor-based or fixed interaction graphs, our community-based framework captures flexible and abstract coordination patterns by allowing each agent to belong to multiple overlapping communities. Each community maintains shared policy and value functions, which are aggregated by individual agents according to personalized membership weights. We also design actor-critic algorithms that exploit this structure: agents inherit community-level estimates for policy updates and value learning, enabling structured information sharing without requiring access to other agents' policies. Importantly, our approach supports both transfer learning by adapting to new agents or tasks via membership estimation, and active learning by prioritizing uncertain communities during exploration. Theoretically, we establish convergence guarantees under linear function approximation for both actor and critic updates. To our knowledge, this is the first MARL framework that integrates community structure, transferability, and active learning with provable guarantees.
<div id='section'>Paperid: <span id='pid'>1561, <a href='https://arxiv.org/pdf/2505.02215.pdf' target='_blank'>https://arxiv.org/pdf/2505.02215.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mannan Bhardwaj
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.02215">Interpretable Emergent Language Using Inter-Agent Transformers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper explores the emergence of language in multi-agent reinforcement learning (MARL) using transformers. Existing methods such as RIAL, DIAL, and CommNet enable agent communication but lack interpretability. We propose Differentiable Inter-Agent Transformers (DIAT), which leverage self-attention to learn symbolic, human-understandable communication protocols. Through experiments, DIAT demonstrates the ability to encode observations into interpretable vocabularies and meaningful embeddings, effectively solving cooperative tasks. These results highlight the potential of DIAT for interpretable communication in complex multi-agent environments.
<div id='section'>Paperid: <span id='pid'>1562, <a href='https://arxiv.org/pdf/2502.16449.pdf' target='_blank'>https://arxiv.org/pdf/2502.16449.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoran Su
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.16449">Facilitating Emergency Vehicle Passage in Congested Urban Areas Using Multi-agent Deep Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Emergency Response Time (ERT) is crucial for urban safety, measuring cities' ability to handle medical, fire, and crime emergencies. In NYC, medical ERT increased 72% from 7.89 minutes in 2014 to 14.27 minutes in 2024, with half of delays due to Emergency Vehicle (EMV) travel times. Each minute's delay in stroke response costs 2 million brain cells, while cardiac arrest survival drops 7-10% per minute.
  This dissertation advances EMV facilitation through three contributions. First, EMVLight, a decentralized multi-agent reinforcement learning framework, integrates EMV routing with traffic signal pre-emption. It achieved 42.6% faster EMV travel times and 23.5% improvement for other vehicles.
  Second, the Dynamic Queue-Jump Lane system uses Multi-Agent Proximal Policy Optimization for coordinated lane-clearing in mixed autonomous and human-driven traffic, reducing EMV travel times by 40%.
  Third, an equity study of NYC Emergency Medical Services revealed disparities across boroughs: Staten Island faces delays due to sparse signalized intersections, while Manhattan struggles with congestion. Solutions include optimized EMS stations and improved intersection designs.
  These contributions enhance EMV mobility and emergency service equity, offering insights for policymakers and urban planners to develop safer, more efficient transportation systems.
<div id='section'>Paperid: <span id='pid'>1563, <a href='https://arxiv.org/pdf/2501.15495.pdf' target='_blank'>https://arxiv.org/pdf/2501.15495.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alberto Castagna
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.15495">Expert-Free Online Transfer Learning in Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reinforcement Learning (RL) enables an intelligent agent to optimise its performance in a task by continuously taking action from an observed state and receiving a feedback from the environment in form of rewards. RL typically uses tables or linear approximators to map state-action tuples that maximises the reward. Combining RL with deep neural networks (DRL) significantly increases its scalability and enables it to address more complex problems than before. However, DRL also inherits downsides from both RL and deep learning. Despite DRL improves generalisation across similar state-action pairs when compared to simpler RL policy representations like tabular methods, it still requires the agent to adequately explore the state-action space. Additionally, deep methods require more training data, with the volume of data escalating with the complexity and size of the neural network. As a result, deep RL requires a long time to collect enough agent-environment samples and to successfully learn the underlying policy. Furthermore, often even a slight alteration to the task invalidates any previous acquired knowledge. To address these shortcomings, Transfer Learning (TL) has been introduced, which enables the use of external knowledge from other tasks or agents to enhance a learning process. The goal of TL is to reduce the learning complexity for an agent dealing with an unfamiliar task by simplifying the exploration process. This is achieved by lowering the amount of new information required by its learning model, resulting in a reduced overall convergence time...
<div id='section'>Paperid: <span id='pid'>1564, <a href='https://arxiv.org/pdf/2501.03187.pdf' target='_blank'>https://arxiv.org/pdf/2501.03187.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dennis Gross
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.03187">Turn-based Multi-Agent Reinforcement Learning Model Checking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we propose a novel approach for verifying the compliance of turn-based multi-agent reinforcement learning (TMARL) agents with complex requirements in stochastic multiplayer games. Our method overcomes the limitations of existing verification approaches, which are inadequate for dealing with TMARL agents and not scalable to large games with multiple agents. Our approach relies on tight integration of TMARL and a verification technique referred to as model checking. We demonstrate the effectiveness and scalability of our technique through experiments in different types of environments. Our experiments show that our method is suited to verify TMARL agents and scales better than naive monolithic model checking.
<div id='section'>Paperid: <span id='pid'>1565, <a href='https://arxiv.org/pdf/2501.00165.pdf' target='_blank'>https://arxiv.org/pdf/2501.00165.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ben McClusky
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.00165">Dynamic Graph Communication for Decentralised Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work presents a novel communication framework for decentralized multi-agent systems operating in dynamic network environments. Integrated into a multi-agent reinforcement learning system, the framework is designed to enhance decision-making by optimizing the network's collective knowledge through efficient communication. Key contributions include adapting a static network packet-routing scenario to a dynamic setting with node failures, incorporating a graph attention network layer in a recurrent message-passing framework, and introducing a multi-round communication targeting mechanism. This approach enables an attention-based aggregation mechanism to be successfully trained within a sparse-reward, dynamic network packet-routing environment using only reinforcement learning. Experimental results show improvements in routing performance, including a 9.5 percent increase in average rewards and a 6.4 percent reduction in communication overhead compared to a baseline system. The study also examines the ethical and legal implications of deploying such systems in critical infrastructure and military contexts, identifies current limitations, and suggests potential directions for future research.
<div id='section'>Paperid: <span id='pid'>1566, <a href='https://arxiv.org/pdf/2412.21088.pdf' target='_blank'>https://arxiv.org/pdf/2412.21088.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Reza Azadeh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.21088">Advances in Multi-agent Reinforcement Learning: Persistent Autonomy and Robot Learning Lab Report 2024</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-Agent Reinforcement Learning (MARL) approaches have emerged as popular solutions to address the general challenges of cooperation in multi-agent environments, where the success of achieving shared or individual goals critically depends on the coordination and collaboration between agents. However, existing cooperative MARL methods face several challenges intrinsic to multi-agent systems, such as the curse of dimensionality, non-stationarity, and the need for a global exploration strategy. Moreover, the presence of agents with constraints (e.g., limited battery life, restricted mobility) or distinct roles further exacerbates these challenges. This document provides an overview of recent advances in Multi-Agent Reinforcement Learning (MARL) conducted at the Persistent Autonomy and Robot Learning (PeARL) lab at the University of Massachusetts Lowell. We briefly discuss various research directions and present a selection of approaches proposed in our most recent publications. For each proposed approach, we also highlight potential future directions to further advance the field.
<div id='section'>Paperid: <span id='pid'>1567, <a href='https://arxiv.org/pdf/2412.20116.pdf' target='_blank'>https://arxiv.org/pdf/2412.20116.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Benedikt Valentin Meylahn
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.20116">Multi-agent reinforcement learning in the all-or-nothing public goods game on networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We study interpersonal trust by means of the all-or-nothing public goods game between agents on a network. The agents are endowed with the simple yet adaptive learning rule, exponential moving average, by which they estimate the behavior of their neighbors in the network. Theoretically we show that in the long-time limit this multi-agent reinforcement learning process always eventually results in indefinite contribution to the public good or indefinite defection (no agent contributing to the public good). However, by simulation of the pre-limit behavior, we see that on complex network structures there may be mixed states in which the process seems to stabilize before actual convergence to states in which agent beliefs and actions are all the same. In these metastable states the local network characteristics can determine whether agents have high or low trust in their neighbors. More generally it is found that more dense networks result in lower rates of contribution to the public good. This has implications for how one can spread global contribution toward a public good by enabling smaller local interactions.
<div id='section'>Paperid: <span id='pid'>1568, <a href='https://arxiv.org/pdf/2411.17724.pdf' target='_blank'>https://arxiv.org/pdf/2411.17724.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aslan S. Dizaji
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.17724">Incentives to Build Houses, Trade Houses, or Trade House Building Skills in Simulated Worlds under Various Governing Systems or Institutions: Comparing Multi-agent Reinforcement Learning to Generative Agent-based Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>It has been shown that social institutions impact human motivations to produce different behaviours, such as amount of working or specialisation in labor. With advancement in artificial intelligence (AI), specifically large language models (LLMs), now it is possible to perform in-silico simulations to test various hypotheses around this topic. Here, I simulate two somewhat similar worlds using multi-agent reinforcement learning (MARL) framework of the AI-Economist and generative agent-based model (GABM) framework of the Concordia. In the extended versions of the AI-Economist and Concordia, the agents are able to build houses, trade houses, and trade house building skill. Moreover, along the individualistic-collectivists axis, there are a set of three governing systems: Full-Libertarian, Semi-Libertarian/Utilitarian, and Full-Utilitarian. Additionally, in the extended AI-Economist, the Semi-Libertarian/Utilitarian system is further divided to a set of three governing institutions along the discriminative axis: Inclusive, Arbitrary, and Extractive. Building on these, I am able to show that among governing systems and institutions of the extended AI-Economist, under the Semi-Libertarian/Utilitarian and Inclusive government, the ratios of building houses to trading houses and trading house building skill are higher than the rest. Furthermore, I am able to show that in the extended Concordia when the central government care about equality in the society, the Full-Utilitarian system generates agents building more houses and trading more house building skill. In contrast, these economic activities are higher under the Full-Libertarian system when the central government cares about productivity in the society. Overall, the focus of this paper is to compare and contrast two advanced techniques of AI, MARL and GABM, to simulate a similar social phenomena with limitations.
<div id='section'>Paperid: <span id='pid'>1569, <a href='https://arxiv.org/pdf/2411.14496.pdf' target='_blank'>https://arxiv.org/pdf/2411.14496.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bao Nguyen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.14496">Multi-agent reinforcement learning strategy to maximize the lifetime of Wireless Rechargeable</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The thesis proposes a generalized charging framework for multiple mobile chargers to maximize the network lifetime and ensure target coverage and connectivity in large scale WRSNs. Moreover, a multi-point charging model is leveraged to enhance charging efficiency, where the MC can charge multiple sensors simultaneously at each charging location. The thesis proposes an effective Decentralized Partially Observable Semi-Markov Decision Process (Dec POSMDP) model that promotes Mobile Chargers (MCs) cooperation and detects optimal charging locations based on realtime network information. Furthermore, the proposal allows reinforcement algorithms to be applied to different networks without requiring extensive retraining. To solve the Dec POSMDP model, the thesis proposes an Asynchronous Multi Agent Reinforcement Learning algorithm (AMAPPO) based on the Proximal Policy Optimization algorithm (PPO).
<div id='section'>Paperid: <span id='pid'>1570, <a href='https://arxiv.org/pdf/2410.11642.pdf' target='_blank'>https://arxiv.org/pdf/2410.11642.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiamian Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.11642">Improve Value Estimation of Q Function and Reshape Reward with Monte Carlo Tree Search</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reinforcement learning has achieved remarkable success in perfect information games such as Go and Atari, enabling agents to compete at the highest levels against human players. However, research in reinforcement learning for imperfect information games has been relatively limited due to the more complex game structures and randomness. Traditional methods face challenges in training and improving performance in imperfect information games due to issues like inaccurate Q value estimation and reward sparsity. In this paper, we focus on Uno, an imperfect information game, and aim to address these problems by reducing Q value overestimation and reshaping reward function. We propose a novel algorithm that utilizes Monte Carlo Tree Search to average the value estimations in Q function. Even though we choose Double Deep Q Learning as the foundational framework in this paper, our method can be generalized and used in any algorithm which needs Q value estimation, such as the Actor-Critic. Additionally, we employ Monte Carlo Tree Search to reshape the reward structure in the game environment. We compare our algorithm with several traditional methods applied to games such as Double Deep Q Learning, Deep Monte Carlo and Neural Fictitious Self Play, and the experiments demonstrate that our algorithm consistently outperforms these approaches, especially as the number of players in Uno increases, indicating a higher level of difficulty.
<div id='section'>Paperid: <span id='pid'>1571, <a href='https://arxiv.org/pdf/2409.03052.pdf' target='_blank'>https://arxiv.org/pdf/2409.03052.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Christopher Amato
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.03052">An Introduction to Centralized Training for Decentralized Execution in Cooperative Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent reinforcement learning (MARL) has exploded in popularity in recent years. Many approaches have been developed but they can be divided into three main types: centralized training and execution (CTE), centralized training for decentralized execution (CTDE), and Decentralized training and execution (DTE).
  CTDE methods are the most common as they can use centralized information during training but execute in a decentralized manner -- using only information available to that agent during execution. CTDE is the only paradigm that requires a separate training phase where any available information (e.g., other agent policies, underlying states) can be used. As a result, they can be more scalable than CTE methods, do not require communication during execution, and can often perform well. CTDE fits most naturally with the cooperative case, but can be potentially applied in competitive or mixed settings depending on what information is assumed to be observed.
  This text is an introduction to CTDE in cooperative MARL. It is meant to explain the setting, basic concepts, and common methods. It does not cover all work in CTDE MARL as the subarea is quite extensive. I have included work that I believe is important for understanding the main concepts in the subarea and apologize to those that I have omitted.
<div id='section'>Paperid: <span id='pid'>1572, <a href='https://arxiv.org/pdf/2407.02613.pdf' target='_blank'>https://arxiv.org/pdf/2407.02613.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Abdelrahman Ramadan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.02613">Wildfire Autonomous Response and Prediction Using Cellular Automata (WARP-CA)</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Wildfires pose a severe challenge to ecosystems and human settlements, exacerbated by climate change and environmental factors. Traditional wildfire modeling, while useful, often fails to adapt to the rapid dynamics of such events. This report introduces the (Wildfire Autonomous Response and Prediction Using Cellular Automata) WARP-CA model, a novel approach that integrates terrain generation using Perlin noise with the dynamism of Cellular Automata (CA) to simulate wildfire spread. We explore the potential of Multi-Agent Reinforcement Learning (MARL) to manage wildfires by simulating autonomous agents, such as UAVs and UGVs, within a collaborative framework. Our methodology combines world simulation techniques and investigates emergent behaviors in MARL, focusing on efficient wildfire suppression and considering critical environmental factors like wind patterns and terrain features.
<div id='section'>Paperid: <span id='pid'>1573, <a href='https://arxiv.org/pdf/2407.00741.pdf' target='_blank'>https://arxiv.org/pdf/2407.00741.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianuo Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.00741">Diffusion Models for Offline Multi-agent Reinforcement Learning with Safety Constraints</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent advancements in Multi-agent Reinforcement Learning (MARL), its application has extended to various safety-critical scenarios. However, most methods focus on online learning, which presents substantial risks when deployed in real-world settings. Addressing this challenge, we introduce an innovative framework integrating diffusion models within the MARL paradigm. This approach notably enhances the safety of actions taken by multiple agents through risk mitigation while modeling coordinated action. Our framework is grounded in the Centralized Training with Decentralized Execution (CTDE) architecture, augmented by a Diffusion Model for prediction trajectory generation. Additionally, we incorporate a specialized algorithm to further ensure operational safety. We evaluate our model against baselines on the DSRL benchmark. Experiment results demonstrate that our model not only adheres to stringent safety constraints but also achieves superior performance compared to existing methodologies. This underscores the potential of our approach in advancing the safety and efficacy of MARL in real-world applications.
<div id='section'>Paperid: <span id='pid'>1574, <a href='https://arxiv.org/pdf/2406.01061.pdf' target='_blank'>https://arxiv.org/pdf/2406.01061.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>yonghao Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.01061">Satellites swarm cooperation for pursuit-attachment tasks with transformer-based reinforcement learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The on-orbit intelligent planning of satellites swarm has attracted increasing attention from scholars. Especially in tasks such as the pursuit and attachment of non-cooperative satellites, satellites swarm must achieve coordinated cooperation with limited resources. The study proposes a reinforcement learning framework that integrates the transformer and expert networks. Firstly, under the constraints of incomplete information about non-cooperative satellites, an implicit multi-satellites cooperation strategy was designed using a communication sharing mechanism. Subsequently, for the characteristics of the pursuit-attachment tasks, the multi-agent reinforcement learning framework is improved by introducing transformers and expert networks inspired by transfer learning ideas. To address the issue of satellites swarm scalability, sequence modelling based on transformers is utilized to craft memory-augmented policy networks, meanwhile increasing the scalability of the swarm. By comparing the convergence curves with other algorithms, it is shown that the proposed method is qualified for pursuit-attachment tasks of satellites swarm. Additionally, simulations under different maneuvering strategies of non-cooperative satellites respectively demonstrate the robustness of the algorithm and the task efficiency of the swarm system. The success rate of pursuit-attachment tasks is analyzed through Monte Carlo simulations.
<div id='section'>Paperid: <span id='pid'>1575, <a href='https://arxiv.org/pdf/2405.06161.pdf' target='_blank'>https://arxiv.org/pdf/2405.06161.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Christopher Amato
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.06161">An Initial Introduction to Cooperative Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent reinforcement learning (MARL) has exploded in popularity in recent years. While numerous approaches have been developed, they can be broadly categorized into three main types: centralized training and execution (CTE), centralized training for decentralized execution (CTDE), and decentralized training and execution (DTE). CTE methods assume centralization during training and execution (e.g., with fast, free, and perfect communication) and have the most information during execution. CTDE methods are the most common, as they leverage centralized information during training while enabling decentralized execution -- using only information available to that agent during execution. Decentralized training and execution methods make the fewest assumptions and are often simple to implement.
  This text is an introduction to cooperative MARL -- MARL in which all agents share a single, joint reward. It is meant to explain the setting, basic concepts, and common methods for the CTE, CTDE, and DTE settings. It does not cover all work in cooperative MARL as the area is quite extensive. I have included work that I believe is important for understanding the main concepts in the area and apologize to those that I have omitted. Topics include simple applications of single-agent methods to CTE as well as some more scalable methods that exploit the multi-agent structure, independent Q-learning and policy gradient methods and their extensions, as well as value function factorization methods including the well-known VDN, QMIX, and QPLEX approaches, and centralized critic methods including MADDPG, COMA, and MAPPO. I also discuss common misconceptions, the relationship between different approaches, and some open questions.
<div id='section'>Paperid: <span id='pid'>1576, <a href='https://arxiv.org/pdf/2403.02369.pdf' target='_blank'>https://arxiv.org/pdf/2403.02369.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aslan S. Dizaji
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.02369">A Multi-agent Reinforcement Learning Study of Evolution of Communication and Teaching under Libertarian and Utilitarian Governing Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Laboratory experiments have shown that communication plays an important role in solving social dilemmas. Here, by extending the AI-Economist, a mixed motive multi-agent reinforcement learning environment, I intend to find an answer to the following descriptive question: which governing system does facilitate the emergence and evolution of communication and teaching among agents? To answer this question, the AI-Economist is extended by a voting mechanism to simulate three different governing systems across individualistic-collectivistic axis, from full-libertarian to Full-Utilitarian governing systems. Moreover, the AI-Economist is further extended to include communication with possible misalignment, a variant of signalling game, by letting agents to build houses together if they are able to name mutually complement material resources by the same letter. Moreover, another extension is made to the AI-Economist to include teaching with possible misalignment, again a variant of signalling game, by letting half the agents as teachers who know how to use mutually complement material resources to build houses but are not capable of building actual houses, and the other half as students who do not have this information but are able to actually build those houses if teachers teach them. I found a strong evidence that collectivistic environment such as Full-Utilitarian system is more favourable for the emergence of communication and teaching, or more precisely, evolution of language alignment. Moreover, I found some evidence that evolution of language alignment through communication and teaching under collectivistic governing systems makes individuals more advantageously inequity averse. As a result, there is a positive correlation between evolution of language alignment and equality in the society.
<div id='section'>Paperid: <span id='pid'>1577, <a href='https://arxiv.org/pdf/2402.15324.pdf' target='_blank'>https://arxiv.org/pdf/2402.15324.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianhong Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.15324">Shapley Value Based Multi-Agent Reinforcement Learning: Theory, Method and Its Application to Energy Network</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent reinforcement learning is an area of rapid advancement in artificial intelligence and machine learning. One of the important questions to be answered is how to conduct credit assignment in a multi-agent system. There have been many schemes designed to conduct credit assignment by multi-agent reinforcement learning algorithms. Although these credit assignment schemes have been proved useful in improving the performance of multi-agent reinforcement learning, most of them are designed heuristically without a rigorous theoretic basis and therefore infeasible to understand how agents cooperate. In this thesis, we aim at investigating the foundation of credit assignment in multi-agent reinforcement learning via cooperative game theory. We first extend a game model called convex game and a payoff distribution scheme called Shapley value in cooperative game theory to Markov decision process, named as Markov convex game and Markov Shapley value respectively. We represent a global reward game as a Markov convex game under the grand coalition. As a result, Markov Shapley value can be reasonably used as a credit assignment scheme in the global reward game. Markov Shapley value possesses the following virtues: (i) efficiency; (ii) identifiability of dummy agents; (iii) reflecting the contribution and (iv) symmetry, which form the fair credit assignment. Based on Markov Shapley value, we propose three multi-agent reinforcement learning algorithms called SHAQ, SQDDPG and SMFPPO. Furthermore, we extend Markov convex game to partial observability to deal with the partially observable problems, named as partially observable Markov convex game. In application, we evaluate SQDDPG and SMFPPO on the real-world problem in energy networks.
<div id='section'>Paperid: <span id='pid'>1578, <a href='https://arxiv.org/pdf/2401.03896.pdf' target='_blank'>https://arxiv.org/pdf/2401.03896.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sunny Howard
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.03896">A Tensor Network Implementation of Multi Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently it has been shown that tensor networks (TNs) have the ability to represent the expected return of a single-agent finite Markov decision process (FMDP). The TN represents a distribution model, where all possible trajectories are considered. When extending these ideas to a multi-agent setting, distribution models suffer from the curse of dimensionality: the exponential relation between the number of possible trajectories and the number of agents. The key advantage of using TNs in this setting is that there exists a large number of established optimisation and decomposition techniques that are specific to TNs, that one can apply to ensure the most efficient representation is found. In this report, these methods are used to form a TN that represents the expected return of a multi-agent reinforcement learning (MARL) task. This model is then applied to a 2 agent random walker example, where it was shown that the policy is correctly optimised using a DMRG technique. Finally, I demonstrate the use of an exact decomposition technique, reducing the number of elements in the tensors by 97.5%, without experiencing any loss of information.
<div id='section'>Paperid: <span id='pid'>1579, <a href='https://arxiv.org/pdf/2312.11834.pdf' target='_blank'>https://arxiv.org/pdf/2312.11834.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hisato Komatsu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.11834">Multi-agent reinforcement learning using echo-state network and its application to pedestrian dynamics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, simulations of pedestrians using the multi-agent reinforcement learning (MARL) have been studied. This study considered the roads on a grid-world environment, and implemented pedestrians as MARL agents using an echo-state network and the least squares policy iteration method. Under this environment, the ability of these agents to learn to move forward by avoiding other agents was investigated. Specifically, we considered two types of tasks: the choice between a narrow direct route and a broad detour, and the bidirectional pedestrian flow in a corridor. The simulations results indicated that the learning was successful when the density of the agents was not that high.
<div id='section'>Paperid: <span id='pid'>1580, <a href='https://arxiv.org/pdf/2310.19903.pdf' target='_blank'>https://arxiv.org/pdf/2310.19903.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aslan S. Dizaji
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.19903">A Multi-agent Reinforcement Learning Study of Emergence of Social Classes out of Arbitrary Governance: The Role of Environment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>There are several theories in economics regarding the roots or causes of prosperity in a society. One of these theories or hypotheses -- named geography hypothesis -- mentions that the reason why some countries are prosperous and some others are poor is the geographical location of the countries in the world as makes their climate and environment favorable or unfavorable regarding natural resources. Another competing hypothesis states that man-made institutions particularly inclusive political institutions are the reasons why some countries are prosperous and some others are poor. On the other hand, there is a specific political theory developed for the long-term social development in Iran -- named Arbitrary Rule and Aridisolatic Society which particularly emphasizes on the role of aridity to shape arbitrary political and economical institutions in Iran, without any functional social classes in the society. In this paper, by extending the AI-Economist -- a recently developed two-level multi-agent reinforcement learning environment -- I show that when the central planner is ruling the environment by arbitrary rules, the society evolves through different paths in different environments. In the environment having band-like vertical isolated patches of natural resources, all mobile agents are equally exploited by the central planner and the central planner is also not gaining any income, while in the society having more uniformly distributed natural resources, the productivity and Maximin are higher and the society generates a heterogeneous stratified social structure. All these findings provide a partial answer to the above debate and reconcile the role of geography and political institutions on the long-term development in a region.
<div id='section'>Paperid: <span id='pid'>1581, <a href='https://arxiv.org/pdf/2306.01270.pdf' target='_blank'>https://arxiv.org/pdf/2306.01270.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shaoming Peng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.01270">Multi-Robot Path Planning Combining Heuristics and Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-robot path finding in dynamic environments is a highly challenging classic problem. In the movement process, robots need to avoid collisions with other moving robots while minimizing their travel distance. Previous methods for this problem either continuously replan paths using heuristic search methods to avoid conflicts or choose appropriate collision avoidance strategies based on learning approaches. The former may result in long travel distances due to frequent replanning, while the latter may have low learning efficiency due to low sample exploration and utilization, and causing high training costs for the model. To address these issues, we propose a path planning method, MAPPOHR, which combines heuristic search, empirical rules, and multi-agent reinforcement learning. The method consists of two layers: a real-time planner based on the multi-agent reinforcement learning algorithm, MAPPO, which embeds empirical rules in the action output layer and reward functions, and a heuristic search planner used to create a global guiding path. During movement, the heuristic search planner replans new paths based on the instructions of the real-time planner. We tested our method in 10 different conflict scenarios. The experiments show that the planning performance of MAPPOHR is better than that of existing learning and heuristic methods. Due to the utilization of empirical knowledge and heuristic search, the learning efficiency of MAPPOHR is higher than that of existing learning methods.
<div id='section'>Paperid: <span id='pid'>1582, <a href='https://arxiv.org/pdf/2305.05573.pdf' target='_blank'>https://arxiv.org/pdf/2305.05573.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Soumajyoti Sarkar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.05573">An Algorithm For Adversary Aware Decentralized Networked MARL</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Decentralized multi-agent reinforcement learning (MARL) algorithms have become popular in the literature since it allows heterogeneous agents to have their own reward functions as opposed to canonical multi-agent Markov Decision Process (MDP) settings which assume common reward functions over all agents. In this work, we follow the existing work on collaborative MARL where agents in a connected time varying network can exchange information among each other in order to reach a consensus. We introduce vulnerabilities in the consensus updates of existing MARL algorithms where agents can deviate from their usual consensus update, who we term as adversarial agents. We then proceed to provide an algorithm that allows non-adversarial agents to reach a consensus in the presence of adversaries under a constrained setting.
<div id='section'>Paperid: <span id='pid'>1583, <a href='https://arxiv.org/pdf/2304.05872.pdf' target='_blank'>https://arxiv.org/pdf/2304.05872.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Philipp Dominic Siedler
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.05872">Learning to Communicate and Collaborate in a Competitive Multi-Agent Setup to Clean the Ocean from Macroplastics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Finding a balance between collaboration and competition is crucial for artificial agents in many real-world applications. We investigate this using a Multi-Agent Reinforcement Learning (MARL) setup on the back of a high-impact problem. The accumulation and yearly growth of plastic in the ocean cause irreparable damage to many aspects of oceanic health and the marina system. To prevent further damage, we need to find ways to reduce macroplastics from known plastic patches in the ocean. Here we propose a Graph Neural Network (GNN) based communication mechanism that increases the agents' observation space. In our custom environment, agents control a plastic collecting vessel. The communication mechanism enables agents to develop a communication protocol using a binary signal. While the goal of the agent collective is to clean up as much as possible, agents are rewarded for the individual amount of macroplastics collected. Hence agents have to learn to communicate effectively while maintaining high individual performance. We compare our proposed communication mechanism with a multi-agent baseline without the ability to communicate. Results show communication enables collaboration and increases collective performance significantly. This means agents have learned the importance of communication and found a balance between collaboration and competition.
<div id='section'>Paperid: <span id='pid'>1584, <a href='https://arxiv.org/pdf/2203.06865.pdf' target='_blank'>https://arxiv.org/pdf/2203.06865.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nelson Vadori
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2203.06865">Calibration of Derivative Pricing Models: a Multi-Agent Reinforcement Learning Perspective</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>One of the most fundamental questions in quantitative finance is the existence of continuous-time diffusion models that fit market prices of a given set of options. Traditionally, one employs a mix of intuition, theoretical and empirical analysis to find models that achieve exact or approximate fits. Our contribution is to show how a suitable game theoretical formulation of this problem can help solve this question by leveraging existing developments in modern deep multi-agent reinforcement learning to search in the space of stochastic processes. Our experiments show that we are able to learn local volatility, as well as path-dependence required in the volatility process to minimize the price of a Bermudan option. Our algorithm can be seen as a particle method \textit{Ã  la} Guyon \textit{et} Henry-Labordere where particles, instead of being designed to ensure $Ï_{loc}(t,S_t)^2 = \mathbb{E}[Ï_t^2|S_t]$, are learning RL-driven agents cooperating towards more general calibration targets.
<div id='section'>Paperid: <span id='pid'>1585, <a href='https://arxiv.org/pdf/2201.02135.pdf' target='_blank'>https://arxiv.org/pdf/2201.02135.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aske Plaat
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2201.02135">Deep Reinforcement Learning, a textbook</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep reinforcement learning has gathered much attention recently. Impressive results were achieved in activities as diverse as autonomous driving, game playing, molecular recombination, and robotics. In all these fields, computer programs have taught themselves to solve difficult problems. They have learned to fly model helicopters and perform aerobatic manoeuvers such as loops and rolls. In some applications they have even become better than the best humans, such as in Atari, Go, poker and StarCraft. The way in which deep reinforcement learning explores complex environments reminds us of how children learn, by playfully trying out things, getting feedback, and trying again. The computer seems to truly possess aspects of human learning; this goes to the heart of the dream of artificial intelligence. The successes in research have not gone unnoticed by educators, and universities have started to offer courses on the subject. The aim of this book is to provide a comprehensive overview of the field of deep reinforcement learning. The book is written for graduate students of artificial intelligence, and for researchers and practitioners who wish to better understand deep reinforcement learning methods and their challenges. We assume an undergraduate-level of understanding of computer science and artificial intelligence; the programming language of this book is Python. We describe the foundations, the algorithms and the applications of deep reinforcement learning. We cover the established model-free and model-based methods that form the basis of the field. Developments go quickly, and we also cover advanced topics: deep multi-agent reinforcement learning, deep hierarchical reinforcement learning, and deep meta learning.
<div id='section'>Paperid: <span id='pid'>1586, <a href='https://arxiv.org/pdf/2106.03007.pdf' target='_blank'>https://arxiv.org/pdf/2106.03007.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shohei Ohsawa
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2106.03007">Truthful Self-Play</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a general framework for evolutionary learning to emergent unbiased state representation without any supervision. Evolutionary frameworks such as self-play converge to bad local optima in case of multi-agent reinforcement learning in non-cooperative partially observable environments with communication due to information asymmetry. Our proposed framework is a simple modification of self-play inspired by mechanism design, also known as {\em reverse game theory}, to elicit truthful signals and make the agents cooperative. The key idea is to add imaginary rewards using the peer prediction method, i.e., a mechanism for evaluating the validity of information exchanged between agents in a decentralized environment. Numerical experiments with predator prey, traffic junction and StarCraft tasks demonstrate that the state-of-the-art performance of our framework.
