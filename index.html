<!DOCTYPE html>
<html>
<head>
<title>arXiv Papers of Multi-Agent Reinforcement Learning</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<style type="text/css">


/* BODY
=============================================================================*/

body {
  font-family: Helvetica, arial, freesans, clean, sans-serif;
  font-size: 14px;
  line-height: 1.6;
  color: #333;
  background-color: #fff;
  padding: 10px 20px 10px 20px;
  max-width: 960px;
  margin: 0 auto;
}

span#pid {
  color:red;
  
}
span#filename{
  font-style: oblique;
}

span#title{
  font-family: Times New Roman, freesans, clean, sans-serif;
  font-style: italic;
  font-size: 20px;
  border:1px solid #B50;
}
span#abs{
  font-family: Times New Roman, freesans, clean, sans-serif;
  font-style: oblique;
  font-size: 18px;
}
</style>
</head>
<body><div id='title' style='font-size:1.3em; font-weight:bold;'>arXiv Papers of Multi-Agent Reinforcement Learning</div><br>
<div id='section'>Paperid: <span id='pid'>1, <a href='https://arxiv.org/pdf/2601.21972.pdf' target='_blank'>https://arxiv.org/pdf/2601.21972.pdf</a></span>   <span><a href='https://github.com/OpenMLRL/CoMLRL/releases/tag/v1.3.2' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuo Liu, Tianle Chen, Ryan Amiri, Christopher Amato
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.21972">Learning Decentralized LLM Collaboration with Multi-Agent Actor Critic</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent work has explored optimizing LLM collaboration through Multi-Agent Reinforcement Learning (MARL). However, most MARL fine-tuning approaches rely on predefined execution protocols, which often require centralized execution. Decentralized LLM collaboration is more appealing in practice, as agents can run inference in parallel with flexible deployments. Also, current approaches use Monte Carlo methods for fine-tuning, which suffer from high variance and thus require more samples to train effectively. Actor-critic methods are prevalent in MARL for dealing with these issues, so we developed Multi-Agent Actor-Critic (MAAC) methods to optimize decentralized LLM collaboration. In this paper, we analyze when and why these MAAC methods are beneficial. We propose 2 MAAC approaches, \textbf{CoLLM-CC} with a \textbf{C}entralized \textbf{C}ritic and \textbf{CoLLM-DC} with \textbf{D}ecentralized \textbf{C}ritics. Our experiments across writing, coding, and game-playing domains show that Monte Carlo methods and CoLLM-DC can achieve performance comparable to CoLLM-CC in short-horizon and dense-reward settings. However, they both underperform CoLLM-CC on long-horizon or sparse-reward tasks, where Monte Carlo methods require substantially more samples and CoLLM-DC struggles to converge. Our code is available at https://github.com/OpenMLRL/CoMLRL/releases/tag/v1.3.2.
<div id='section'>Paperid: <span id='pid'>2, <a href='https://arxiv.org/pdf/2601.10156.pdf' target='_blank'>https://arxiv.org/pdf/2601.10156.pdf</a></span>   <span><a href='https://github.com/MurrayTom/ToolSafe' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yutao Mou, Zhangchi Xue, Lijun Li, Peiyang Liu, Shikun Zhang, Wei Ye, Jing Shao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.10156">ToolSafe: Enhancing Tool Invocation Safety of LLM-based agents via Proactive Step-level Guardrail and Feedback</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While LLM-based agents can interact with environments via invoking external tools, their expanded capabilities also amplify security risks. Monitoring step-level tool invocation behaviors in real time and proactively intervening before unsafe execution is critical for agent deployment, yet remains under-explored. In this work, we first construct TS-Bench, a novel benchmark for step-level tool invocation safety detection in LLM agents. We then develop a guardrail model, TS-Guard, using multi-task reinforcement learning. The model proactively detects unsafe tool invocation actions before execution by reasoning over the interaction history. It assesses request harmfulness and action-attack correlations, producing interpretable and generalizable safety judgments and feedback. Furthermore, we introduce TS-Flow, a guardrail-feedback-driven reasoning framework for LLM agents, which reduces harmful tool invocations of ReAct-style agents by 65 percent on average and improves benign task completion by approximately 10 percent under prompt injection attacks.
<div id='section'>Paperid: <span id='pid'>3, <a href='https://arxiv.org/pdf/2512.17444.pdf' target='_blank'>https://arxiv.org/pdf/2512.17444.pdf</a></span>   <span><a href='https://github.com/jjgonzalez2491/MARLEY_V1' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Javier Gonzalez-Ruiz, Carlos Rodriguez-Pardo, Iacopo Savelli, Alice Di Bella, Massimo Tavoni
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.17444">Assessing Long-Term Electricity Market Design for Ambitious Decarbonization Targets using Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Electricity systems are key to transforming today's society into a carbon-free economy. Long-term electricity market mechanisms, including auctions, support schemes, and other policy instruments, are critical in shaping the electricity generation mix. In light of the need for more advanced tools to support policymakers and other stakeholders in designing, testing, and evaluating long-term markets, this work presents a multi-agent reinforcement learning model capable of capturing the key features of decarbonizing energy systems. Profit-maximizing generation companies make investment decisions in the wholesale electricity market, responding to system needs, competitive dynamics, and policy signals. The model employs independent proximal policy optimization, which was selected for suitability to the decentralized and competitive environment. Nevertheless, given the inherent challenges of independent learning in multi-agent settings, an extensive hyperparameter search ensures that decentralized training yields market outcomes consistent with competitive behavior. The model is applied to a stylized version of the Italian electricity system and tested under varying levels of competition, market designs, and policy scenarios. Results highlight the critical role of market design for decarbonizing the electricity sector and avoiding price volatility. The proposed framework allows assessing long-term electricity markets in which multiple policy and market mechanisms interact simultaneously, with market participants responding and adapting to decarbonization pathways.
<div id='section'>Paperid: <span id='pid'>4, <a href='https://arxiv.org/pdf/2512.16444.pdf' target='_blank'>https://arxiv.org/pdf/2512.16444.pdf</a></span>   <span><a href='https://github.com/dooliu/SC2BA' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/dooliu/SC2BA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yadong Li, Tong Zhang, Bo Huang, Zhen Cui
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.16444">StarCraft+: Benchmarking Multi-agent Algorithms in Adversary Paradigm</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep multi-agent reinforcement learning (MARL) algorithms are booming in the field of collaborative intelligence, and StarCraft multi-agent challenge (SMAC) is widely-used as the benchmark therein. However, imaginary opponents of MARL algorithms are practically configured and controlled in a fixed built-in AI mode, which causes less diversity and versatility in algorithm evaluation. To address this issue, in this work, we establish a multi-agent algorithm-vs-algorithm environment, named StarCraft II battle arena (SC2BA), to refresh the benchmarking of MARL algorithms in an adversary paradigm. Taking StarCraft as infrastructure, the SC2BA environment is specifically created for inter-algorithm adversary with the consideration of fairness, usability and customizability, and meantime an adversarial PyMARL (APyMARL) library is developed with easy-to-use interfaces/modules. Grounding in SC2BA, we benchmark those classic MARL algorithms in two types of adversarial modes: dual-algorithm paired adversary and multi-algorithm mixed adversary, where the former conducts the adversary of pairwise algorithms while the latter focuses on the adversary to multiple behaviors from a group of algorithms. The extensive benchmark experiments exhibit some thought-provoking observations/problems in the effectivity, sensibility and scalability of these completed algorithms. The SC2BA environment as well as reproduced experiments are released in \href{https://github.com/dooliu/SC2BA}{Github}, and we believe that this work could mark a new step for the MARL field in the coming years.
<div id='section'>Paperid: <span id='pid'>5, <a href='https://arxiv.org/pdf/2512.06581.pdf' target='_blank'>https://arxiv.org/pdf/2512.06581.pdf</a></span>   <span><a href='https://yuhaosu.github.io/MedGRPO/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuhao Su, Anwesa Choudhuri, Zhongpai Gao, Benjamin Planche, Van Nguyen Nguyen, Meng Zheng, Yuhan Shen, Arun Innanje, Terrence Chen, Ehsan Elhamifar, Ziyan Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.06581">MedGRPO: Multi-Task Reinforcement Learning for Heterogeneous Medical Video Understanding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large vision-language models struggle with medical video understanding, where spatial precision, temporal reasoning, and clinical semantics are critical. To address this, we first introduce \textbf{MedVidBench}, a large-scale benchmark of 531,850 video-instruction pairs across 8 medical sources spanning video, segment, and frame-level tasks, curated through a rigorous quality assurance pipeline with expert-guided prompting and dual-model validation. While supervised fine-tuning on MedVidBench yields noticeable gains, standard Reinforcement Learning (RL) fails due to imbalanced reward scales across datasets, which destabilizes optimization and leads to training collapse. To overcome this, we introduce \textbf{MedGRPO}, a novel RL framework for balanced multi-dataset training with two key innovations: (1) \emph{cross-dataset reward normalization} that maps each dataset's median performance to a common reward value, ensuring fair optimization regardless of difficulty, and (2) a \emph{medical LLM judge} that evaluates caption quality on five clinical dimensions through comparative similarity scoring. Supervised fine-tuning Qwen2.5-VL-7B on MedVidBench substantially outperforms GPT-4.1 and Gemini-2.5-Flash across all tasks, demonstrating MedVidBench's efficacy, while our MedGRPO framework further improves upon the SFT baseline across grounding and captioning tasks. Our work establishes a foundational benchmark and robust training methodology for advancing vision-language models in medical domains. Our project website is available at https://yuhaosu.github.io/MedGRPO/.
<div id='section'>Paperid: <span id='pid'>6, <a href='https://arxiv.org/pdf/2512.04463.pdf' target='_blank'>https://arxiv.org/pdf/2512.04463.pdf</a></span>   <span><a href='https://pallman14.github.io/MARL-QMIX-Warehouse-Robots/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Price Allman, Lian Thang, Dre Simmons, Salmon Riaz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.04463">MARL Warehouse Robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a comparative study of multi-agent reinforcement learning (MARL) algorithms for cooperative warehouse robotics. We evaluate QMIX and IPPO on the Robotic Warehouse (RWARE) environment and a custom Unity 3D simulation. Our experiments reveal that QMIX's value decomposition significantly outperforms independent learning approaches (achieving 3.25 mean return vs. 0.38 for advanced IPPO), but requires extensive hyperparameter tuning -- particularly extended epsilon annealing (5M+ steps) for sparse reward discovery. We demonstrate successful deployment in Unity ML-Agents, achieving consistent package delivery after 1M training steps. While MARL shows promise for small-scale deployments (2-4 robots), significant scaling challenges remain. Code and analyses: https://pallman14.github.io/MARL-QMIX-Warehouse-Robots/
<div id='section'>Paperid: <span id='pid'>7, <a href='https://arxiv.org/pdf/2511.10030.pdf' target='_blank'>https://arxiv.org/pdf/2511.10030.pdf</a></span>   <span><a href='https://github.com/LAMDA-RL/MAICC' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tao Jiang, Zichuan Lin, Lihe Li, Yi-Chen Li, Cong Guan, Lei Yuan, Zongzhang Zhang, Yang Yu, Deheng Ye
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.10030">Multi-agent In-context Coordination via Decentralized Memory Retrieval</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large transformer models, trained on diverse datasets, have demonstrated impressive few-shot performance on previously unseen tasks without requiring parameter updates. This capability has also been explored in Reinforcement Learning (RL), where agents interact with the environment to retrieve context and maximize cumulative rewards, showcasing strong adaptability in complex settings. However, in cooperative Multi-Agent Reinforcement Learning (MARL), where agents must coordinate toward a shared goal, decentralized policy deployment can lead to mismatches in task alignment and reward assignment, limiting the efficiency of policy adaptation. To address this challenge, we introduce Multi-agent In-context Coordination via Decentralized Memory Retrieval (MAICC), a novel approach designed to enhance coordination by fast adaptation. Our method involves training a centralized embedding model to capture fine-grained trajectory representations, followed by decentralized models that approximate the centralized one to obtain team-level task information. Based on the learned embeddings, relevant trajectories are retrieved as context, which, combined with the agents' current sub-trajectories, inform decision-making. During decentralized execution, we introduce a novel memory mechanism that effectively balances test-time online data with offline memory. Based on the constructed memory, we propose a hybrid utility score that incorporates both individual- and team-level returns, ensuring credit assignment across agents. Extensive experiments on cooperative MARL benchmarks, including Level-Based Foraging (LBF) and SMAC (v1/v2), show that MAICC enables faster adaptation to unseen tasks compared to existing methods. Code is available at https://github.com/LAMDA-RL/MAICC.
<div id='section'>Paperid: <span id='pid'>8, <a href='https://arxiv.org/pdf/2511.08832.pdf' target='_blank'>https://arxiv.org/pdf/2511.08832.pdf</a></span>   <span><a href='https://github.com/Nikunj-Gupta/tiger-marl' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Nikunj Gupta, Ludwika Twardecka, James Zachary Hare, Jesse Milzman, Rajgopal Kannan, Viktor Prasanna
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.08832">TIGER-MARL: Enhancing Multi-Agent Reinforcement Learning with Temporal Information through Graph-based Embeddings and Representations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we propose capturing and utilizing \textit{Temporal Information through Graph-based Embeddings and Representations} or \textbf{TIGER} to enhance multi-agent reinforcement learning (MARL). We explicitly model how inter-agent coordination structures evolve over time. While most MARL approaches rely on static or per-step relational graphs, they overlook the temporal evolution of interactions that naturally arise as agents adapt, move, or reorganize cooperation strategies. Capturing such evolving dependencies is key to achieving robust and adaptive coordination. To this end, TIGER constructs dynamic temporal graphs of MARL agents, connecting their current and historical interactions. It then employs a temporal attention-based encoder to aggregate information across these structural and temporal neighborhoods, yielding time-aware agent embeddings that guide cooperative policy learning. Through extensive experiments on two coordination-intensive benchmarks, we show that TIGER consistently outperforms diverse value-decomposition and graph-based MARL baselines in task performance and sample efficiency. Furthermore, we conduct comprehensive ablation studies to isolate the impact of key design parameters in TIGER, revealing how structural and temporal factors can jointly shape effective policy learning in MARL. All codes can be found here: https://github.com/Nikunj-Gupta/tiger-marl.
<div id='section'>Paperid: <span id='pid'>9, <a href='https://arxiv.org/pdf/2511.07071.pdf' target='_blank'>https://arxiv.org/pdf/2511.07071.pdf</a></span>   <span><a href='https://github.com/Nerozud/dl_reference_models' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/Nerozud/FTS_simpel' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Marcel Müller
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.07071">Multi-Agent Reinforcement Learning for Deadlock Handling among Autonomous Mobile Robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This dissertation explores the application of multi-agent reinforcement learning (MARL) for handling deadlocks in intralogistics systems that rely on autonomous mobile robots (AMRs). AMRs enhance operational flexibility but also increase the risk of deadlocks, which degrade system throughput and reliability. Existing approaches often neglect deadlock handling in the planning phase and rely on rigid control rules that cannot adapt to dynamic operational conditions. To address these shortcomings, this work develops a structured methodology for integrating MARL into logistics planning and operational control. It introduces reference models that explicitly consider deadlock-capable multi-agent pathfinding (MAPF) problems, enabling systematic evaluation of MARL strategies. Using grid-based environments and an external simulation software, the study compares traditional deadlock handling strategies with MARL-based solutions, focusing on PPO and IMPALA algorithms under different training and execution modes. Findings reveal that MARL-based strategies, particularly when combined with centralized training and decentralized execution (CTDE), outperform rule-based methods in complex, congested environments. In simpler environments or those with ample spatial freedom, rule-based methods remain competitive due to their lower computational demands. These results highlight that MARL provides a flexible and scalable solution for deadlock handling in dynamic intralogistics scenarios, but requires careful tailoring to the operational context.
<div id='section'>Paperid: <span id='pid'>10, <a href='https://arxiv.org/pdf/2511.02136.pdf' target='_blank'>https://arxiv.org/pdf/2511.02136.pdf</a></span>   <span><a href='https://github.com/vmohl/JaxMARL-HFT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Valentin Mohl, Sascha Frey, Reuben Leyland, Kang Li, George Nigmatulin, Mihai Cucuringu, Stefan Zohren, Jakob Foerster, Anisoara Calinescu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.02136">JaxMARL-HFT: GPU-Accelerated Large-Scale Multi-Agent Reinforcement Learning for High-Frequency Trading</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Agent-based modelling (ABM) approaches for high-frequency financial markets are difficult to calibrate and validate, partly due to the large parameter space created by defining fixed agent policies. Multi-agent reinforcement learning (MARL) enables more realistic agent behaviour and reduces the number of free parameters, but the heavy computational cost has so far limited research efforts. To address this, we introduce JaxMARL-HFT (JAX-based Multi-Agent Reinforcement Learning for High-Frequency Trading), the first GPU-accelerated open-source multi-agent reinforcement learning environment for high-frequency trading (HFT) on market-by-order (MBO) data. Extending the JaxMARL framework and building on the JAX-LOB implementation, JaxMARL-HFT is designed to handle a heterogeneous set of agents, enabling diverse observation/action spaces and reward functions. It is designed flexibly, so it can also be used for single-agent RL, or extended to act as an ABM with fixed-policy agents. Leveraging JAX enables up to a 240x reduction in end-to-end training time, compared with state-of-the-art reference implementations on the same hardware. This significant speed-up makes it feasible to exploit the large, granular datasets available in high-frequency trading, and to perform the extensive hyperparameter sweeps required for robust and efficient MARL research in trading. We demonstrate the use of JaxMARL-HFT with independent Proximal Policy Optimization (IPPO) for a two-player environment, with an order execution and a market making agent, using one year of LOB data (400 million orders), and show that these agents learn to outperform standard benchmarks. The code for the JaxMARL-HFT framework is available on GitHub.
<div id='section'>Paperid: <span id='pid'>11, <a href='https://arxiv.org/pdf/2511.01008.pdf' target='_blank'>https://arxiv.org/pdf/2511.01008.pdf</a></span>   <span><a href='https://github.com/YangHaolin0526/MARS-SQL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Haolin Yang, Jipeng Zhang, Zhitao He, Yi R. Fung
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.01008">MARS-SQL: A multi-agent reinforcement learning framework for Text-to-SQL</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Translating natural language to SQL remains difficult for complex queries. Such queries often need environmental interaction and self-correction. To address this, we introduce MARS-SQL, a novel multi-agent framework that combines principled task decomposition and interactive reinforcement learning (RL). Our system comprises three specialized agents: a Grounding Agent for schema linking, a Generation Agent for query generation, and a Validation Agent for final selection. The core of our framework is the Generation agent, which is trained via a multi-turn RL policy. Adopting a ReAct-style Think-Act-Observe loop, the agent iteratively generates thoughts, executes SQL actions against a live database, and revises its strategy based on execution feedback, enabling dynamic, stateful reasoning and self-correction. At inference time, we generate multiple interaction trajectories to explore diverse reasoning paths. The Validation agent, then selects the optimal trajectory by modeling verification as a next-token prediction task and choosing the solution with the highest generation probability. This structured workflow pipelines specialized agents. It combines interactive RL for generation with generative modeling for verification. The approach proves highly effective for robust and accurate SQL generation. Experiments show that MARS-SQL achieves state-of-the-art Execution Accuracy of 77.84% on the BIRD dev set and 89.75% on the Spider test set. Our code is available at https://github.com/YangHaolin0526/MARS-SQL.
<div id='section'>Paperid: <span id='pid'>12, <a href='https://arxiv.org/pdf/2510.26089.pdf' target='_blank'>https://arxiv.org/pdf/2510.26089.pdf</a></span>   <span><a href='https://github.com/Arianhgh/HHAN' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Fazel Arasteh, Arian Haghparast, Manos Papagelis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.26089">Network-Constrained Policy Optimization for Adaptive Multi-agent Vehicle Routing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Traffic congestion in urban road networks leads to longer trip times and higher emissions, especially during peak periods. While the Shortest Path First (SPF) algorithm is optimal for a single vehicle in a static network, it performs poorly in dynamic, multi-vehicle settings, often worsening congestion by routing all vehicles along identical paths. We address dynamic vehicle routing through a multi-agent reinforcement learning (MARL) framework for coordinated, network-aware fleet navigation. We first propose Adaptive Navigation (AN), a decentralized MARL model where each intersection agent provides routing guidance based on (i) local traffic and (ii) neighborhood state modeled using Graph Attention Networks (GAT). To improve scalability in large networks, we further propose Hierarchical Hub-based Adaptive Navigation (HHAN), an extension of AN that assigns agents only to key intersections (hubs). Vehicles are routed hub-to-hub under agent control, while SPF handles micro-routing within each hub region. For hub coordination, HHAN adopts centralized training with decentralized execution (CTDE) under the Attentive Q-Mixing (A-QMIX) framework, which aggregates asynchronous vehicle decisions via attention. Hub agents use flow-aware state features that combine local congestion and predictive dynamics for proactive routing. Experiments on synthetic grids and real urban maps (Toronto, Manhattan) show that AN reduces average travel time versus SPF and learning baselines, maintaining 100% routing success. HHAN scales to networks with hundreds of intersections, achieving up to 15.9% improvement under heavy traffic. These findings highlight the potential of network-constrained MARL for scalable, coordinated, and congestion-aware routing in intelligent transportation systems.
<div id='section'>Paperid: <span id='pid'>13, <a href='https://arxiv.org/pdf/2510.23535.pdf' target='_blank'>https://arxiv.org/pdf/2510.23535.pdf</a></span>   <span><a href='https://github.com/lamda-bbo/seq-madac' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chen Lu, Ke Xue, Lei Yuan, Yao Wang, Yaoyuan Wang, Sheng Fu, Chao Qian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.23535">Sequential Multi-Agent Dynamic Algorithm Configuration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Dynamic algorithm configuration (DAC) is a recent trend in automated machine learning, which can dynamically adjust the algorithm's configuration during the execution process and relieve users from tedious trial-and-error tuning tasks. Recently, multi-agent reinforcement learning (MARL) approaches have improved the configuration of multiple heterogeneous hyperparameters, making various parameter configurations for complex algorithms possible. However, many complex algorithms have inherent inter-dependencies among multiple parameters (e.g., determining the operator type first and then the operator's parameter), which are, however, not considered in previous approaches, thus leading to sub-optimal results. In this paper, we propose the sequential multi-agent DAC (Seq-MADAC) framework to address this issue by considering the inherent inter-dependencies of multiple parameters. Specifically, we propose a sequential advantage decomposition network, which can leverage action-order information through sequential advantage decomposition. Experiments from synthetic functions to the configuration of multi-objective optimization algorithms demonstrate Seq-MADAC's superior performance over state-of-the-art MARL methods and show strong generalization across problem classes. Seq-MADAC establishes a new paradigm for the widespread dependency-aware automated algorithm configuration. Our code is available at https://github.com/lamda-bbo/seq-madac.
<div id='section'>Paperid: <span id='pid'>14, <a href='https://arxiv.org/pdf/2510.22740.pdf' target='_blank'>https://arxiv.org/pdf/2510.22740.pdf</a></span>   <span><a href='https://github.com/herolab-uga/policies-over-poses' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sai Krishna Ghanta, Ramviyas Parasuraman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.22740">Policies over Poses: Reinforcement Learning based Distributed Pose-Graph Optimization for Multi-Robot SLAM</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We consider the distributed pose-graph optimization (PGO) problem, which is fundamental in accurate trajectory estimation in multi-robot simultaneous localization and mapping (SLAM). Conventional iterative approaches linearize a highly non-convex optimization objective, requiring repeated solving of normal equations, which often converge to local minima and thus produce suboptimal estimates. We propose a scalable, outlier-robust distributed planar PGO framework using Multi-Agent Reinforcement Learning (MARL). We cast distributed PGO as a partially observable Markov game defined on local pose-graphs, where each action refines a single edge's pose estimate. A graph partitioner decomposes the global pose graph, and each robot runs a recurrent edge-conditioned Graph Neural Network (GNN) encoder with adaptive edge-gating to denoise noisy edges. Robots sequentially refine poses through a hybrid policy that utilizes prior action memory and graph embeddings. After local graph correction, a consensus scheme reconciles inter-robot disagreements to produce a globally consistent estimate. Our extensive evaluations on a comprehensive suite of synthetic and real-world datasets demonstrate that our learned MARL-based actors reduce the global objective by an average of 37.5% more than the state-of-the-art distributed PGO framework, while enhancing inference efficiency by at least 6X. We also demonstrate that actor replication allows a single learned policy to scale effortlessly to substantially larger robot teams without any retraining. Code is publicly available at https://github.com/herolab-uga/policies-over-poses.
<div id='section'>Paperid: <span id='pid'>15, <a href='https://arxiv.org/pdf/2510.18258.pdf' target='_blank'>https://arxiv.org/pdf/2510.18258.pdf</a></span>   <span><a href='https://github.com/jianke0604/NTKMTL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaohan Qin, Xiaoxing Wang, Ning Liao, Junchi Yan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.18258">NTKMTL: Mitigating Task Imbalance in Multi-Task Learning from Neural Tangent Kernel Perspective</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-Task Learning (MTL) enables a single model to learn multiple tasks simultaneously, leveraging knowledge transfer among tasks for enhanced generalization, and has been widely applied across various domains. However, task imbalance remains a major challenge in MTL. Although balancing the convergence speeds of different tasks is an effective approach to address this issue, it is highly challenging to accurately characterize the training dynamics and convergence speeds of multiple tasks within the complex MTL system. To this end, we attempt to analyze the training dynamics in MTL by leveraging Neural Tangent Kernel (NTK) theory and propose a new MTL method, NTKMTL. Specifically, we introduce an extended NTK matrix for MTL and adopt spectral analysis to balance the convergence speeds of multiple tasks, thereby mitigating task imbalance. Based on the approximation via shared representation, we further propose NTKMTL-SR, achieving training efficiency while maintaining competitive performance. Extensive experiments demonstrate that our methods achieve state-of-the-art performance across a wide range of benchmarks, including both multi-task supervised learning and multi-task reinforcement learning. Source code is available at https://github.com/jianke0604/NTKMTL.
<div id='section'>Paperid: <span id='pid'>16, <a href='https://arxiv.org/pdf/2510.11824.pdf' target='_blank'>https://arxiv.org/pdf/2510.11824.pdf</a></span>   <span><a href='https://github.com/BUAA-TrustworthyMARL/adv_marl_benchmark' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Simin Li, Zihao Mao, Hanxiao Li, Zonglei Jing, Zhuohang bian, Jun Guo, Li Wang, Zhuoran Han, Ruixiao Xu, Xin Yu, Chengdong Ma, Yuqing Ma, Bo An, Yaodong Yang, Weifeng Lv, Xianglong Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.11824">Empirical Study on Robustness and Resilience in Cooperative Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In cooperative Multi-Agent Reinforcement Learning (MARL), it is a common practice to tune hyperparameters in ideal simulated environments to maximize cooperative performance. However, policies tuned for cooperation often fail to maintain robustness and resilience under real-world uncertainties. Building trustworthy MARL systems requires a deep understanding of robustness, which ensures stability under uncertainties, and resilience, the ability to recover from disruptions--a concept extensively studied in control systems but largely overlooked in MARL. In this paper, we present a large-scale empirical study comprising over 82,620 experiments to evaluate cooperation, robustness, and resilience in MARL across 4 real-world environments, 13 uncertainty types, and 15 hyperparameters. Our key findings are: (1) Under mild uncertainty, optimizing cooperation improves robustness and resilience, but this link weakens as perturbations intensify. Robustness and resilience also varies by algorithm and uncertainty type. (2) Robustness and resilience do not generalize across uncertainty modalities or agent scopes: policies robust to action noise for all agents may fail under observation noise on a single agent. (3) Hyperparameter tuning is critical for trustworthy MARL: surprisingly, standard practices like parameter sharing, GAE, and PopArt can hurt robustness, while early stopping, high critic learning rates, and Leaky ReLU consistently help. By optimizing hyperparameters only, we observe substantial improvement in cooperation, robustness and resilience across all MARL backbones, with the phenomenon also generalizing to robust MARL methods across these backbones. Code and results available at https://github.com/BUAA-TrustworthyMARL/adv_marl_benchmark .
<div id='section'>Paperid: <span id='pid'>17, <a href='https://arxiv.org/pdf/2510.11062.pdf' target='_blank'>https://arxiv.org/pdf/2510.11062.pdf</a></span>   <span><a href='https://github.com/pettingllms-ai/PettingLLMs' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yujie Zhao, Lanxiang Hu, Yang Wang, Minmin Hou, Hao Zhang, Ke Ding, Jishen Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.11062">Stronger-MAS: Multi-Agent Reinforcement Learning for Collaborative LLMs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent systems (MAS) and reinforcement learning (RL) are widely used to enhance the agentic capabilities of large language models (LLMs). MAS improves task performance through role-based orchestration, while RL uses environmental rewards to learn stronger policies, such as GRPO-style optimization. However, applying on-policy RL to MAS remains underexplored and presents unique challenges. Algorithmically, standard GRPO grouping assumptions break down because prompts vary by role and by turn. System-wise, the training stack must support MAS-workflow rollouts and on-policy updates for both single-policy and multi-policy models. We propose AT-GRPO, which includes (i) an agent- and turn-wise grouped RL algorithm tailored to MAS and (ii) a training system that supports both single- and multi-policy regimes. Across game, planning, coding, and math tasks, AT-GRPO delivers substantial gains. On long-horizon planning, it increases accuracy from a 14.0 to 47.0 percent single-agent RL baseline to 96.0 to 99.5 percent. It also improves reasoning performance, with average gains of 3.87 to 7.62 percent on coding tasks and 9.0 to 17.93 percent on math. Code and environments are available at: https://github.com/pettingllms-ai/PettingLLMs.
<div id='section'>Paperid: <span id='pid'>18, <a href='https://arxiv.org/pdf/2510.04508.pdf' target='_blank'>https://arxiv.org/pdf/2510.04508.pdf</a></span>   <span><a href='https://github.com/xiewilliams/MARCO' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Lili Xie, Yi Zhang, Ruihong Qiu, Jiajun Liu, Sen Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.04508">MARCO: A Cooperative Knowledge Transfer Framework for Personalized Cross-domain Recommendations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recommender systems frequently encounter data sparsity issues, particularly when addressing cold-start scenarios involving new users or items. Multi-source cross-domain recommendation (CDR) addresses these challenges by transferring valuable knowledge from multiple source domains to enhance recommendations in a target domain. However, existing reinforcement learning (RL)-based CDR methods typically rely on a single-agent framework, leading to negative transfer issues caused by inconsistent domain contributions and inherent distributional discrepancies among source domains. To overcome these limitations, MARCO, a Multi-Agent Reinforcement Learning-based Cross-Domain recommendation framework, is proposed. It leverages cooperative multi-agent reinforcement learning, where each agent is dedicated to estimating the contribution from an individual source domain, effectively managing credit assignment and mitigating negative transfer. In addition, an entropy-based action diversity penalty is introduced to enhance policy expressiveness and stabilize training by encouraging diverse agents' joint actions. Extensive experiments across four benchmark datasets demonstrate MARCO's superior performance over state-of-the-art methods, highlighting its robustness and strong generalization capabilities. The code is at https://github.com/xiewilliams/MARCO.
<div id='section'>Paperid: <span id='pid'>19, <a href='https://arxiv.org/pdf/2510.03257.pdf' target='_blank'>https://arxiv.org/pdf/2510.03257.pdf</a></span>   <span><a href='https://github.com/RS2002/Triple-BERT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zijian Zhao, Sen Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.03257">Triple-BERT: Do We Really Need MARL for Order Dispatch on Ride-Sharing Platforms?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>On-demand ride-sharing platforms, such as Uber and Lyft, face the intricate real-time challenge of bundling and matching passengers-each with distinct origins and destinations-to available vehicles, all while navigating significant system uncertainties. Due to the extensive observation space arising from the large number of drivers and orders, order dispatching, though fundamentally a centralized task, is often addressed using Multi-Agent Reinforcement Learning (MARL). However, independent MARL methods fail to capture global information and exhibit poor cooperation among workers, while Centralized Training Decentralized Execution (CTDE) MARL methods suffer from the curse of dimensionality. To overcome these challenges, we propose Triple-BERT, a centralized Single Agent Reinforcement Learning (MARL) method designed specifically for large-scale order dispatching on ride-sharing platforms. Built on a variant TD3, our approach addresses the vast action space through an action decomposition strategy that breaks down the joint action probability into individual driver action probabilities. To handle the extensive observation space, we introduce a novel BERT-based network, where parameter reuse mitigates parameter growth as the number of drivers and orders increases, and the attention mechanism effectively captures the complex relationships among the large pool of driver and orders. We validate our method using a real-world ride-hailing dataset from Manhattan. Triple-BERT achieves approximately an 11.95% improvement over current state-of-the-art methods, with a 4.26% increase in served orders and a 22.25% reduction in pickup times. Our code, trained model parameters, and processed data are publicly available at the repository https://github.com/RS2002/Triple-BERT .
<div id='section'>Paperid: <span id='pid'>20, <a href='https://arxiv.org/pdf/2510.01264.pdf' target='_blank'>https://arxiv.org/pdf/2510.01264.pdf</a></span>   <span><a href='https://github.com/DIRECTLab/IsaacLab-HARL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Isaac Peterson, Christopher Allred, Jacob Morrey, Mario Harper
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.01264">A Framework for Scalable Heterogeneous Multi-Agent Adversarial Reinforcement Learning in IsaacLab</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-Agent Reinforcement Learning (MARL) is central to robotic systems cooperating in dynamic environments. While prior work has focused on these collaborative settings, adversarial interactions are equally critical for real-world applications such as pursuit-evasion, security, and competitive manipulation. In this work, we extend the IsaacLab framework to support scalable training of adversarial policies in high-fidelity physics simulations. We introduce a suite of adversarial MARL environments featuring heterogeneous agents with asymmetric goals and capabilities. Our platform integrates a competitive variant of Heterogeneous Agent Reinforcement Learning with Proximal Policy Optimization (HAPPO), enabling efficient training and evaluation under adversarial dynamics. Experiments across several benchmark scenarios demonstrate the framework's ability to model and train robust policies for morphologically diverse multi-agent competition while maintaining high throughput and simulation realism. Code and benchmarks are available at: https://github.com/DIRECTLab/IsaacLab-HARL .
<div id='section'>Paperid: <span id='pid'>21, <a href='https://arxiv.org/pdf/2509.25550.pdf' target='_blank'>https://arxiv.org/pdf/2509.25550.pdf</a></span>   <span><a href='https://dongsuleetech.github.io/projects/IWoL/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Dongsu Lee, Daehee Lee, Yaru Niu, Honguk Woo, Amy Zhang, Ding Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.25550">Learning to Interact in World Latent for Team Coordination</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work presents a novel representation learning framework, interactive world latent (IWoL), to facilitate team coordination in multi-agent reinforcement learning (MARL). Building effective representation for team coordination is a challenging problem, due to the intricate dynamics emerging from multi-agent interaction and incomplete information induced by local observations. Our key insight is to construct a learnable representation space that jointly captures inter-agent relations and task-specific world information by directly modeling communication protocols. This representation, we maintain fully decentralized execution with implicit coordination, all while avoiding the inherent drawbacks of explicit message passing, e.g., slower decision-making, vulnerability to malicious attackers, and sensitivity to bandwidth constraints. In practice, our representation can be used not only as an implicit latent for each agent, but also as an explicit message for communication. Across four challenging MARL benchmarks, we evaluate both variants and show that IWoL provides a simple yet powerful key for team coordination. Moreover, we demonstrate that our representation can be combined with existing MARL algorithms to further enhance their performance.
<div id='section'>Paperid: <span id='pid'>22, <a href='https://arxiv.org/pdf/2508.20818.pdf' target='_blank'>https://arxiv.org/pdf/2508.20818.pdf</a></span>   <span><a href='https://github.com/DaRL-LibSignal/cMALC-D' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Anirudh Satheesh, Keenan Powell, Hua Wei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.20818">cMALC-D: Contextual Multi-Agent LLM-Guided Curriculum Learning with Diversity-Based Context Blending</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Many multi-agent reinforcement learning (MARL) algorithms are trained in fixed simulation environments, making them brittle when deployed in real-world scenarios with more complex and uncertain conditions. Contextual MARL (cMARL) addresses this by parameterizing environments with context variables and training a context-agnostic policy that performs well across all environment configurations. Existing cMARL methods attempt to use curriculum learning to help train and evaluate context-agnostic policies, but they often rely on unreliable proxy signals, such as value estimates or generalized advantage estimates that are noisy and unstable in multi-agent settings due to inter-agent dynamics and partial observability. To address these issues, we propose Contextual Multi-Agent LLM-Guided Curriculum Learning with Diversity-Based Context Blending (cMALC-D), a framework that uses Large Language Models (LLMs) to generate semantically meaningful curricula and provide a more robust evaluation signal. To prevent mode collapse and encourage exploration, we introduce a novel diversity-based context blending mechanism that creates new training scenarios by combining features from prior contexts. Experiments in traffic signal control domains demonstrate that cMALC-D significantly improves both generalization and sample efficiency compared to existing curriculum learning baselines. We provide code at https://github.com/DaRL-LibSignal/cMALC-D.
<div id='section'>Paperid: <span id='pid'>23, <a href='https://arxiv.org/pdf/2508.04416.pdf' target='_blank'>https://arxiv.org/pdf/2508.04416.pdf</a></span>   <span><a href='https://zhang9302002.github.io/thinkingwithvideos-page/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoji Zhang, Xin Gu, Jiawen Li, Chixiang Ma, Sule Bai, Chubin Zhang, Bowen Zhang, Zhichao Zhou, Dongliang He, Yansong Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.04416">Thinking With Videos: Multimodal Tool-Augmented Reinforcement Learning for Long Video Reasoning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The video reasoning ability of multimodal large language models (MLLMs) is crucial for downstream tasks like video question answering and temporal grounding. While recent approaches have explored text-based chain-of-thought (CoT) reasoning for MLLMs, these methods often suffer from limited cross-modal interaction and increased hallucination, especially with longer videos or reasoning chains. To address these challenges, we propose Video Intelligence via Tool-Augmented Learning (VITAL), a novel end-to-end agentic video reasoning framework. With a visual toolbox, the model can densely sample new video frames on demand and generate multimodal CoT for precise long video reasoning. We observe that temporal grounding and question answering are mutually beneficial for video understanding tasks. Therefore, we construct two high-quality multi-task video reasoning datasets MTVR-CoT-72k for supervised fine-tuning and MTVR-RL-110k for reinforcement learning. Moreover, we propose a Difficulty-aware Group Relative Policy Optimization algorithm (DGRPO) to mitigate difficulty imbalance in multi-task reinforcement learning. Extensive experiments on 11 challenging video understanding benchmarks demonstrate the advanced reasoning ability of VITAL, outperforming existing methods in video question answering and temporal grounding tasks, especially in long video scenarios. Code is available at https://zhang9302002.github.io/thinkingwithvideos-page/.
<div id='section'>Paperid: <span id='pid'>24, <a href='https://arxiv.org/pdf/2507.23172.pdf' target='_blank'>https://arxiv.org/pdf/2507.23172.pdf</a></span>   <span><a href='https://github.com/Viraj-Joshi/MTBench' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Viraj Joshi, Zifan Xu, Bo Liu, Peter Stone, Amy Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.23172">Benchmarking Massively Parallelized Multi-Task Reinforcement Learning for Robotics Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-task Reinforcement Learning (MTRL) has emerged as a critical training paradigm for applying reinforcement learning (RL) to a set of complex real-world robotic tasks, which demands a generalizable and robust policy. At the same time, \emph{massively parallelized training} has gained popularity, not only for significantly accelerating data collection through GPU-accelerated simulation but also for enabling diverse data collection across multiple tasks by simulating heterogeneous scenes in parallel. However, existing MTRL research has largely been limited to off-policy methods like SAC in the low-parallelization regime. MTRL could capitalize on the higher asymptotic performance of on-policy algorithms, whose batches require data from the current policy, and as a result, take advantage of massive parallelization offered by GPU-accelerated simulation. To bridge this gap, we introduce a massively parallelized $\textbf{M}$ulti-$\textbf{T}$ask $\textbf{Bench}$mark for robotics (MTBench), an open-sourced benchmark featuring a broad distribution of 50 manipulation tasks and 20 locomotion tasks, implemented using the GPU-accelerated simulator IsaacGym. MTBench also includes four base RL algorithms combined with seven state-of-the-art MTRL algorithms and architectures, providing a unified framework for evaluating their performance. Our extensive experiments highlight the superior speed of evaluating MTRL approaches using MTBench, while also uncovering unique challenges that arise from combining massive parallelism with MTRL. Code is available at https://github.com/Viraj-Joshi/MTBench
<div id='section'>Paperid: <span id='pid'>25, <a href='https://arxiv.org/pdf/2507.18059.pdf' target='_blank'>https://arxiv.org/pdf/2507.18059.pdf</a></span>   <span><a href='https://github.com/liyheng/MAGPO' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yueheng Li, Guangming Xie, Zongqing Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.18059">Multi-Agent Guided Policy Optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Due to practical constraints such as partial observability and limited communication, Centralized Training with Decentralized Execution (CTDE) has become the dominant paradigm in cooperative Multi-Agent Reinforcement Learning (MARL). However, existing CTDE methods often underutilize centralized training or lack theoretical guarantees. We propose Multi-Agent Guided Policy Optimization (MAGPO), a novel framework that better leverages centralized training by integrating centralized guidance with decentralized execution. MAGPO uses an auto-regressive joint policy for scalable, coordinated exploration and explicitly aligns it with decentralized policies to ensure deployability under partial observability. We provide theoretical guarantees of monotonic policy improvement and empirically evaluate MAGPO on 43 tasks across 6 diverse environments. Results show that MAGPO consistently outperforms strong CTDE baselines and matches or surpasses fully centralized approaches, offering a principled and practical solution for decentralized multi-agent learning. Our code and experimental data can be found in https://github.com/liyheng/MAGPO.
<div id='section'>Paperid: <span id='pid'>26, <a href='https://arxiv.org/pdf/2507.12110.pdf' target='_blank'>https://arxiv.org/pdf/2507.12110.pdf</a></span>   <span><a href='https://github.com/leoPub/tpemarl' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/leoPub/tpemarl' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ye Han, Lijun Zhang, Dejian Meng, Zhuang Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.12110">Topology Enhanced MARL for Multi-Vehicle Cooperative Decision-Making of CAVs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The exploration-exploitation trade-off constitutes one of the fundamental challenges in reinforcement learning (RL), which is exacerbated in multi-agent reinforcement learning (MARL) due to the exponential growth of joint state-action spaces. This paper proposes a topology-enhanced MARL (TPE-MARL) method for optimizing cooperative decision-making of connected and autonomous vehicles (CAVs) in mixed traffic. This work presents two primary contributions: First, we construct a game topology tensor for dynamic traffic flow, effectively compressing high-dimensional traffic state information and decrease the search space for MARL algorithms. Second, building upon the designed game topology tensor and using QMIX as the backbone RL algorithm, we establish a topology-enhanced MARL framework incorporating visit counts and agent mutual information. Extensive simulations across varying traffic densities and CAV penetration rates demonstrate the effectiveness of TPE-MARL. Evaluations encompassing training dynamics, exploration patterns, macroscopic traffic performance metrics, and microscopic vehicle behaviors reveal that TPE-MARL successfully balances exploration and exploitation. Consequently, it exhibits superior performance in terms of traffic efficiency, safety, decision smoothness, and task completion. Furthermore, the algorithm demonstrates decision-making rationality comparable to or exceeding that of human drivers in both mixed-autonomy and fully autonomous traffic scenarios. Code of our work is available at \href{https://github.com/leoPub/tpemarl}{https://github.com/leoPub/tpemarl}.
<div id='section'>Paperid: <span id='pid'>27, <a href='https://arxiv.org/pdf/2507.06825.pdf' target='_blank'>https://arxiv.org/pdf/2507.06825.pdf</a></span>   <span><a href='https://github.com/strakam/generals-bots' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Matej Straka, Martin Schmid
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.06825">Artificial Generals Intelligence: Mastering Generals.io with Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce a real-time strategy game environment based on Generals.io, a game with thousands of weekly active players. Our environment is fully compatible with Gymnasium and PettingZoo and is capable of running thousands of frames per second on commodity hardware. We also present a reference agent, trained with supervised pre-training and self-play, which reached the top 0.003% of the 1v1 human leaderboard after only 36 hours on a single H100 GPU. To accelerate learning, we incorporate potential-based reward shaping and memory features. Our contributions of a modular RTS benchmark and a competitive baseline agent provide an accessible yet challenging platform for advancing multi-agent reinforcement learning research. The documented code, together with examples and tutorials, is available at https://github.com/strakam/generals-bots.
<div id='section'>Paperid: <span id='pid'>28, <a href='https://arxiv.org/pdf/2507.06690.pdf' target='_blank'>https://arxiv.org/pdf/2507.06690.pdf</a></span>   <span><a href='https://github.com/WindyLab/MT-MARL-SG' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Guobin Zhu, Rui Zhou, Wenkang Ji, Hongyin Zhang, Donglin Wang, Shiyu Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.06690">Multi-Task Multi-Agent Reinforcement Learning via Skill Graphs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-task multi-agent reinforcement learning (MT-MARL) has recently gained attention for its potential to enhance MARL's adaptability across multiple tasks. However, it is challenging for existing multi-task learning methods to handle complex problems, as they are unable to handle unrelated tasks and possess limited knowledge transfer capabilities. In this paper, we propose a hierarchical approach that efficiently addresses these challenges. The high-level module utilizes a skill graph, while the low-level module employs a standard MARL algorithm. Our approach offers two contributions. First, we consider the MT-MARL problem in the context of unrelated tasks, expanding the scope of MTRL. Second, the skill graph is used as the upper layer of the standard hierarchical approach, with training independent of the lower layer, effectively handling unrelated tasks and enhancing knowledge transfer capabilities. Extensive experiments are conducted to validate these advantages and demonstrate that the proposed method outperforms the latest hierarchical MAPPO algorithms. Videos and code are available at https://github.com/WindyLab/MT-MARL-SG
<div id='section'>Paperid: <span id='pid'>29, <a href='https://arxiv.org/pdf/2507.01823.pdf' target='_blank'>https://arxiv.org/pdf/2507.01823.pdf</a></span>   <span><a href='https://github.com/dmytro-kuzmenko/td-mpc-opt' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Dmytro Kuzmenko, Nadiya Shvai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.01823">TD-MPC-Opt: Distilling Model-Based Multi-Task Reinforcement Learning Agents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a novel approach to knowledge transfer in model-based reinforcement learning, addressing the critical challenge of deploying large world models in resource-constrained environments. Our method efficiently distills a high-capacity multi-task agent (317M parameters) into a compact model (1M parameters) on the MT30 benchmark, significantly improving performance across diverse tasks. Our distilled model achieves a state-of-the-art normalized score of 28.45, surpassing the original 1M parameter model score of 18.93. This improvement demonstrates the ability of our distillation technique to capture and consolidate complex multi-task knowledge. We further optimize the distilled model through FP16 post-training quantization, reducing its size by $\sim$50\%. Our approach addresses practical deployment limitations and offers insights into knowledge representation in large world models, paving the way for more efficient and accessible multi-task reinforcement learning systems in robotics and other resource-constrained applications. Code available at https://github.com/dmytro-kuzmenko/td-mpc-opt.
<div id='section'>Paperid: <span id='pid'>30, <a href='https://arxiv.org/pdf/2506.16718.pdf' target='_blank'>https://arxiv.org/pdf/2506.16718.pdf</a></span>   <span><a href='https://github.com/vcis-wangchenxu/MRDG.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chenxu Wang, Yonggang Jin, Cheng Hu, Youpeng Zhao, Zipeng Dai, Jian Zhao, Shiyu Huang, Liuyu Xiang, Junge Zhang, Zhaofeng He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.16718">Generalizable Agent Modeling for Agent Collaboration-Competition Adaptation with Multi-Retrieval and Dynamic Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Adapting a single agent to a new multi-agent system brings challenges, necessitating adjustments across various tasks, environments, and interactions with unknown teammates and opponents. Addressing this challenge is highly complex, and researchers have proposed two simplified scenarios, Multi-agent reinforcement learning for zero-shot learning and Ad-Hoc Teamwork. Building on these foundations, we propose a more comprehensive setting, Agent Collaborative-Competitive Adaptation (ACCA), which evaluates an agent to generalize across diverse scenarios, tasks, and interactions with both unfamiliar opponents and teammates. In ACCA, agents adjust to task and environmental changes, collaborate with unseen teammates, and compete against unknown opponents. We introduce a new modeling approach, Multi-Retrieval and Dynamic Generation (MRDG), that effectively models both teammates and opponents using their behavioral trajectories. This method incorporates a positional encoder for varying team sizes and a hypernetwork module to boost agents' learning and adaptive capabilities. Additionally, a viewpoint alignment module harmonizes the observational perspectives of retrieved teammates and opponents with the learning agent. Extensive tests in benchmark scenarios like SMAC, Overcooked-AI, and Melting Pot show that MRDG significantly improves robust collaboration and competition with unseen teammates and opponents, surpassing established baselines. Our code is available at: https://github.com/vcis-wangchenxu/MRDG.git
<div id='section'>Paperid: <span id='pid'>31, <a href='https://arxiv.org/pdf/2506.10326.pdf' target='_blank'>https://arxiv.org/pdf/2506.10326.pdf</a></span>   <span><a href='https://github.com/cameronangliss/vgc-bench' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Cameron Angliss, Jiaxun Cui, Jiaheng Hu, Arrasy Rahman, Peter Stone
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.10326">VGC-Bench: Towards Mastering Diverse Team Strategies in Competitive Pokémon</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Developing AI agents that can robustly adapt to varying strategic landscapes without retraining is a central challenge in multi-agent learning. Pokémon Video Game Championships (VGC) is a domain with a vast space of approximately $10^{139}$ team configurations, far larger than those of other games such as Chess, Go, Poker, StarCraft, or Dota. The combinatorial nature of team building in Pokémon VGC causes optimal strategies to vary substantially depending on both the controlled team and the opponent's team, making generalization uniquely challenging. To advance research on this problem, we introduce VGC-Bench: a benchmark that provides critical infrastructure, standardizes evaluation protocols, and supplies a human-play dataset of over 700,000 battle logs and a range of baseline agents based on heuristics, large language models, behavior cloning, and multi-agent reinforcement learning with empirical game-theoretic methods such as self-play, fictitious play, and double oracle. In the restricted setting where an agent is trained and evaluated in a mirror match with a single team configuration, our methods can win against a professional VGC competitor. We repeat this training and evaluation with progressively larger team sets and find that as the number of teams increases, the best-performing algorithm in the single-team setting has worse performance and is more exploitable, but has improved generalization to unseen teams. Our code and dataset are open-sourced at https://github.com/cameronangliss/vgc-bench and https://huggingface.co/datasets/cameronangliss/vgc-battle-logs.
<div id='section'>Paperid: <span id='pid'>32, <a href='https://arxiv.org/pdf/2506.07548.pdf' target='_blank'>https://arxiv.org/pdf/2506.07548.pdf</a></span>   <span><a href='https://github.com/NICE-HKU/CL2MARL-SMAC' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Weiqiang Jin, Hongyang Du, Guizhong Liu, Dong In Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.07548">Curriculum Learning With Counterfactual Group Relative Policy Advantage For Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent reinforcement learning (MARL) has achieved strong performance in cooperative adversarial tasks. However, most existing methods typically train agents against fixed opponent strategies and rely on such meta-static difficulty conditions, which limits their adaptability to changing environments and often leads to suboptimal policies. Inspired by the success of curriculum learning (CL) in supervised tasks, we propose a dynamic CL framework for MARL that employs an self-adaptive difficulty adjustment mechanism. This mechanism continuously modulates opponent strength based on real-time agent training performance, allowing agents to progressively learn from easier to more challenging scenarios. However, the dynamic nature of CL introduces instability due to nonstationary environments and sparse global rewards. To address this challenge, we develop a Counterfactual Group Relative Policy Advantage (CGRPA), which is tightly coupled with the curriculum by providing intrinsic credit signals that reflect each agent's impact under evolving task demands. CGRPA constructs a counterfactual advantage function that isolates individual contributions within group behavior, facilitating more reliable policy updates throughout the curriculum. CGRPA evaluates each agent's contribution through constructing counterfactual action advantage function, providing intrinsic rewards that enhance credit assignment and stabilize learning under non-stationary conditions. Extensive experiments demonstrate that our method improves both training stability and final performance, achieving competitive results against state-of-the-art methods. The code is available at https://github.com/NICE-HKU/CL2MARL-SMAC.
<div id='section'>Paperid: <span id='pid'>33, <a href='https://arxiv.org/pdf/2506.01538.pdf' target='_blank'>https://arxiv.org/pdf/2506.01538.pdf</a></span>   <span><a href='https://windylab.github.io/LAMARL/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Guobin Zhu, Rui Zhou, Wenkang Ji, Shiyu Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.01538">LAMARL: LLM-Aided Multi-Agent Reinforcement Learning for Cooperative Policy Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Although Multi-Agent Reinforcement Learning (MARL) is effective for complex multi-robot tasks, it suffers from low sample efficiency and requires iterative manual reward tuning. Large Language Models (LLMs) have shown promise in single-robot settings, but their application in multi-robot systems remains largely unexplored. This paper introduces a novel LLM-Aided MARL (LAMARL) approach, which integrates MARL with LLMs, significantly enhancing sample efficiency without requiring manual design. LAMARL consists of two modules: the first module leverages LLMs to fully automate the generation of prior policy and reward functions. The second module is MARL, which uses the generated functions to guide robot policy training effectively. On a shape assembly benchmark, both simulation and real-world experiments demonstrate the unique advantages of LAMARL. Ablation studies show that the prior policy improves sample efficiency by an average of 185.9% and enhances task completion, while structured prompts based on Chain-of-Thought (CoT) and basic APIs improve LLM output success rates by 28.5%-67.5%. Videos and code are available at https://windylab.github.io/LAMARL/
<div id='section'>Paperid: <span id='pid'>34, <a href='https://arxiv.org/pdf/2506.00228.pdf' target='_blank'>https://arxiv.org/pdf/2506.00228.pdf</a></span>   <span><a href='https://github.com/social-ai-uoft/sorrel' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Rebekah A. GelpÃ­, Yibing Ju, Ethan C. Jackson, Yikai Tang, Shon Verch, Claas Voelcker, William A. Cunningham
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.00228">Sorrel: A simple and flexible framework for multi-agent reinforcement learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce Sorrel (https://github.com/social-ai-uoft/sorrel), a simple Python interface for generating and testing new multi-agent reinforcement learning environments. This interface places a high degree of emphasis on simplicity and accessibility, and uses a more psychologically intuitive structure for the basic agent-environment loop, making it a useful tool for social scientists to investigate how learning and social interaction leads to the development and change of group dynamics. In this short paper, we outline the basic design philosophy and features of Sorrel.
<div id='section'>Paperid: <span id='pid'>35, <a href='https://arxiv.org/pdf/2505.08459.pdf' target='_blank'>https://arxiv.org/pdf/2505.08459.pdf</a></span>   <span><a href='https://github.com/hsushuai/SAP' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuai Xu, Sijia Cui, Yanna Wang, Bo Xu, Qi Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.08459">Strategy-Augmented Planning for Large Language Models via Opponent Exploitation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Efficiently modeling and exploiting opponents is a long-standing challenge in adversarial domains. Large Language Models (LLMs) trained on extensive textual data have recently demonstrated outstanding performance in general tasks, introducing new research directions for opponent modeling. Some studies primarily focus on directly using LLMs to generate decisions based on the elaborate prompt context that incorporates opponent descriptions, while these approaches are limited to scenarios where LLMs possess adequate domain expertise. To address that, we introduce a two-stage Strategy-Augmented Planning (SAP) framework that significantly enhances the opponent exploitation capabilities of LLM-based agents by utilizing a critical component, the Strategy Evaluation Network (SEN). Specifically, in the offline stage, we construct an explicit strategy space and subsequently collect strategy-outcome pair data for training the SEN network. During the online phase, SAP dynamically recognizes the opponent's strategies and greedily exploits them by searching best response strategy on the well-trained SEN, finally translating strategy to a course of actions by carefully designed prompts. Experimental results show that SAP exhibits robust generalization capabilities, allowing it to perform effectively not only against previously encountered opponent strategies but also against novel, unseen strategies. In the MicroRTS environment, SAP achieves a $85.35\%$ performance improvement over baseline methods and matches the competitiveness of reinforcement learning approaches against state-of-the-art (SOTA) rule-based AI. Our code is available at https://github.com/hsushuai/SAP.
<div id='section'>Paperid: <span id='pid'>36, <a href='https://arxiv.org/pdf/2505.07008.pdf' target='_blank'>https://arxiv.org/pdf/2505.07008.pdf</a></span>   <span><a href='https://github.com/Fernadoo/Const-Mem.}$' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Fengming Zhu, Fangzhen Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.07008">Constant-Memory Strategies in Stochastic Games: Best Responses and Equilibria</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Stochastic games have become a prevalent framework for studying long-term multi-agent interactions, especially in the context of multi-agent reinforcement learning. In this work, we comprehensively investigate the concept of constant-memory strategies in stochastic games. We first establish some results on best responses and Nash equilibria for behavioral constant-memory strategies, followed by a discussion on the computational hardness of best responding to mixed constant-memory strategies. Those theoretic insights are later verified on several sequential decision-making testbeds, including the $\textit{Iterated Prisoner's Dilemma}$, the $\textit{Iterated Traveler's Dilemma}$, and the $\textit{Pursuit}$ domain. This work aims to enhance the understanding of theoretical issues in single-agent planning under multi-agent systems, and uncover the connection between decision models in single-agent and multi-agent contexts. The code is available at $\texttt{https://github.com/Fernadoo/Const-Mem.}$
<div id='section'>Paperid: <span id='pid'>37, <a href='https://arxiv.org/pdf/2505.06771.pdf' target='_blank'>https://arxiv.org/pdf/2505.06771.pdf</a></span>   <span><a href='https://github.com/GT-STAR-Lab/JaxRobotarium' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/GT-STAR-Lab/JaxRobotarium' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shalin Anand Jain, Jiazhen Liu, Siva Kailas, Harish Ravichandar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.06771">JaxRobotarium: Training and Deploying Multi-Robot Policies in 10 Minutes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent reinforcement learning (MARL) has emerged as a promising solution for learning complex and scalable coordination behaviors in multi-robot systems. However, established MARL platforms (e.g., SMAC and MPE) lack robotics relevance and hardware deployment, leaving multi-robot learning researchers to develop bespoke environments and hardware testbeds dedicated to the development and evaluation of their individual contributions. The Multi-Agent RL Benchmark and Learning Environment for the Robotarium (MARBLER) is an exciting recent step in providing a standardized robotics-relevant platform for MARL, by bridging the Robotarium testbed with existing MARL software infrastructure. However, MARBLER lacks support for parallelization and GPU/TPU execution, making the platform prohibitively slow compared to modern MARL environments and hindering adoption. We contribute JaxRobotarium, a Jax-powered end-to-end simulation, learning, deployment, and benchmarking platform for the Robotarium. JaxRobotarium enables rapid training and deployment of multi-robot RL (MRRL) policies with realistic robot dynamics and safety constraints, supporting parallelization and hardware acceleration. Our generalizable learning interface integrates easily with SOTA MARL libraries (e.g., JaxMARL). In addition, JaxRobotarium includes eight standardized coordination scenarios, including four novel scenarios that bring established MARL benchmark tasks (e.g., RWARE and Level-Based Foraging) to a robotics setting. We demonstrate that JaxRobotarium retains high simulation fidelity while achieving dramatic speedups over baseline (20x in training and 150x in simulation), and provides an open-access sim-to-real evaluation pipeline through the Robotarium testbed, accelerating and democratizing access to multi-robot learning research and evaluation. Our code is available at https://github.com/GT-STAR-Lab/JaxRobotarium.
<div id='section'>Paperid: <span id='pid'>38, <a href='https://arxiv.org/pdf/2505.03586.pdf' target='_blank'>https://arxiv.org/pdf/2505.03586.pdf</a></span>   <span><a href='https://github.com/linkjoker1006' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Songchen Fu, Siang Chen, Shaojing Zhao, Letian Bai, Ta Li, Yonghong Yan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.03586">Rainbow Delay Compensation: A Multi-Agent Reinforcement Learning Framework for Mitigating Delayed Observation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In real-world multi-agent systems (MASs), observation delays are ubiquitous, preventing agents from making decisions based on the environment's true state. An individual agent's local observation often consists of multiple components from other agents or dynamic entities in the environment. These discrete observation components with varying delay characteristics pose significant challenges for multi-agent reinforcement learning (MARL). In this paper, we first formulate the decentralized stochastic individual delay partially observable Markov decision process (DSID-POMDP) by extending the standard Dec-POMDP. We then propose the Rainbow Delay Compensation (RDC), a MARL training framework for addressing stochastic individual delays, along with recommended implementations for its constituent modules. We implement the DSID-POMDP's observation generation pattern using standard MARL benchmarks, including MPE and SMAC. Experiments demonstrate that baseline MARL methods suffer severe performance degradation under fixed and unfixed delays. The RDC-enhanced approach mitigates this issue, remarkably achieving ideal delay-free performance in certain delay scenarios while maintaining generalizability. Our work provides a novel perspective on multi-agent delayed observation problems and offers an effective solution framework. The source code is available at https://anonymous.4open.science/r/RDC-pymarl-4512/.
<div id='section'>Paperid: <span id='pid'>39, <a href='https://arxiv.org/pdf/2505.02293.pdf' target='_blank'>https://arxiv.org/pdf/2505.02293.pdf</a></span>   <span><a href='https://dinamo-mit.github.io/Layered-Safe-MARL/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jason J. Choi, Jasmine Jerry Aloor, Jingqi Li, Maria G. Mendoza, Hamsa Balakrishnan, Claire J. Tomlin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.02293">Resolving Conflicting Constraints in Multi-Agent Reinforcement Learning with Layered Safety</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Preventing collisions in multi-robot navigation is crucial for deployment. This requirement hinders the use of learning-based approaches, such as multi-agent reinforcement learning (MARL), on their own due to their lack of safety guarantees. Traditional control methods, such as reachability and control barrier functions, can provide rigorous safety guarantees when interactions are limited only to a small number of robots. However, conflicts between the constraints faced by different agents pose a challenge to safe multi-agent coordination.
  To overcome this challenge, we propose a method that integrates multiple layers of safety by combining MARL with safety filters. First, MARL is used to learn strategies that minimize multiple agent interactions, where multiple indicates more than two. Particularly, we focus on interactions likely to result in conflicting constraints within the engagement distance. Next, for agents that enter the engagement distance, we prioritize pairs requiring the most urgent corrective actions. Finally, a dedicated safety filter provides tactical corrective actions to resolve these conflicts. Crucially, the design decisions for all layers of this framework are grounded in reachability analysis and a control barrier-value function-based filtering mechanism.
  We validate our Layered Safe MARL framework in 1) hardware experiments using Crazyflie drones and 2) high-density advanced aerial mobility (AAM) operation scenarios, where agents navigate to designated waypoints while avoiding collisions. The results show that our method significantly reduces conflict while maintaining safety without sacrificing much efficiency (i.e., shorter travel time and distance) compared to baselines that do not incorporate layered safety. The project website is available at https://dinamo-mit.github.io/Layered-Safe-MARL/
<div id='section'>Paperid: <span id='pid'>40, <a href='https://arxiv.org/pdf/2504.16129.pdf' target='_blank'>https://arxiv.org/pdf/2504.16129.pdf</a></span>   <span><a href='https://github.com/jwliao-ai/MARFT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Junwei Liao, Muning Wen, Jun Wang, Weinan Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.16129">MARFT: Multi-Agent Reinforcement Fine-Tuning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>LLM-based Multi-Agent Systems have demonstrated remarkable capabilities in addressing complex, agentic tasks, from generating high-quality presentation slides to even conducting sophisticated scientific research. Meanwhile, RL has been widely recognized for its effectiveness in enhancing agent intelligence, but limited research has investigated the fine-tuning of LaMAS using foundational RL techniques. Moreover, the direct application of MARL methods to LaMAS introduces significant challenges, stemming from the unique characteristics and mechanisms inherent to LaMAS. To address these challenges, this article presents a comprehensive study of LLM-based MARL and proposes a novel paradigm termed Multi-Agent Reinforcement Fine-Tuning (MARFT). We introduce a brand-new POMDP called Flex-POMDP, which aligns with the LaMAS optimization in real-world applications and a universal algorithmic framework tailored specifically for LaMAS, outlining the conceptual foundations, key distinctions, and practical implementation strategies. We review the evolution from RL to RFT, setting the stage for a parallel analysis in the multi-agent domain. In the context of LaMAS, we elucidate critical differences between MARL and MARFT. These differences motivate a transition toward a LaMAS-oriented formulation of RFT. Central to this work is a robust and scalable MARFT framework. We detail the core algorithm and provide a complete, open-source implementation to facilitate adoption and further research. The latter sections of the paper explore real-world application perspectives and opening challenges in MARFT. By bridging theoretical underpinnings with practical methodologies, this work serves as a roadmap for researchers seeking to advance MARFT toward resilient and adaptive solutions in agentic systems. Our implementation of the proposed framework is publicly available at: https://github.com/jwliao-ai/MARFT.
<div id='section'>Paperid: <span id='pid'>41, <a href='https://arxiv.org/pdf/2503.23626.pdf' target='_blank'>https://arxiv.org/pdf/2503.23626.pdf</a></span>   <span><a href='https://github.com/Asatheesh6561/MAPPO-LCE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Anirudh Satheesh, Keenan Powell
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.23626">A Constrained Multi-Agent Reinforcement Learning Approach to Autonomous Traffic Signal Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Traffic congestion in modern cities is exacerbated by the limitations of traditional fixed-time traffic signal systems, which fail to adapt to dynamic traffic patterns. Adaptive Traffic Signal Control (ATSC) algorithms have emerged as a solution by dynamically adjusting signal timing based on real-time traffic conditions. However, the main limitation of such methods is that they are not transferable to environments under real-world constraints, such as balancing efficiency, minimizing collisions, and ensuring fairness across intersections. In this paper, we view the ATSC problem as a constrained multi-agent reinforcement learning (MARL) problem and propose a novel algorithm named Multi-Agent Proximal Policy Optimization with Lagrange Cost Estimator (MAPPO-LCE) to produce effective traffic signal control policies. Our approach integrates the Lagrange multipliers method to balance rewards and constraints, with a cost estimator for stable adjustment. We also introduce three constraints on the traffic network: GreenTime, GreenSkip, and PhaseSkip, which penalize traffic policies that do not conform to real-world scenarios. Our experimental results on three real-world datasets demonstrate that MAPPO-LCE outperforms three baseline MARL algorithms by across all environments and traffic constraints (improving on MAPPO by 12.60%, IPPO by 10.29%, and QTRAN by 13.10%). Our results show that constrained MARL is a valuable tool for traffic planners to deploy scalable and efficient ATSC methods in real-world traffic networks. We provide code at https://github.com/Asatheesh6561/MAPPO-LCE.
<div id='section'>Paperid: <span id='pid'>42, <a href='https://arxiv.org/pdf/2503.15049.pdf' target='_blank'>https://arxiv.org/pdf/2503.15049.pdf</a></span>   <span><a href='https://github.com/RoboSafe-Lab/Sim4AD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Cheng Wang, Lingxin Kong, Massimiliano Tamborski, Stefano V. Albrecht
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.15049">HAD-Gen: Human-like and Diverse Driving Behavior Modeling for Controllable Scenario Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Simulation-based testing has emerged as an essential tool for verifying and validating autonomous vehicles (AVs). However, contemporary methodologies, such as deterministic and imitation learning-based driver models, struggle to capture the variability of human-like driving behavior. Given these challenges, we propose HAD-Gen, a general framework for realistic traffic scenario generation that simulates diverse human-like driving behaviors. The framework first clusters the vehicle trajectory data into different driving styles according to safety features. It then employs maximum entropy inverse reinforcement learning on each of the clusters to learn the reward function corresponding to each driving style. Using these reward functions, the method integrates offline reinforcement learning pre-training and multi-agent reinforcement learning algorithms to obtain general and robust driving policies. Multi-perspective simulation results show that our proposed scenario generation framework can simulate diverse, human-like driving behaviors with strong generalization capability. The proposed framework achieves a 90.96% goal-reaching rate, an off-road rate of 2.08%, and a collision rate of 6.91% in the generalization test, outperforming prior approaches by over 20% in goal-reaching performance. The source code is released at https://github.com/RoboSafe-Lab/Sim4AD.
<div id='section'>Paperid: <span id='pid'>43, <a href='https://arxiv.org/pdf/2503.14555.pdf' target='_blank'>https://arxiv.org/pdf/2503.14555.pdf</a></span>   <span><a href='https://github.com/chandar-lab/R3D2-A-Generalist-Hanabi-Agent' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/chandar-lab/R3D2-A-Generalist-Hanabi-Agent' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Arjun V Sudhakar, Hadi Nekoei, Mathieu Reymond, Miao Liu, Janarthanan Rajendran, Sarath Chandar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.14555">A Generalist Hanabi Agent</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Traditional multi-agent reinforcement learning (MARL) systems can develop cooperative strategies through repeated interactions. However, these systems are unable to perform well on any other setting than the one they have been trained on, and struggle to successfully cooperate with unfamiliar collaborators. This is particularly visible in the Hanabi benchmark, a popular 2-to-5 player cooperative card-game which requires complex reasoning and precise assistance to other agents. Current MARL agents for Hanabi can only learn one specific game-setting (e.g., 2-player games), and play with the same algorithmic agents. This is in stark contrast to humans, who can quickly adjust their strategies to work with unfamiliar partners or situations. In this paper, we introduce Recurrent Replay Relevance Distributed DQN (R3D2), a generalist agent for Hanabi, designed to overcome these limitations. We reformulate the task using text, as language has been shown to improve transfer. We then propose a distributed MARL algorithm that copes with the resulting dynamic observation- and action-space. In doing so, our agent is the first that can play all game settings concurrently, and extend strategies learned from one setting to other ones. As a consequence, our agent also demonstrates the ability to collaborate with different algorithmic agents -- agents that are themselves unable to do so. The implementation code is available at: $\href{https://github.com/chandar-lab/R3D2-A-Generalist-Hanabi-Agent}{R3D2-A-Generalist-Hanabi-Agent}$
<div id='section'>Paperid: <span id='pid'>44, <a href='https://arxiv.org/pdf/2503.12122.pdf' target='_blank'>https://arxiv.org/pdf/2503.12122.pdf</a></span>   <span><a href='https://yanoyoshiki.github.io/ICCO/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yoshiki Yano, Kazuki Shibata, Maarten Kokshoorn, Takamitsu Matsubara
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.12122">ICCO: Learning an Instruction-conditioned Coordinator for Language-guided Task-aligned Multi-robot Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in Large Language Models (LLMs) have permitted the development of language-guided multi-robot systems, which allow robots to execute tasks based on natural language instructions. However, achieving effective coordination in distributed multi-agent environments remains challenging due to (1) misalignment between instructions and task requirements and (2) inconsistency in robot behaviors when they independently interpret ambiguous instructions. To address these challenges, we propose Instruction-Conditioned Coordinator (ICCO), a Multi-Agent Reinforcement Learning (MARL) framework designed to enhance coordination in language-guided multi-robot systems. ICCO consists of a Coordinator agent and multiple Local Agents, where the Coordinator generates Task-Aligned and Consistent Instructions (TACI) by integrating language instructions with environmental states, ensuring task alignment and behavioral consistency. The Coordinator and Local Agents are jointly trained to optimize a reward function that balances task efficiency and instruction following. A Consistency Enhancement Term is added to the learning objective to maximize mutual information between instructions and robot behaviors, further improving coordination. Simulation and real-world experiments validate the effectiveness of ICCO in achieving language-guided task-aligned multi-robot control. The demonstration can be found at https://yanoyoshiki.github.io/ICCO/.
<div id='section'>Paperid: <span id='pid'>45, <a href='https://arxiv.org/pdf/2503.11726.pdf' target='_blank'>https://arxiv.org/pdf/2503.11726.pdf</a></span>   <span><a href='https://github.com/funny-rl/SPECTra' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hyunwoo Park, Baekryun Seong, Sang-Ki Ko
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.11726">SPECTra: Scalable Multi-Agent Reinforcement Learning with Permutation-Free Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In cooperative multi-agent reinforcement learning (MARL), the permutation problem where the state space grows exponentially with the number of agents reduces sample efficiency. Additionally, many existing architectures struggle with scalability, relying on a fixed structure tied to a specific number of agents, limiting their applicability to environments with a variable number of entities. While approaches such as graph neural networks (GNNs) and self-attention mechanisms have progressed in addressing these challenges, they have significant limitations as dense GNNs and self-attention mechanisms incur high computational costs. To overcome these limitations, we propose a novel agent network and a non-linear mixing network that ensure permutation-equivariance and scalability, allowing them to generalize to environments with various numbers of agents. Our agent network significantly reduces computational complexity, and our scalable hypernetwork enables efficient weight generation for non-linear mixing. Additionally, we introduce curriculum learning to improve training efficiency. Experiments on SMACv2 and Google Research Football (GRF) demonstrate that our approach achieves superior learning performance compared to existing methods. By addressing both permutation-invariance and scalability in MARL, our work provides a more efficient and adaptable framework for cooperative MARL. Our code is available at https://github.com/funny-rl/SPECTra.
<div id='section'>Paperid: <span id='pid'>46, <a href='https://arxiv.org/pdf/2503.09501.pdf' target='_blank'>https://arxiv.org/pdf/2503.09501.pdf</a></span>   <span><a href='https://github.com/ziyuwan/ReMA-public' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziyu Wan, Yunxiang Li, Xiaoyu Wen, Yan Song, Hanjing Wang, Linyi Yang, Mark Schmidt, Jun Wang, Weinan Zhang, Shuyue Hu, Ying Wen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.09501">ReMA: Learning to Meta-think for LLMs with Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent research on Reasoning of Large Language Models (LLMs) has sought to further enhance their performance by integrating meta-thinking -- enabling models to monitor, evaluate, and control their reasoning processes for more adaptive and effective problem-solving. However, current single-agent work lacks a specialized design for acquiring meta-thinking, resulting in low efficacy. To address this challenge, we introduce Reinforced Meta-thinking Agents (ReMA), a novel framework that leverages Multi-Agent Reinforcement Learning (MARL) to elicit meta-thinking behaviors, encouraging LLMs to think about thinking. ReMA decouples the reasoning process into two hierarchical agents: a high-level meta-thinking agent responsible for generating strategic oversight and plans, and a low-level reasoning agent for detailed executions. Through iterative reinforcement learning with aligned objectives, these agents explore and learn collaboration, leading to improved generalization and robustness. Empirical results from single-turn experiments demonstrate that ReMA outperforms single-agent RL baselines on complex reasoning tasks, including competitive-level mathematical benchmarks and LLM-as-a-Judge benchmarks. Additionally, we further extend ReMA to multi-turn interaction settings, leveraging turn-level ratio and parameter sharing to improve efficiency. Comprehensive ablation studies further illustrate the evolving dynamics of each distinct agent, providing valuable insights into how the meta-thinking reasoning process enhances the reasoning capabilities of LLMs. Our code can be found in https://github.com/ziyuwan/ReMA-public
<div id='section'>Paperid: <span id='pid'>47, <a href='https://arxiv.org/pdf/2503.01017.pdf' target='_blank'>https://arxiv.org/pdf/2503.01017.pdf</a></span>   <span><a href='https://github.com/Lab-Work/marl-vsl-controller' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuhang Zhang, Zhiyao Zhang, Junyi Ji, Marcos QuiÃ±ones-Grueiro, William Barbour, Derek Gloudemans, Gergely ZachÃ¡r, Clay Weston, Gautam Biswas, Daniel B. Work
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.01017">Real-World Deployment and Assessment of a Multi-Agent Reinforcement Learning-Based Variable Speed Limit Control System</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This article presents the first field deployment of a multi-agent reinforcement learning (MARL) based variable speed limit (VSL) control system on Interstate 24 (I-24) near Nashville, Tennessee. We design and demonstrate a full pipeline from training MARL agents in a traffic simulator to a field deployment on a 17-mile segment of I-24 encompassing 67 VSL controllers. The system was launched on March 8th, 2024, and has made approximately 35 million decisions on 28 million trips in six months of operation. We apply an invalid action masking mechanism and several safety guards to ensure real-world constraints. The MARL-based implementation operates up to 98% of the time, with the safety guards overriding the MARL decisions for the remaining time. We evaluate the performance of the MARL-based algorithm in comparison to a previously deployed non-RL VSL benchmark algorithm on I-24. Results show that the MARL-based VSL control system achieves a superior performance. The accuracy of correctly warning drivers about slowing traffic ahead is improved by 14% and the response delay to non-recurrent congestion is reduced by 75%. The preliminary data shows that the VSL control system has reduced the crash rate by 26% and the secondary crash rate by 50%. We open-sourced the deployed MARL-based VSL algorithm at https://github.com/Lab-Work/marl-vsl-controller.
<div id='section'>Paperid: <span id='pid'>48, <a href='https://arxiv.org/pdf/2502.19717.pdf' target='_blank'>https://arxiv.org/pdf/2502.19717.pdf</a></span>   <span><a href='https://github.com/LXXXXR/ExpoComm' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinran Li, Xiaolu Wang, Chenjia Bai, Jun Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.19717">Exponential Topology-enabled Scalable Communication in Multi-agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In cooperative multi-agent reinforcement learning (MARL), well-designed communication protocols can effectively facilitate consensus among agents, thereby enhancing task performance. Moreover, in large-scale multi-agent systems commonly found in real-world applications, effective communication plays an even more critical role due to the escalated challenge of partial observability compared to smaller-scale setups. In this work, we endeavor to develop a scalable communication protocol for MARL. Unlike previous methods that focus on selecting optimal pairwise communication links-a task that becomes increasingly complex as the number of agents grows-we adopt a global perspective on communication topology design. Specifically, we propose utilizing the exponential topology to enable rapid information dissemination among agents by leveraging its small-diameter and small-size properties. This approach leads to a scalable communication protocol, named ExpoComm. To fully unlock the potential of exponential graphs as communication topologies, we employ memory-based message processors and auxiliary tasks to ground messages, ensuring that they reflect global information and benefit decision-making. Extensive experiments on large-scale cooperative benchmarks, including MAgent and Infrastructure Management Planning, demonstrate the superior performance and robust zero-shot transferability of ExpoComm compared to existing communication strategies. The code is publicly available at https://github.com/LXXXXR/ExpoComm.
<div id='section'>Paperid: <span id='pid'>49, <a href='https://arxiv.org/pdf/2502.10233.pdf' target='_blank'>https://arxiv.org/pdf/2502.10233.pdf</a></span>   <span><a href='http://github.com/LTluttmann/marl4msprp' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Laurin Luttmann, Lin Xie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.10233">Learning to Solve the Min-Max Mixed-Shelves Picker-Routing Problem via Hierarchical and Parallel Decoding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The Mixed-Shelves Picker Routing Problem (MSPRP) is a fundamental challenge in warehouse logistics, where pickers must navigate a mixed-shelves environment to retrieve SKUs efficiently. Traditional heuristics and optimization-based approaches struggle with scalability, while recent machine learning methods often rely on sequential decision-making, leading to high solution latency and suboptimal agent coordination. In this work, we propose a novel hierarchical and parallel decoding approach for solving the min-max variant of the MSPRP via multi-agent reinforcement learning. While our approach generates a joint distribution over agent actions, allowing for fast decoding and effective picker coordination, our method introduces a sequential action selection to avoid conflicts in the multi-dimensional action space. Experiments show state-of-the-art performance in both solution quality and inference speed, particularly for large-scale and out-of-distribution instances. Our code is publicly available at http://github.com/LTluttmann/marl4msprp.
<div id='section'>Paperid: <span id='pid'>50, <a href='https://arxiv.org/pdf/2502.05453.pdf' target='_blank'>https://arxiv.org/pdf/2502.05453.pdf</a></span>   <span><a href='https://happyeureka.github.io/damcs' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hanqing Yang, Jingdi Chen, Marie Siew, Tania Lorido-Botran, Carlee Joe-Wong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.05453">LLM-Powered Decentralized Generative Agents with Adaptive Hierarchical Knowledge Graph for Cooperative Planning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Developing intelligent agents for long-term cooperation in dynamic open-world scenarios is a major challenge in multi-agent systems. Traditional Multi-agent Reinforcement Learning (MARL) frameworks like centralized training decentralized execution (CTDE) struggle with scalability and flexibility. They require centralized long-term planning, which is difficult without custom reward functions, and face challenges in processing multi-modal data. CTDE approaches also assume fixed cooperation strategies, making them impractical in dynamic environments where agents need to adapt and plan independently. To address decentralized multi-agent cooperation, we propose Decentralized Adaptive Knowledge Graph Memory and Structured Communication System (DAMCS) in a novel Multi-agent Crafter environment. Our generative agents, powered by Large Language Models (LLMs), are more scalable than traditional MARL agents by leveraging external knowledge and language for long-term planning and reasoning. Instead of fully sharing information from all past experiences, DAMCS introduces a multi-modal memory system organized as a hierarchical knowledge graph and a structured communication protocol to optimize agent cooperation. This allows agents to reason from past interactions and share relevant information efficiently. Experiments on novel multi-agent open-world tasks show that DAMCS outperforms both MARL and LLM baselines in task efficiency and collaboration. Compared to single-agent scenarios, the two-agent scenario achieves the same goal with 63% fewer steps, and the six-agent scenario with 74% fewer steps, highlighting the importance of adaptive memory and structured communication in achieving long-term goals. We publicly release our project at: https://happyeureka.github.io/damcs.
<div id='section'>Paperid: <span id='pid'>51, <a href='https://arxiv.org/pdf/2502.04492.pdf' target='_blank'>https://arxiv.org/pdf/2502.04492.pdf</a></span>   <span><a href='https://github.com/sftekin/rl-focal' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Selim Furkan Tekin, Fatih Ilhan, Tiansheng Huang, Sihao Hu, Zachary Yahn, Ling Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.04492">Multi-Agent Reinforcement Learning with Focal Diversity Optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The advancement of Large Language Models (LLMs) and their finetuning strategies has triggered the renewed interests in multi-agent reinforcement learning. In this paper, we introduce a focal diversity-optimized multi-agent reinforcement learning approach, coined as MARL-Focal, with three unique characteristics. First, we develop an agent-fusion framework for encouraging multiple LLM based agents to collaborate in producing the final inference output for each LLM query. Second, we develop a focal-diversity optimized agent selection algorithm that can choose a small subset of the available agents based on how well they can complement one another to generate the query output. Finally, we design a conflict-resolution method to detect output inconsistency among multiple agents and produce our MARL-Focal output through reward-aware and policy-adaptive inference fusion. Extensive evaluations on five benchmarks show that MARL-Focal is cost-efficient and adversarial-robust. Our multi-agent fusion model achieves performance improvement of 5.51\% compared to the best individual LLM-agent and offers stronger robustness over the TruthfulQA benchmark. Code is available at https://github.com/sftekin/rl-focal
<div id='section'>Paperid: <span id='pid'>52, <a href='https://arxiv.org/pdf/2502.04028.pdf' target='_blank'>https://arxiv.org/pdf/2502.04028.pdf</a></span>   <span><a href='https://github.com/Nikunj-Gupta/dmcg-marl' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Nikunj Gupta, James Zachary Hare, Rajgopal Kannan, Viktor Prasanna
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.04028">Deep Meta Coordination Graphs for Multi-agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents deep meta coordination graphs (DMCG) for learning cooperative policies in multi-agent reinforcement learning (MARL). Coordination graph formulations encode local interactions and accordingly factorize the joint value function of all agents to improve efficiency in MARL. However, existing approaches rely solely on pairwise relations between agents, which potentially oversimplifies complex multi-agent interactions. DMCG goes beyond these simple direct interactions by also capturing useful higher-order and indirect relationships among agents. It generates novel graph structures accommodating multiple types of interactions and arbitrary lengths of multi-hop connections in coordination graphs to model such interactions. It then employs a graph convolutional network module to learn powerful representations in an end-to-end manner. We demonstrate its effectiveness in multiple coordination problems in MARL where other state-of-the-art methods can suffer from sample inefficiency or fail entirely. All codes can be found here: https://github.com/Nikunj-Gupta/dmcg-marl.
<div id='section'>Paperid: <span id='pid'>53, <a href='https://arxiv.org/pdf/2502.02844.pdf' target='_blank'>https://arxiv.org/pdf/2502.02844.pdf</a></span>   <span><a href='https://github.com/sunwoolee0504/WALL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sunwoo Lee, Jaebak Hwang, Yonghyeon Jo, Seungyul Han
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.02844">Wolfpack Adversarial Attack for Robust Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Traditional robust methods in multi-agent reinforcement learning (MARL) often struggle against coordinated adversarial attacks in cooperative scenarios. To address this limitation, we propose the Wolfpack Adversarial Attack framework, inspired by wolf hunting strategies, which targets an initial agent and its assisting agents to disrupt cooperation. Additionally, we introduce the Wolfpack-Adversarial Learning for MARL (WALL) framework, which trains robust MARL policies to defend against the proposed Wolfpack attack by fostering systemwide collaboration. Experimental results underscore the devastating impact of the Wolfpack attack and the significant robustness improvements achieved by WALL. Our code is available at https://github.com/sunwoolee0504/WALL.
<div id='section'>Paperid: <span id='pid'>54, <a href='https://arxiv.org/pdf/2502.00345.pdf' target='_blank'>https://arxiv.org/pdf/2502.00345.pdf</a></span>   <span><a href='https://github.com/Yurui-Li/CTC' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yurui Li, Yuxuan Chen, Li Zhang, Shijian Li, Gang Pan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.00345">The Composite Task Challenge for Cooperative Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The significant role of division of labor (DOL) in promoting cooperation is widely recognized in real-world applications.Many cooperative multi-agent reinforcement learning (MARL) methods have incorporated the concept of DOL to improve cooperation among agents.However, the tasks used in existing testbeds typically correspond to tasks where DOL is often not a necessary feature for achieving optimal policies.Additionally, the full utilize of DOL concept in MARL methods remains unrealized due to the absence of appropriate tasks.To enhance the generality and applicability of MARL methods in real-world scenarios, there is a necessary to develop tasks that demand multi-agent DOL and cooperation.In this paper, we propose a series of tasks designed to meet these requirements, drawing on real-world rules as the guidance for their design.We guarantee that DOL and cooperation are necessary condition for completing tasks and introduce three factors to expand the diversity of proposed tasks to cover more realistic situations.We evaluate 10 cooperative MARL methods on the proposed tasks.The results indicate that all baselines perform poorly on these tasks.To further validate the solvability of these tasks, we also propose simplified variants of proposed tasks.Experimental results show that baselines are able to handle these simplified variants, providing evidence of the solvability of the proposed tasks.The source files is available at https://github.com/Yurui-Li/CTC.
<div id='section'>Paperid: <span id='pid'>55, <a href='https://arxiv.org/pdf/2501.15228.pdf' target='_blank'>https://arxiv.org/pdf/2501.15228.pdf</a></span>   <span><a href='https://github.com/chenyiqun/MMOA-RAG' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiqun Chen, Lingyong Yan, Weiwei Sun, Xinyu Ma, Yi Zhang, Shuaiqiang Wang, Dawei Yin, Yiming Yang, Jiaxin Mao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.15228">Improving Retrieval-Augmented Generation through Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Retrieval-augmented generation (RAG) is extensively utilized to incorporate external, current knowledge into large language models, thereby minimizing hallucinations. A standard RAG pipeline may comprise several components, such as query rewriting, document retrieval, document filtering, and answer generation. However, these components are typically optimized separately through supervised fine-tuning, which can lead to misalignments between the objectives of individual modules and the overarching aim of generating accurate answers in question-answering (QA) tasks. Although recent efforts have explored reinforcement learning (RL) to optimize specific RAG components, these approaches often focus on overly simplistic pipelines with only two components or do not adequately address the complex interdependencies and collaborative interactions among the modules. To overcome these challenges, we propose treating the RAG pipeline as a multi-agent cooperative task, with each component regarded as an RL agent. Specifically, we present MMOA-RAG, a Multi-Module joint Optimization Algorithm for RAG, which employs multi-agent reinforcement learning to harmonize all agents' goals towards a unified reward, such as the F1 score of the final answer. Experiments conducted on various QA datasets demonstrate that MMOA-RAG improves the overall pipeline performance and outperforms existing baselines. Furthermore, comprehensive ablation studies validate the contributions of individual components and the adaptability of MMOA-RAG across different RAG components and datasets. The code of MMOA-RAG is on https://github.com/chenyiqun/MMOA-RAG.
<div id='section'>Paperid: <span id='pid'>56, <a href='https://arxiv.org/pdf/2501.13200.pdf' target='_blank'>https://arxiv.org/pdf/2501.13200.pdf</a></span>   <span><a href='https://github.com/Aloriosa/srmt' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Alsu Sagirova, Yuri Kuratov, Mikhail Burtsev
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.13200">SRMT: Shared Memory for Multi-agent Lifelong Pathfinding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent reinforcement learning (MARL) demonstrates significant progress in solving cooperative and competitive multi-agent problems in various environments. One of the principal challenges in MARL is the need for explicit prediction of the agents' behavior to achieve cooperation. To resolve this issue, we propose the Shared Recurrent Memory Transformer (SRMT) which extends memory transformers to multi-agent settings by pooling and globally broadcasting individual working memories, enabling agents to exchange information implicitly and coordinate their actions. We evaluate SRMT on the Partially Observable Multi-Agent Pathfinding problem in a toy Bottleneck navigation task that requires agents to pass through a narrow corridor and on a POGEMA benchmark set of tasks. In the Bottleneck task, SRMT consistently outperforms a variety of reinforcement learning baselines, especially under sparse rewards, and generalizes effectively to longer corridors than those seen during training. On POGEMA maps, including Mazes, Random, and MovingAI, SRMT is competitive with recent MARL, hybrid, and planning-based algorithms. These results suggest that incorporating shared recurrent memory into the transformer-based architectures can enhance coordination in decentralized multi-agent systems. The source code for training and evaluation is available on GitHub: https://github.com/Aloriosa/srmt.
<div id='section'>Paperid: <span id='pid'>57, <a href='https://arxiv.org/pdf/2501.10593.pdf' target='_blank'>https://arxiv.org/pdf/2501.10593.pdf</a></span>   <span><a href='https://github.com/andreyrisukhin/ColorGrid' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Andrey Risukhin, Kavel Rao, Ben Caffee, Alan Fan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.10593">ColorGrid: A Multi-Agent Non-Stationary Environment for Goal Inference and Assistance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autonomous agents' interactions with humans are increasingly focused on adapting to their changing preferences in order to improve assistance in real-world tasks. Effective agents must learn to accurately infer human goals, which are often hidden, to collaborate well. However, existing Multi-Agent Reinforcement Learning (MARL) environments lack the necessary attributes required to rigorously evaluate these agents' learning capabilities. To this end, we introduce ColorGrid, a novel MARL environment with customizable non-stationarity, asymmetry, and reward structure. We investigate the performance of Independent Proximal Policy Optimization (IPPO), a state-of-the-art (SOTA) MARL algorithm, in ColorGrid and find through extensive ablations that, particularly with simultaneous non-stationary and asymmetric goals between a ``leader'' agent representing a human and a ``follower'' assistant agent, ColorGrid is unsolved by IPPO. To support benchmarking future MARL algorithms, we release our environment code, model checkpoints, and trajectory visualizations at https://github.com/andreyrisukhin/ColorGrid.
<div id='section'>Paperid: <span id='pid'>58, <a href='https://arxiv.org/pdf/2501.02977.pdf' target='_blank'>https://arxiv.org/pdf/2501.02977.pdf</a></span>   <span><a href='https://github.com/ai4co/camp' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chuanbo Hua, Federico Berto, Jiwoo Son, Seunghyun Kang, Changhyun Kwon, Jinkyoo Park
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.02977">CAMP: Collaborative Attention Model with Profiles for Vehicle Routing Problems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The profiled vehicle routing problem (PVRP) is a generalization of the heterogeneous capacitated vehicle routing problem (HCVRP) in which the objective is to optimize the routes of vehicles to serve client demands subject to different vehicle profiles, with each having a preference or constraint on a per-client basis. While existing learning methods have shown promise for solving the HCVRP in real-time, no learning method exists to solve the more practical and challenging PVRP. In this paper, we propose a Collaborative Attention Model with Profiles (CAMP), a novel approach that learns efficient solvers for PVRP using multi-agent reinforcement learning. CAMP employs a specialized attention-based encoder architecture to embed profiled client embeddings in parallel for each vehicle profile. We design a communication layer between agents for collaborative decision-making across profiled embeddings at each decoding step and a batched pointer mechanism to attend to the profiled embeddings to evaluate the likelihood of the next actions. We evaluate CAMP on two variants of PVRPs: PVRP with preferences, which explicitly influence the reward function, and PVRP with zone constraints with different numbers of agents and clients, demonstrating that our learned solvers achieve competitive results compared to both classical state-of-the-art neural multi-agent models in terms of solution quality and computational efficiency. We make our code openly available at https://github.com/ai4co/camp.
<div id='section'>Paperid: <span id='pid'>59, <a href='https://arxiv.org/pdf/2412.17707.pdf' target='_blank'>https://arxiv.org/pdf/2412.17707.pdf</a></span>   <span><a href='https://github.com/devindeng94/smac-hard' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yue Deng, Yan Yu, Weiyu Ma, Zirui Wang, Wenhui Zhu, Jian Zhao, Yin Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.17707">SMAC-Hard: Enabling Mixed Opponent Strategy Script and Self-play on SMAC</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The availability of challenging simulation environments is pivotal for advancing the field of Multi-Agent Reinforcement Learning (MARL). In cooperative MARL settings, the StarCraft Multi-Agent Challenge (SMAC) has gained prominence as a benchmark for algorithms following centralized training with decentralized execution paradigm. However, with continual advancements in SMAC, many algorithms now exhibit near-optimal performance, complicating the evaluation of their true effectiveness. To alleviate this problem, in this work, we highlight a critical issue: the default opponent policy in these environments lacks sufficient diversity, leading MARL algorithms to overfit and exploit unintended vulnerabilities rather than learning robust strategies. To overcome these limitations, we propose SMAC-HARD, a novel benchmark designed to enhance training robustness and evaluation comprehensiveness. SMAC-HARD supports customizable opponent strategies, randomization of adversarial policies, and interfaces for MARL self-play, enabling agents to generalize to varying opponent behaviors and improve model stability. Furthermore, we introduce a black-box testing framework wherein agents are trained without exposure to the edited opponent scripts but are tested against these scripts to evaluate the policy coverage and adaptability of MARL algorithms. We conduct extensive evaluations of widely used and state-of-the-art algorithms on SMAC-HARD, revealing the substantial challenges posed by edited and mixed strategy opponents. Additionally, the black-box strategy tests illustrate the difficulty of transferring learned policies to unseen adversaries. We envision SMAC-HARD as a critical step toward benchmarking the next generation of MARL algorithms, fostering progress in self-play methods for multi-agent systems. Our code is available at https://github.com/devindeng94/smac-hard.
<div id='section'>Paperid: <span id='pid'>60, <a href='https://arxiv.org/pdf/2412.04233.pdf' target='_blank'>https://arxiv.org/pdf/2412.04233.pdf</a></span>   <span><a href='https://github.com/KaleabTessera/HyperMARL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kale-ab Abebe Tessera, Arrasy Rahman, Amos Storkey, Stefano V. Albrecht
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.04233">HyperMARL: Adaptive Hypernetworks for Multi-Agent RL</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Adaptability to specialised or homogeneous behaviours is critical in cooperative multi-agent reinforcement learning (MARL). Parameter sharing (PS) techniques, common for efficient adaptation, often limit behavioural diversity due to cross-agent gradient interference, which we show can be exacerbated by the coupling of observations and agent IDs. Current remedies typically add complexity through altered objectives, manual preset diversity levels, or sequential updates. We ask: can shared policies adapt without these complexities? We propose HyperMARL, a PS approach using hypernetworks for dynamic agent-specific parameters, without altering the RL objective or requiring preset diversity levels. HyperMARL's explicit decoupling of observation- and agent-conditioned gradients empirically reduces policy gradient variance, facilitates shared-policy adaptation (including specialisation), and helps mitigate cross-agent interference. Across diverse MARL benchmarks (up to 20 agents), requiring homogeneous, heterogeneous, or mixed behaviours, HyperMARL achieves competitive performance against key baselines -- fully shared, non-parameter sharing, and three diversity-promoting methods -- while preserving behavioural diversity comparable to non-parameter sharing. These findings establish HyperMARL as a versatile approach for adaptive MARL. The code is publicly available at https://github.com/KaleabTessera/HyperMARL.
<div id='section'>Paperid: <span id='pid'>61, <a href='https://arxiv.org/pdf/2411.04672.pdf' target='_blank'>https://arxiv.org/pdf/2411.04672.pdf</a></span>   <span><a href='https://github.com/qiongwu86/Semantic-Aware-Resource-Management-for-C-V2X-Platooning-via-Multi-Agent-Reinforcement-Learning' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenjun Zhang, Qiong Wu, Pingyi Fan, Kezhi Wang, Nan Cheng, Wen Chen, Khaled B. Letaief
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.04672">Semantic-Aware Resource Management for C-V2X Platooning via Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Semantic communication transmits the extracted features of information rather than raw data, significantly reducing redundancy, which is crucial for addressing spectrum and energy challenges in 6G networks. In this paper, we introduce semantic communication into a cellular vehicle-to-everything (C-V2X)- based autonomous vehicle platoon system for the first time, aiming to achieve efficient management of communication resources in a dynamic environment. Firstly, we construct a mathematical model for semantic communication in platoon systems, in which the DeepSC model and MU-DeepSC model are used to semantically encode and decode unimodal and multi-modal data, respectively. Then, we propose the quality of experience (QoE) metric based on semantic similarity and semantic rate. Meanwhile, we consider the success rate of semantic information transmission (SRS) metric to ensure the fairness of channel resource allocation. Next, the optimization problem is posed with the aim of maximizing the QoE in vehicle-to-vehicle (V2V) links while improving SRS. To solve this mixed integer nonlinear programming problem (MINLP) and adapt to time-varying channel conditions, the paper proposes a distributed semantic-aware multi-modal resource allocation (SAMRA) algorithm based on multi-agent reinforcement learning (MARL), referred to as SAMRAMARL. The algorithm can dynamically allocate channels and power and determine semantic symbol length based on the contextual importance of the transmitted information, ensuring efficient resource utilization. Finally, extensive simulations have demonstrated that SAMRAMARL outperforms existing methods, achieving significant gains in QoE, SRS, and communication delay in C-V2X platooning scenarios.
<div id='section'>Paperid: <span id='pid'>62, <a href='https://arxiv.org/pdf/2410.19372.pdf' target='_blank'>https://arxiv.org/pdf/2410.19372.pdf</a></span>   <span><a href='https://github.com/giangbang/Strong-Pareto-MARL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Bang Giang Le, Viet Cuong Ta
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.19372">Toward Finding Strong Pareto Optimal Policies in Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this work, we study the problem of finding Pareto optimal policies in multi-agent reinforcement learning problems with cooperative reward structures. We show that any algorithm where each agent only optimizes their reward is subject to suboptimal convergence. Therefore, to achieve Pareto optimality, agents have to act altruistically by considering the rewards of others. This observation bridges the multi-objective optimization framework and multi-agent reinforcement learning together. We first propose a framework for applying the Multiple Gradient Descent algorithm (MGDA) for learning in multi-agent settings. We further show that standard MGDA is subjected to weak Pareto convergence, a problem that is often overlooked in other learning settings but is prevalent in multi-agent reinforcement learning. To mitigate this issue, we propose MGDA++, an improvement of the existing algorithm to handle the weakly optimal convergence of MGDA properly. Theoretically, we prove that MGDA++ converges to strong Pareto optimal solutions in convex, smooth bi-objective problems. We further demonstrate the superiority of our MGDA++ in cooperative settings in the Gridworld benchmark. The results highlight that our proposed method can converge efficiently and outperform the other methods in terms of the optimality of the convergent policies. The source code is available at \url{https://github.com/giangbang/Strong-Pareto-MARL}.
<div id='section'>Paperid: <span id='pid'>63, <a href='https://arxiv.org/pdf/2410.08540.pdf' target='_blank'>https://arxiv.org/pdf/2410.08540.pdf</a></span>   <span><a href='https://github.com/LXXXXR/Kaleidoscope' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinran Li, Ling Pan, Jun Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.08540">Kaleidoscope: Learnable Masks for Heterogeneous Multi-agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In multi-agent reinforcement learning (MARL), parameter sharing is commonly employed to enhance sample efficiency. However, the popular approach of full parameter sharing often leads to homogeneous policies among agents, potentially limiting the performance benefits that could be derived from policy diversity. To address this critical limitation, we introduce \emph{Kaleidoscope}, a novel adaptive partial parameter sharing scheme that fosters policy heterogeneity while still maintaining high sample efficiency. Specifically, Kaleidoscope maintains one set of common parameters alongside multiple sets of distinct, learnable masks for different agents, dictating the sharing of parameters. It promotes diversity among policy networks by encouraging discrepancy among these masks, without sacrificing the efficiencies of parameter sharing. This design allows Kaleidoscope to dynamically balance high sample efficiency with a broad policy representational capacity, effectively bridging the gap between full parameter sharing and non-parameter sharing across various environments. We further extend Kaleidoscope to critic ensembles in the context of actor-critic algorithms, which could help improve value estimations.Our empirical evaluations across extensive environments, including multi-agent particle environment, multi-agent MuJoCo and StarCraft multi-agent challenge v2, demonstrate the superior performance of Kaleidoscope compared with existing parameter sharing approaches, showcasing its potential for performance enhancement in MARL. The code is publicly available at \url{https://github.com/LXXXXR/Kaleidoscope}.
<div id='section'>Paperid: <span id='pid'>64, <a href='https://arxiv.org/pdf/2410.08345.pdf' target='_blank'>https://arxiv.org/pdf/2410.08345.pdf</a></span>   <span><a href='https://github.com/hegasz/large-legislative-models' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Henry Gasztowtt, Benjamin Smith, Vincent Zhu, Qinxun Bai, Edwin Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.08345">Large Legislative Models: Towards Efficient AI Policymaking in Economic Simulations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The improvement of economic policymaking presents an opportunity for broad societal benefit, a notion that has inspired research towards AI-driven policymaking tools. AI policymaking holds the potential to surpass human performance through the ability to process data quickly at scale. However, existing RL-based methods exhibit sample inefficiency, and are further limited by an inability to flexibly incorporate nuanced information into their decision-making processes. Thus, we propose a novel method in which we instead utilize pre-trained Large Language Models (LLMs), as sample-efficient policymakers in socially complex multi-agent reinforcement learning (MARL) scenarios. We demonstrate significant efficiency gains, outperforming existing methods across three environments. Our code is available at https://github.com/hegasz/large-legislative-models.
<div id='section'>Paperid: <span id='pid'>65, <a href='https://arxiv.org/pdf/2410.06372.pdf' target='_blank'>https://arxiv.org/pdf/2410.06372.pdf</a></span>   <span><a href='https://github.com/mylad13/CATMiP' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Milad Farjadnasab, Shahin Sirouspour
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.06372">Cooperative and Asynchronous Transformer-based Mission Planning for Heterogeneous Teams of Mobile Robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cooperative mission planning for heterogeneous teams of mobile robots presents a unique set of challenges, particularly when operating under communication constraints and limited computational resources. To address these challenges, we propose the Cooperative and Asynchronous Transformer-based Mission Planning (CATMiP) framework, which leverages multi-agent reinforcement learning (MARL) to coordinate distributed decision making among agents with diverse sensing, motion, and actuation capabilities, operating under sporadic ad hoc communication. A Class-based Macro-Action Decentralized Partially Observable Markov Decision Process (CMacDec-POMDP) is also formulated to effectively model asynchronous decision-making for heterogeneous teams of agents. The framework utilizes an asynchronous centralized training and distributed execution scheme, enabled by the proposed Asynchronous Multi-Agent Transformer (AMAT) architecture. This design allows a single trained model to generalize to larger environments and accommodate varying team sizes and compositions. We evaluate CATMiP in a 2D grid-world simulation environment and compare its performance against planning-based exploration methods. Results demonstrate CATMiP's superior efficiency, scalability, and robustness to communication dropouts and input noise, highlighting its potential for real-world heterogeneous mobile robot systems. The code is available at https://github.com/mylad13/CATMiP
<div id='section'>Paperid: <span id='pid'>66, <a href='https://arxiv.org/pdf/2409.11741.pdf' target='_blank'>https://arxiv.org/pdf/2409.11741.pdf</a></span>   <span><a href='https://github.com/huawen-hu/HARP' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Huawen Hu, Enze Shi, Chenxi Yue, Shuocun Yang, Zihao Wu, Yiwei Li, Tianyang Zhong, Tuo Zhang, Tianming Liu, Shu Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.11741">HARP: Human-Assisted Regrouping with Permutation Invariant Critic for Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human-in-the-loop reinforcement learning integrates human expertise to accelerate agent learning and provide critical guidance and feedback in complex fields. However, many existing approaches focus on single-agent tasks and require continuous human involvement during the training process, significantly increasing the human workload and limiting scalability. In this paper, we propose HARP (Human-Assisted Regrouping with Permutation Invariant Critic), a multi-agent reinforcement learning framework designed for group-oriented tasks. HARP integrates automatic agent regrouping with strategic human assistance during deployment, enabling and allowing non-experts to offer effective guidance with minimal intervention. During training, agents dynamically adjust their groupings to optimize collaborative task completion. When deployed, they actively seek human assistance and utilize the Permutation Invariant Group Critic to evaluate and refine human-proposed groupings, allowing non-expert users to contribute valuable suggestions. In multiple collaboration scenarios, our approach is able to leverage limited guidance from non-experts and enhance performance. The project can be found at https://github.com/huawen-hu/HARP.
<div id='section'>Paperid: <span id='pid'>67, <a href='https://arxiv.org/pdf/2409.00985.pdf' target='_blank'>https://arxiv.org/pdf/2409.00985.pdf</a></span>   <span><a href='https://github.com/yuqian2003/Co_Learning' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiapeng Yu, Yuqian Wu, Yajing Zhan, Wenhao Guo, Zhou Xu, Raymond Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.00985">Co-Learning: Code Learning for Multi-Agent Reinforcement Collaborative Framework with Conversational Natural Language Interfaces</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Online question-and-answer (Q\&A) systems based on the Large Language Model (LLM) have progressively diverged from recreational to professional use. This paper proposed a Multi-Agent framework with environmentally reinforcement learning (E-RL) for code correction called Code Learning (Co-Learning) community, assisting beginners to correct code errors independently. It evaluates the performance of multiple LLMs from an original dataset with 702 error codes, uses it as a reward or punishment criterion for E-RL; Analyzes input error codes by the current agent; selects the appropriate LLM-based agent to achieve optimal error correction accuracy and reduce correction time. Experiment results showed that 3\% improvement in Precision score and 15\% improvement in time cost as compared with no E-RL method respectively. Our source code is available at: https://github.com/yuqian2003/Co_Learning
<div id='section'>Paperid: <span id='pid'>68, <a href='https://arxiv.org/pdf/2411.01146.pdf' target='_blank'>https://arxiv.org/pdf/2411.01146.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziqing Fan, Shengchao Hu, Yuhang Zhou, Li Shen, Ya Zhang, Yanfeng Wang, Dacheng Tao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.01146">Task-Aware Harmony Multi-Task Decision Transformer for Offline Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The purpose of offline multi-task reinforcement learning (MTRL) is to develop a unified policy applicable to diverse tasks without the need for online environmental interaction. Recent advancements approach this through sequence modeling, leveraging the Transformer architecture's scalability and the benefits of parameter sharing to exploit task similarities. However, variations in task content and complexity pose significant challenges in policy formulation, necessitating judicious parameter sharing and management of conflicting gradients for optimal policy performance. Furthermore, identifying the optimal parameter subspace for each task often necessitates prior knowledge of the task identifier during inference, limiting applicability in real-world scenarios with variable task content and unknown current tasks. In this work, we introduce the Harmony Multi-Task Decision Transformer (HarmoDT), a novel solution designed to identify an optimal harmony subspace of parameters for each task. We formulate this as a bi-level optimization problem within a meta-learning framework, where the upper level learns masks to define the harmony subspace, while the inner level focuses on updating parameters to improve the overall performance of the unified policy. To eliminate the need for task identifiers, we further design a group-wise variant (G-HarmoDT) that clusters tasks into coherent groups based on gradient information, and utilizes a gating network to determine task identifiers during inference. Empirical evaluations across various benchmarks highlight the superiority of our approach, demonstrating its effectiveness in the multi-task context with specific improvements of 8% gain in task-provided settings, 5% in task-agnostic settings, and 10% in unseen settings.
<div id='section'>Paperid: <span id='pid'>69, <a href='https://arxiv.org/pdf/2511.05972.pdf' target='_blank'>https://arxiv.org/pdf/2511.05972.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Guangyuan Liu, Yinqiu Liu, Ruichen Zhang, Dusit Niyato, Jiawen Kang, Sumei Sun, Abbas Jamalipour, Ping Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.05972">DWM-RO: Decentralized World Models with Reasoning Offloading for SWIPT-enabled Satellite-Terrestrial HetNets</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Wireless networks are undergoing a paradigm shift toward massive connectivity with energy-efficient operation, driving the integration of satellite-terrestrial architectures with simultaneous wireless information and power transfer (SWIPT). Optimizing transmit beamforming and power splitting in such systems faces formidable challenges, e.g., time-varying channels and multi-tier interference, which create a complex decision landscape where conventional model-free multi-agent reinforcement learning (MARL) suffers from sample inefficiency due to rarely-encountered state transitions and poor coordination as decentralized agents act independently. This paper proposes the Decentralized World Model with Reasoning Offloading (DWM-RO) framework to address these fundamental limitations. Specifically, each agent employs a world model to learn compact predictive representations of environment dynamics, enabling imagination-based policy training that dramatically reduces required environment interactions. An uncertainty-aware offloading gate monitors local interference levels and model reconstruction errors to trigger selective edge coordination. When activated, a lightweight latent decorrelation mechanism at the edge refines agents' strategic representations, guiding them toward orthogonal actions that minimize resource conflicts. Extensive simulations demonstrate that DWM-RO converges 5 times faster than state-of-the-art baselines while achieving 34.7% higher spectral efficiency and reducing constraint violations by 40%. In dense network scenarios with 10 users, DWM-RO maintains violation rates below 20% while baselines exceed 70%, validating superior robustness.
<div id='section'>Paperid: <span id='pid'>70, <a href='https://arxiv.org/pdf/2504.08134.pdf' target='_blank'>https://arxiv.org/pdf/2504.08134.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Minrui Xu, Dusit Niyato, Jiawen Kang, Zehui Xiong, Mingzhe Chen, Dong In Kim, Xuemin, Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.08134">Hybrid Reinforcement Learning-based Sustainable Multi-User Computation Offloading for Mobile Edge-Quantum Computing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Exploiting quantum computing at the mobile edge holds immense potential for facilitating large-scale network design, processing multimodal data, optimizing resource management, and enhancing network security. In this paper, we propose a pioneering paradigm of mobile edge quantum computing (MEQC) that integrates quantum computing capabilities into classical edge computing servers that are proximate to mobile devices. To conceptualize the MEQC, we first design an MEQC system, where mobile devices can offload classical and quantum computation tasks to edge servers equipped with classical and quantum computers. We then formulate the hybrid classical-quantum computation offloading problem whose goal is to minimize system cost in terms of latency and energy consumption. To solve the offloading problem efficiently, we propose a hybrid discrete-continuous multi-agent reinforcement learning algorithm to learn long-term sustainable offloading and partitioning strategies. Finally, numerical results demonstrate that the proposed algorithm can reduce the MEQC system cost by up to 30% compared to existing baselines.
<div id='section'>Paperid: <span id='pid'>71, <a href='https://arxiv.org/pdf/2505.24378.pdf' target='_blank'>https://arxiv.org/pdf/2505.24378.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yilun Kong, Guozheng Ma, Qi Zhao, Haoyu Wang, Li Shen, Xueqian Wang, Dacheng Tao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.24378">Mastering Massive Multi-Task Reinforcement Learning via Mixture-of-Expert Decision Transformer</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite recent advancements in offline multi-task reinforcement learning (MTRL) have harnessed the powerful capabilities of the Transformer architecture, most approaches focus on a limited number of tasks, with scaling to extremely massive tasks remaining a formidable challenge. In this paper, we first revisit the key impact of task numbers on current MTRL method, and further reveal that naively expanding the parameters proves insufficient to counteract the performance degradation as the number of tasks escalates. Building upon these insights, we propose M3DT, a novel mixture-of-experts (MoE) framework that tackles task scalability by further unlocking the model's parameter scalability. Specifically, we enhance both the architecture and the optimization of the agent, where we strengthen the Decision Transformer (DT) backbone with MoE to reduce task load on parameter subsets, and introduce a three-stage training mechanism to facilitate efficient training with optimal performance. Experimental results show that, by increasing the number of experts, M3DT not only consistently enhances its performance as model expansion on the fixed task numbers, but also exhibits remarkable task scalability, successfully extending to 160 tasks with superior performance.
<div id='section'>Paperid: <span id='pid'>72, <a href='https://arxiv.org/pdf/2504.13554.pdf' target='_blank'>https://arxiv.org/pdf/2504.13554.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xin Tang, Qian Chen, Wenjie Weng, Chao Jin, Zhang Liu, Jiacheng Wang, Geng Sun, Xiaohuan Li, Dusit Niyato
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.13554">Task Assignment and Exploration Optimization for Low Altitude UAV Rescue via Generative AI Enhanced Multi-agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The integration of emerging uncrewed aerial vehicles (UAVs) with artificial intelligence (AI) and ground-embedded robots (GERs) has transformed emergency rescue operations in unknown environments. However, the high computational demands often exceed a single UAV's capacity, making it difficult to continuously provide stable high-level services. To address this, this paper proposes a cooperation framework involving UAVs, GERs, and airships. The framework enables resource pooling through UAV-to-GER (U2G) and UAV-to-airship (U2A) links, offering computing services for offloaded tasks. Specifically, we formulate the multi-objective problem of task assignment and exploration as a dynamic long-term optimization problem aiming to minimize task completion time and energy use while ensuring stability. Using Lyapunov optimization, we transform it into a per-slot deterministic problem and propose HG-MADDPG, which combines the Hungarian algorithm with a GDM-based multi-agent deep deterministic policy gradient. Simulations demonstrate significant improvements in offloading efficiency, latency, and system stability over baselines.
<div id='section'>Paperid: <span id='pid'>73, <a href='https://arxiv.org/pdf/2601.21609.pdf' target='_blank'>https://arxiv.org/pdf/2601.21609.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bingqian Li, Xiaolei Wang, Junyi Li, Weitao Li, Long Zhang, Sheng Chen, Wayne Xin Zhao, Ji-Rong Wen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.21609">RecNet: Self-Evolving Preference Propagation for Agentic Recommender Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Agentic recommender systems leverage Large Language Models (LLMs) to model complex user behaviors and support personalized decision-making. However, existing methods primarily model preference changes based on explicit user-item interactions, which are sparse, noisy, and unable to reflect the real-time, mutual influences among users and items. To address these limitations, we propose RecNet, a self-evolving preference propagation framework that proactively propagates real-time preference updates across related users and items. RecNet consists of two complementary phases. In the forward phase, the centralized preference routing mechanism leverages router agents to integrate preference updates and dynamically propagate them to the most relevant agents. To ensure accurate and personalized integration of propagated preferences, we further introduce a personalized preference reception mechanism, which combines a message buffer for temporary caching and an optimizable, rule-based filter memory to guide selective preference assimilation based on past experience and interests. In the backward phase, the feedback-driven propagation optimization mechanism simulates a multi-agent reinforcement learning framework, using LLMs for credit assignment, gradient analysis, and module-level optimization, enabling continuous self-evolution of propagation strategies. Extensive experiments on various scenarios demonstrate the effectiveness of RecNet in modeling preference propagation for recommender systems.
<div id='section'>Paperid: <span id='pid'>74, <a href='https://arxiv.org/pdf/2510.04935.pdf' target='_blank'>https://arxiv.org/pdf/2510.04935.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Guoxin Chen, Zile Qiao, Wenqing Wang, Donglei Yu, Xuanzhong Chen, Hao Sun, Minpeng Liao, Kai Fan, Yong Jiang, Penguin Xie, Wayne Xin Zhao, Ruihua Song, Fei Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.04935">MARS: Optimizing Dual-System Deep Research via Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large Reasoning Models (LRMs) often exhibit a tendency for overanalysis in simple tasks, where the models excessively utilize System 2-type, deliberate reasoning, leading to inefficient token generation. Furthermore, these models face challenges in adapting their reasoning capabilities to rapidly changing environments due to the static nature of their pretraining data. To address these issues, advancing Large Language Models (LLMs) for complex reasoning tasks requires innovative approaches that bridge intuitive and deliberate cognitive processes, akin to human cognition's dual-system dynamic. This paper introduces a Multi-Agent System for Deep ReSearch (MARS) enabling seamless integration of System 1's fast, intuitive thinking with System 2's deliberate reasoning within LLMs. MARS strategically integrates multiple external tools, such as Google Search, Google Scholar, and Python Interpreter, to access up-to-date information and execute complex computations, while creating a specialized division of labor where System 1 efficiently processes and summarizes high-volume external information, providing distilled insights that expand System 2's reasoning context without overwhelming its capacity. Furthermore, we propose a multi-agent reinforcement learning framework extending Group Relative Policy Optimization to simultaneously optimize both systems with multi-turn tool interactions, bin-packing optimization, and sample balancing strategies that enhance collaborative efficiency. Extensive experiments demonstrate MARS achieves substantial improvements of 3.86% on the challenging Humanity's Last Exam (HLE) benchmark and an average gain of 8.9% across 7 knowledge-intensive tasks, validating the effectiveness of our dual-system paradigm for complex reasoning in dynamic information environments.
<div id='section'>Paperid: <span id='pid'>75, <a href='https://arxiv.org/pdf/2502.13569.pdf' target='_blank'>https://arxiv.org/pdf/2502.13569.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yan Yu, Wengang Zhou, Yaodong Yang, Wanxuan Lu, Yingyan Hou, Houqiang Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.13569">Model Evolution Framework with Genetic Algorithm for Multi-Task Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-task reinforcement learning employs a single policy to complete various tasks, aiming to develop an agent with generalizability across different scenarios. Given the shared characteristics of tasks, the agent's learning efficiency can be enhanced through parameter sharing. Existing approaches typically use a routing network to generate specific routes for each task and reconstruct a set of modules into diverse models to complete multiple tasks simultaneously. However, due to the inherent difference between tasks, it is crucial to allocate resources based on task difficulty, which is constrained by the model's structure. To this end, we propose a Model Evolution framework with Genetic Algorithm (MEGA), which enables the model to evolve during training according to the difficulty of the tasks. When the current model is insufficient for certain tasks, the framework will automatically incorporate additional modules, enhancing the model's capabilities. Moreover, to adapt to our model evolution framework, we introduce a genotype module-level model, using binary sequences as genotype policies for model reconstruction, while leveraging a non-gradient genetic algorithm to optimize these genotype policies. Unlike routing networks with fixed output dimensions, our approach allows for the dynamic adjustment of the genotype policy length, enabling it to accommodate models with a varying number of modules. We conducted experiments on various robotics manipulation tasks in the Meta-World benchmark. Our state-of-the-art performance demonstrated the effectiveness of the MEGA framework. We will release our source code to the public.
<div id='section'>Paperid: <span id='pid'>76, <a href='https://arxiv.org/pdf/2502.01932.pdf' target='_blank'>https://arxiv.org/pdf/2502.01932.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zelai Xu, Ruize Zhang, Chao Yu, Huining Yuan, Xiangmin Yi, Shilong Ji, Chuqi Wang, Wenhao Tang, Feng Gao, Wenbo Ding, Xinlei Chen, Yu Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.01932">VolleyBots: A Testbed for Multi-Drone Volleyball Game Combining Motion Control and Strategic Play</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robot sports, characterized by well-defined objectives, explicit rules, and dynamic interactions, present ideal scenarios for demonstrating embodied intelligence. In this paper, we present VolleyBots, a novel robot sports testbed where multiple drones cooperate and compete in the sport of volleyball under physical dynamics. VolleyBots integrates three features within a unified platform: competitive and cooperative gameplay, turn-based interaction structure, and agile 3D maneuvering. Competitive and cooperative gameplay challenges each drone to coordinate with its teammates while anticipating and countering opposing teams' tactics. Turn-based interaction demands precise timing, accurate state prediction, and management of long-horizon temporal dependencies. Agile 3D maneuvering requires rapid accelerations, sharp turns, and precise 3D positioning despite the quadrotor's underactuated dynamics. These intertwined features yield a complex problem combining motion control and strategic play, with no available expert demonstrations. We provide a comprehensive suite of tasks ranging from single-drone drills to multi-drone cooperative and competitive tasks, accompanied by baseline evaluations of representative multi-agent reinforcement learning (MARL) and game-theoretic algorithms. Simulation results show that on-policy reinforcement learning (RL) methods outperform off-policy methods in single-agent tasks, but both approaches struggle in complex tasks that combine motion control and strategic play. We additionally design a hierarchical policy which achieves a 69.5% percent win rate against the strongest baseline in the 3 vs 3 task, underscoring its potential as an effective solution for tackling the complex interplay between low-level control and high-level strategy. The project page is at https://sites.google.com/view/thu-volleybots.
<div id='section'>Paperid: <span id='pid'>77, <a href='https://arxiv.org/pdf/2409.15866.pdf' target='_blank'>https://arxiv.org/pdf/2409.15866.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiayu Chen, Chao Yu, Guosheng Li, Wenhao Tang, Shilong Ji, Xinyi Yang, Botian Xu, Huazhong Yang, Yu Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.15866">Online Planning for Multi-UAV Pursuit-Evasion in Unknown Environments Using Deep Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-UAV pursuit-evasion, where pursuers aim to capture evaders, poses a key challenge for UAV swarm intelligence. Multi-agent reinforcement learning (MARL) has demonstrated potential in modeling cooperative behaviors, but most RL-based approaches remain constrained to simplified simulations with limited dynamics or fixed scenarios. Previous attempts to deploy RL policy to real-world pursuit-evasion are largely restricted to two-dimensional scenarios, such as ground vehicles or UAVs at fixed altitudes. In this paper, we address multi-UAV pursuit-evasion by considering UAV dynamics and physical constraints. We introduce an evader prediction-enhanced network to tackle partial observability in cooperative strategy learning. Additionally, we propose an adaptive environment generator within MARL training, enabling higher exploration efficiency and better policy generalization across diverse scenarios. Simulations show our method significantly outperforms all baselines in challenging scenarios, generalizing to unseen scenarios with a 100% capture rate. Finally, we derive a feasible policy via a two-stage reward refinement and deploy the policy on real quadrotors in a zero-shot manner. To our knowledge, this is the first work to derive and deploy an RL-based policy using collective thrust and body rates control commands for multi-UAV pursuit-evasion in unknown environments. The open-source code and videos are available at https://sites.google.com/view/pursuit-evasion-rl.
<div id='section'>Paperid: <span id='pid'>78, <a href='https://arxiv.org/pdf/2511.19524.pdf' target='_blank'>https://arxiv.org/pdf/2511.19524.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Boyu Chen, Zikang Wang, Zhengrong Yue, Kainan Yan, Chenyun Yu, Yi Huang, Zijun Liu, Yafei Wen, Xiaoxin Chen, Yang Liu, Peng Li, Yali Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.19524">VideoChat-M1: Collaborative Policy Planning for Video Understanding via Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>By leveraging tool-augmented Multimodal Large Language Models (MLLMs), multi-agent frameworks are driving progress in video understanding. However, most of them adopt static and non-learnable tool invocation mechanisms, which limit the discovery of diverse clues essential for robust perception and reasoning regarding temporally or spatially complex videos. To address this challenge, we propose a novel Multi-agent system for video understanding, namely VideoChat-M1. Instead of using a single or fixed policy, VideoChat-M1 adopts a distinct Collaborative Policy Planning (CPP) paradigm with multiple policy agents, which comprises three key processes. (1) Policy Generation: Each agent generates its unique tool invocation policy tailored to the user's query; (2) Policy Execution: Each agent sequentially invokes relevant tools to execute its policy and explore the video content; (3) Policy Communication: During the intermediate stages of policy execution, agents interact with one another to update their respective policies. Through this collaborative framework, all agents work in tandem, dynamically refining their preferred policies based on contextual insights from peers to effectively respond to the user's query. Moreover, we equip our CPP paradigm with a concise Multi-Agent Reinforcement Learning (MARL) method. Consequently, the team of policy agents can be jointly optimized to enhance VideoChat-M1's performance, guided by both the final answer reward and intermediate collaborative process feedback. Extensive experiments demonstrate that VideoChat-M1 achieves SOTA performance across eight benchmarks spanning four tasks. Notably, on LongVideoBench, our method outperforms the SOTA model Gemini 2.5 pro by 3.6% and GPT-4o by 15.6%.
<div id='section'>Paperid: <span id='pid'>79, <a href='https://arxiv.org/pdf/2502.14496.pdf' target='_blank'>https://arxiv.org/pdf/2502.14496.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhitao He, Zijun Liu, Peng Li, Yi R. Fung, Ming Yan, Ji Zhang, Fei Huang, Yang Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.14496">Advancing Language Multi-Agent Learning with Credit Re-Assignment for Interactive Environment Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>LLM-based agents have made significant advancements in interactive environments, such as mobile operations and web browsing, and other domains beyond computer using. Current multi-agent systems universally excel in performance, compared to single agents, but struggle with generalization across environments due to predefined roles and inadequate strategies for generalizing language agents. The challenge of achieving both strong performance and good generalization has hindered the progress of multi-agent systems for interactive environments. To address these issues, we propose CollabUIAgents, a multi-agent reinforcement learning framework with a novel multi-agent credit re-assignment (CR) strategy, assigning process rewards with LLMs rather than environment-specific rewards and learning with synthesized preference data, in order to foster generalizable, collaborative behaviors among the role-free agents' policies. Empirical results show that our framework improves both performance and cross-environment generalizability of multi-agent systems. Moreover, our 7B-parameter system achieves results on par with or exceed strong closed-source models, and the LLM that guides the CR. We also provide insights in using granular CR rewards effectively for environment generalization, and accommodating trained LLMs in multi-agent systems.
<div id='section'>Paperid: <span id='pid'>80, <a href='https://arxiv.org/pdf/2510.00477.pdf' target='_blank'>https://arxiv.org/pdf/2510.00477.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chengzhen Li, Likun Zhang, Chuang Zhang, Jiahui Li, Changyuan Zhao, Ruichen Zhang, Geng Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.00477">Wireless Laser Power Transfer for Low-altitude Uncrewed Aerial Vehicle-assisted Internet of Things: Paradigms, Challenges, and Solutions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Low-altitude uncrewed aerial vehicles (UAVs) have become integral enablers for the Internet of Things (IoT) by offering enhanced coverage, improved connectivity and access to remote areas. A critical challenge limiting their operational capacity lies in the energy constraints of both aerial platforms and ground-based sensors. This paper explores WLPT as a transformative solution for sustainable energy provisioning in UAV-assisted IoT networks. We first systematically investigate the fundamental principles of WLPT and analysis the comparative advantages. Then, we introduce three operational paradigms for system integration, identify key challenges, and discuss corresponding potential solutions. In case study, we propose a multi-agent reinforcement learning framework to address the coordination and optimization challenges in WLPT-enabled UAV-assisted IoT data collection. Simulation results demonstrate that our framework significantly improves energy sustainability and data freshness. Finally, we discuss some future directions.
<div id='section'>Paperid: <span id='pid'>81, <a href='https://arxiv.org/pdf/2412.13551.pdf' target='_blank'>https://arxiv.org/pdf/2412.13551.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuhan Zuo, Minghao Wang, Tianqing Zhu, Shui Yu, Wanlei Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.13551">Large Language Model Federated Learning with Blockchain and Unlearning for Cross-Organizational Collaboration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large language models (LLMs) have transformed the way computers understand and process human language, but using them effectively across different organizations remains still difficult. When organizations work together to improve LLMs, they face several main challenges. First, organizations hesitate to share their valuable data with others. Second, competition between organizations creates trust problems during collaboration. Third, new privacy laws require organizations to be able to delete specific data when requested, which is especially difficult when multiple organizations are learning from shared data. Traditional federated learning approaches do not address these interconnected challenges, particularly in scenarios where participants cannot fully trust each other or the central aggregator. To overcome these limitations, we propose a hybrid blockchain-based federated learning framework that uniquely combines public and private blockchain architectures with multi-agent reinforcement learning. Our framework enables transparent sharing of model update through the public blockchain while protecting sensitive computations in private chains. Each organization operates as an intelligent agent, using Q-learning to optimize its participation strategy and resource allocation, thus aligning individual incentives with collective goals. Notably, we introduce an efficient unlearning mechanism based on Low-Rank Adaptation (LoRA) that enables selective removal of specific data contributions without compromising the model's overall performance. Through extensive experimentation on real-world datasets, we demonstrate that our framework effectively balances privacy protection, trust establishment, and regulatory compliance while maintaining high model performance.
<div id='section'>Paperid: <span id='pid'>82, <a href='https://arxiv.org/pdf/2410.24152.pdf' target='_blank'>https://arxiv.org/pdf/2410.24152.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiaqi Liu, Chengkai Xu, Peng Hang, Jian Sun, Wei Zhan, Masayoshi Tomizuka, Mingyu Ding
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.24152">Language-Driven Policy Distillation for Cooperative Driving in Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The cooperative driving technology of Connected and Autonomous Vehicles (CAVs) is crucial for improving the efficiency and safety of transportation systems. Learning-based methods, such as Multi-Agent Reinforcement Learning (MARL), have demonstrated strong capabilities in cooperative decision-making tasks. However, existing MARL approaches still face challenges in terms of learning efficiency and performance. In recent years, Large Language Models (LLMs) have rapidly advanced and shown remarkable abilities in various sequential decision-making tasks. To enhance the learning capabilities of cooperative agents while ensuring decision-making efficiency and cost-effectiveness, we propose LDPD, a language-driven policy distillation method for guiding MARL exploration. In this framework, a teacher agent based on LLM trains smaller student agents to achieve cooperative decision-making through its own decision-making demonstrations. The teacher agent enhances the observation information of CAVs and utilizes LLMs to perform complex cooperative decision-making reasoning, which also leverages carefully designed decision-making tools to achieve expert-level decisions, providing high-quality teaching experiences. The student agent then refines the teacher's prior knowledge into its own model through gradient policy updates. The experiments demonstrate that the students can rapidly improve their capabilities with minimal guidance from the teacher and eventually surpass the teacher's performance. Extensive experiments show that our approach demonstrates better performance and learning efficiency compared to baseline methods.
<div id='section'>Paperid: <span id='pid'>83, <a href='https://arxiv.org/pdf/2601.19726.pdf' target='_blank'>https://arxiv.org/pdf/2601.19726.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lige Huang, Zicheng Liu, Jie Zhang, Lewen Yan, Dongrui Liu, Jing Shao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.19726">RvB: Automating AI System Hardening via Iterative Red-Blue Games</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The dual offensive and defensive utility of Large Language Models (LLMs) highlights a critical gap in AI security: the lack of unified frameworks for dynamic, iterative adversarial adaptation hardening. To bridge this gap, we propose the Red Team vs. Blue Team (RvB) framework, formulated as a training-free, sequential, imperfect-information game. In this process, the Red Team exposes vulnerabilities, driving the Blue Team to learning effective solutions without parameter updates. We validate our framework across two challenging domains: dynamic code hardening against CVEs and guardrail optimization against jailbreaks. Our empirical results show that this interaction compels the Blue Team to learn fundamental defensive principles, leading to robust remediations that are not merely overfitted to specific exploits. RvB achieves Defense Success Rates of 90\% and 45\% across the respective tasks while maintaining near 0\% False Positive Rates, significantly surpassing baselines. This work establishes the iterative adversarial interaction framework as a practical paradigm that automates the continuous hardening of AI systems.
<div id='section'>Paperid: <span id='pid'>84, <a href='https://arxiv.org/pdf/2510.01586.pdf' target='_blank'>https://arxiv.org/pdf/2510.01586.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhenyu Pan, Yiting Zhang, Zhuo Liu, Yolo Yunlong Tang, Zeliang Zhang, Haozheng Luo, Yuwei Han, Jianshu Zhang, Dennis Wu, Hong-Yu Chen, Haoran Lu, Haoyang Fang, Manling Li, Chenliang Xu, Philip S. Yu, Han Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.01586">AdvEvo-MARL: Shaping Internalized Safety through Adversarial Co-Evolution in Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>LLM-based multi-agent systems excel at planning, tool use, and role coordination, but their openness and interaction complexity also expose them to jailbreak, prompt-injection, and adversarial collaboration. Existing defenses fall into two lines: (i) self-verification that asks each agent to pre-filter unsafe instructions before execution, and (ii) external guard modules that police behaviors. The former often underperforms because a standalone agent lacks sufficient capacity to detect cross-agent unsafe chains and delegation-induced risks; the latter increases system overhead and creates a single-point-of-failure-once compromised, system-wide safety collapses, and adding more guards worsens cost and complexity. To solve these challenges, we propose AdvEvo-MARL, a co-evolutionary multi-agent reinforcement learning framework that internalizes safety into task agents. Rather than relying on external guards, AdvEvo-MARL jointly optimizes attackers (which synthesize evolving jailbreak prompts) and defenders (task agents trained to both accomplish their duties and resist attacks) in adversarial learning environments. To stabilize learning and foster cooperation, we introduce a public baseline for advantage estimation: agents within the same functional group share a group-level mean-return baseline, enabling lower-variance updates and stronger intra-group coordination. Across representative attack scenarios, AdvEvo-MARL consistently keeps attack-success rate (ASR) below 20%, whereas baselines reach up to 38.33%, while preserving-and sometimes improving-task accuracy (up to +3.67% on reasoning tasks). These results show that safety and utility can be jointly improved without relying on extra guard agents or added system overhead.
<div id='section'>Paperid: <span id='pid'>85, <a href='https://arxiv.org/pdf/2505.24155.pdf' target='_blank'>https://arxiv.org/pdf/2505.24155.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ehtesamul Azim, Dongjie Wang, Tae Hyun Hwang, Yanjie Fu, Wei Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.24155">Biological Pathway Guided Gene Selection Through Collaborative Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Gene selection in high-dimensional genomic data is essential for understanding disease mechanisms and improving therapeutic outcomes. Traditional feature selection methods effectively identify predictive genes but often ignore complex biological pathways and regulatory networks, leading to unstable and biologically irrelevant signatures. Prior approaches, such as Lasso-based methods and statistical filtering, either focus solely on individual gene-outcome associations or fail to capture pathway-level interactions, presenting a key challenge: how to integrate biological pathway knowledge while maintaining statistical rigor in gene selection? To address this gap, we propose a novel two-stage framework that integrates statistical selection with biological pathway knowledge using multi-agent reinforcement learning (MARL). First, we introduce a pathway-guided pre-filtering strategy that leverages multiple statistical methods alongside KEGG pathway information for initial dimensionality reduction. Next, for refined selection, we model genes as collaborative agents in a MARL framework, where each agent optimizes both predictive power and biological relevance. Our framework incorporates pathway knowledge through Graph Neural Network-based state representations, a reward mechanism combining prediction performance with gene centrality and pathway coverage, and collaborative learning strategies using shared memory and a centralized critic component. Extensive experiments on multiple gene expression datasets demonstrate that our approach significantly improves both prediction accuracy and biological interpretability compared to traditional methods.
<div id='section'>Paperid: <span id='pid'>86, <a href='https://arxiv.org/pdf/2504.14824.pdf' target='_blank'>https://arxiv.org/pdf/2504.14824.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wei Wang, Nan Cheng, Conghao Zhou, Haixia Peng, Haibo Zhou, Zhou Su, Xuemin, Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.14824">An Enhanced Dual-Currency VCG Auction Mechanism for Resource Allocation in IoV: A Value of Information Perspective</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The Internet of Vehicles (IoV) is undergoing a transformative evolution, enabled by advancements in future 6G network technologies, to support intelligent, highly reliable, and low-latency vehicular services. However, the enhanced capabilities of loV have heightened the demands for efficient network resource allocation while simultaneously giving rise to diverse vehicular service requirements. For network service providers (NSPs), meeting the customized resource-slicing requirements of vehicle service providers (VSPs) while maximizing social welfare has become a significant challenge. This paper proposes an innovative solution by integrating a mean-field multi-agent reinforcement learning (MFMARL) framework with an enhanced Vickrey-Clarke-Groves (VCG) auction mechanism to address the problem of social welfare maximization under the condition of unknown VSP utility functions. The core of this solution is introducing the ``value of information" as a novel monetary metric to estimate the expected benefits of VSPs, thereby ensuring the effective execution of the VCG auction mechanism. MFMARL is employed to optimize resource allocation for social welfare maximization while adapting to the intelligent and dynamic requirements of IoV. The proposed enhanced VCG auction mechanism not only protects the privacy of VSPs but also reduces the likelihood of collusion among VSPs, and it is theoretically proven to be dominant-strategy incentive compatible (DSIC). The simulation results demonstrate that, compared to the VCG mechanism implemented using quantization methods, the proposed mechanism exhibits significant advantages in convergence speed, social welfare maximization, and resistance to collusion, providing new insights into resource allocation in intelligent 6G networks.
<div id='section'>Paperid: <span id='pid'>87, <a href='https://arxiv.org/pdf/2502.05812.pdf' target='_blank'>https://arxiv.org/pdf/2502.05812.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiayi Zhang, Ziheng Liu, Yiyang Zhu, Enyu Shi, Bokai Xu, Chau Yuen, Dusit Niyato, MÃ©rouane Debbah, Shi Jin, Bo Ai, Xuemin, Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.05812">Multi-Agent Reinforcement Learning in Wireless Distributed Networks for 6G</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The introduction of intelligent interconnectivity between the physical and human worlds has attracted great attention for future sixth-generation (6G) networks, emphasizing massive capacity, ultra-low latency, and unparalleled reliability. Wireless distributed networks and multi-agent reinforcement learning (MARL), both of which have evolved from centralized paradigms, are two promising solutions for the great attention. Given their distinct capabilities, such as decentralization and collaborative mechanisms, integrating these two paradigms holds great promise for unleashing the full power of 6G, attracting significant research and development attention. This paper provides a comprehensive study on MARL-assisted wireless distributed networks for 6G. In particular, we introduce the basic mathematical background and evolution of wireless distributed networks and MARL, as well as demonstrate their interrelationships. Subsequently, we analyze different structures of wireless distributed networks from the perspectives of homogeneous and heterogeneous. Furthermore, we introduce the basic concepts of MARL and discuss two typical categories, including model-based and model-free. We then present critical challenges faced by MARL-assisted wireless distributed networks, providing important guidance and insights for actual implementation. We also explore an interplay between MARL-assisted wireless distributed networks and emerging techniques, such as information bottleneck and mirror learning, delivering in-depth analyses and application scenarios. Finally, we outline several compelling research directions for future MARL-assisted wireless distributed networks.
<div id='section'>Paperid: <span id='pid'>88, <a href='https://arxiv.org/pdf/2410.20151.pdf' target='_blank'>https://arxiv.org/pdf/2410.20151.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shanshan Song, Bingwen Huangfu, Jiani Guo, Jun Liu, Junhong Cui, Xuemin, Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.20151">A Digital Twin-based Intelligent Network Architecture for Underwater Acoustic Sensor Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Underwater acoustic sensor networks (UASNs) drive toward strong environmental adaptability, intelligence, and multifunctionality. However, due to unique UASN characteristics, such as long propagation delay, dynamic channel quality, and high attenuation, existing studies present untimeliness, inefficiency, and inflexibility in real practice. Digital twin (DT) technology is promising for UASNs to break the above bottlenecks by providing high-fidelity status prediction and exploring optimal schemes. In this article, we propose a Digital Twin-based Network Architecture (DTNA), enhancing UASNs' environmental adaptability, intelligence, and multifunctionality. By extracting real UASN information from local (node) and global (network) levels, we first design a layered architecture to improve the DT replica fidelity and UASN control flexibility. In local DT, we develop a resource allocation paradigm (RAPD), which rapidly perceives performance variations and iteratively optimizes allocation schemes to improve real-time environmental adaptability of resource allocation algorithms. In global DT, we aggregate decentralized local DT data and propose a collaborative Multi-agent reinforcement learning framework (CMFD) and a task-oriented network slicing (TNSD). CMFD patches scarce real data and provides extensive DT data to accelerate AI model training. TNSD unifies heterogeneous tasks' demand extraction and efficiently provides comprehensive network status, improving the flexibility of multi-task scheduling algorithms. Finally, practical and simulation experiments verify the high fidelity of DT. Compared with the original UASN architecture, experiment results demonstrate that DTNA can: (i) improve the timeliness and robustness of resource allocation; (ii) greatly reduce the training time of AI algorithms; (iii) more rapidly obtain network status for multi-task scheduling at a low cost.
<div id='section'>Paperid: <span id='pid'>89, <a href='https://arxiv.org/pdf/2504.17355.pdf' target='_blank'>https://arxiv.org/pdf/2504.17355.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaohan Huang, Dongjie Wang, Zhiyuan Ning, Ziyue Qiao, Qingqing Long, Haowei Zhu, Yi Du, Min Wu, Yuanchun Zhou, Meng Xiao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.17355">Collaborative Multi-Agent Reinforcement Learning for Automated Feature Transformation with Graph-Driven Path Optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Feature transformation methods aim to find an optimal mathematical feature-feature crossing process that generates high-value features and improves the performance of downstream machine learning tasks. Existing frameworks, though designed to mitigate manual costs, often treat feature transformations as isolated operations, ignoring dynamic dependencies between transformation steps. To address the limitations, we propose TCTO, a collaborative multi-agent reinforcement learning framework that automates feature engineering through graph-driven path optimization. The framework's core innovation lies in an evolving interaction graph that models features as nodes and transformations as edges. Through graph pruning and backtracking, it dynamically eliminates low-impact edges, reduces redundant operations, and enhances exploration stability. This graph also provides full traceability to empower TCTO to reuse high-utility subgraphs from historical transformations. To demonstrate the efficacy and adaptability of our approach, we conduct comprehensive experiments and case studies, which show superior performance across a range of datasets.
<div id='section'>Paperid: <span id='pid'>90, <a href='https://arxiv.org/pdf/2503.20507.pdf' target='_blank'>https://arxiv.org/pdf/2503.20507.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rakesh Nadig, Vamanan Arulchelvan, Rahul Bera, Taha Shahroodi, Gagandeep Singh, Andreas Kakolyris, Mohammad Sadrosadati, Jisung Park, Onur Mutlu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.20507">Harmonia: A Multi-Agent Reinforcement Learning Approach to Data Placement and Migration in Hybrid Storage Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Hybrid storage systems (HSS) integrate multiple storage devices with diverse characteristics to deliver high performance and capacity at low cost. The performance of an HSS highly depends on the effectiveness of two key policies: (1) the data-placement policy, which determines the best-fit storage device for incoming data, and (2) the data-migration policy, which dynamically rearranges stored data (i.e., prefetches hot data and evicts cold data) across the devices to sustain high HSS performance. Prior works optimize either data placement or data migration in isolation, which leads to suboptimal HSS performance. Unfortunately, no prior work tries to optimize both policies together. Our goal is to design a holistic data-management technique that optimizes both data-placement and data-migration policies to fully exploit the potential of an HSS, and thus significantly improve system performance. We propose Harmonia, a multi-agent reinforcement learning (RL)-based data-management technique that employs two lightweight autonomous RL agents, a data-placement agent and a data-migration agent, that adapt their policies for the current workload and HSS configuration while coordinating with each other to improve overall HSS performance. We evaluate Harmonia on real HSS configurations with up to four heterogeneous storage devices and seventeen data-intensive workloads. On performance-optimized (cost-optimized) HSS with two storage devices, Harmonia outperforms the best-performing prior approach by 49.5% (31.7%) on average. On an HSS with three (four) devices, Harmonia outperforms the best-performing prior work by 37.0% (42.0%) on average. Harmonia's performance benefits come with low latency (240ns for inference) and storage overheads (206 KiB in DRAM for both RL agents combined). We will open-source Harmonia's implementation to aid future research on HSS.
<div id='section'>Paperid: <span id='pid'>91, <a href='https://arxiv.org/pdf/2506.02718.pdf' target='_blank'>https://arxiv.org/pdf/2506.02718.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Guanzhong Chen, Shaoxiong Yang, Chao Li, Wei Liu, Jian Luan, Zenglin Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.02718">Heterogeneous Group-Based Reinforcement Learning for LLM-based Multi-Agent Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large Language Models (LLMs) have achieved remarkable success across diverse natural language processing tasks, yet their deployment in real-world applications is hindered by fixed knowledge cutoffs and difficulties in generating controllable, accurate outputs in a single inference. Multi-agent systems (MAS) built from specialized LLM agents offer a promising solution, enabling dynamic collaboration and iterative reasoning. However, optimizing these systems remains a challenge, as conventional methods such as prompt engineering and supervised fine-tuning entail high engineering overhead and limited adaptability. Reinforcement learning (RL), particularly multi-agent reinforcement learning (MARL), provides a scalable framework by refining agent policies based on system-level feedback. Nevertheless, existing MARL algorithms, such as Multi-Agent Proximal Policy Optimization (MAPPO), rely on Critic networks, which can cause training instability and increase computational burden. To address these limitations and target the prototypical Multi-Agent Search System (MASS), we propose Multi-Agent Heterogeneous Group Policy Optimization (MHGPO), a novel Critic-free algorithm that guides policy updates by estimating relative reward advantages across heterogeneous groups of rollouts. MHGPO eliminates the need for Critic networks, enhancing stability and reducing computational overhead. Additionally, we introduce three group rollout sampling strategies that trade off between efficiency and effectiveness. Experiments on a multi-agent LLM-based search system demonstrate that MHGPO consistently outperforms MAPPO in both task performance and computational efficiency, without requiring warm-up, underscoring its potential for stable and scalable optimization of complex LLM-based MAS.
<div id='section'>Paperid: <span id='pid'>92, <a href='https://arxiv.org/pdf/2501.00950.pdf' target='_blank'>https://arxiv.org/pdf/2501.00950.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Cleverson Nahum, Salvatore D'Oro, Pedro Batista, Cristiano Both, Kleber Cardoso, Aldebaro Klautau, Tommaso Melodia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.00950">Intent-based Radio Scheduler for RAN Slicing: Learning to deal with different network scenarios</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The future mobile network has the complex mission of distributing available radio resources among various applications with different requirements. The radio access network slicing enables the creation of different logical networks by isolating and using dedicated resources for each group of applications. In this scenario, the radio resource scheduling (RRS) is responsible for distributing the radio resources available among the slices to fulfill their service-level agreement (SLA) requirements, prioritizing critical slices while minimizing the number of intent violations. Moreover, ensuring that the RRS can deal with a high diversity of network scenarios is essential. Several recent papers present advances in machine learning-based RRS. However, the scenarios and slice variety are restricted, which inhibits solid conclusions about the generalization capabilities of the models after deployment in real networks. This paper proposes an intent-based RRS using multi-agent reinforcement learning in a radio access network (RAN) slicing context. The proposed method protects high-priority slices when the available radio resources cannot fulfill all the slices. It uses transfer learning to reduce the number of training steps required. The proposed method and baselines are evaluated in different network scenarios that comprehend combinations of different slice types, channel trajectories, number of active slices and users' equipment (UEs), and UE characteristics. The proposed method outperformed the baselines in protecting slices with higher priority, obtaining an improvement of 40% and, when considering all the slices, obtaining an improvement of 20% in relation to the baselines. The results show that by using transfer learning, the required number of training steps could be reduced by a factor of eight without hurting performance.
<div id='section'>Paperid: <span id='pid'>93, <a href='https://arxiv.org/pdf/2409.14177.pdf' target='_blank'>https://arxiv.org/pdf/2409.14177.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhihao Lin, Wei Ma, Mingyi Zhou, Yanjie Zhao, Haoyu Wang, Yang Liu, Jun Wang, Li Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.14177">PathSeeker: Exploring LLM Security Vulnerabilities with a Reinforcement Learning-Based Jailbreak Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, Large Language Models (LLMs) have gained widespread use, raising concerns about their security. Traditional jailbreak attacks, which often rely on the model internal information or have limitations when exploring the unsafe behavior of the victim model, limiting their reducing their general applicability. In this paper, we introduce PathSeeker, a novel black-box jailbreak method, which is inspired by the game of rats escaping a maze. We think that each LLM has its unique "security maze", and attackers attempt to find the exit learning from the received feedback and their accumulated experience to compromise the target LLM's security defences. Our approach leverages multi-agent reinforcement learning, where smaller models collaborate to guide the main LLM in performing mutation operations to achieve the attack objectives. By progressively modifying inputs based on the model's feedback, our system induces richer, harmful responses. During our manual attempts to perform jailbreak attacks, we found that the vocabulary of the response of the target model gradually became richer and eventually produced harmful responses. Based on the observation, we also introduce a reward mechanism that exploits the expansion of vocabulary richness in LLM responses to weaken security constraints. Our method outperforms five state-of-the-art attack techniques when tested across 13 commercial and open-source LLMs, achieving high attack success rates, especially in strongly aligned commercial models like GPT-4o-mini, Claude-3.5, and GLM-4-air with strong safety alignment. This study aims to improve the understanding of LLM security vulnerabilities and we hope that this sturdy can contribute to the development of more robust defenses.
<div id='section'>Paperid: <span id='pid'>94, <a href='https://arxiv.org/pdf/2510.17697.pdf' target='_blank'>https://arxiv.org/pdf/2510.17697.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Anjie Liu, Jianhong Wang, Samuel Kaski, Jun Wang, Mengyue Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.17697">A Principle of Targeted Intervention for Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Steering cooperative multi-agent reinforcement learning (MARL) towards desired outcomes is challenging, particularly when the global guidance from a human on the whole multi-agent system is impractical in a large-scale MARL. On the other hand, designing mechanisms to coordinate agents most relies on empirical studies, lacking a easy-to-use research tool. In this work, we employ multi-agent influence diagrams (MAIDs) as a graphical framework to address the above issues. First, we introduce interaction paradigms that leverage MAIDs to analyze and visualize existing approaches in MARL. Then, we design a new interaction paradigm based on MAIDs, referred to as targeted intervention that is applied to only a single targeted agent, so the problem of global guidance can be mitigated. In our implementation, we introduce a causal inference technique-referred to as Pre-Strategy Intervention (PSI)-to realize the targeted intervention paradigm. Since MAIDs can be regarded as a special class of causal diagrams, a composite desired outcome that integrates the primary task goal and an additional desired outcome can be achieved by maximizing the corresponding causal effect through the PSI. Moreover, the bundled relevance graph analysis of MAIDs provides a tool to identify whether an MARL learning paradigm is workable under the design of an interaction paradigm. In experiments, we demonstrate the effectiveness of our proposed targeted intervention, and verify the result of relevance graph analysis.
<div id='section'>Paperid: <span id='pid'>95, <a href='https://arxiv.org/pdf/2502.19675.pdf' target='_blank'>https://arxiv.org/pdf/2502.19675.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiyang Zhu, Jiayi Zhang, Enyu Shi, Ziheng Liu, Chau Yuen, Bo Ai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.19675">Joint Power Allocation and Phase Shift Design for Stacked Intelligent Metasurfaces-aided Cell-Free Massive MIMO Systems with MARL</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cell-free (CF) massive multiple-input multiple-output (mMIMO) systems offer high spectral efficiency (SE) through multiple distributed access points (APs). However, the large number of antennas increases power consumption. We propose incorporating stacked intelligent metasurfaces (SIM) into CF mMIMO systems as a cost-effective, energy-efficient solution. This paper focuses on optimizing the joint power allocation of APs and the phase shift of SIMs to maximize the sum SE. To address this complex problem, we introduce a fully distributed multi-agent reinforcement learning (MARL) algorithm. Our novel algorithm, the noisy value method with a recurrent policy in multi-agent policy optimization (NVR-MAPPO), enhances performance by encouraging diverse exploration under centralized training and decentralized execution. Simulations demonstrate that NVR-MAPPO significantly improves sum SE and robustness across various scenarios.
<div id='section'>Paperid: <span id='pid'>96, <a href='https://arxiv.org/pdf/2412.02581.pdf' target='_blank'>https://arxiv.org/pdf/2412.02581.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziheng Liu, Jiayi Zhang, Yiyang Zhu, Enyu Shi, Bo Ai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.02581">Mobile Cell-Free Massive MIMO with Multi-Agent Reinforcement Learning: A Scalable Framework</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cell-free massive multiple-input multiple-output (mMIMO) offers significant advantages in mobility scenarios, mainly due to the elimination of cell boundaries and strong macro diversity. In this paper, we examine the downlink performance of cell-free mMIMO systems equipped with mobile-APs utilizing the concept of unmanned aerial vehicles, where mobility and power control are jointly considered to effectively enhance coverage and suppress interference. However, the high computational complexity, poor collaboration, limited scalability, and uneven reward distribution of conventional optimization schemes lead to serious performance degradation and instability. These factors complicate the provision of consistent and high-quality service across all user equipments in downlink cell-free mMIMO systems. Consequently, we propose a novel scalable framework enhanced by multi-agent reinforcement learning (MARL) to tackle these challenges. The established framework incorporates a graph neural network (GNN)-aided communication mechanism to facilitate effective collaboration among agents, a permutation architecture to improve scalability, and a directional decoupling architecture to accurately distinguish contributions. In the numerical results, we present comparisons of different optimization schemes and network architectures, which reveal that the proposed scheme can effectively enhance system performance compared to conventional schemes due to the adoption of advanced technologies. In particular, appropriately compressing the observation space of agents is beneficial for achieving a better balance between performance and convergence.
<div id='section'>Paperid: <span id='pid'>97, <a href='https://arxiv.org/pdf/2411.11070.pdf' target='_blank'>https://arxiv.org/pdf/2411.11070.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Enyu Shi, Jiayi Zhang, Ziheng Liu, Yiyang Zhu, Chau Yuen, Derrick Wing Kwan Ng, Marco Di Renzo, Bo Ai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.11070">Joint Precoding and AP Selection for Energy Efficient RIS-aided Cell-Free Massive MIMO Using Multi-agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cell-free (CF) massive multiple-input multiple-output (mMIMO) and reconfigurable intelligent surface (RIS) are two advanced transceiver technologies for realizing future sixth-generation (6G) networks. In this paper, we investigate the joint precoding and access point (AP) selection for energy efficient RIS-aided CF mMIMO system. To address the associated computational complexity and communication power consumption, we advocate for user-centric dynamic networks in which each user is served by a subset of APs rather than by all of them. Based on the user-centric network, we formulate a joint precoding and AP selection problem to maximize the energy efficiency (EE) of the considered system. To solve this complex nonconvex problem, we propose an innovative double-layer multi-agent reinforcement learning (MARL)-based scheme. Moreover, we propose an adaptive power threshold-based AP selection scheme to further enhance the EE of the considered system. To reduce the computational complexity of the RIS-aided CF mMIMO system, we introduce a fuzzy logic (FL) strategy into the MARL scheme to accelerate convergence. The simulation results show that the proposed FL-based MARL cooperative architecture effectively improves EE performance, offering a 85\% enhancement over the zero-forcing (ZF) method, and achieves faster convergence speed compared with MARL. It is important to note that increasing the transmission power of the APs or the number of RIS elements can effectively enhance the spectral efficiency (SE) performance, which also leads to an increase in power consumption, resulting in a non-trivial trade-off between the quality of service and EE performance.
<div id='section'>Paperid: <span id='pid'>98, <a href='https://arxiv.org/pdf/2410.06506.pdf' target='_blank'>https://arxiv.org/pdf/2410.06506.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziheng Liu, Jiayi Zhang, Enyu Shi, Yiyang Zhu, Derrick Wing Kwan Ng, Bo Ai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.06506">Cooperative Multi-Target Positioning for Cell-Free Massive MIMO with Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cell-free massive multiple-input multiple-output (mMIMO) is a promising technology to empower next-generation mobile communication networks. In this paper, to address the computational complexity associated with conventional fingerprint positioning, we consider a novel cooperative positioning architecture that involves certain relevant access points (APs) to establish positioning similarity coefficients. Then, we propose an innovative joint positioning and correction framework employing multi-agent reinforcement learning (MARL) to tackle the challenges of high-dimensional sophisticated signal processing, which mainly leverages on the received signal strength information for preliminary positioning, supplemented by the angle of arrival information to refine the initial position estimation. Moreover, to mitigate the bias effects originating from remote APs, we design a cooperative weighted K-nearest neighbor (Co-WKNN)-based estimation scheme to select APs with a high correlation to participate in user positioning. In the numerical results, we present comparisons of various user positioning schemes, which reveal that the proposed MARL-based positioning scheme with Co-WKNN can effectively improve positioning performance. It is important to note that the cooperative positioning architecture is a critical element in striking a balance between positioning performance and computational complexity.
<div id='section'>Paperid: <span id='pid'>99, <a href='https://arxiv.org/pdf/2410.04871.pdf' target='_blank'>https://arxiv.org/pdf/2410.04871.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziheng Liu, Jiayi Zhang, Enyu Shi, Yiyang Zhu, Derrick Wing Kwan Ng, Bo Ai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.04871">Distributed Collaborative User Positioning for Cell-Free Massive MIMO with Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we investigate a cell-free massive multiple-input multiple-output system, which exhibits great potential in enhancing the capabilities of next-generation mobile communication networks. We first study the distributed positioning problem to lay the groundwork for solving resource allocation and interference management issues. Instead of relying on computationally and spatially complex fingerprint positioning methods, we propose a novel two-stage distributed collaborative positioning architecture with multi-agent reinforcement learning (MARL) network, consisting of a received signal strength-based preliminary positioning network and an angle of arrival-based auxiliary correction network. Our experimental results demonstrate that the two-stage distributed collaborative user positioning architecture can outperform conventional fingerprint positioning methods in terms of positioning accuracy.
<div id='section'>Paperid: <span id='pid'>100, <a href='https://arxiv.org/pdf/2512.03528.pdf' target='_blank'>https://arxiv.org/pdf/2512.03528.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Guang Yang, Tianpei Yang, Jingwen Qiao, Yanqing Wu, Jing Huo, Xingguo Chen, Yang Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.03528">Multi-Agent Reinforcement Learning with Communication-Constrained Priors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Communication is one of the effective means to improve the learning of cooperative policy in multi-agent systems. However, in most real-world scenarios, lossy communication is a prevalent issue. Existing multi-agent reinforcement learning with communication, due to their limited scalability and robustness, struggles to apply to complex and dynamic real-world environments. To address these challenges, we propose a generalized communication-constrained model to uniformly characterize communication conditions across different scenarios. Based on this, we utilize it as a learning prior to distinguish between lossy and lossless messages for specific scenarios. Additionally, we decouple the impact of lossy and lossless messages on distributed decision-making, drawing on a dual mutual information estimatior, and introduce a communication-constrained multi-agent reinforcement learning framework, quantifying the impact of communication messages into the global reward. Finally, we validate the effectiveness of our approach across several communication-constrained benchmarks.
<div id='section'>Paperid: <span id='pid'>101, <a href='https://arxiv.org/pdf/2510.22969.pdf' target='_blank'>https://arxiv.org/pdf/2510.22969.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kechen Meng, Sinuo Zhang, Rongpeng Li, Xiangming Meng, Chan Wang, Ming Lei, Zhifeng Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.22969">Multi-Agent Conditional Diffusion Model with Mean Field Communication as Wireless Resource Allocation Planner</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In wireless communication systems, efficient and adaptive resource allocation plays a crucial role in enhancing overall Quality of Service (QoS). While centralized Multi-Agent Reinforcement Learning (MARL) frameworks rely on a central coordinator for policy training and resource scheduling, they suffer from scalability issues and privacy risks. In contrast, the Distributed Training with Decentralized Execution (DTDE) paradigm enables distributed learning and decision-making, but it struggles with non-stationarity and limited inter-agent cooperation, which can severely degrade system performance. To overcome these challenges, we propose the Multi-Agent Conditional Diffusion Model Planner (MA-CDMP) for decentralized communication resource management. Built upon the Model-Based Reinforcement Learning (MBRL) paradigm, MA-CDMP employs Diffusion Models (DMs) to capture environment dynamics and plan future trajectories, while an inverse dynamics model guides action generation, thereby alleviating the sample inefficiency and slow convergence of conventional DTDE methods. Moreover, to approximate large-scale agent interactions, a Mean-Field (MF) mechanism is introduced as an assistance to the classifier in DMs. This design mitigates inter-agent non-stationarity and enhances cooperation with minimal communication overhead in distributed settings. We further theoretically establish an upper bound on the distributional approximation error introduced by the MF-based diffusion generation, guaranteeing convergence stability and reliable modeling of multi-agent stochastic dynamics. Extensive experiments demonstrate that MA-CDMP consistently outperforms existing MARL baselines in terms of average reward and QoS metrics, showcasing its scalability and practicality for real-world wireless network optimization.
<div id='section'>Paperid: <span id='pid'>102, <a href='https://arxiv.org/pdf/2510.16035.pdf' target='_blank'>https://arxiv.org/pdf/2510.16035.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yingguang Yang, Xianghua Zeng, Qi Wu, Hao Peng, Yutong Xia, Hao Liu, Bin Chong, Philip S. Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.16035">RoBCtrl: Attacking GNN-Based Social Bot Detectors via Reinforced Manipulation of Bots Control Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Social networks have become a crucial source of real-time information for individuals. The influence of social bots within these platforms has garnered considerable attention from researchers, leading to the development of numerous detection technologies. However, the vulnerability and robustness of these detection methods is still underexplored. Existing Graph Neural Network (GNN)-based methods cannot be directly applied due to the issues of limited control over social agents, the black-box nature of bot detectors, and the heterogeneity of bots. To address these challenges, this paper proposes the first adversarial multi-agent Reinforcement learning framework for social Bot control attacks (RoBCtrl) targeting GNN-based social bot detectors. Specifically, we use a diffusion model to generate high-fidelity bot accounts by reconstructing existing account data with minor modifications, thereby evading detection on social platforms. To the best of our knowledge, this is the first application of diffusion models to mimic the behavior of evolving social bots effectively. We then employ a Multi-Agent Reinforcement Learning (MARL) method to simulate bots adversarial behavior. We categorize social accounts based on their influence and budget. Different agents are then employed to control bot accounts across various categories, optimizing the attachment strategy through reinforcement learning. Additionally, a hierarchical state abstraction based on structural entropy is designed to accelerate the reinforcement learning. Extensive experiments on social bot detection datasets demonstrate that our framework can effectively undermine the performance of GNN-based detectors.
<div id='section'>Paperid: <span id='pid'>103, <a href='https://arxiv.org/pdf/2508.16037.pdf' target='_blank'>https://arxiv.org/pdf/2508.16037.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Renxuan Tan, Rongpeng Li, Xiaoxue Yu, Xianfu Chen, Xing Xu, Zhifeng Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.16037">Pareto Actor-Critic for Communication and Computation Co-Optimization in Non-Cooperative Federated Learning Services</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Federated learning (FL) in multi-service provider (SP) ecosystems is fundamentally hampered by non-cooperative dynamics, where privacy constraints and competing interests preclude the centralized optimization of multi-SP communication and computation resources. In this paper, we introduce PAC-MCoFL, a game-theoretic multi-agent reinforcement learning (MARL) framework where SPs act as agents to jointly optimize client assignment, adaptive quantization, and resource allocation. Within the framework, we integrate Pareto Actor-Critic (PAC) principles with expectile regression, enabling agents to conjecture optimal joint policies to achieve Pareto-optimal equilibria while modeling heterogeneous risk profiles. To manage the high-dimensional action space, we devise a ternary Cartesian decomposition (TCAD) mechanism that facilitates fine-grained control. Further, we develop PAC-MCoFL-p, a scalable variant featuring a parameterized conjecture generator that substantially reduces computational complexity with a provably bounded error. Alongside theoretical convergence guarantees, our framework's superiority is validated through extensive simulations -- PAC-MCoFL achieves approximately 5.8% and 4.2% improvements in total reward and hypervolume indicator (HVI), respectively, over the latest MARL solutions. The results also demonstrate that our method can more effectively balance individual SP and system performance in scaled deployments and under diverse data heterogeneity.
<div id='section'>Paperid: <span id='pid'>104, <a href='https://arxiv.org/pdf/2507.01378.pdf' target='_blank'>https://arxiv.org/pdf/2507.01378.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziyao Wang, Rongpeng Li, Sizhao Li, Yuming Xiang, Haiping Wang, Zhifeng Zhao, Honggang Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.01378">RALLY: Role-Adaptive LLM-Driven Yoked Navigation for Agentic UAV Swarms</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Intelligent control of Unmanned Aerial Vehicles (UAVs) swarms has emerged as a critical research focus, and it typically requires the swarm to navigate effectively while avoiding obstacles and achieving continuous coverage over multiple mission targets. Although traditional Multi-Agent Reinforcement Learning (MARL) approaches offer dynamic adaptability, they are hindered by the semantic gap in numerical communication and the rigidity of homogeneous role structures, resulting in poor generalization and limited task scalability. Recent advances in Large Language Model (LLM)-based control frameworks demonstrate strong semantic reasoning capabilities by leveraging extensive prior knowledge. However, due to the lack of online learning and over-reliance on static priors, these works often struggle with effective exploration, leading to reduced individual potential and overall system performance. To address these limitations, we propose a Role-Adaptive LLM-Driven Yoked navigation algorithm RALLY. Specifically, we first develop an LLM-driven semantic decision framework that uses structured natural language for efficient semantic communication and collaborative reasoning. Afterward, we introduce a dynamic role-heterogeneity mechanism for adaptive role switching and personalized decision-making. Furthermore, we propose a Role-value Mixing Network (RMIX)-based assignment strategy that integrates LLM offline priors with MARL online policies to enable semi-offline training of role selection strategies. Experiments in the Multi-Agent Particle Environment (MPE) environment and a Software-In-The-Loop (SITL) platform demonstrate that RALLY outperforms conventional approaches in terms of task coverage, convergence speed, and generalization, highlighting its strong potential for collaborative navigation in agentic multi-UAV systems.
<div id='section'>Paperid: <span id='pid'>105, <a href='https://arxiv.org/pdf/2506.12453.pdf' target='_blank'>https://arxiv.org/pdf/2506.12453.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rongpeng Li, Jianhang Zhu, Jiahao Huang, Zhifeng Zhao, Honggang Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.12453">Topology-Assisted Spatio-Temporal Pattern Disentangling for Scalable MARL in Large-scale Autonomous Traffic Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Intelligent Transportation Systems (ITSs) have emerged as a promising solution towards ameliorating urban traffic congestion, with Traffic Signal Control (TSC) identified as a critical component. Although Multi-Agent Reinforcement Learning (MARL) algorithms have shown potential in optimizing TSC through real-time decision-making, their scalability and effectiveness often suffer from large-scale and complex environments. Typically, these limitations primarily stem from a fundamental mismatch between the exponential growth of the state space driven by the environmental heterogeneities and the limited modeling capacity of current solutions. To address these issues, this paper introduces a novel MARL framework that integrates Dynamic Graph Neural Networks (DGNNs) and Topological Data Analysis (TDA), aiming to enhance the expressiveness of environmental representations and improve agent coordination. Furthermore, inspired by the Mixture of Experts (MoE) architecture in Large Language Models (LLMs), a topology-assisted spatial pattern disentangling (TSD)-enhanced MoE is proposed, which leverages topological signatures to decouple graph features for specialized processing, thus improving the model's ability to characterize dynamic and heterogeneous local observations. The TSD module is also integrated into the policy and value networks of the Multi-agent Proximal Policy Optimization (MAPPO) algorithm, further improving decision-making efficiency and robustness. Extensive experiments conducted on real-world traffic scenarios, together with comprehensive theoretical analysis, validate the superior performance of the proposed framework, highlighting the model's scalability and effectiveness in addressing the complexities of large-scale TSC tasks.
<div id='section'>Paperid: <span id='pid'>106, <a href='https://arxiv.org/pdf/2505.06706.pdf' target='_blank'>https://arxiv.org/pdf/2505.06706.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuxuan Zheng, Yihe Zhou, Feiyang Xu, Mingli Song, Shunyu Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.06706">Bi-level Mean Field: Dynamic Grouping for Large-Scale MARL</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large-scale Multi-Agent Reinforcement Learning (MARL) often suffers from the curse of dimensionality, as the exponential growth in agent interactions significantly increases computational complexity and impedes learning efficiency. To mitigate this, existing efforts that rely on Mean Field (MF) simplify the interaction landscape by approximating neighboring agents as a single mean agent, thus reducing overall complexity to pairwise interactions. However, these MF methods inevitably fail to account for individual differences, leading to aggregation noise caused by inaccurate iterative updates during MF learning. In this paper, we propose a Bi-level Mean Field (BMF) method to capture agent diversity with dynamic grouping in large-scale MARL, which can alleviate aggregation noise via bi-level interaction. Specifically, BMF introduces a dynamic group assignment module, which employs a Variational AutoEncoder (VAE) to learn the representations of agents, facilitating their dynamic grouping over time. Furthermore, we propose a bi-level interaction module to model both inter- and intra-group interactions for effective neighboring aggregation. Experiments across various tasks demonstrate that the proposed BMF yields results superior to the state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>107, <a href='https://arxiv.org/pdf/2505.04339.pdf' target='_blank'>https://arxiv.org/pdf/2505.04339.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao Peng, Xiang Huang, Shuo Sun, Ruitong Zhang, Philip S. Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.04339">Adaptive and Robust DBSCAN with Multi-agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>DBSCAN, a well-known density-based clustering algorithm, has gained widespread popularity and usage due to its effectiveness in identifying clusters of arbitrary shapes and handling noisy data. However, it encounters challenges in producing satisfactory cluster results when confronted with datasets of varying density scales, a common scenario in real-world applications. In this paper, we propose a novel Adaptive and Robust DBSCAN with Multi-agent Reinforcement Learning cluster framework, namely AR-DBSCAN. First, we model the initial dataset as a two-level encoding tree and categorize the data vertices into distinct density partitions according to the information uncertainty determined in the encoding tree. Each partition is then assigned to an agent to find the best clustering parameters without manual assistance. The allocation is density-adaptive, enabling AR-DBSCAN to effectively handle diverse density distributions within the dataset by utilizing distinct agents for different partitions. Second, a multi-agent deep reinforcement learning guided automatic parameter searching process is designed. The process of adjusting the parameter search direction by perceiving the clustering environment is modeled as a Markov decision process. Using a weakly-supervised reward training policy network, each agent adaptively learns the optimal clustering parameters by interacting with the clusters. Third, a recursive search mechanism adaptable to the data's scale is presented, enabling efficient and controlled exploration of large parameter spaces. Extensive experiments are conducted on nine artificial datasets and a real-world dataset. The results of offline and online tasks show that AR-DBSCAN not only improves clustering accuracy by up to 144.1% and 175.3% in the NMI and ARI metrics, respectively, but also is capable of robustly finding dominant parameters.
<div id='section'>Paperid: <span id='pid'>108, <a href='https://arxiv.org/pdf/2502.09846.pdf' target='_blank'>https://arxiv.org/pdf/2502.09846.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziqiong Wang, Xiaoxue Yu, Rongpeng Li, Zhifeng Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.09846">Robust Event-Triggered Integrated Communication and Control with Graph Information Bottleneck Optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Integrated communication and control serves as a critical ingredient in Multi-Agent Reinforcement Learning. However, partial observability limitations will impair collaboration effectiveness, and a potential solution is to establish consensus through well-calibrated latent variables obtained from neighboring agents. Nevertheless, the rigid transmission of less informative content can still result in redundant information exchanges. Therefore, we propose a Consensus-Driven Event-Based Graph Information Bottleneck (CDE-GIB) method, which integrates the communication graph and information flow through a GIB regularizer to extract more concise message representations while avoiding the high computational complexity of inner-loop operations. To further minimize the communication volume required for establishing consensus during interactions, we also develop a variable-threshold event-triggering mechanism. By simultaneously considering historical data and current observations, this mechanism capably evaluates the importance of information to determine whether an event should be triggered. Experimental results demonstrate that our proposed method outperforms existing state-of-the-art methods in terms of both efficiency and adaptability.
<div id='section'>Paperid: <span id='pid'>109, <a href='https://arxiv.org/pdf/2505.13834.pdf' target='_blank'>https://arxiv.org/pdf/2505.13834.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhi Su, Yuman Gao, Emily Lukas, Yunfei Li, Jiaze Cai, Faris Tulbah, Fei Gao, Chao Yu, Zhongyu Li, Yi Wu, Koushil Sreenath
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.13834">Toward Real-World Cooperative and Competitive Soccer with Quadrupedal Robot Teams</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Achieving coordinated teamwork among legged robots requires both fine-grained locomotion control and long-horizon strategic decision-making. Robot soccer offers a compelling testbed for this challenge, combining dynamic, competitive, and multi-agent interactions. In this work, we present a hierarchical multi-agent reinforcement learning (MARL) framework that enables fully autonomous and decentralized quadruped robot soccer. First, a set of highly dynamic low-level skills is trained for legged locomotion and ball manipulation, such as walking, dribbling, and kicking. On top of these, a high-level strategic planning policy is trained with Multi-Agent Proximal Policy Optimization (MAPPO) via Fictitious Self-Play (FSP). This learning framework allows agents to adapt to diverse opponent strategies and gives rise to sophisticated team behaviors, including coordinated passing, interception, and dynamic role allocation. With an extensive ablation study, the proposed learning method shows significant advantages in the cooperative and competitive multi-agent soccer game. We deploy the learned policies to real quadruped robots relying solely on onboard proprioception and decentralized localization, with the resulting system supporting autonomous robot-robot and robot-human soccer matches on indoor and outdoor soccer courts.
<div id='section'>Paperid: <span id='pid'>110, <a href='https://arxiv.org/pdf/2508.03864.pdf' target='_blank'>https://arxiv.org/pdf/2508.03864.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhenyu Pan, Yiting Zhang, Yutong Zhang, Jianshu Zhang, Haozheng Luo, Yuwei Han, Dennis Wu, Hong-Yu Chen, Philip S. Yu, Manling Li, Han Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.03864">Evo-MARL: Co-Evolutionary Multi-Agent Reinforcement Learning for Internalized Safety</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent systems (MAS) built on multimodal large language models exhibit strong collaboration and performance. However, their growing openness and interaction complexity pose serious risks, notably jailbreak and adversarial attacks. Existing defenses typically rely on external guard modules, such as dedicated safety agents, to handle unsafe behaviors. Unfortunately, this paradigm faces two challenges: (1) standalone agents offer limited protection, and (2) their independence leads to single-point failure-if compromised, system-wide safety collapses. Naively increasing the number of guard agents further raises cost and complexity. To address these challenges, we propose Evo-MARL, a novel multi-agent reinforcement learning (MARL) framework that enables all task agents to jointly acquire defensive capabilities. Rather than relying on external safety modules, Evo-MARL trains each agent to simultaneously perform its primary function and resist adversarial threats, ensuring robustness without increasing system overhead or single-node failure. Furthermore, Evo-MARL integrates evolutionary search with parameter-sharing reinforcement learning to co-evolve attackers and defenders. This adversarial training paradigm internalizes safety mechanisms and continually enhances MAS performance under co-evolving threats. Experiments show that Evo-MARL reduces attack success rates by up to 22% while boosting accuracy by up to 5% on reasoning tasks-demonstrating that safety and utility can be jointly improved.
<div id='section'>Paperid: <span id='pid'>111, <a href='https://arxiv.org/pdf/2502.20217.pdf' target='_blank'>https://arxiv.org/pdf/2502.20217.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jimmy Chiun, Shizhe Zhang, Yizhuo Wang, Yuhong Cao, Guillaume Sartoretti
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.20217">MARVEL: Multi-Agent Reinforcement Learning for constrained field-of-View multi-robot Exploration in Large-scale environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In multi-robot exploration, a team of mobile robot is tasked with efficiently mapping an unknown environments. While most exploration planners assume omnidirectional sensors like LiDAR, this is impractical for small robots such as drones, where lightweight, directional sensors like cameras may be the only option due to payload constraints. These sensors have a constrained field-of-view (FoV), which adds complexity to the exploration problem, requiring not only optimal robot positioning but also sensor orientation during movement. In this work, we propose MARVEL, a neural framework that leverages graph attention networks, together with novel frontiers and orientation features fusion technique, to develop a collaborative, decentralized policy using multi-agent reinforcement learning (MARL) for robots with constrained FoV. To handle the large action space of viewpoints planning, we further introduce a novel information-driven action pruning strategy. MARVEL improves multi-robot coordination and decision-making in challenging large-scale indoor environments, while adapting to various team sizes and sensor configurations (i.e., FoV and sensor range) without additional training. Our extensive evaluation shows that MARVEL's learned policies exhibit effective coordinated behaviors, outperforming state-of-the-art exploration planners across multiple metrics. We experimentally demonstrate MARVEL's generalizability in large-scale environments, of up to 90m by 90m, and validate its practical applicability through successful deployment on a team of real drone hardware.
<div id='section'>Paperid: <span id='pid'>112, <a href='https://arxiv.org/pdf/2601.21919.pdf' target='_blank'>https://arxiv.org/pdf/2601.21919.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiqun Chen, Jinyuan Feng, Wei Yang, Meizhi Zhong, Zhengliang Shi, Rui Li, Xiaochi Wei, Yan Gao, Yi Wu, Yao Hu, Zhiqiang Pu, Jiaxin Mao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.21919">Self-Compression of Chain-of-Thought via Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The inference overhead induced by redundant reasoning undermines the interactive experience and severely bottlenecks the deployment of Large Reasoning Models. Existing reinforcement learning (RL)-based solutions tackle this problem by coupling a length penalty with outcome-based rewards. This simplistic reward weighting struggles to reconcile brevity with accuracy, as enforcing brevity may compromise critical reasoning logic. In this work, we address this limitation by proposing a multi-agent RL framework that selectively penalizes redundant chunks, while preserving essential reasoning logic. Our framework, Self-Compression via MARL (SCMA), instantiates redundancy detection and evaluation through two specialized agents: \textbf{a Segmentation Agent} for decomposing the reasoning process into logical chunks, and \textbf{a Scoring Agent} for quantifying the significance of each chunk. The Segmentation and Scoring agents collaboratively define an importance-weighted length penalty during training, incentivizing \textbf{a Reasoning Agent} to prioritize essential logic without introducing inference overhead during deployment. Empirical evaluations across model scales demonstrate that SCMA reduces response length by 11.1\% to 39.0\% while boosting accuracy by 4.33\% to 10.02\%. Furthermore, ablation studies and qualitative analysis validate that the synergistic optimization within the MARL framework fosters emergent behaviors, yielding more powerful LRMs compared to vanilla RL paradigms.
<div id='section'>Paperid: <span id='pid'>113, <a href='https://arxiv.org/pdf/2503.20685.pdf' target='_blank'>https://arxiv.org/pdf/2503.20685.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuhao Huang, Ao Chang, Haoran Dou, Xing Tao, Xinrui Zhou, Yan Cao, Ruobing Huang, Alejandro F Frangi, Lingyun Bao, Xin Yang, Dong Ni
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.20685">Flip Learning: Weakly Supervised Erase to Segment Nodules in Breast Ultrasound</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate segmentation of nodules in both 2D breast ultrasound (BUS) and 3D automated breast ultrasound (ABUS) is crucial for clinical diagnosis and treatment planning. Therefore, developing an automated system for nodule segmentation can enhance user independence and expedite clinical analysis. Unlike fully-supervised learning, weakly-supervised segmentation (WSS) can streamline the laborious and intricate annotation process. However, current WSS methods face challenges in achieving precise nodule segmentation, as many of them depend on inaccurate activation maps or inefficient pseudo-mask generation algorithms. In this study, we introduce a novel multi-agent reinforcement learning-based WSS framework called Flip Learning, which relies solely on 2D/3D boxes for accurate segmentation. Specifically, multiple agents are employed to erase the target from the box to facilitate classification tag flipping, with the erased region serving as the predicted segmentation mask. The key contributions of this research are as follows: (1) Adoption of a superpixel/supervoxel-based approach to encode the standardized environment, capturing boundary priors and expediting the learning process. (2) Introduction of three meticulously designed rewards, comprising a classification score reward and two intensity distribution rewards, to steer the agents' erasing process precisely, thereby avoiding both under- and over-segmentation. (3) Implementation of a progressive curriculum learning strategy to enable agents to interact with the environment in a progressively challenging manner, thereby enhancing learning efficiency. Extensively validated on the large in-house BUS and ABUS datasets, our Flip Learning method outperforms state-of-the-art WSS methods and foundation models, and achieves comparable performance as fully-supervised learning algorithms.
<div id='section'>Paperid: <span id='pid'>114, <a href='https://arxiv.org/pdf/2507.19050.pdf' target='_blank'>https://arxiv.org/pdf/2507.19050.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qiong Wu, Yu Xie, Pingyi Fan, Dong Qin, Kezhi Wang, Nan Cheng, Khaled B. Letaief
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.19050">Large Language Model-Based Task Offloading and Resource Allocation for Digital Twin Edge Computing Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we propose a general digital twin edge computing network comprising multiple vehicles and a server. Each vehicle generates multiple computing tasks within a time slot, leading to queuing challenges when offloading tasks to the server. The study investigates task offloading strategies, queue stability, and resource allocation. Lyapunov optimization is employed to transform long-term constraints into tractable short-term decisions. To solve the resulting problem, an in-context learning approach based on large language model (LLM) is adopted, replacing the conventional multi-agent reinforcement learning (MARL) framework. Experimental results demonstrate that the LLM-based method achieves comparable or even superior performance to MARL.
<div id='section'>Paperid: <span id='pid'>115, <a href='https://arxiv.org/pdf/2506.07468.pdf' target='_blank'>https://arxiv.org/pdf/2506.07468.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mickel Liu, Liwei Jiang, Yancheng Liang, Simon Shaolei Du, Yejin Choi, Tim Althoff, Natasha Jaques
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.07468">Chasing Moving Targets with Online Self-Play Reinforcement Learning for Safer Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Conventional language model (LM) safety alignment relies on a reactive, disjoint procedure: attackers exploit a static model, followed by defensive fine-tuning to patch exposed vulnerabilities. This sequential approach creates a mismatch -- attackers overfit to obsolete defenses, while defenders perpetually lag behind emerging threats. To address this, we propose Self-RedTeam, an online self-play reinforcement learning algorithm where an attacker and defender agent co-evolve through continuous interaction. We cast safety alignment as a two-player zero-sum game, where a single model alternates between attacker and defender roles -- generating adversarial prompts and safeguarding against them -- while a reward LM adjudicates outcomes. This enables dynamic co-adaptation. Grounded in the game-theoretic framework of zero-sum games, we establish a theoretical safety guarantee which motivates the design of our method: if self-play converges to a Nash Equilibrium, the defender will reliably produce safe responses to any adversarial input. Empirically, Self-RedTeam uncovers more diverse attacks (+21.8% SBERT) compared to attackers trained against static defenders and achieves higher robustness on safety benchmarks (e.g., +65.5% on WildJailBreak) than defenders trained against static attackers. We further propose hidden Chain-of-Thought, allowing agents to plan privately, which boosts adversarial diversity and reduces over-refusals. Our results motivate a shift from reactive patching to proactive co-evolution in LM safety training, enabling scalable, autonomous, and robust self-improvement of LMs via multi-agent reinforcement learning (MARL).
<div id='section'>Paperid: <span id='pid'>116, <a href='https://arxiv.org/pdf/2503.10049.pdf' target='_blank'>https://arxiv.org/pdf/2503.10049.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziqi Jia, Junjie Li, Xiaoyang Qu, Jianzong Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.10049">Enhancing Multi-Agent Systems via Reinforcement Learning with LLM-based Planner and Graph-based Policy</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent systems (MAS) have shown great potential in executing complex tasks, but coordination and safety remain significant challenges. Multi-Agent Reinforcement Learning (MARL) offers a promising framework for agent collaboration, but it faces difficulties in handling complex tasks and designing reward functions. The introduction of Large Language Models (LLMs) has brought stronger reasoning and cognitive abilities to MAS, but existing LLM-based systems struggle to respond quickly and accurately in dynamic environments. To address these challenges, we propose LLM-based Graph Collaboration MARL (LGC-MARL), a framework that efficiently combines LLMs and MARL. This framework decomposes complex tasks into executable subtasks and achieves efficient collaboration among multiple agents through graph-based coordination. Specifically, LGC-MARL consists of two main components: an LLM planner and a graph-based collaboration meta policy. The LLM planner transforms complex task instructions into a series of executable subtasks, evaluates the rationality of these subtasks using a critic model, and generates an action dependency graph. The graph-based collaboration meta policy facilitates communication and collaboration among agents based on the action dependency graph, and adapts to new task environments through meta-learning. Experimental results on the AI2-THOR simulation platform demonstrate the superior performance and scalability of LGC-MARL in completing various complex tasks.
<div id='section'>Paperid: <span id='pid'>117, <a href='https://arxiv.org/pdf/2501.01266.pdf' target='_blank'>https://arxiv.org/pdf/2501.01266.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Michael KÃ¶lle, Johannes Tochtermann, Julian SchÃ¶nberger, Gerhard Stenzel, Philipp Altmann, Claudia Linnhoff-Popien
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.01266">PIMAEX: Multi-Agent Exploration through Peer Incentivization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While exploration in single-agent reinforcement learning has been studied extensively in recent years, considerably less work has focused on its counterpart in multi-agent reinforcement learning. To address this issue, this work proposes a peer-incentivized reward function inspired by previous research on intrinsic curiosity and influence-based rewards. The \textit{PIMAEX} reward, short for Peer-Incentivized Multi-Agent Exploration, aims to improve exploration in the multi-agent setting by encouraging agents to exert influence over each other to increase the likelihood of encountering novel states. We evaluate the \textit{PIMAEX} reward in conjunction with \textit{PIMAEX-Communication}, a multi-agent training algorithm that employs a communication channel for agents to influence one another. The evaluation is conducted in the \textit{Consume/Explore} environment, a partially observable environment with deceptive rewards, specifically designed to challenge the exploration vs.\ exploitation dilemma and the credit-assignment problem. The results empirically demonstrate that agents using the \textit{PIMAEX} reward with \textit{PIMAEX-Communication} outperform those that do not.
<div id='section'>Paperid: <span id='pid'>118, <a href='https://arxiv.org/pdf/2601.12416.pdf' target='_blank'>https://arxiv.org/pdf/2601.12416.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wei Huang, Hanchen Wang, Dong Wen, Xin Cao, Ying Zhang, Wenjie Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.12416">RLMiner: Finding the Most Frequent k-sized Subgraph via Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Identifying the most frequent induced subgraph of size $k$ in a target graph is a fundamental graph mining problem with direct implications for Web-related data mining and social network analysis. Despite its importance, finding the most frequent induced subgraph remains computationally expensive due to the NP-hard nature of the subgraph counting task. Traditional exact enumeration algorithms often suffer from high time complexity, especially for a large graph size $k$. To mitigate this, existing approaches often utilize frequency measurement with the Downward Closure Property to reduce the search space, imposing additional constraints on the task. In this paper, we first formulate this task as a Markov Decision Process and approach it using a multi-task reinforcement learning framework. Specifically, we introduce RLMiner, a novel framework that integrates reinforcement learning with our proposed task-state-aware Graph Neural Network to find the most frequent induced subgraph of size $k$ with a time complexity linear to $k$. Extensive experiments on real-world datasets demonstrate that our proposed RLMiner effectively identifies subgraphs with frequencies closely matching the ground-truth most frequent induced subgraphs, while achieving significantly shorter and more stable running times compared to traditional methods.
<div id='section'>Paperid: <span id='pid'>119, <a href='https://arxiv.org/pdf/2411.07104.pdf' target='_blank'>https://arxiv.org/pdf/2411.07104.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuming Feng, Chuye Hong, Yaru Niu, Shiqi Liu, Yuxiang Yang, Wenhao Yu, Tingnan Zhang, Jie Tan, Ding Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.07104">Learning Multi-Agent Loco-Manipulation for Long-Horizon Quadrupedal Pushing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, quadrupedal locomotion has achieved significant success, but their manipulation capabilities, particularly in handling large objects, remain limited, restricting their usefulness in demanding real-world applications such as search and rescue, construction, industrial automation, and room organization. This paper tackles the task of obstacle-aware, long-horizon pushing by multiple quadrupedal robots. We propose a hierarchical multi-agent reinforcement learning framework with three levels of control. The high-level controller integrates an RRT planner and a centralized adaptive policy to generate subgoals, while the mid-level controller uses a decentralized goal-conditioned policy to guide the robots toward these sub-goals. A pre-trained low-level locomotion policy executes the movement commands. We evaluate our method against several baselines in simulation, demonstrating significant improvements over baseline approaches, with 36.0% higher success rates and 24.5% reduction in completion time than the best baseline. Our framework successfully enables long-horizon, obstacle-aware manipulation tasks like Push-Cuboid and Push-T on Go1 robots in the real world.
<div id='section'>Paperid: <span id='pid'>120, <a href='https://arxiv.org/pdf/2507.13370.pdf' target='_blank'>https://arxiv.org/pdf/2507.13370.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shijun Guo, Haoran Xu, Yaming Yang, Ziyu Guan, Wei Zhao, Xinyi Zhang, Yishan Song, Jiwei Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.13370">H-NeiFi: Non-Invasive and Consensus-Efficient Multi-Agent Opinion Guidance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The openness of social media enables the free exchange of opinions, but it also presents challenges in guiding opinion evolution towards global consensus. Existing methods often directly modify user views or enforce cross-group connections. These intrusive interventions undermine user autonomy, provoke psychological resistance, and reduce the efficiency of global consensus. Additionally, due to the lack of a long-term perspective, promoting local consensus often exacerbates divisions at the macro level. To address these issues, we propose the hierarchical, non-intrusive opinion guidance framework, H-NeiFi. It first establishes a two-layer dynamic model based on social roles, considering the behavioral characteristics of both experts and non-experts. Additionally, we introduce a non-intrusive neighbor filtering method that adaptively controls user communication channels. Using multi-agent reinforcement learning (MARL), we optimize information propagation paths through a long-term reward function, avoiding direct interference with user interactions. Experiments show that H-NeiFi increases consensus speed by 22.0% to 30.7% and maintains global convergence even in the absence of experts. This approach enables natural and efficient consensus guidance by protecting user interaction autonomy, offering a new paradigm for social network governance.
<div id='section'>Paperid: <span id='pid'>121, <a href='https://arxiv.org/pdf/2506.00439.pdf' target='_blank'>https://arxiv.org/pdf/2506.00439.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuqian Fu, Yuanheng Zhu, Jiajun Chai, Guojun Yin, Wei Lin, Qichao Zhang, Dongbin Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.00439">RLAE: Reinforcement Learning-Assisted Ensemble for LLMs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Ensembling large language models (LLMs) can effectively combine diverse strengths of different models, offering a promising approach to enhance performance across various tasks. However, existing methods typically rely on fixed weighting strategies that fail to adapt to the dynamic, context-dependent characteristics of LLM capabilities. In this work, we propose Reinforcement Learning-Assisted Ensemble for LLMs (RLAE), a novel framework that reformulates LLM ensemble through the lens of a Markov Decision Process (MDP). Our approach introduces a RL agent that dynamically adjusts ensemble weights by considering both input context and intermediate generation states, with the agent being trained using rewards that directly correspond to the quality of final outputs. We implement RLAE using both single-agent and multi-agent reinforcement learning algorithms ($\text{RLAE}_\text{PPO}$ and $\text{RLAE}_\text{MAPPO}$ ), demonstrating substantial improvements over conventional ensemble methods. Extensive evaluations on a diverse set of tasks show that RLAE outperforms existing approaches by up to $3.3\%$ accuracy points, offering a more effective framework for LLM ensembling. Furthermore, our method exhibits superior generalization capabilities across different tasks without the need for retraining, while simultaneously achieving lower time latency.
<div id='section'>Paperid: <span id='pid'>122, <a href='https://arxiv.org/pdf/2501.00226.pdf' target='_blank'>https://arxiv.org/pdf/2501.00226.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tadahiro Taniguchi, Ryo Ueda, Tomoaki Nakamura, Masahiro Suzuki, Akira Taniguchi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.00226">Generative Emergent Communication: Large Language Model is a Collective World Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large Language Models (LLMs) have demonstrated a remarkable ability to capture extensive world knowledge, yet how this is achieved without direct sensorimotor experience remains a fundamental puzzle. This study proposes a novel theoretical solution by introducing the Collective World Model hypothesis. We argue that an LLM does not learn a world model from scratch; instead, it learns a statistical approximation of a collective world model that is already implicitly encoded in human language through a society-wide process of embodied, interactive sense-making. To formalize this process, we introduce generative emergent communication (Generative EmCom), a framework built on the Collective Predictive Coding (CPC). This framework models the emergence of language as a process of decentralized Bayesian inference over the internal states of multiple agents. We argue that this process effectively creates an encoder-decoder structure at a societal scale: human society collectively encodes its grounded, internal representations into language, and an LLM subsequently decodes these symbols to reconstruct a latent space that mirrors the structure of the original collective representations. This perspective provides a principled, mathematical explanation for how LLMs acquire their capabilities. The main contributions of this paper are: 1) the formalization of the Generative EmCom framework, clarifying its connection to world models and multi-agent reinforcement learning, and 2) its application to interpret LLMs, explaining phenomena such as distributional semantics as a natural consequence of representation reconstruction. This work provides a unified theory that bridges individual cognitive development, collective language evolution, and the foundations of large-scale AI.
<div id='section'>Paperid: <span id='pid'>123, <a href='https://arxiv.org/pdf/2512.11862.pdf' target='_blank'>https://arxiv.org/pdf/2512.11862.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiahao You, Ziye Jia, Can Cui, Chao Dong, Qihui Wu, Zhu Han
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.11862">Hierarchical Task Offloading and Trajectory Optimization in Low-Altitude Intelligent Networks Via Auction and Diffusion-based MARL</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The low-altitude intelligent networks (LAINs) emerge as a promising architecture for delivering low-latency and energy-efficient edge intelligence in dynamic and infrastructure-limited environments. By integrating unmanned aerial vehicles (UAVs), aerial base stations, and terrestrial base stations, LAINs can support mission-critical applications such as disaster response, environmental monitoring, and real-time sensing. However, these systems face key challenges, including energy-constrained UAVs, stochastic task arrivals, and heterogeneous computing resources. To address these issues, we propose an integrated air-ground collaborative network and formulate a time-dependent integer nonlinear programming problem that jointly optimizes UAV trajectory planning and task offloading decisions. The problem is challenging to solve due to temporal coupling among decision variables. Therefore, we design a hierarchical learning framework with two timescales. At the large timescale, a Vickrey-Clarke-Groves auction mechanism enables the energy-aware and incentive-compatible trajectory assignment. At the small timescale, we propose the diffusion-heterogeneous-agent proximal policy optimization, a generative multi-agent reinforcement learning algorithm that embeds latent diffusion models into actor networks. Each UAV samples actions from a Gaussian prior and refines them via observation-conditioned denoising, enhancing adaptability and policy diversity. Extensive simulations show that our framework outperforms baselines in energy efficiency, task success rate, and convergence performance.
<div id='section'>Paperid: <span id='pid'>124, <a href='https://arxiv.org/pdf/2504.21278.pdf' target='_blank'>https://arxiv.org/pdf/2504.21278.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuyan Ma, Yawen Wang, Junjie Wang, Xiaofei Xie, Boyu Wu, Shoubin Li, Fanjiang Xu, Qing Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.21278">Robust Multi-agent Communication Based on Decentralization-Oriented Adversarial Training</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In typical multi-agent reinforcement learning (MARL) problems, communication is important for agents to share information and make the right decisions. However, due to the complexity of training multi-agent communication, existing methods often fall into the dilemma of local optimization, which leads to the concentration of communication in a limited number of channels and presents an unbalanced structure. Such unbalanced communication policy are vulnerable to abnormal conditions, where the damage of critical communication channels can trigger the crash of the entire system. Inspired by decentralization theory in sociology, we propose DMAC, which enhances the robustness of multi-agent communication policies by retraining them into decentralized patterns. Specifically, we train an adversary DMAC\_Adv which can dynamically identify and mask the critical communication channels, and then apply the adversarial samples generated by DMAC\_Adv to the adversarial learning of the communication policy to force the policy in exploring other potential communication schemes and transition to a decentralized structure. As a training method to improve robustness, DMAC can be fused with any learnable communication policy algorithm. The experimental results in two communication policies and four multi-agent tasks demonstrate that DMAC achieves higher improvement on robustness and performance of communication policy compared with two state-of-the-art and commonly-used baselines. Also, the results demonstrate that DMAC can achieve decentralized communication structure with acceptable communication cost.
<div id='section'>Paperid: <span id='pid'>125, <a href='https://arxiv.org/pdf/2502.08119.pdf' target='_blank'>https://arxiv.org/pdf/2502.08119.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiahao You, Ziye Jia, Chao Dong, Qihui Wu, Zhu Han
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.08119">Generative AI-Enhanced Cooperative MEC of UAVs and Ground Stations for Unmanned Surface Vehicles</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The increasing deployment of unmanned surface vehicles (USVs) require computational support and coverage in applications such as maritime search and rescue. Unmanned aerial vehicles (UAVs) can offer low-cost, flexible aerial services, and ground stations (GSs) can provide powerful supports, which can cooperate to help the USVs in complex scenarios. However, the collaboration between UAVs and GSs for USVs faces challenges of task uncertainties, USVs trajectory uncertainties, heterogeneities, and limited computational resources. To address these issues, we propose a cooperative UAV and GS based robust multi-access edge computing framework to assist USVs in completing computational tasks. Specifically, we formulate the optimization problem of joint task offloading and UAV trajectory to minimize the total execution time, which is in the form of mixed integer nonlinear programming and NP-hard to tackle. Therefore, we propose the algorithm of generative artificial intelligence-enhanced heterogeneous agent proximal policy optimization (GAI-HAPPO). The proposed algorithm integrates GAI models to enhance the actor network ability to model complex environments and extract high-level features, thereby allowing the algorithm to predict uncertainties and adapt to dynamic conditions. Additionally, GAI stabilizes the critic network, addressing the instability of multi-agent reinforcement learning approaches. Finally, extensive simulations demonstrate that the proposed algorithm outperforms the existing benchmark methods, thus highlighting the potentials in tackling intricate, cross-domain issues in the considered scenarios.
<div id='section'>Paperid: <span id='pid'>126, <a href='https://arxiv.org/pdf/2412.10442.pdf' target='_blank'>https://arxiv.org/pdf/2412.10442.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ching-Chun Chang, Isao Echizen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.10442">Steganography in Game Actions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The exchange of messages has always carried with it the timeless challenge of secrecy. From whispers in shadows to the enigmatic notes written in the margins of history, humanity has long sought ways to convey thoughts that remain imperceptible to all but the chosen few. The challenge of subliminal communication has been addressed in various forms of steganography. However, the field faces a fundamental paradox: as the art of concealment advances, so too does the science of revelation, leading to an ongoing evolutionary interplay. This study seeks to extend the boundaries of what is considered a viable steganographic medium. We explore a steganographic paradigm, in which hidden information is communicated through the episodes of multiple agents interacting with an environment. Each agent, acting as an encoder, learns a policy to disguise the very existence of hidden messages within actions seemingly directed toward innocent objectives. Meanwhile, an observer, serving as a decoder, learns to associate behavioural patterns with their respective agents despite their dynamic nature, thereby unveiling the hidden messages. The interactions of agents are governed by the framework of multi-agent reinforcement learning and shaped by feedback from the observer. This framework encapsulates a game-theoretic dilemma, wherein agents face decisions between cooperating to create distinguishable behavioural patterns or defecting to pursue individually optimal yet potentially overlapping episodic actions. As a proof of concept, we exemplify action steganography through the game of labyrinth, a navigation task where subliminal communication is concealed within the act of steering toward a destination, and systematically validate the stego-system in terms of distortion, capacity, secrecy and robustness when subjected to simulated passive and active adversaries.
<div id='section'>Paperid: <span id='pid'>127, <a href='https://arxiv.org/pdf/2410.21616.pdf' target='_blank'>https://arxiv.org/pdf/2410.21616.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiwen Qiu, Yujia Zheng, Kun Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.21616">Identifying Selections for Unsupervised Subtask Discovery</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>When solving long-horizon tasks, it is intriguing to decompose the high-level task into subtasks. Decomposing experiences into reusable subtasks can improve data efficiency, accelerate policy generalization, and in general provide promising solutions to multi-task reinforcement learning and imitation learning problems. However, the concept of subtasks is not sufficiently understood and modeled yet, and existing works often overlook the true structure of the data generation process: subtasks are the results of a $\textit{selection}$ mechanism on actions, rather than possible underlying confounders or intermediates. Specifically, we provide a theory to identify, and experiments to verify the existence of selection variables in such data. These selections serve as subgoals that indicate subtasks and guide policy. In light of this idea, we develop a sequential non-negative matrix factorization (seq- NMF) method to learn these subgoals and extract meaningful behavior patterns as subtasks. Our empirical results on a challenging Kitchen environment demonstrate that the learned subtasks effectively enhance the generalization to new tasks in multi-task imitation learning scenarios. The codes are provided at https://anonymous.4open.science/r/Identifying\_Selections\_for\_Unsupervised\_Subtask\_Discovery/README.md.
<div id='section'>Paperid: <span id='pid'>128, <a href='https://arxiv.org/pdf/2505.03558.pdf' target='_blank'>https://arxiv.org/pdf/2505.03558.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Giacomo Avanzi, Marco Giordani, Michele Zorzi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.03558">Multi-Agent Reinforcement Learning Scheduling to Support Low Latency in Teleoperated Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The teleoperated driving (TD) scenario comes with stringent Quality of Service (QoS) communication constraints, especially in terms of end-to-end (E2E) latency and reliability. In this context, Predictive Quality of Service (PQoS), possibly combined with Reinforcement Learning (RL) techniques, is a powerful tool to estimate QoS degradation and react accordingly. For example, an intelligent agent can be trained to select the optimal compression configuration for automotive data, and reduce the file size whenever QoS conditions deteriorate. However, compression may inevitably compromise data quality, with negative implications for the TD application. An alternative strategy involves operating at the Radio Access Network (RAN) level to optimize radio parameters based on current network conditions, while preserving data quality. In this paper, we propose Multi-Agent Reinforcement Learning (MARL) scheduling algorithms, based on Proximal Policy Optimization (PPO), to dynamically and intelligently allocate radio resources to minimize E2E latency in a TD scenario. We evaluate two training paradigms, i.e., decentralized learning with local observations (IPPO) vs. centralized aggregation (MAPPO), in conjunction with two resource allocation strategies, i.e., proportional allocation (PA) and greedy allocation (GA). We prove via ns-3 simulations that MAPPO, combined with GA, achieves the best results in terms of latency, especially as the number of vehicles increases.
<div id='section'>Paperid: <span id='pid'>129, <a href='https://arxiv.org/pdf/2502.03723.pdf' target='_blank'>https://arxiv.org/pdf/2502.03723.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Muhan Lin, Shuyang Shi, Yue Guo, Vaishnav Tadiparthi, Behdad Chalaki, Ehsan Moradi Pari, Simon Stepputtis, Woojun Kim, Joseph Campbell, Katia Sycara
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.03723">Speaking the Language of Teamwork: LLM-Guided Credit Assignment in Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Credit assignment, the process of attributing credit or blame to individual agents for their contributions to a team's success or failure, remains a fundamental challenge in multi-agent reinforcement learning (MARL), particularly in environments with sparse rewards. Commonly-used approaches such as value decomposition often lead to suboptimal policies in these settings, and designing dense reward functions that align with human intuition can be complex and labor-intensive. In this work, we propose a novel framework where a large language model (LLM) generates dense, agent-specific rewards based on a natural language description of the task and the overall team goal. By learning a potential-based reward function over multiple queries, our method reduces the impact of ranking errors while allowing the LLM to evaluate each agent's contribution to the overall task. Through extensive experiments, we demonstrate that our approach achieves faster convergence and higher policy returns compared to state-of-the-art MARL baselines.
<div id='section'>Paperid: <span id='pid'>130, <a href='https://arxiv.org/pdf/2511.19146.pdf' target='_blank'>https://arxiv.org/pdf/2511.19146.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qian Zhang, Zhuo Sun, Yao Zhang, Zhiwen Yu, Bin Guo, Jun Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.19146">VIL2C: Value-of-Information Aware Low-Latency Communication for Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Inter-agent communication serves as an effective mechanism for enhancing performance in collaborative multi-agent reinforcement learning(MARL) systems. However, the inherent communication latency in practical systems induces both action decision delays and outdated information sharing, impeding MARL performance gains, particularly in time-critical applications like autonomous driving. In this work, we propose a Value-of-Information aware Low-latency Communication(VIL2C) scheme that proactively adjusts the latency distribution to mitigate its effects in MARL systems. Specifically, we define a Value of Information (VOI) metric to quantify the importance of delayed message transmission based on each delayed message's importance. Moreover, we propose a progressive message reception mechanism to adaptively adjust the reception duration based on received messages. We derive the optimized VoI aware resource allocation and theoretically prove the performance advantage of the proposed VIL2C scheme. Extensive experiments demonstrate that VIL2C outperforms existing approaches under various communication conditions. These gains are attributed to the low-latency transmission of high-VoI messages via resource allocation and the elimination of unnecessary waiting periods via adaptive reception duration.
<div id='section'>Paperid: <span id='pid'>131, <a href='https://arxiv.org/pdf/2509.20648.pdf' target='_blank'>https://arxiv.org/pdf/2509.20648.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiyuan Pan, Zhe Liu, Hesheng Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.20648">Wonder Wins Ways: Curiosity-Driven Exploration through Multi-Agent Contextual Calibration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autonomous exploration in complex multi-agent reinforcement learning (MARL) with sparse rewards critically depends on providing agents with effective intrinsic motivation. While artificial curiosity offers a powerful self-supervised signal, it often confuses environmental stochasticity with meaningful novelty. Moreover, existing curiosity mechanisms exhibit a uniform novelty bias, treating all unexpected observations equally. However, peer behavior novelty, which encode latent task dynamics, are often overlooked, resulting in suboptimal exploration in decentralized, communication-free MARL settings. To this end, inspired by how human children adaptively calibrate their own exploratory behaviors via observing peers, we propose a novel approach to enhance multi-agent exploration. We introduce CERMIC, a principled framework that empowers agents to robustly filter noisy surprise signals and guide exploration by dynamically calibrating their intrinsic curiosity with inferred multi-agent context. Additionally, CERMIC generates theoretically-grounded intrinsic rewards, encouraging agents to explore state transitions with high information gain. We evaluate CERMIC on benchmark suites including VMAS, Meltingpot, and SMACv2. Empirical results demonstrate that exploration with CERMIC significantly outperforms SoTA algorithms in sparse-reward environments.
<div id='section'>Paperid: <span id='pid'>132, <a href='https://arxiv.org/pdf/2505.19714.pdf' target='_blank'>https://arxiv.org/pdf/2505.19714.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhaopeng Feng, Yupu Liang, Shaosheng Cao, Jiayuan Su, Jiahan Ren, Zhe Xu, Yao Hu, Wenxuan Huang, Jian Wu, Zuozhu Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.19714">MT$^{3}$: Scaling MLLM-based Text Image Machine Translation via Multi-Task Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text Image Machine Translation (TIMT)-the task of translating textual content embedded in images-is critical for applications in accessibility, cross-lingual information access, and real-world document understanding. However, TIMT remains a complex challenge due to the need for accurate optical character recognition (OCR), robust visual-text reasoning, and high-quality translation, often requiring cascading multi-stage pipelines. Recent advances in large-scale Reinforcement Learning (RL) have improved reasoning in Large Language Models (LLMs) and Multimodal LLMs (MLLMs), but their application to end-to-end TIMT is still underexplored. To bridge this gap, we introduce MT$^{3}$, the first framework to apply Multi-Task RL to MLLMs for end-to-end TIMT. MT$^{3}$ adopts a multi-task optimization paradigm targeting three key sub-skills: text recognition, context-aware reasoning, and translation. It is trained using a novel multi-mixed reward mechanism that adapts rule-based RL strategies to TIMT's intricacies, offering fine-grained, non-binary feedback across tasks. Furthermore, to facilitate the evaluation of TIMT in authentic cross-cultural and real-world social media contexts, we introduced XHSPost, the first social media TIMT benchmark. Our MT$^{3}$-7B-Zero achieves state-of-the-art results on the latest in-domain MIT-10M benchmark, outperforming strong baselines such as Qwen2.5-VL-72B and InternVL2.5-78B by notable margins across multiple metrics. Additionally, the model shows strong generalization to out-of-distribution language pairs and datasets. In-depth analyses reveal how multi-task synergy, reinforcement learning initialization, curriculum design, and reward formulation contribute to advancing MLLM-driven TIMT.
<div id='section'>Paperid: <span id='pid'>133, <a href='https://arxiv.org/pdf/2601.06382.pdf' target='_blank'>https://arxiv.org/pdf/2601.06382.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Philipp Altmann, Thomy Phan, Maximilian Zorn, Claudia Linnhoff-Popien, Sven Koenig
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.06382">Dynamic Incentivized Cooperation under Changing Rewards</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Peer incentivization (PI) is a popular multi-agent reinforcement learning approach where all agents can reward or penalize each other to achieve cooperation in social dilemmas. Despite their potential for scalable cooperation, current PI methods heavily depend on fixed incentive values that need to be appropriately chosen with respect to the environmental rewards and thus are highly sensitive to their changes. Therefore, they fail to maintain cooperation under changing rewards in the environment, e.g., caused by modified specifications, varying supply and demand, or sensory flaws - even when the conditions for mutual cooperation remain the same. In this paper, we propose Dynamic Reward Incentives for Variable Exchange (DRIVE), an adaptive PI approach to cooperation in social dilemmas with changing rewards. DRIVE agents reciprocally exchange reward differences to incentivize mutual cooperation in a completely decentralized way. We show how DRIVE achieves mutual cooperation in the general Prisoner's Dilemma and empirically evaluate DRIVE in more complex sequential social dilemmas with changing rewards, demonstrating its ability to achieve and maintain cooperation, in contrast to current state-of-the-art PI methods.
<div id='section'>Paperid: <span id='pid'>134, <a href='https://arxiv.org/pdf/2504.14520.pdf' target='_blank'>https://arxiv.org/pdf/2504.14520.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ahsan Bilal, Muhammad Ahmed Mohsin, Muhammad Umer, Muhammad Awais Khan Bangash, Muhammad Ali Jamshed
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.14520">Meta-Thinking in LLMs via Multi-Agent Reinforcement Learning: A Survey</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This survey explores the development of meta-thinking capabilities in Large Language Models (LLMs) from a Multi-Agent Reinforcement Learning (MARL) perspective. Meta-thinking self-reflection, assessment, and control of thinking processes is an important next step in enhancing LLM reliability, flexibility, and performance, particularly for complex or high-stakes tasks. The survey begins by analyzing current LLM limitations, such as hallucinations and the lack of internal self-assessment mechanisms. It then talks about newer methods, including RL from human feedback (RLHF), self-distillation, and chain-of-thought prompting, and each of their limitations. The crux of the survey is to talk about how multi-agent architectures, namely supervisor-agent hierarchies, agent debates, and theory of mind frameworks, can emulate human-like introspective behavior and enhance LLM robustness. By exploring reward mechanisms, self-play, and continuous learning methods in MARL, this survey gives a comprehensive roadmap to building introspective, adaptive, and trustworthy LLMs. Evaluation metrics, datasets, and future research avenues, including neuroscience-inspired architectures and hybrid symbolic reasoning, are also discussed.
<div id='section'>Paperid: <span id='pid'>135, <a href='https://arxiv.org/pdf/2509.23154.pdf' target='_blank'>https://arxiv.org/pdf/2509.23154.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinzhe Pan, Jingqing Wang, Yuehui Ouyang, Wenchi Cheng, Wei Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.23154">AI-Enhanced Distributed Channel Access for Collision Avoidance in Future Wi-Fi 8</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The exponential growth of wireless devices and stringent reliability requirements of emerging applications demand fundamental improvements in distributed channel access mechanisms for unlicensed bands. Current Wi-Fi systems, which rely on binary exponential backoff (BEB), suffer from suboptimal collision resolution in dense deployments and persistent fairness challenges due to inherent randomness. This paper introduces a multi-agent reinforcement learning framework that integrates artificial intelligence (AI) optimization with legacy device coexistence. We first develop a dynamic backoff selection mechanism that adapts to real-time channel conditions through access deferral events while maintaining full compatibility with conventional CSMA/CA operations. Second, we introduce a fairness quantification metric aligned with enhanced distributed channel access (EDCA) principles to ensure equitable medium access opportunities. Finally, we propose a centralized training decentralized execution (CTDE) architecture incorporating neighborhood activity patterns as observational inputs, optimized via constrained multi-agent proximal policy optimization (MAPPO) to jointly minimize collisions and guarantee fairness. Experimental results demonstrate that our solution significantly reduces collision probability compared to conventional BEB while preserving backward compatibility with commercial Wi-Fi devices. The proposed fairness metric effectively eliminates starvation risks in heterogeneous scenarios.
<div id='section'>Paperid: <span id='pid'>136, <a href='https://arxiv.org/pdf/2506.14187.pdf' target='_blank'>https://arxiv.org/pdf/2506.14187.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiaming Yu, Le Liang, Hao Ye, Shi Jin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.14187">Hierarchical Multi-Agent Reinforcement Learning-based Coordinated Spatial Reuse for Next Generation WLANs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>High-density Wi-Fi deployments often result in significant co-channel interference, which degrades overall network performance. To address this issue, coordination of multi access points (APs) has been considered to enable coordinated spatial reuse (CSR) in next generation wireless local area networks. This paper tackles the challenge of downlink spatial reuse in Wi-Fi networks, specifically in scenarios involving overlapping basic service sets, by employing hierarchical multi-agent reinforcement learning (HMARL). We decompose the CSR process into two phases, i.e., a polling phase and a decision phase, and introduce the HMARL algorithm to enable efficient CSR. To enhance training efficiency, the proposed HMARL algorithm employs a hierarchical structure, where station selection and power control are determined by a high- and low-level policy network, respectively. Simulation results demonstrate that this approach consistently outperforms baseline methods in terms of throughput and latency across various network topologies. Moreover, the algorithm exhibits robust performance when coexisting with legacy APs. Additional experiments in a representative topology further reveal that the carefully designed reward function not only maximizes the overall network throughput, but also improves fairness in transmission opportunities for APs in high-interference regions.
<div id='section'>Paperid: <span id='pid'>137, <a href='https://arxiv.org/pdf/2505.03533.pdf' target='_blank'>https://arxiv.org/pdf/2505.03533.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiacheng Wang, Le Liang, Hao Ye, Chongtao Guo, Shi Jin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.03533">Small-Scale-Fading-Aware Resource Allocation in Wireless Federated Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Judicious resource allocation can effectively enhance federated learning (FL) training performance in wireless networks by addressing both system and statistical heterogeneity. However, existing strategies typically rely on block fading assumptions, which overlooks rapid channel fluctuations within each round of FL gradient uploading, leading to a degradation in FL training performance. Therefore, this paper proposes a small-scale-fading-aware resource allocation strategy using a multi-agent reinforcement learning (MARL) framework. Specifically, we establish a one-step convergence bound of the FL algorithm and formulate the resource allocation problem as a decentralized partially observable Markov decision process (Dec-POMDP), which is subsequently solved using the QMIX algorithm. In our framework, each client serves as an agent that dynamically determines spectrum and power allocations within each coherence time slot, based on local observations and a reward derived from the convergence analysis. The MARL setting reduces the dimensionality of the action space and facilitates decentralized decision-making, enhancing the scalability and practicality of the solution. Experimental results demonstrate that our QMIX-based resource allocation strategy significantly outperforms baseline methods across various degrees of statistical heterogeneity. Additionally, ablation studies validate the critical importance of incorporating small-scale fading dynamics, highlighting its role in optimizing FL performance.
<div id='section'>Paperid: <span id='pid'>138, <a href='https://arxiv.org/pdf/2412.14218.pdf' target='_blank'>https://arxiv.org/pdf/2412.14218.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiaming Yu, Le Liang, Chongtao Guo, Ziyang Guo, Shi Jin, Geoffrey Ye Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.14218">Heterogeneous Multi-Agent Reinforcement Learning for Distributed Channel Access in WLANs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper investigates the use of multi-agent reinforcement learning (MARL) to address distributed channel access in wireless local area networks. In particular, we consider the challenging yet more practical case where the agents heterogeneously adopt value-based or policy-based reinforcement learning algorithms to train the model. We propose a heterogeneous MARL training framework, named QPMIX, which adopts a centralized training with distributed execution paradigm to enable heterogeneous agents to collaborate. Moreover, we theoretically prove the convergence of the proposed heterogeneous MARL method when using the linear value function approximation. Our method maximizes the network throughput and ensures fairness among stations, therefore, enhancing the overall network performance. Simulation results demonstrate that the proposed QPMIX algorithm improves throughput, mean delay, delay jitter, and collision rates compared with conventional carrier-sense multiple access with collision avoidance (CSMA/CA) mechanism in the saturated traffic scenario. Furthermore, the QPMIX algorithm is robust in unsaturated and delay-sensitive traffic scenarios. It coexists well with the conventional CSMA/CA mechanism and promotes cooperation among heterogeneous agents.
<div id='section'>Paperid: <span id='pid'>139, <a href='https://arxiv.org/pdf/2412.03072.pdf' target='_blank'>https://arxiv.org/pdf/2412.03072.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinyu Qiao, Yudong Hu, Congying Han, Weiyan Wu, Tiande Guo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.03072">Preference-based opponent shaping in differentiable games</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Strategy learning in game environments with multi-agent is a challenging problem. Since each agent's reward is determined by the joint strategy, a greedy learning strategy that aims to maximize its own reward may fall into a local optimum. Recent studies have proposed the opponent modeling and shaping methods for game environments. These methods enhance the efficiency of strategy learning by modeling the strategies and updating processes of other agents. However, these methods often rely on simple predictions of opponent strategy changes. Due to the lack of modeling behavioral preferences such as cooperation and competition, they are usually applicable only to predefined scenarios and lack generalization capabilities. In this paper, we propose a novel Preference-based Opponent Shaping (PBOS) method to enhance the strategy learning process by shaping agents' preferences towards cooperation. We introduce the preference parameter, which is incorporated into the agent's loss function, thus allowing the agent to directly consider the opponent's loss function when updating the strategy. We update the preference parameters concurrently with strategy learning to ensure that agents can adapt to any cooperative or competitive game environment. Through a series of experiments, we verify the performance of PBOS algorithm in a variety of differentiable games. The experimental results show that the PBOS algorithm can guide the agent to learn the appropriate preference parameters, so as to achieve better reward distribution in multiple game environments.
<div id='section'>Paperid: <span id='pid'>140, <a href='https://arxiv.org/pdf/2409.05712.pdf' target='_blank'>https://arxiv.org/pdf/2409.05712.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiaqi Liu, Peng Hang, Xiaoxiang Na, Chao Huang, Jian Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.05712">Cooperative Decision-Making for CAVs at Unsignalized Intersections: A MARL Approach with Attention and Hierarchical Game Priors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The development of autonomous vehicles has shown great potential to enhance the efficiency and safety of transportation systems. However, the decision-making issue in complex human-machine mixed traffic scenarios, such as unsignalized intersections, remains a challenge for autonomous vehicles. While reinforcement learning (RL) has been used to solve complex decision-making problems, existing RL methods still have limitations in dealing with cooperative decision-making of multiple connected autonomous vehicles (CAVs), ensuring safety during exploration, and simulating realistic human driver behaviors. In this paper, a novel and efficient algorithm, Multi-Agent Game-prior Attention Deep Deterministic Policy Gradient (MA-GA-DDPG), is proposed to address these limitations. Our proposed algorithm formulates the decision-making problem of CAVs at unsignalized intersections as a decentralized multi-agent reinforcement learning problem and incorporates an attention mechanism to capture interaction dependencies between ego CAV and other agents. The attention weights between the ego vehicle and other agents are then used to screen interaction objects and obtain prior hierarchical game relations, based on which a safety inspector module is designed to improve the traffic safety. Furthermore, both simulation and hardware-in-the-loop experiments were conducted, demonstrating that our method outperforms other baseline approaches in terms of driving safety, efficiency, and comfort.
<div id='section'>Paperid: <span id='pid'>141, <a href='https://arxiv.org/pdf/2601.08327.pdf' target='_blank'>https://arxiv.org/pdf/2601.08327.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gabriele Calzolari, Vidya Sumathy, Christoforos Kanellakis, George Nikolakopoulos
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.08327">Safe Heterogeneous Multi-Agent RL with Communication Regularization for Coordinated Target Acquisition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces a decentralized multi-agent reinforcement learning framework enabling structurally heterogeneous teams of agents to jointly discover and acquire randomly located targets in environments characterized by partial observability, communication constraints, and dynamic interactions. Each agent's policy is trained with the Multi-Agent Proximal Policy Optimization algorithm and employs a Graph Attention Network encoder that integrates simulated range-sensing data with communication embeddings exchanged among neighboring agents, enabling context-aware decision-making from both local sensing and relational information. In particular, this work introduces a unified framework that integrates graph-based communication and trajectory-aware safety through safety filters. The architecture is supported by a structured reward formulation designed to encourage effective target discovery and acquisition, collision avoidance, and de-correlation between the agents' communication vectors by promoting informational orthogonality. The effectiveness of the proposed reward function is demonstrated through a comprehensive ablation study. Moreover, simulation results demonstrate safe and stable task execution, confirming the framework's effectiveness.
<div id='section'>Paperid: <span id='pid'>142, <a href='https://arxiv.org/pdf/2502.03845.pdf' target='_blank'>https://arxiv.org/pdf/2502.03845.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhuohui Zhang, Bin Cheng, Zhipeng Wang, Yanmin Zhou, Gang Li, Ping Lu, Bin He, Jie Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.03845">PAGNet: Pluggable Adaptive Generative Networks for Information Completion in Multi-Agent Communication</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>For partially observable cooperative tasks, multi-agent systems must develop effective communication and understand the interplay among agents in order to achieve cooperative goals. However, existing multi-agent reinforcement learning (MARL) with communication methods lack evaluation metrics for information weights and information-level communication modeling. This causes agents to neglect the aggregation of multiple messages, thereby significantly reducing policy learning efficiency. In this paper, we propose pluggable adaptive generative networks (PAGNet), a novel framework that integrates generative models into MARL to enhance communication and decision-making. PAGNet enables agents to synthesize global states representations from weighted local observations and use these representations alongside learned communication weights for coordinated decision-making. This pluggable approach reduces the computational demands typically associated with the joint training of communication and policy networks. Extensive experimental evaluations across diverse benchmarks and communication scenarios demonstrate the significant performance improvements achieved by PAGNet. Furthermore, we analyze the emergent communication patterns and the quality of generated global states, providing insights into operational mechanisms.
<div id='section'>Paperid: <span id='pid'>143, <a href='https://arxiv.org/pdf/2412.20075.pdf' target='_blank'>https://arxiv.org/pdf/2412.20075.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gabriele Calzolari, Vidya Sumathy, Christoforos Kanellakis, George Nikolakopoulos
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.20075">Investigating the Impact of Communication-Induced Action Space on Exploration of Unknown Environments with Decentralized Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces a novel enhancement to the Decentralized Multi-Agent Reinforcement Learning (D-MARL) exploration by proposing communication-induced action space to improve the mapping efficiency of unknown environments using homogeneous agents. Efficient exploration of large environments relies heavily on inter-agent communication as real-world scenarios are often constrained by data transmission limits, such as signal latency and bandwidth. Our proposed method optimizes each agent's policy using the heterogeneous-agent proximal policy optimization algorithm, allowing agents to autonomously decide whether to communicate or to explore, that is whether to share the locally collected maps or continue the exploration. We propose and compare multiple novel reward functions that integrate inter-agent communication and exploration, enhance mapping efficiency and robustness, and minimize exploration overlap. This article presents a framework developed in ROS2 to evaluate and validate the investigated architecture. Specifically, four TurtleBot3 Burgers have been deployed in a Gazebo-designed environment filled with obstacles to evaluate the efficacy of the trained policies in mapping the exploration arena.
<div id='section'>Paperid: <span id='pid'>144, <a href='https://arxiv.org/pdf/2510.04862.pdf' target='_blank'>https://arxiv.org/pdf/2510.04862.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sam Earle, Zehua Jiang, Eugene Vinitsky, Julian Togelius
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.04862">Video Game Level Design as a Multi-Agent Reinforcement Learning Problem</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Procedural Content Generation via Reinforcement Learning (PCGRL) offers a method for training controllable level designer agents without the need for human datasets, using metrics that serve as proxies for level quality as rewards. Existing PCGRL research focuses on single generator agents, but are bottlenecked by the need to frequently recalculate heuristics of level quality and the agent's need to navigate around potentially large maps. By framing level generation as a multi-agent problem, we mitigate the efficiency bottleneck of single-agent PCGRL by reducing the number of reward calculations relative to the number of agent actions. We also find that multi-agent level generators are better able to generalize to out-of-distribution map shapes, which we argue is due to the generators' learning more local, modular design policies. We conclude that treating content generation as a distributed, multi-agent task is beneficial for generating functional artifacts at scale.
<div id='section'>Paperid: <span id='pid'>145, <a href='https://arxiv.org/pdf/2503.18816.pdf' target='_blank'>https://arxiv.org/pdf/2503.18816.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chak Lam Shek, Amrit Singh Bedi, Anjon Basak, Ellen Novoseller, Nick Waytowich, Priya Narayanan, Dinesh Manocha, Pratap Tokekar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.18816">Learning Multi-Robot Coordination through Locality-Based Factorized Multi-Agent Actor-Critic Algorithm</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this work, we present a novel cooperative multi-agent reinforcement learning method called \textbf{Loc}ality based \textbf{Fac}torized \textbf{M}ulti-Agent \textbf{A}ctor-\textbf{C}ritic (Loc-FACMAC). Existing state-of-the-art algorithms, such as FACMAC, rely on global reward information, which may not accurately reflect the quality of individual robots' actions in decentralized systems. We integrate the concept of locality into critic learning, where strongly related robots form partitions during training. Robots within the same partition have a greater impact on each other, leading to more precise policy evaluation. Additionally, we construct a dependency graph to capture the relationships between robots, facilitating the partitioning process. This approach mitigates the curse of dimensionality and prevents robots from using irrelevant information. Our method improves existing algorithms by focusing on local rewards and leveraging partition-based learning to enhance training efficiency and performance. We evaluate the performance of Loc-FACMAC in three environments: Hallway, Multi-cartpole, and Bounded-Cooperative-Navigation. We explore the impact of partition sizes on the performance and compare the result with baseline MARL algorithms such as LOMAQ, FACMAC, and QMIX. The experiments reveal that, if the locality structure is defined properly, Loc-FACMAC outperforms these baseline algorithms up to 108\%, indicating that exploiting the locality structure in the actor-critic framework improves the MARL performance.
<div id='section'>Paperid: <span id='pid'>146, <a href='https://arxiv.org/pdf/2512.03963.pdf' target='_blank'>https://arxiv.org/pdf/2512.03963.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tao Wu, Li Yang, Gen Zhan, Yabin Zhang, Yiting Liao, Junlin Li, Deliang Fu, Li Zhang, Limin Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.03963">TempR1: Improving Temporal Understanding of MLLMs via Temporal-Aware Multi-Task Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Enhancing the temporal understanding of Multimodal Large Language Models (MLLMs) is essential for advancing long-form video analysis, enabling tasks such as temporal localization, action detection, and time-sensitive question answering. While reinforcement learning (RL) has recently been explored for improving temporal reasoning, existing approaches are often confined to limited task types and data, restricting their generalization across diverse temporal understanding scenarios. To address this challenge, we present TempR1, a temporal-aware multi-task reinforcement learning framework that systematically strengthens MLLMs' temporal comprehension. We curate a multi-task corpus that exposes the model to diverse temporal structures and semantics, and build upon the Group Relative Policy Optimization (GRPO) algorithm to achieve stable and effective cross-task optimization. Specifically, we categorize temporal tasks into three correspondence types between predicted intervals and ground-truth instances, and design tailored localization rewards for each, enabling TempR1 to capture fine-grained temporal dependencies and adapt to different temporal patterns. Extensive experiments demonstrate that TempR1 attains state-of-the-art performance across multiple benchmarks. Moreover, its joint optimization over complementary tasks yields a strong synergistic effect, enhancing both generalization and single-task performance, establishing a scalable and principled paradigm for temporal reasoning in MLLMs.
<div id='section'>Paperid: <span id='pid'>147, <a href='https://arxiv.org/pdf/2511.19368.pdf' target='_blank'>https://arxiv.org/pdf/2511.19368.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianyang Duan, Zongyuan Zhang, Zheng Lin, Songxiao Guo, Xiuxian Guan, Guangyu Wu, Zihan Fang, Haotian Meng, Xia Du, Ji-Zhe Zhou, Heming Cui, Jun Luo, Yue Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.19368">LLM-Driven Stationarity-Aware Expert Demonstrations for Multi-Agent Reinforcement Learning in Mobile Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent reinforcement learning (MARL) has been increasingly adopted in many real-world applications. While MARL enables decentralized deployment on resource-constrained edge devices, it suffers from severe non-stationarity due to the synchronous updates of agent policies. This non stationarity results in unstable training and poor policy con vergence, especially as the number of agents increases. In this paper, we propose RELED, a scalable MARL framework that integrates large language model (LLM)-driven expert demonstrations with autonomous agent exploration. RELED incorporates a Stationarity-Aware Expert Demonstration module, which leverages theoretical non-stationarity bounds to enhance the quality of LLM-generated expert trajectories, thus providing high reward and training-stable samples for each agent. Moreover, a Hybrid Expert-Agent Policy Optimization module adaptively balances each agent's learning from both expert-generated and agent-generated trajectories, accelerating policy convergence and improving generalization. Extensive experiments with real city networks based on OpenStreetMap demonstrate that RELED achieves superior performance compared to state-of-the-art MARL methods.
<div id='section'>Paperid: <span id='pid'>148, <a href='https://arxiv.org/pdf/2509.15103.pdf' target='_blank'>https://arxiv.org/pdf/2509.15103.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Simin Li, Zheng Yuwei, Zihao Mao, Linhao Wang, Ruixiao Xu, Chengdong Ma, Xin Yu, Yuqing Ma, Qi Dou, Xin Wang, Jie Luo, Bo An, Yaodong Yang, Weifeng Lv, Xianglong Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.15103">Vulnerable Agent Identification in Large-Scale Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Partial agent failure becomes inevitable when systems scale up, making it crucial to identify the subset of agents whose compromise would most severely degrade overall performance. In this paper, we study this Vulnerable Agent Identification (VAI) problem in large-scale multi-agent reinforcement learning (MARL). We frame VAI as a Hierarchical Adversarial Decentralized Mean Field Control (HAD-MFC), where the upper level involves an NP-hard combinatorial task of selecting the most vulnerable agents, and the lower level learns worst-case adversarial policies for these agents using mean-field MARL. The two problems are coupled together, making HAD-MFC difficult to solve. To solve this, we first decouple the hierarchical process by Fenchel-Rockafellar transform, resulting a regularized mean-field Bellman operator for upper level that enables independent learning at each level, thus reducing computational complexity. We then reformulate the upper-level combinatorial problem as a MDP with dense rewards from our regularized mean-field Bellman operator, enabling us to sequentially identify the most vulnerable agents by greedy and RL algorithms. This decomposition provably preserves the optimal solution of the original HAD-MFC. Experiments show our method effectively identifies more vulnerable agents in large-scale MARL and the rule-based system, fooling system into worse failures, and learns a value function that reveals the vulnerability of each agent.
<div id='section'>Paperid: <span id='pid'>149, <a href='https://arxiv.org/pdf/2509.14680.pdf' target='_blank'>https://arxiv.org/pdf/2509.14680.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianyang Duan, Zongyuan Zhang, Songxiao Guo, Dong Huang, Yuanye Zhao, Zheng Lin, Zihan Fang, Dianxin Luan, Heming Cui, Yong Cui
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.14680">LEED: A Highly Efficient and Scalable LLM-Empowered Expert Demonstrations Framework for Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent reinforcement learning (MARL) holds substantial promise for intelligent decision-making in complex environments. However, it suffers from a coordination and scalability bottleneck as the number of agents increases. To address these issues, we propose the LLM-empowered expert demonstrations framework for multi-agent reinforcement learning (LEED). LEED consists of two components: a demonstration generation (DG) module and a policy optimization (PO) module. Specifically, the DG module leverages large language models to generate instructions for interacting with the environment, thereby producing high-quality demonstrations. The PO module adopts a decentralized training paradigm, where each agent utilizes the generated demonstrations to construct an expert policy loss, which is then integrated with its own policy loss. This enables each agent to effectively personalize and optimize its local policy based on both expert knowledge and individual experience. Experimental results show that LEED achieves superior sample efficiency, time efficiency, and robust scalability compared to state-of-the-art baselines.
<div id='section'>Paperid: <span id='pid'>150, <a href='https://arxiv.org/pdf/2506.24119.pdf' target='_blank'>https://arxiv.org/pdf/2506.24119.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bo Liu, Leon Guertler, Simon Yu, Zichen Liu, Penghui Qi, Daniel Balcells, Mickel Liu, Cheston Tan, Weiyan Shi, Min Lin, Wee Sun Lee, Natasha Jaques
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.24119">SPIRAL: Self-Play on Zero-Sum Games Incentivizes Reasoning via Multi-Agent Multi-Turn Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in reinforcement learning have shown that language models can develop sophisticated reasoning through training on tasks with verifiable rewards, but these approaches depend on human-curated problem-answer pairs and domain-specific reward engineering. We introduce SPIRAL, a self-play framework where models learn by playing multi-turn, zero-sum games against continuously improving versions of themselves, eliminating the need for human supervision. Through self-play, SPIRAL generates an infinite curriculum of progressively challenging problems as models must constantly adapt to stronger opponents. To enable this self-play training at scale, We implement a fully online, multi-turn, multi-agent reinforcement learning system for LLMs and propose role-conditioned advantage estimation (RAE) to stabilize multi-agent training. Using SPIRAL, self-play on zero-sum games produces reasoning capabilities that transfer broadly. Training Qwen3-4B-Base on Kuhn Poker alone achieves 8.6% improvement on math and 8.4% on general reasoning, outperforming SFT on 25,000 expert game trajectories. Analysis reveals that this transfer occurs through three cognitive patterns: systematic decomposition, expected value calculation, and case-by-case analysis. Multi-game training (TicTacToe, Kuhn Poker, Simple Negotiation) further enhances performance as each game develops distinct reasoning strengths. Applying SPIRAL to a strong reasoning model (DeepSeek-R1-Distill-Qwen-7B) can still lead to 2.0% average improvement. These results demonstrate that zero-sum games naturally develop transferable reasoning capabilities, highlighting a promising direction for autonomous reasoning development.
<div id='section'>Paperid: <span id='pid'>151, <a href='https://arxiv.org/pdf/2506.14990.pdf' target='_blank'>https://arxiv.org/pdf/2506.14990.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tristan Tomilin, Luka van den Boogaard, Samuel Garcin, Bram Grooten, Meng Fang, Yali Du, Mykola Pechenizkiy
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.14990">MEAL: A Benchmark for Continual Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Benchmarks play a crucial role in the development and analysis of reinforcement learning (RL) algorithms, with environment availability strongly impacting research. One particularly underexplored intersection is continual learning (CL) in cooperative multi-agent settings. To remedy this, we introduce MEAL (Multi-agent Environments for Adaptive Learning), the first benchmark tailored for continual multi-agent reinforcement learning (CMARL). Existing CL benchmarks run environments on the CPU, leading to computational bottlenecks and limiting the length of task sequences. MEAL leverages JAX for GPU acceleration, enabling continual learning across sequences of 100 tasks on a standard desktop PC in a few hours. We show that naively combining popular CL and MARL methods yields strong performance on simple environments, but fails to scale to more complex settings requiring sustained coordination and adaptation. Our ablation study identifies architectural and algorithmic features critical for CMARL on MEAL.
<div id='section'>Paperid: <span id='pid'>152, <a href='https://arxiv.org/pdf/2503.07817.pdf' target='_blank'>https://arxiv.org/pdf/2503.07817.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kefan Song, Runnan Jiang, Rohan Chandra, Shangtong Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.07817">Group Fairness in Multi-Task Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper addresses a critical societal consideration in the application of Reinforcement Learning (RL): ensuring equitable outcomes across different demographic groups in multi-task settings. While previous work has explored fairness in single-task RL, many real-world applications are multi-task in nature and require policies to maintain fairness across all tasks. We introduce a novel formulation of multi-task group fairness in RL and propose a constrained optimization algorithm that explicitly enforces fairness constraints across multiple tasks simultaneously. We have shown that our proposed algorithm does not violate fairness constraints with high probability and with sublinear regret in the finite-horizon episodic setting. Through experiments in RiverSwim and MuJoCo environments, we demonstrate that our approach better ensures group fairness across multiple tasks compared to previous methods that lack explicit multi-task fairness constraints in both the finite-horizon setting and the infinite-horizon setting. Our results show that the proposed algorithm achieves smaller fairness gaps while maintaining comparable returns across different demographic groups and tasks, suggesting its potential for addressing fairness concerns in real-world multi-task RL applications.
<div id='section'>Paperid: <span id='pid'>153, <a href='https://arxiv.org/pdf/2412.10700.pdf' target='_blank'>https://arxiv.org/pdf/2412.10700.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiying Wang, Gang Sun, Yuhui Wang, Hongfang Yu, Dusit Niyato
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.10700">Cluster-Based Multi-Agent Task Scheduling for Space-Air-Ground Integrated Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The Space-Air-Ground Integrated Network (SAGIN) framework is a crucial foundation for future networks, where satellites and aerial nodes assist in computational task offloading. The low-altitude economy, leveraging the flexibility and multifunctionality of Unmanned Aerial Vehicles (UAVs) in SAGIN, holds significant potential for development in areas such as communication and sensing. However, effective coordination is needed to streamline information exchange and enable efficient system resource allocation. In this paper, we propose a Clustering-based Multi-agent Deep Deterministic Policy Gradient (CMADDPG) algorithm to address the multi-UAV cooperative task scheduling challenges in SAGIN. The CMADDPG algorithm leverages dynamic UAV clustering to partition UAVs into clusters, each managed by a Cluster Head (CH) UAV, facilitating a distributed-centralized control approach. Within each cluster, UAVs delegate offloading decisions to the CH UAV, reducing intra-cluster communication costs and decision conflicts, thereby enhancing task scheduling efficiency. Additionally, by employing a multi-agent reinforcement learning framework, the algorithm leverages the extensive coverage of satellites to achieve centralized training and distributed execution of multi-agent tasks, while maximizing overall system profit through optimized task offloading decision-making. Simulation results reveal that the CMADDPG algorithm effectively optimizes resource allocation, minimizes queue delays, maintains balanced load distribution, and surpasses existing methods by achieving at least a 25\% improvement in system profit, showcasing its robustness and adaptability across diverse scenarios.
<div id='section'>Paperid: <span id='pid'>154, <a href='https://arxiv.org/pdf/2410.15841.pdf' target='_blank'>https://arxiv.org/pdf/2410.15841.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenzhe Fan, Zishun Yu, Chengdong Ma, Changye Li, Yaodong Yang, Xinhua Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.15841">Towards Efficient Collaboration via Graph Modeling in Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In multi-agent reinforcement learning, a commonly considered paradigm is centralized training with decentralized execution. However, in this framework, decentralized execution restricts the development of coordinated policies due to the local observation limitation. In this paper, we consider the cooperation among neighboring agents during execution and formulate their interactions as a graph. Thus, we introduce a novel encoder-decoder architecture named Factor-based Multi-Agent Transformer ($f$-MAT) that utilizes a transformer to enable communication between neighboring agents during both training and execution. By dividing agents into different overlapping groups and representing each group with a factor, $f$-MAT achieves efficient message passing and parallel action generation through factor-based attention layers. Empirical results in networked systems such as traffic scheduling and power control demonstrate that $f$-MAT achieves superior performance compared to strong baselines, thereby paving the way for handling complex collaborative problems.
<div id='section'>Paperid: <span id='pid'>155, <a href='https://arxiv.org/pdf/2501.14488.pdf' target='_blank'>https://arxiv.org/pdf/2501.14488.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuhan Hu, Yirong Sun, Yanjun Chen, Xinghao Chen, Xiaoyu Shen, Wei Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.14488">Breaking the Pre-Planning Barrier: Adaptive Real-Time Coordination of Heterogeneous UAVs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Unmanned Aerial Vehicles (UAVs) offer significant potential in dynamic, perception-intensive tasks such as search and rescue and environmental monitoring; however, their effectiveness is severely restricted by conventional pre-planned routing methods, which lack the flexibility to respond in real-time to evolving task demands, unexpected disturbances, and localized view limitations in real-world scenarios. To address this fundamental limitation, we introduce a novel multi-agent reinforcement learning framework named \textbf{H}eterogeneous \textbf{G}raph \textbf{A}ttention \textbf{M}ulti-agent Deep Deterministic Policy Gradient (HGAM), uniquely designed to enable adaptive real-time coordination between mission UAVs (MUAVs) and charging UAVs (CUAVs). HGAM specifically addresses the previously unsolved challenge of enabling precise, decentralized continuous-action coordination solely based on local, heterogeneous graph-based observations. Extensive simulations demonstrate that HGAM substantially surpasses existing methods, achieving, for example, a 30\% improvement in data collection coverage and a 20\% increase in charging efficiency, providing crucial insights and foundations for the future deployment of intelligent, flexible UAV networks in complex, dynamic environments.
<div id='section'>Paperid: <span id='pid'>156, <a href='https://arxiv.org/pdf/2510.05571.pdf' target='_blank'>https://arxiv.org/pdf/2510.05571.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chengzhi Liu, Yuzhe Yang, Kaiwen Zhou, Zhen Zhang, Yue Fan, Yannan Xie, Peng Qi, Xin Eric Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.05571">Presenting a Paper is an Art: Self-Improvement Aesthetic Agents for Academic Presentations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The promotion of academic papers has become an important means of enhancing research visibility. However, existing automated methods struggle limited storytelling, insufficient aesthetic quality, and constrained self-adjustment, making it difficult to achieve efficient and engaging dissemination. At the heart of those challenges is a simple principle: \emph{there is no way to improve it when you cannot evaluate it right}. To address this, we introduce \textbf{EvoPresent}, a self-improvement agent framework that unifies coherent narratives, aesthetic-aware designs, and realistic presentation delivery via virtual characters. Central to EvoPresent is \textbf{PresAesth}, a multi-task reinforcement learning (RL) aesthetic model that provides reliable aesthetic scoring, defect adjustment, and comparative feedback, enabling iterative self-improvement even under limited aesthetic training data. To systematically evaluate the methods, we introduce \textbf{EvoPresent Benchmark}, a comprehensive benchmark comprising: \textit{Presentation Generation Quality}, built on 650 top-tier AI conference papers with multimodal resources (slides, videos and scripts) to assess both content and design; and \textit{Aesthetic Awareness}, consisting of 2,000 slide pairs with varying aesthetic levels, supporting joint training and evaluation on scoring, defect adjustment, and comparison. Our findings highlight that (i) High-quality feedback is essential for agent self-improvement, while initial capability alone does not guarantee effective self-correction. (ii) Automated generation pipelines exhibit a trade-off between visual design and content construction. (iii) Multi-task RL training shows stronger generalization in aesthetic awareness tasks.
<div id='section'>Paperid: <span id='pid'>157, <a href='https://arxiv.org/pdf/2510.14112.pdf' target='_blank'>https://arxiv.org/pdf/2510.14112.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Huiliang Zhang, Di Wu, Arnaud Zinflou, Benoit Boulet
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.14112">STEMS: Spatial-Temporal Enhanced Safe Multi-Agent Coordination for Building Energy Management</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Building energy management is essential for achieving carbon reduction goals, improving occupant comfort, and reducing energy costs. Coordinated building energy management faces critical challenges in exploiting spatial-temporal dependencies while ensuring operational safety across multi-building systems. Current multi-building energy systems face three key challenges: insufficient spatial-temporal information exploitation, lack of rigorous safety guarantees, and system complexity. This paper proposes Spatial-Temporal Enhanced Safe Multi-Agent Coordination (STEMS), a novel safety-constrained multi-agent reinforcement learning framework for coordinated building energy management. STEMS integrates two core components: (1) a spatial-temporal graph representation learning framework using a GCN-Transformer fusion architecture to capture inter-building relationships and temporal patterns, and (2) a safety-constrained multi-agent RL algorithm incorporating Control Barrier Functions to provide mathematical safety guarantees. Extensive experiments on real-world building datasets demonstrate STEMS's superior performance over existing methods, showing that STEMS achieves 21% cost reduction, 18% emission reduction, and dramatically reduces safety violations from 35.1% to 5.6% while maintaining optimal comfort with only 0.13 discomfort proportion. The framework also demonstrates strong robustness during extreme weather conditions and maintains effectiveness across different building types.
<div id='section'>Paperid: <span id='pid'>158, <a href='https://arxiv.org/pdf/2510.09156.pdf' target='_blank'>https://arxiv.org/pdf/2510.09156.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jing Li, Zhijie Sun, Zhicheng Zhou, Suming Qiu, Junjie Huang, Haijia Sun, Linyuan Qiu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.09156">Agentic-KGR: Co-evolutionary Knowledge Graph Construction through Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current knowledge-enhanced large language models (LLMs) rely on static, pre-constructed knowledge bases that suffer from coverage gaps and temporal obsolescence, limiting their effectiveness in dynamic information environments. We present Agentic-KGR, a novel framework enabling co-evolution between LLMs and knowledge graphs (KGs) through multi-round reinforcement learning (RL). Our approach introduces three key innovations: (1) a dynamic schema expansion mechanism that systematically extends graph ontologies beyond pre-defined boundaries during training; (2) a retrieval-augmented memory system enabling synergistic co-evolution between model parameters and knowledge structures through continuous optimization; (3) a learnable multi-scale prompt compression approach that preserves critical information while reducing computational complexity through adaptive sequence optimization. Experimental results demonstrate substantial improvements over supervised baselines and single-round RL approaches in knowledge extraction tasks. When integrated with GraphRAG, our method achieves superior performance in downstream QA tasks, with significant gains in both accuracy and knowledge coverage compared to existing methods.
<div id='section'>Paperid: <span id='pid'>159, <a href='https://arxiv.org/pdf/2503.18221.pdf' target='_blank'>https://arxiv.org/pdf/2503.18221.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wen-Tse Chen, Minh Nguyen, Zhongyu Li, Guo Ning Sue, Koushil Sreenath
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.18221">Decentralized Navigation of a Cable-Towed Load using Quadrupedal Robot Team via MARL</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work addresses the challenge of enabling a team of quadrupedal robots to collaboratively tow a cable-connected load through cluttered and unstructured environments while avoiding obstacles. Leveraging cables allows the multi-robot system to navigate narrow spaces by maintaining slack when necessary. However, this introduces hybrid physical interactions due to alternating taut and slack states, with computational complexity that scales exponentially as the number of agents increases. To tackle these challenges, we developed a scalable and decentralized system capable of dynamically coordinating a variable number of quadrupedal robots while managing the hybrid physical interactions inherent in the load-towing task. At the core of this system is a novel multi-agent reinforcement learning (MARL)-based planner, designed for decentralized coordination. The MARL-based planner is trained using a centralized training with decentralized execution (CTDE) framework, enabling each robot to make decisions autonomously using only local (ego) observations. To accelerate learning and ensure effective collaboration across varying team sizes, we introduce a tailored training curriculum for MARL. Experimental results highlight the flexibility and scalability of the framework, demonstrating successful deployment with one to four robots in real-world scenarios and up to twelve robots in simulation. The decentralized planner maintains consistent inference times, regardless of the team size. Additionally, the proposed system demonstrates robustness to environment perturbations and adaptability to varying load weights. This work represents a step forward in achieving flexible and efficient multi-legged robotic collaboration in complex and real-world environments.
<div id='section'>Paperid: <span id='pid'>160, <a href='https://arxiv.org/pdf/2512.16813.pdf' target='_blank'>https://arxiv.org/pdf/2512.16813.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bahman Abolhassani, Tugba Erpek, Kemal Davaslioglu, Yalin E. Sagduyu, Sastry Kompella
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.16813">Coordinated Anti-Jamming Resilience in Swarm Networks via Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reactive jammers pose a severe security threat to robotic-swarm networks by selectively disrupting inter-agent communications and undermining formation integrity and mission success. Conventional countermeasures such as fixed power control or static channel hopping are largely ineffective against such adaptive adversaries. This paper presents a multi-agent reinforcement learning (MARL) framework based on the QMIX algorithm to improve the resilience of swarm communications under reactive jamming. We consider a network of multiple transmitter-receiver pairs sharing channels while a reactive jammer with Markovian threshold dynamics senses aggregate power and reacts accordingly. Each agent jointly selects transmit frequency (channel) and power, and QMIX learns a centralized but factorizable action-value function that enables coordinated yet decentralized execution. We benchmark QMIX against a genie-aided optimal policy in a no-channel-reuse setting, and against local Upper Confidence Bound (UCB) and a stateless reactive policy in a more general fading regime with channel reuse enabled. Simulation results show that QMIX rapidly converges to cooperative policies that nearly match the genie-aided bound, while achieving higher throughput and lower jamming incidence than the baselines, thereby demonstrating MARL's effectiveness for securing autonomous swarms in contested environments.
<div id='section'>Paperid: <span id='pid'>161, <a href='https://arxiv.org/pdf/2502.16496.pdf' target='_blank'>https://arxiv.org/pdf/2502.16496.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kun Hu, Muning Wen, Xihuai Wang, Shao Zhang, Yiwei Shi, Minne Li, Minglong Li, Ying Wen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.16496">PMAT: Optimizing Action Generation Order in Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent reinforcement learning (MARL) faces challenges in coordinating agents due to complex interdependencies within multi-agent systems. Most MARL algorithms use the simultaneous decision-making paradigm but ignore the action-level dependencies among agents, which reduces coordination efficiency. In contrast, the sequential decision-making paradigm provides finer-grained supervision for agent decision order, presenting the potential for handling dependencies via better decision order management. However, determining the optimal decision order remains a challenge. In this paper, we introduce Action Generation with Plackett-Luce Sampling (AGPS), a novel mechanism for agent decision order optimization. We model the order determination task as a Plackett-Luce sampling process to address issues such as ranking instability and vanishing gradient during the network training process. AGPS realizes credit-based decision order determination by establishing a bridge between the significance of agents' local observations and their decision credits, thus facilitating order optimization and dependency management. Integrating AGPS with the Multi-Agent Transformer, we propose the Prioritized Multi-Agent Transformer (PMAT), a sequential decision-making MARL algorithm with decision order optimization. Experiments on benchmarks including StarCraft II Multi-Agent Challenge, Google Research Football, and Multi-Agent MuJoCo show that PMAT outperforms state-of-the-art algorithms, greatly enhancing coordination efficiency.
<div id='section'>Paperid: <span id='pid'>162, <a href='https://arxiv.org/pdf/2507.23698.pdf' target='_blank'>https://arxiv.org/pdf/2507.23698.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shaofei Cai, Zhancun Mu, Haiwen Xia, Bowei Zhang, Anji Liu, Yitao Liang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.23698">Scalable Multi-Task Reinforcement Learning for Generalizable Spatial Intelligence in Visuomotor Agents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While Reinforcement Learning (RL) has achieved remarkable success in language modeling, its triumph hasn't yet fully translated to visuomotor agents. A primary challenge in RL models is their tendency to overfit specific tasks or environments, thereby hindering the acquisition of generalizable behaviors across diverse settings. This paper provides a preliminary answer to this challenge by demonstrating that RL-finetuned visuomotor agents in Minecraft can achieve zero-shot generalization to unseen worlds. Specifically, we explore RL's potential to enhance generalizable spatial reasoning and interaction capabilities in 3D worlds. To address challenges in multi-task RL representation, we analyze and establish cross-view goal specification as a unified multi-task goal space for visuomotor policies. Furthermore, to overcome the significant bottleneck of manual task design, we propose automated task synthesis within the highly customizable Minecraft environment for large-scale multi-task RL training, and we construct an efficient distributed RL framework to support this. Experimental results show RL significantly boosts interaction success rates by $4\times$ and enables zero-shot generalization of spatial reasoning across diverse environments, including real-world settings. Our findings underscore the immense potential of RL training in 3D simulated environments, especially those amenable to large-scale task generation, for significantly advancing visuomotor agents' spatial reasoning.
<div id='section'>Paperid: <span id='pid'>163, <a href='https://arxiv.org/pdf/2507.07671.pdf' target='_blank'>https://arxiv.org/pdf/2507.07671.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jovan Prodanov, BlaÅ¾ BertalaniÄ, Carolina Fortuna, Shih-Kai Chou, MatjaÅ¾ Branko JuriÄ, Ramon Sanchez-Iborra, Jernej Hribar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.07671">Multi-agent Reinforcement Learning-based In-place Scaling Engine for Edge-cloud Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Modern edge-cloud systems face challenges in efficiently scaling resources to handle dynamic and unpredictable workloads. Traditional scaling approaches typically rely on static thresholds and predefined rules, which are often inadequate for optimizing resource utilization and maintaining performance in distributed and dynamic environments. This inefficiency hinders the adaptability and performance required in edge-cloud infrastructures, which can only be achieved through the newly proposed in-place scaling. To address this problem, we propose the Multi-Agent Reinforcement Learning-based In-place Scaling Engine (MARLISE) that enables seamless, dynamic, reactive control with in-place resource scaling. We develop our solution using two Deep Reinforcement Learning algorithms: Deep Q-Network (DQN), and Proximal Policy Optimization (PPO). We analyze each version of the proposed MARLISE solution using dynamic workloads, demonstrating their ability to ensure low response times of microservices and scalability. Our results show that MARLISE-based approaches outperform heuristic method in managing resource elasticity while maintaining microservice response times and achieving higher resource efficiency.
<div id='section'>Paperid: <span id='pid'>164, <a href='https://arxiv.org/pdf/2502.04399.pdf' target='_blank'>https://arxiv.org/pdf/2502.04399.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bokeng Zheng, Bo Rao, Tianxiang Zhu, Chee Wei Tan, Jingpu Duan, Zhi Zhou, Xu Chen, Xiaoxi Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.04399">Online Location Planning for AI-Defined Vehicles: Optimizing Joint Tasks of Order Serving and Spatio-Temporal Heterogeneous Model Fine-Tuning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Advances in artificial intelligence (AI) including foundation models (FMs), are increasingly transforming human society, with smart city driving the evolution of urban living.Meanwhile, vehicle crowdsensing (VCS) has emerged as a key enabler, leveraging vehicles' mobility and sensor-equipped capabilities. In particular, ride-hailing vehicles can effectively facilitate flexible data collection and contribute towards urban intelligence, despite resource limitations. Therefore, this work explores a promising scenario, where edge-assisted vehicles perform joint tasks of order serving and the emerging foundation model fine-tuning using various urban data. However, integrating the VCS AI task with the conventional order serving task is challenging, due to their inconsistent spatio-temporal characteristics: (i) The distributions of ride orders and data point-of-interests (PoIs) may not coincide in geography, both following a priori unknown patterns; (ii) they have distinct forms of temporal effects, i.e., prolonged waiting makes orders become instantly invalid while data with increased staleness gradually reduces its utility for model fine-tuning.To overcome these obstacles, we propose an online framework based on multi-agent reinforcement learning (MARL) with careful augmentation. A new quality-of-service (QoS) metric is designed to characterize and balance the utility of the two joint tasks, under the effects of varying data volumes and staleness. We also integrate graph neural networks (GNNs) with MARL to enhance state representations, capturing graph-structured, time-varying dependencies among vehicles and across locations. Extensive experiments on our testbed simulator, utilizing various real-world foundation model fine-tuning tasks and the New York City Taxi ride order dataset, demonstrate the advantage of our proposed method.
<div id='section'>Paperid: <span id='pid'>165, <a href='https://arxiv.org/pdf/2411.01166.pdf' target='_blank'>https://arxiv.org/pdf/2411.01166.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Weifan Long, Wen Wen, Peng Zhai, Lihua Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.01166">Role Play: Learning Adaptive Role-Specific Strategies in Multi-Agent Interactions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Zero-shot coordination problem in multi-agent reinforcement learning (MARL), which requires agents to adapt to unseen agents, has attracted increasing attention. Traditional approaches often rely on the Self-Play (SP) framework to generate a diverse set of policies in a policy pool, which serves to improve the generalization capability of the final agent. However, these frameworks may struggle to capture the full spectrum of potential strategies, especially in real-world scenarios that demand agents balance cooperation with competition. In such settings, agents need strategies that can adapt to varying and often conflicting goals. Drawing inspiration from Social Value Orientation (SVO)-where individuals maintain stable value orientations during interactions with others-we propose a novel framework called \emph{Role Play} (RP). RP employs role embeddings to transform the challenge of policy diversity into a more manageable diversity of roles. It trains a common policy with role embedding observations and employs a role predictor to estimate the joint role embeddings of other agents, helping the learning agent adapt to its assigned role. We theoretically prove that an approximate optimal policy can be achieved by optimizing the expected cumulative reward relative to an approximate role-based policy. Experimental results in both cooperative (Overcooked) and mixed-motive games (Harvest, CleanUp) reveal that RP consistently outperforms strong baselines when interacting with unseen agents, highlighting its robustness and adaptability in complex environments.
<div id='section'>Paperid: <span id='pid'>166, <a href='https://arxiv.org/pdf/2410.05127.pdf' target='_blank'>https://arxiv.org/pdf/2410.05127.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Noboru Isobe, Kenshi Abe, Kaito Ariu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.05127">Last Iterate Convergence in Monotone Mean Field Games</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Mean Field Game (MFG) is a framework for modeling and approximating the behavior of large numbers of agents. Computing equilibria in MFG has been of interest in multi-agent reinforcement learning. The theoretical guarantee that the last updated policy converges to an equilibrium has been limited. We propose the use of a simple, proximal-point (PP) type method to compute equilibria for MFGs. We then provide the first last-iterate convergence (LIC) guarantee under the Lasry--Lions-type monotonicity condition. We also propose an approximation of the update rule of PP ($\mathtt{APP}$) based on the observation that it is equivalent to solving the regularized MFG, which can be solved by mirror descent. We further establish that the regularized mirror descent achieves LIC at an exponential rate. Our numerical experiment demonstrates that $\mathtt{APP}$ efficiently computes the equilibrium.
<div id='section'>Paperid: <span id='pid'>167, <a href='https://arxiv.org/pdf/2409.06366.pdf' target='_blank'>https://arxiv.org/pdf/2409.06366.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nico Bohlinger, Grzegorz Czechmanowski, Maciej Krupka, Piotr Kicki, Krzysztof Walas, Jan Peters, Davide Tateo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.06366">One Policy to Run Them All: an End-to-end Learning Approach to Multi-Embodiment Locomotion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep Reinforcement Learning techniques are achieving state-of-the-art results in robust legged locomotion. While there exists a wide variety of legged platforms such as quadruped, humanoids, and hexapods, the field is still missing a single learning framework that can control all these different embodiments easily and effectively and possibly transfer, zero or few-shot, to unseen robot embodiments. We introduce URMA, the Unified Robot Morphology Architecture, to close this gap. Our framework brings the end-to-end Multi-Task Reinforcement Learning approach to the realm of legged robots, enabling the learned policy to control any type of robot morphology. The key idea of our method is to allow the network to learn an abstract locomotion controller that can be seamlessly shared between embodiments thanks to our morphology-agnostic encoders and decoders. This flexible architecture can be seen as a potential first step in building a foundation model for legged robot locomotion. Our experiments show that URMA can learn a locomotion policy on multiple embodiments that can be easily transferred to unseen robot platforms in simulation and the real world.
<div id='section'>Paperid: <span id='pid'>168, <a href='https://arxiv.org/pdf/2510.20176.pdf' target='_blank'>https://arxiv.org/pdf/2510.20176.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuhang Zhou, Mingrui Zhang, Ke Li, Mingyi Wang, Qiao Liu, Qifei Wang, Jiayi Liu, Fei Liu, Serena Li, Weiwei Li, Mingze Gao, Abhishek Kumar, Xiangjun Fan, Zhuokai Zhao, Lizhu Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.20176">Mixture-of-Minds: Multi-Agent Reinforcement Learning for Table Understanding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding and reasoning over tables is a critical capability for many real-world applications. Large language models (LLMs) have shown promise on this task, but current approaches remain limited. Fine-tuning based methods strengthen language reasoning; yet they are prone to arithmetic errors and hallucination. In contrast, tool-based methods enable precise table manipulation but rely on rigid schemas and lack semantic understanding. These complementary drawbacks highlight the need for approaches that integrate robust reasoning with reliable table processing. In this work, we propose Mixture-of-Minds, a multi-agent framework that decomposes table reasoning into three specialized roles: planning, coding, and answering. This design enables each agent to focus on a specific aspect of the task while leveraging code execution for precise table manipulation. Building on this workflow, we introduce a self-improvement training framework that employs Monte Carlo Tree Search (MCTS) rollouts to generate pseudo-gold trajectories and optimize agents with reinforcement learning (RL). Extensive experiments show that Mixture-of-Minds delivers substantial gains, reaching 62.13% on TableBench and surpassing OpenAI-o4-mini-high. These results demonstrate the promise of combining structured multi-agent workflows with RL to advance table understanding.
<div id='section'>Paperid: <span id='pid'>169, <a href='https://arxiv.org/pdf/2411.03603.pdf' target='_blank'>https://arxiv.org/pdf/2411.03603.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuqian Fu, Yuanheng Zhu, Haoran Li, Zijie Zhao, Jiajun Chai, Dongbin Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.03603">CPIG: Leveraging Consistency Policy with Intention Guidance for Multi-agent Exploration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Efficient exploration is crucial in cooperative multi-agent reinforcement learning (MARL), especially in sparse-reward settings. However, due to the reliance on the unimodal policy, existing methods are prone to falling into the local optima, hindering the effective exploration of better policies. Furthermore, in sparse-reward settings, each agent tends to receive a scarce reward, which poses significant challenges to inter-agent cooperation. This not only increases the difficulty of policy learning but also degrades the overall performance of multi-agent tasks. To address these issues, we propose a Consistency Policy with Intention Guidance (CPIG), with two primary components: (a) introducing a multimodal policy to enhance the agent's exploration capability, and (b) sharing the intention among agents to foster agent cooperation. For component (a), CPIG incorporates a Consistency model as the policy, leveraging its multimodal nature and stochastic characteristics to facilitate exploration. Regarding component (b), we introduce an Intention Learner to deduce the intention on the global state from each agent's local observation. This intention then serves as a guidance for the Consistency Policy, promoting cooperation among agents. The proposed method is evaluated in multi-agent particle environments (MPE) and multi-agent MuJoCo (MAMuJoCo). Empirical results demonstrate that our method not only achieves comparable performance to various baselines in dense-reward environments but also significantly enhances performance in sparse-reward settings, outperforming state-of-the-art (SOTA) algorithms by 20%.
<div id='section'>Paperid: <span id='pid'>170, <a href='https://arxiv.org/pdf/2509.12117.pdf' target='_blank'>https://arxiv.org/pdf/2509.12117.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aryaman Reddi, Gabriele Tiboni, Jan Peters, Carlo D'Eramo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.12117">$K$-Level Policy Gradients for Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Actor-critic algorithms for deep multi-agent reinforcement learning (MARL) typically employ a policy update that responds to the current strategies of other agents. While being straightforward, this approach does not account for the updates of other agents at the same update step, resulting in miscoordination. In this paper, we introduce the $K$-Level Policy Gradient (KPG), a method that recursively updates each agent against the updated policies of other agents, speeding up the discovery of effective coordinated policies. We theoretically prove that KPG with finite iterates achieves monotonic convergence to a local Nash equilibrium under certain conditions. We provide principled implementations of KPG by applying it to the deep MARL algorithms MAPPO, MADDPG, and FACMAC. Empirically, we demonstrate superior performance over existing deep MARL algorithms in StarCraft II and multi-agent MuJoCo.
<div id='section'>Paperid: <span id='pid'>171, <a href='https://arxiv.org/pdf/2506.07829.pdf' target='_blank'>https://arxiv.org/pdf/2506.07829.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jan Corazza, Hadi Partovi Aria, Hyohun Kim, Daniel Neider, Zhe Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.07829">Decentralizing Multi-Agent Reinforcement Learning with Temporal Causal Information</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reinforcement learning (RL) algorithms can find an optimal policy for a single agent to accomplish a particular task. However, many real-world problems require multiple agents to collaborate in order to achieve a common goal. For example, a robot executing a task in a warehouse may require the assistance of a drone to retrieve items from high shelves. In Decentralized Multi-Agent RL (DMARL), agents learn independently and then combine their policies at execution time, but often must satisfy constraints on compatibility of local policies to ensure that they can achieve the global task when combined. In this paper, we study how providing high-level symbolic knowledge to agents can help address unique challenges of this setting, such as privacy constraints, communication limitations, and performance concerns. In particular, we extend the formal tools used to check the compatibility of local policies with the team task, making decentralized training with theoretical guarantees usable in more scenarios. Furthermore, we empirically demonstrate that symbolic knowledge about the temporal evolution of events in the environment can significantly expedite the learning process in DMARL.
<div id='section'>Paperid: <span id='pid'>172, <a href='https://arxiv.org/pdf/2511.23193.pdf' target='_blank'>https://arxiv.org/pdf/2511.23193.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuchen Shi, Huaxin Pei, Yi Zhang, Danya Yao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.23193">Fault-Tolerant MARL for CAVs under Observation Perturbations for Highway On-Ramp Merging</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-Agent Reinforcement Learning (MARL) holds significant promise for enabling cooperative driving among Connected and Automated Vehicles (CAVs). However, its practical application is hindered by a critical limitation, i.e., insufficient fault tolerance against observational faults. Such faults, which appear as perturbations in the vehicles' perceived data, can substantially compromise the performance of MARL-based driving systems. Addressing this problem presents two primary challenges. One is to generate adversarial perturbations that effectively stress the policy during training, and the other is to equip vehicles with the capability to mitigate the impact of corrupted observations. To overcome the challenges, we propose a fault-tolerant MARL method for cooperative on-ramp vehicles incorporating two key agents. First, an adversarial fault injection agent is co-trained to generate perturbations that actively challenge and harden the vehicle policies. Second, we design a novel fault-tolerant vehicle agent equipped with a self-diagnosis capability, which leverages the inherent spatio-temporal correlations in vehicle state sequences to detect faults and reconstruct credible observations, thereby shielding the policy from misleading inputs. Experiments in a simulated highway merging scenario demonstrate that our method significantly outperforms baseline MARL approaches, achieving near-fault-free levels of safety and efficiency under various observation fault patterns.
<div id='section'>Paperid: <span id='pid'>173, <a href='https://arxiv.org/pdf/2511.21934.pdf' target='_blank'>https://arxiv.org/pdf/2511.21934.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tao Zhe, Huazhen Fang, Kunpeng Liu, Qian Lou, Tamzidul Hoque, Dongjie Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.21934">Heterogeneous Multi-Agent Reinforcement Learning with Attention for Cooperative and Scalable Feature Transformation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Feature transformation enhances downstream task performance by generating informative features through mathematical feature crossing. Despite the advancements in deep learning, feature transformation remains essential for structured data, where deep models often struggle to capture complex feature interactions. Prior literature on automated feature transformation has achieved success but often relies on heuristics or exhaustive searches, leading to inefficient and time-consuming processes. Recent works employ reinforcement learning (RL) to enhance traditional approaches through a more effective trial-and-error way. However, two limitations remain: 1) Dynamic feature expansion during the transformation process, which causes instability and increases the learning complexity for RL agents; 2) Insufficient cooperation and communication between agents, which results in suboptimal feature crossing operations and degraded model performance. To address them, we propose a novel heterogeneous multi-agent RL framework to enable cooperative and scalable feature transformation. The framework comprises three heterogeneous agents, grouped into two types, each designed to select essential features and operations for feature crossing. To enhance communication among these agents, we implement a shared critic mechanism that facilitates information exchange during feature transformation. To handle the dynamically expanding feature space, we tailor multi-head attention-based feature agents to select suitable features for feature crossing. Additionally, we introduce a state encoding technique during the optimization process to stabilize and enhance the learning dynamics of the RL agents, resulting in more robust and reliable transformation policies. Finally, we conduct extensive experiments to validate the effectiveness, efficiency, robustness, and interpretability of our model.
<div id='section'>Paperid: <span id='pid'>174, <a href='https://arxiv.org/pdf/2505.20922.pdf' target='_blank'>https://arxiv.org/pdf/2505.20922.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yang Zhang, Xinran Li, Jianing Ye, Delin Qu, Shuang Qiu, Chongjie Zhang, Xiu Li, Chenjia Bai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.20922">Revisiting Multi-Agent World Modeling from a Diffusion-Inspired Perspective</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>World models have recently attracted growing interest in Multi-Agent Reinforcement Learning (MARL) due to their ability to improve sample efficiency for policy learning. However, accurately modeling environments in MARL is challenging due to the exponentially large joint action space and highly uncertain dynamics inherent in multi-agent systems. To address this, we reduce modeling complexity by shifting from jointly modeling the entire state-action transition dynamics to focusing on the state space alone at each timestep through sequential agent modeling. Specifically, our approach enables the model to progressively resolve uncertainty while capturing the structured dependencies among agents, providing a more accurate representation of how agents influence the state. Interestingly, this sequential revelation of agents' actions in a multi-agent system aligns with the reverse process in diffusion models--a class of powerful generative models known for their expressiveness and training stability compared to autoregressive or latent variable models. Leveraging this insight, we develop a flexible and robust world model for MARL using diffusion models. Our method, Diffusion-Inspired Multi-Agent world model (DIMA), achieves state-of-the-art performance across multiple multi-agent control benchmarks, significantly outperforming prior world models in terms of final return and sample efficiency, including MAMuJoCo and Bi-DexHands. DIMA establishes a new paradigm for constructing multi-agent world models, advancing the frontier of MARL research.
<div id='section'>Paperid: <span id='pid'>175, <a href='https://arxiv.org/pdf/2501.09429.pdf' target='_blank'>https://arxiv.org/pdf/2501.09429.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Benjamin Patrick Evans, Sihan Zeng, Sumitra Ganesh, Leo Ardon
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.09429">ADAGE: A generic two-layer framework for adaptive agent based modelling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Agent-based models (ABMs) are valuable for modelling complex, potentially out-of-equilibria scenarios. However, ABMs have long suffered from the Lucas critique, stating that agent behaviour should adapt to environmental changes. Furthermore, the environment itself often adapts to these behavioural changes, creating a complex bi-level adaptation problem. Recent progress integrating multi-agent reinforcement learning into ABMs introduces adaptive agent behaviour, beginning to address the first part of this critique, however, the approaches are still relatively ad hoc, lacking a general formulation, and furthermore, do not tackle the second aspect of simultaneously adapting environmental level characteristics in addition to the agent behaviours. In this work, we develop a generic two-layer framework for ADaptive AGEnt based modelling (ADAGE) for addressing these problems. This framework formalises the bi-level problem as a Stackelberg game with conditional behavioural policies, providing a consolidated framework for adaptive agent-based modelling based on solving a coupled set of non-linear equations. We demonstrate how this generic approach encapsulates several common (previously viewed as distinct) ABM tasks, such as policy design, calibration, scenario generation, and robust behavioural learning under one unified framework. We provide example simulations on multiple complex economic and financial environments, showing the strength of the novel framework under these canonical settings, addressing long-standing critiques of traditional ABMs.
<div id='section'>Paperid: <span id='pid'>176, <a href='https://arxiv.org/pdf/2412.12442.pdf' target='_blank'>https://arxiv.org/pdf/2412.12442.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiaxu Xing, Ismail Geles, Yunlong Song, Elie Aljalbout, Davide Scaramuzza
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.12442">Multi-Task Reinforcement Learning for Quadrotors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reinforcement learning (RL) has shown great effectiveness in quadrotor control, enabling specialized policies to develop even human-champion-level performance in single-task scenarios. However, these specialized policies often struggle with novel tasks, requiring a complete retraining of the policy from scratch. To address this limitation, this paper presents a novel multi-task reinforcement learning (MTRL) framework tailored for quadrotor control, leveraging the shared physical dynamics of the platform to enhance sample efficiency and task performance. By employing a multi-critic architecture and shared task encoders, our framework facilitates knowledge transfer across tasks, enabling a single policy to execute diverse maneuvers, including high-speed stabilization, velocity tracking, and autonomous racing. Our experimental results, validated both in simulation and real-world scenarios, demonstrate that our framework outperforms baseline approaches in terms of sample efficiency and overall task performance.
<div id='section'>Paperid: <span id='pid'>177, <a href='https://arxiv.org/pdf/2412.00534.pdf' target='_blank'>https://arxiv.org/pdf/2412.00534.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuchen Shi, Huaxin Pei, Liang Feng, Yi Zhang, Danya Yao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.00534">Towards Fault Tolerance in Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Agent faults pose a significant threat to the performance of multi-agent reinforcement learning (MARL) algorithms, introducing two key challenges. First, agents often struggle to extract critical information from the chaotic state space created by unexpected faults. Second, transitions recorded before and after faults in the replay buffer affect training unevenly, leading to a sample imbalance problem. To overcome these challenges, this paper enhances the fault tolerance of MARL by combining optimized model architecture with a tailored training data sampling strategy. Specifically, an attention mechanism is incorporated into the actor and critic networks to automatically detect faults and dynamically regulate the attention given to faulty agents. Additionally, a prioritization mechanism is introduced to selectively sample transitions critical to current training needs. To further support research in this area, we design and open-source a highly decoupled code platform for fault-tolerant MARL, aimed at improving the efficiency of studying related problems. Experimental results demonstrate the effectiveness of our method in handling various types of faults, faults occurring in any agent, and faults arising at random times.
<div id='section'>Paperid: <span id='pid'>178, <a href='https://arxiv.org/pdf/2411.14264.pdf' target='_blank'>https://arxiv.org/pdf/2411.14264.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pedro Enrique Iturria-Rivera, Raimundas Gaigalas, Medhat Elsayed, Majid Bavand, Yigit Ozcan, Melike Erol-Kantarci
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.14264">Explainable Multi-Agent Reinforcement Learning for Extended Reality Codec Adaptation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Extended Reality (XR) services are set to transform applications over 5th and 6th generation wireless networks, delivering immersive experiences. Concurrently, Artificial Intelligence (AI) advancements have expanded their role in wireless networks, however, trust and transparency in AI remain to be strengthened. Thus, providing explanations for AI-enabled systems can enhance trust. We introduce Value Function Factorization (VFF)-based Explainable (X) Multi-Agent Reinforcement Learning (MARL) algorithms, explaining reward design in XR codec adaptation through reward decomposition. We contribute four enhancements to XMARL algorithms. Firstly, we detail architectural modifications to enable reward decomposition in VFF-based MARL algorithms: Value Decomposition Networks (VDN), Mixture of Q-Values (QMIX), and Q-Transformation (Q-TRAN). Secondly, inspired by multi-task learning, we reduce the overhead of vanilla XMARL algorithms. Thirdly, we propose a new explainability metric, Reward Difference Fluctuation Explanation (RDFX), suitable for problems with adjustable parameters. Lastly, we propose adaptive XMARL, leveraging network gradients and reward decomposition for improved action selection. Simulation results indicate that, in XR codec adaptation, the Packet Delivery Ratio reward is the primary contributor to optimal performance compared to the initial composite reward, which included delay and Data Rate Ratio components. Modifications to VFF-based XMARL algorithms, incorporating multi-headed structures and adaptive loss functions, enable the best-performing algorithm, Multi-Headed Adaptive (MHA)-QMIX, to achieve significant average gains over the Adjust Packet Size baseline up to 10.7%, 41.4%, 33.3%, and 67.9% in XR index, jitter, delay, and Packet Loss Ratio (PLR), respectively.
<div id='section'>Paperid: <span id='pid'>179, <a href='https://arxiv.org/pdf/2511.02762.pdf' target='_blank'>https://arxiv.org/pdf/2511.02762.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xun Wang, Zhuoran Li, Yanshan Lin, Hai Zhong, Longbo Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.02762">From Solo to Symphony: Orchestrating Multi-Agent Collaboration with Single-Agent Demos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Training a team of agents from scratch in multi-agent reinforcement learning (MARL) is highly inefficient, much like asking beginners to play a symphony together without first practicing solo. Existing methods, such as offline or transferable MARL, can ease this burden, but they still rely on costly multi-agent data, which often becomes the bottleneck. In contrast, solo experiences are far easier to obtain in many important scenarios, e.g., collaborative coding, household cooperation, and search-and-rescue. To unlock their potential, we propose Solo-to-Collaborative RL (SoCo), a framework that transfers solo knowledge into cooperative learning. SoCo first pretrains a shared solo policy from solo demonstrations, then adapts it for cooperation during multi-agent training through a policy fusion mechanism that combines an MoE-like gating selector and an action editor. Experiments across diverse cooperative tasks show that SoCo significantly boosts the training efficiency and performance of backbone algorithms. These results demonstrate that solo demonstrations provide a scalable and effective complement to multi-agent data, making cooperative learning more practical and broadly applicable.
<div id='section'>Paperid: <span id='pid'>180, <a href='https://arxiv.org/pdf/2509.23960.pdf' target='_blank'>https://arxiv.org/pdf/2509.23960.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Manan Tayal, Aditya Singh, Shishir Kolathaya, Somil Bansal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.23960">MAD-PINN: A Decentralized Physics-Informed Machine Learning Framework for Safe and Optimal Multi-Agent Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Co-optimizing safety and performance in large-scale multi-agent systems remains a fundamental challenge. Existing approaches based on multi-agent reinforcement learning (MARL), safety filtering, or Model Predictive Control (MPC) either lack strict safety guarantees, suffer from conservatism, or fail to scale effectively. We propose MAD-PINN, a decentralized physics-informed machine learning framework for solving the multi-agent state-constrained optimal control problem (MASC-OCP). Our method leverages an epigraph-based reformulation of SC-OCP to simultaneously capture performance and safety, and approximates its solution via a physics-informed neural network. Scalability is achieved by training the SC-OCP value function on reduced-agent systems and deploying them in a decentralized fashion, where each agent relies only on local observations of its neighbours for decision-making. To further enhance safety and efficiency, we introduce an Hamilton-Jacobi (HJ) reachability-based neighbour selection strategy to prioritize safety-critical interactions, and a receding-horizon policy execution scheme that adapts to dynamic interactions while reducing computational burden. Experiments on multi-agent navigation tasks demonstrate that MAD-PINN achieves superior safety-performance trade-offs, maintains scalability as the number of agents grows, and consistently outperforms state-of-the-art baselines.
<div id='section'>Paperid: <span id='pid'>181, <a href='https://arxiv.org/pdf/2509.18545.pdf' target='_blank'>https://arxiv.org/pdf/2509.18545.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ioannis Panitsas, Tolga O. Atalay, Dragoslav Stojadinovic, Angelos Stavrou, Leandros Tassiulas
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.18545">Accelerating Network Slice Placement with Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cellular networks are increasingly realized through software-based entities, with core functions deployed as Virtual Network Functions (VNFs) on Commercial-off-the-Shelf (COTS) hardware. Network slicing has emerged as a key enabler of 5G by providing logically isolated Quality of Service (QoS) guarantees for diverse applications. With the adoption of cloud-native infrastructures, the placement of network slices across heterogeneous multi-cloud environments poses new challenges due to variable resource capabilities and slice-specific requirements. This paper introduces a modular framework for autonomous and near-optimal VNF placement based on a disaggregated Multi-Agent Reinforcement Learning (MARL) approach. The framework incorporates real traffic profiles to estimate slice resource demands and employs a MARL-based scheduler to minimize deployment cost while meeting QoS constraints. Experimental evaluation on a multi-cloud testbed shows a 19x speed-up compared to combinatorial optimization, with deployment costs within 7.8% of the optimal. While the method incurs up to 2.42x more QoS violations under high load, the trade-off provides significantly faster decision-making and reduced computational complexity. These results suggest that MARL-based approaches offer a scalable and cost-efficient solution for real-time network slice placement in heterogeneous infrastructures.
<div id='section'>Paperid: <span id='pid'>182, <a href='https://arxiv.org/pdf/2508.06269.pdf' target='_blank'>https://arxiv.org/pdf/2508.06269.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhuoran Li, Xun Wang, Hai Zhong, Longbo Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.06269">OM2P: Offline Multi-Agent Mean-Flow Policy</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generative models, especially diffusion and flow-based models, have been promising in offline multi-agent reinforcement learning. However, integrating powerful generative models into this framework poses unique challenges. In particular, diffusion and flow-based policies suffer from low sampling efficiency due to their iterative generation processes, making them impractical in time-sensitive or resource-constrained settings. To tackle these difficulties, we propose OM2P (Offline Multi-Agent Mean-Flow Policy), a novel offline MARL algorithm to achieve efficient one-step action sampling. To address the misalignment between generative objectives and reward maximization, we introduce a reward-aware optimization scheme that integrates a carefully-designed mean-flow matching loss with Q-function supervision. Additionally, we design a generalized timestep distribution and a derivative-free estimation strategy to reduce memory overhead and improve training stability. Empirical evaluations on Multi-Agent Particle and MuJoCo benchmarks demonstrate that OM2P achieves superior performance, with up to a 3.8x reduction in GPU memory usage and up to a 10.8x speed-up in training time. Our approach represents the first to successfully integrate mean-flow model into offline MARL, paving the way for practical and scalable generative policies in cooperative multi-agent settings.
<div id='section'>Paperid: <span id='pid'>183, <a href='https://arxiv.org/pdf/2507.09179.pdf' target='_blank'>https://arxiv.org/pdf/2507.09179.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ronghua Shi, Yiou Liu, Xinyu Ying, Yang Tan, Yuchun Feng, Lynn Ai, Bill Shi, Xuhui Wang, Zhuang Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.09179">Hide-and-Shill: A Reinforcement Learning Framework for Market Manipulation Detection in Symphony-a Decentralized Multi-Agent System</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Decentralized finance (DeFi) has introduced a new era of permissionless financial innovation but also led to unprecedented market manipulation. Without centralized oversight, malicious actors coordinate shilling campaigns and pump-and-dump schemes across various platforms. We propose a Multi-Agent Reinforcement Learning (MARL) framework for decentralized manipulation detection, modeling the interaction between manipulators and detectors as a dynamic adversarial game. This framework identifies suspicious patterns using delayed token price reactions as financial indicators.Our method introduces three innovations: (1) Group Relative Policy Optimization (GRPO) to enhance learning stability in sparse-reward and partially observable settings; (2) a theory-based reward function inspired by rational expectations and information asymmetry, differentiating price discovery from manipulation noise; and (3) a multi-modal agent pipeline that integrates LLM-based semantic features, social graph signals, and on-chain market data for informed decision-making.The framework is integrated within the Symphony system, a decentralized multi-agent architecture enabling peer-to-peer agent execution and trust-aware learning through distributed logs, supporting chain-verifiable evaluation. Symphony promotes adversarial co-evolution among strategic actors and maintains robust manipulation detection without centralized oracles, enabling real-time surveillance across global DeFi ecosystems.Trained on 100,000 real-world discourse episodes and validated in adversarial simulations, Hide-and-Shill achieves top performance in detection accuracy and causal attribution. This work bridges multi-agent systems with financial surveillance, advancing a new paradigm for decentralized market intelligence. All resources are available at the Hide-and-Shill GitHub repository to promote open research and reproducibility.
<div id='section'>Paperid: <span id='pid'>184, <a href='https://arxiv.org/pdf/2504.12961.pdf' target='_blank'>https://arxiv.org/pdf/2504.12961.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhouyang Jiang, Bin Zhang, Airong Wei, Zhiwei Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.12961">QLLM: Do We Really Need a Mixing Network for Credit Assignment in Multi-Agent Reinforcement Learning?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Credit assignment has remained a fundamental challenge in multi-agent reinforcement learning (MARL). Previous studies have primarily addressed this issue through value decomposition methods under the centralized training with decentralized execution paradigm, where neural networks are utilized to approximate the nonlinear relationship between individual Q-values and the global Q-value. Although these approaches have achieved considerable success in various benchmark tasks, they still suffer from several limitations, including imprecise attribution of contributions, limited interpretability, and poor scalability in high-dimensional state spaces. To address these challenges, we propose a novel algorithm, \textbf{QLLM}, which facilitates the automatic construction of credit assignment functions using large language models (LLMs). Specifically, the concept of \textbf{TFCAF} is introduced, wherein the credit allocation process is represented as a direct and expressive nonlinear functional formulation. A custom-designed \textit{coder-evaluator} framework is further employed to guide the generation, verification, and refinement of executable code by LLMs, significantly mitigating issues such as hallucination and shallow reasoning during inference. Extensive experiments conducted on several standard MARL benchmarks demonstrate that the proposed method consistently outperforms existing state-of-the-art baselines. Moreover, QLLM exhibits strong generalization capability and maintains compatibility with a wide range of MARL algorithms that utilize mixing networks, positioning it as a promising and versatile solution for complex multi-agent scenarios.
<div id='section'>Paperid: <span id='pid'>185, <a href='https://arxiv.org/pdf/2502.08985.pdf' target='_blank'>https://arxiv.org/pdf/2502.08985.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xun Wang, Zhuoran Li, Hai Zhong, Longbo Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.08985">Few is More: Task-Efficient Skill-Discovery for Multi-Task Offline Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As a data-driven approach, offline MARL learns superior policies solely from offline datasets, ideal for domains rich in historical data but with high interaction costs and risks. However, most existing methods are task-specific, requiring retraining for new tasks, leading to redundancy and inefficiency. To address this issue, in this paper, we propose a task-efficient multi-task offline MARL algorithm, Skill-Discovery Conservative Q-Learning (SD-CQL). Unlike existing offline skill-discovery methods, SD-CQL discovers skills by reconstructing the next observation. It then evaluates fixed and variable actions separately and employs behavior-regularized conservative Q-learning to execute the optimal action for each skill. This approach eliminates the need for local-global alignment and enables strong multi-task generalization from limited small-scale source tasks. Substantial experiments on StarCraftII demonstrates the superior generalization performance and task-efficiency of SD-CQL. It achieves the best performance on $\textbf{10}$ out of $14$ task sets, with up to $\textbf{65%}$ improvement on individual task sets, and is within $4\%$ of the best baseline on the remaining four.
<div id='section'>Paperid: <span id='pid'>186, <a href='https://arxiv.org/pdf/2412.08845.pdf' target='_blank'>https://arxiv.org/pdf/2412.08845.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kuan-Cheng Chen, Samuel Yen-Chi Chen, Chen-Yu Liu, Kin K. Leung
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.08845">Quantum-Train-Based Distributed Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we introduce Quantum-Train-Based Distributed Multi-Agent Reinforcement Learning (Dist-QTRL), a novel approach to addressing the scalability challenges of traditional Reinforcement Learning (RL) by integrating quantum computing principles. Quantum-Train Reinforcement Learning (QTRL) leverages parameterized quantum circuits to efficiently generate neural network parameters, achieving a \(poly(\log(N))\) reduction in the dimensionality of trainable parameters while harnessing quantum entanglement for superior data representation. The framework is designed for distributed multi-agent environments, where multiple agents, modeled as Quantum Processing Units (QPUs), operate in parallel, enabling faster convergence and enhanced scalability. Additionally, the Dist-QTRL framework can be extended to high-performance computing (HPC) environments by utilizing distributed quantum training for parameter reduction in classical neural networks, followed by inference using classical CPUs or GPUs. This hybrid quantum-HPC approach allows for further optimization in real-world applications. In this paper, we provide a mathematical formulation of the Dist-QTRL framework and explore its convergence properties, supported by empirical results demonstrating performance improvements over centric QTRL models. The results highlight the potential of quantum-enhanced RL in tackling complex, high-dimensional tasks, particularly in distributed computing settings, where our framework achieves significant speedups through parallelization without compromising model accuracy. This work paves the way for scalable, quantum-enhanced RL systems in practical applications, leveraging both quantum and classical computational resources.
<div id='section'>Paperid: <span id='pid'>187, <a href='https://arxiv.org/pdf/2410.19450.pdf' target='_blank'>https://arxiv.org/pdf/2410.19450.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hai Zhong, Xun Wang, Zhuoran Li, Longbo Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.19450">Offline-to-Online Multi-Agent Reinforcement Learning with Offline Value Function Memory and Sequential Exploration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Offline-to-Online Reinforcement Learning has emerged as a powerful paradigm, leveraging offline data for initialization and online fine-tuning to enhance both sample efficiency and performance. However, most existing research has focused on single-agent settings, with limited exploration of the multi-agent extension, i.e., Offline-to-Online Multi-Agent Reinforcement Learning (O2O MARL). In O2O MARL, two critical challenges become more prominent as the number of agents increases: (i) the risk of unlearning pre-trained Q-values due to distributional shifts during the transition from offline-to-online phases, and (ii) the difficulty of efficient exploration in the large joint state-action space. To tackle these challenges, we propose a novel O2O MARL framework called Offline Value Function Memory with Sequential Exploration (OVMSE). First, we introduce the Offline Value Function Memory (OVM) mechanism to compute target Q-values, preserving knowledge gained during offline training, ensuring smoother transitions, and enabling efficient fine-tuning. Second, we propose a decentralized Sequential Exploration (SE) strategy tailored for O2O MARL, which effectively utilizes the pre-trained offline policy for exploration, thereby significantly reducing the joint state-action space to be explored. Extensive experiments on the StarCraft Multi-Agent Challenge (SMAC) demonstrate that OVMSE significantly outperforms existing baselines, achieving superior sample efficiency and overall performance.
<div id='section'>Paperid: <span id='pid'>188, <a href='https://arxiv.org/pdf/2409.19391.pdf' target='_blank'>https://arxiv.org/pdf/2409.19391.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pihe Hu, Shaolong Li, Zhuoran Li, Ling Pan, Longbo Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.19391">Value-Based Deep Multi-Agent Reinforcement Learning with Dynamic Sparse Training</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep Multi-agent Reinforcement Learning (MARL) relies on neural networks with numerous parameters in multi-agent scenarios, often incurring substantial computational overhead. Consequently, there is an urgent need to expedite training and enable model compression in MARL. This paper proposes the utilization of dynamic sparse training (DST), a technique proven effective in deep supervised learning tasks, to alleviate the computational burdens in MARL training. However, a direct adoption of DST fails to yield satisfactory MARL agents, leading to breakdowns in value learning within deep sparse value-based MARL models. Motivated by this challenge, we introduce an innovative Multi-Agent Sparse Training (MAST) framework aimed at simultaneously enhancing the reliability of learning targets and the rationality of sample distribution to improve value learning in sparse models. Specifically, MAST incorporates the Soft Mellowmax Operator with a hybrid TD-($Î»$) schema to establish dependable learning targets. Additionally, it employs a dual replay buffer mechanism to enhance the distribution of training samples. Building upon these aspects, MAST utilizes gradient-based topology evolution to exclusively train multiple MARL agents using sparse networks. Our comprehensive experimental investigation across various value-based MARL algorithms on multiple benchmarks demonstrates, for the first time, significant reductions in redundancy of up to $20\times$ in Floating Point Operations (FLOPs) for both training and inference, with less than $3\%$ performance degradation.
<div id='section'>Paperid: <span id='pid'>189, <a href='https://arxiv.org/pdf/2512.07417.pdf' target='_blank'>https://arxiv.org/pdf/2512.07417.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Giray Önür, Azita Dabiri, Bart De Schutter
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.07417">Adaptive Tuning of Parameterized Traffic Controllers via Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Effective traffic control is essential for mitigating congestion in transportation networks. Conventional traffic management strategies, including route guidance, ramp metering, and traffic signal control, often rely on state feedback controllers, used for their simplicity and reactivity; however, they lack the adaptability required to cope with complex and time-varying traffic dynamics. This paper proposes a multi-agent reinforcement learning framework in which each agent adaptively tunes the parameters of a state feedback traffic controller, combining the reactivity of state feedback controllers with the adaptability of reinforcement learning. By tuning parameters at a lower frequency rather than directly determining control actions at a high frequency, the reinforcement learning agents achieve improved training efficiency while maintaining adaptability to varying traffic conditions. The multi-agent structure further enhances system robustness, as local controllers can operate independently in the event of partial failures. The proposed framework is evaluated on a simulated multi-class transportation network under varying traffic conditions. Results show that the proposed multi-agent framework outperforms the no control and fixed-parameter state feedback control cases, while performing on par with the single-agent RL-based adaptive state feedback control, with a much better resilience to partial failures.
<div id='section'>Paperid: <span id='pid'>190, <a href='https://arxiv.org/pdf/2511.08412.pdf' target='_blank'>https://arxiv.org/pdf/2511.08412.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruochuan Shi, Runyu Lu, Yuanheng Zhu, Dongbin Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.08412">ARAC: Adaptive Regularized Multi-Agent Soft Actor-Critic in Graph-Structured Adversarial Games</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In graph-structured multi-agent reinforcement learning (MARL) adversarial tasks such as pursuit and confrontation, agents must coordinate under highly dynamic interactions, where sparse rewards hinder efficient policy learning. We propose Adaptive Regularized Multi-Agent Soft Actor-Critic (ARAC), which integrates an attention-based graph neural network (GNN) for modeling agent dependencies with an adaptive divergence regularization mechanism. The GNN enables expressive representation of spatial relations and state features in graph environments. Divergence regularization can serve as policy guidance to alleviate the sparse reward problem, but it may lead to suboptimal convergence when the reference policy itself is imperfect. The adaptive divergence regularization mechanism enables the framework to exploit reference policies for efficient exploration in the early stages, while gradually reducing reliance on them as training progresses to avoid inheriting their limitations. Experiments in pursuit and confrontation scenarios demonstrate that ARAC achieves faster convergence, higher final success rates, and stronger scalability across varying numbers of agents compared with MARL baselines, highlighting its effectiveness in complex graph-structured environments.
<div id='section'>Paperid: <span id='pid'>191, <a href='https://arxiv.org/pdf/2507.06628.pdf' target='_blank'>https://arxiv.org/pdf/2507.06628.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinmin He, Kai Li, Yifan Zang, Haobo Fu, Qiang Fu, Junliang Xing, Jian Cheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.06628">Goal-Oriented Skill Abstraction for Offline Multi-Task Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Offline multi-task reinforcement learning aims to learn a unified policy capable of solving multiple tasks using only pre-collected task-mixed datasets, without requiring any online interaction with the environment. However, it faces significant challenges in effectively sharing knowledge across tasks. Inspired by the efficient knowledge abstraction observed in human learning, we propose Goal-Oriented Skill Abstraction (GO-Skill), a novel approach designed to extract and utilize reusable skills to enhance knowledge transfer and task performance. Our approach uncovers reusable skills through a goal-oriented skill extraction process and leverages vector quantization to construct a discrete skill library. To mitigate class imbalances between broadly applicable and task-specific skills, we introduce a skill enhancement phase to refine the extracted skills. Furthermore, we integrate these skills using hierarchical policy learning, enabling the construction of a high-level policy that dynamically orchestrates discrete skills to accomplish specific tasks. Extensive experiments on diverse robotic manipulation tasks within the MetaWorld benchmark demonstrate the effectiveness and versatility of GO-Skill.
<div id='section'>Paperid: <span id='pid'>192, <a href='https://arxiv.org/pdf/2507.06615.pdf' target='_blank'>https://arxiv.org/pdf/2507.06615.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinmin He, Kai Li, Yifan Zang, Haobo Fu, Qiang Fu, Junliang Xing, Jian Cheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.06615">Efficient Multi-Task Reinforcement Learning with Cross-Task Policy Guidance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-task reinforcement learning endeavors to efficiently leverage shared information across various tasks, facilitating the simultaneous learning of multiple tasks. Existing approaches primarily focus on parameter sharing with carefully designed network structures or tailored optimization procedures. However, they overlook a direct and complementary way to exploit cross-task similarities: the control policies of tasks already proficient in some skills can provide explicit guidance for unmastered tasks to accelerate skills acquisition. To this end, we present a novel framework called Cross-Task Policy Guidance (CTPG), which trains a guide policy for each task to select the behavior policy interacting with the environment from all tasks' control policies, generating better training trajectories. In addition, we propose two gating mechanisms to improve the learning efficiency of CTPG: one gate filters out control policies that are not beneficial for guidance, while the other gate blocks tasks that do not necessitate guidance. CTPG is a general framework adaptable to existing parameter sharing approaches. Empirical evaluations demonstrate that incorporating CTPG with these approaches significantly enhances performance in manipulation and locomotion benchmarks.
<div id='section'>Paperid: <span id='pid'>193, <a href='https://arxiv.org/pdf/2506.07232.pdf' target='_blank'>https://arxiv.org/pdf/2506.07232.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinran Li, Chenjia Bai, Zijian Li, Jiakun Zheng, Ting Xiao, Jun Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.07232">Learn as Individuals, Evolve as a Team: Multi-agent LLMs Adaptation in Embodied Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large language models (LLMs) possess extensive knowledge bases and strong reasoning capabilities, making them promising tools for complex, multi-agent planning in embodied environments. However, despite LLMs' advanced abilities and the sophisticated modular design of agentic methods, existing LLM-based planning algorithms remain limited by weak adaptation capabilities to multi-agent embodied scenarios. We address this limitation by introducing a framework that enables LLM agents to learn and evolve both before and during test time, equipping them with environment-relevant knowledge for better planning and enhanced communication for improved cooperation. Inspired by centralized training with decentralized execution in multi-agent reinforcement learning, we propose a \textit{Learn as Individuals, Evolve as a Team (LIET)} paradigm for multi-agent LLMs adaptation. At the individual level, LLM agents learn a local utility function from exploratory datasets to better comprehend the embodied environment, which is then queried during test time to support informed decision-making. At the team level, LLM agents collaboratively and iteratively maintain and update a shared cooperation knowledge list based on new experiences, using it to guide more effective communication. By combining individual learning with team evolution, LIET enables comprehensive and flexible adaptation for LLM agents. Our experiments on Communicative Watch-And-Help and ThreeD-World Multi-Agent Transport benchmarks demonstrate that LIET, instantiated with both LLaMA and GPT-4o, outperforms existing baselines and exhibits strong cooperative planning abilities.
<div id='section'>Paperid: <span id='pid'>194, <a href='https://arxiv.org/pdf/2412.03925.pdf' target='_blank'>https://arxiv.org/pdf/2412.03925.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Talha Azfar, Kaicong Huang, Andrew Tracy, Sandra Misiewicz, Chenxi Liu, Ruimin Ke
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.03925">Traffic Co-Simulation Framework Empowered by Infrastructure Camera Sensing and Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Traffic simulations are commonly used to optimize urban traffic flow, with reinforcement learning (RL) showing promising potential for automated traffic signal control, particularly in intelligent transportation systems involving connected automated vehicles. Multi-agent reinforcement learning (MARL) is particularly effective for learning control strategies for traffic lights in a network using iterative simulations. However, existing methods often assume perfect vehicle detection, which overlooks real-world limitations related to infrastructure availability and sensor reliability. This study proposes a co-simulation framework integrating CARLA and SUMO, which combines high-fidelity 3D modeling with large-scale traffic flow simulation. Cameras mounted on traffic light poles within the CARLA environment use a YOLO-based computer vision system to detect and count vehicles, providing real-time traffic data as input for adaptive signal control in SUMO. MARL agents trained with four different reward structures leverage this visual feedback to optimize signal timings and improve network-wide traffic flow. Experiments in a multi-intersection test-bed demonstrate the effectiveness of the proposed MARL approach in enhancing traffic conditions using real-time camera based detection. The framework also evaluates the robustness of MARL under faulty or sparse sensing and compares the performance of YOLOv5 and YOLOv8 for vehicle detection. Results show that while better accuracy improves performance, MARL agents can still achieve significant improvements with imperfect detection, demonstrating scalability and adaptability for real-world scenarios.
<div id='section'>Paperid: <span id='pid'>195, <a href='https://arxiv.org/pdf/2410.17221.pdf' target='_blank'>https://arxiv.org/pdf/2410.17221.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhaolin Ren, Runyu Zhang, Bo Dai, Na Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.17221">Scalable spectral representations for multi-agent reinforcement learning in network MDPs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Network Markov Decision Processes (MDPs), a popular model for multi-agent control, pose a significant challenge to efficient learning due to the exponential growth of the global state-action space with the number of agents. In this work, utilizing the exponential decay property of network dynamics, we first derive scalable spectral local representations for network MDPs, which induces a network linear subspace for the local $Q$-function of each agent. Building on these local spectral representations, we design a scalable algorithmic framework for continuous state-action network MDPs, and provide end-to-end guarantees for the convergence of our algorithm. Empirically, we validate the effectiveness of our scalable representation-based approach on two benchmark problems, and demonstrate the advantages of our approach over generic function approximation approaches to representing the local $Q$-functions.
<div id='section'>Paperid: <span id='pid'>196, <a href='https://arxiv.org/pdf/2410.07863.pdf' target='_blank'>https://arxiv.org/pdf/2410.07863.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fanqi Kong, Yizhe Huang, Song-Chun Zhu, Siyuan Qi, Xue Feng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.07863">Learning to Balance Altruism and Self-interest Based on Empathy in Mixed-Motive Games</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Real-world multi-agent scenarios often involve mixed motives, demanding altruistic agents capable of self-protection against potential exploitation. However, existing approaches often struggle to achieve both objectives. In this paper, based on that empathic responses are modulated by inferred social relationships between agents, we propose LASE Learning to balance Altruism and Self-interest based on Empathy), a distributed multi-agent reinforcement learning algorithm that fosters altruistic cooperation through gifting while avoiding exploitation by other agents in mixed-motive games. LASE allocates a portion of its rewards to co-players as gifts, with this allocation adapting dynamically based on the social relationship -- a metric evaluating the friendliness of co-players estimated by counterfactual reasoning. In particular, social relationship measures each co-player by comparing the estimated $Q$-function of current joint action to a counterfactual baseline which marginalizes the co-player's action, with its action distribution inferred by a perspective-taking module. Comprehensive experiments are performed in spatially and temporally extended mixed-motive games, demonstrating LASE's ability to promote group collaboration without compromising fairness and its capacity to adapt policies to various types of interactive co-players.
<div id='section'>Paperid: <span id='pid'>197, <a href='https://arxiv.org/pdf/2510.08240.pdf' target='_blank'>https://arxiv.org/pdf/2510.08240.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jingyu Zhang, Haozhu Wang, Eric Michael Smith, Sid Wang, Amr Sharaf, Mahesh Pasupuleti, Benjamin Van Durme, Daniel Khashabi, Jason Weston, Hongyuan Zhan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.08240">The Alignment Waltz: Jointly Training Agents to Collaborate for Safety</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Harnessing the power of LLMs requires a delicate dance between being helpful and harmless. This creates a fundamental tension between two competing challenges: vulnerability to adversarial attacks that elicit unsafe content, and a tendency for overrefusal on benign but sensitive prompts. Current approaches often navigate this dance with safeguard models that completely reject any content that contains unsafe portions. This approach cuts the music entirely-it may exacerbate overrefusals and fails to provide nuanced guidance for queries it refuses. To teach models a more coordinated choreography, we propose WaltzRL, a novel multi-agent reinforcement learning framework that formulates safety alignment as a collaborative, positive-sum game. WaltzRL jointly trains a conversation agent and a feedback agent, where the latter is incentivized to provide useful suggestions that improve the safety and helpfulness of the conversation agent's responses. At the core of WaltzRL is a Dynamic Improvement Reward (DIR) that evolves over time based on how well the conversation agent incorporates the feedback. At inference time, unsafe or overrefusing responses from the conversation agent are improved rather than discarded. The feedback agent is deployed together with the conversation agent and only engages adaptively when needed, preserving helpfulness and low latency on safe queries. Our experiments, conducted across five diverse datasets, demonstrate that WaltzRL significantly reduces both unsafe responses (e.g., from 39.0% to 4.6% on WildJailbreak) and overrefusals (from 45.3% to 9.9% on OR-Bench) compared to various baselines. By enabling the conversation and feedback agents to co-evolve and adaptively apply feedback, WaltzRL enhances LLM safety without degrading general capabilities, thereby advancing the Pareto front between helpfulness and harmlessness.
<div id='section'>Paperid: <span id='pid'>198, <a href='https://arxiv.org/pdf/2509.24047.pdf' target='_blank'>https://arxiv.org/pdf/2509.24047.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Runyu Zhang, Na Li, Asuman Ozdaglar, Jeff Shamma, Gioele Zardini
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.24047">Optimism as Risk-Seeking in Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Risk sensitivity has become a central theme in reinforcement learning (RL), where convex risk measures and robust formulations provide principled ways to model preferences beyond expected return. Recent extensions to multi-agent RL (MARL) have largely emphasized the risk-averse setting, prioritizing robustness to uncertainty. In cooperative MARL, however, such conservatism often leads to suboptimal equilibria, and a parallel line of work has shown that optimism can promote cooperation. Existing optimistic methods, though effective in practice, are typically heuristic and lack theoretical grounding. Building on the dual representation for convex risk measures, we propose a principled framework that interprets risk-seeking objectives as optimism. We introduce optimistic value functions, which formalize optimism as divergence-penalized risk-seeking evaluations. Building on this foundation, we derive a policy-gradient theorem for optimistic value functions, including explicit formulas for the entropic risk/KL-penalty setting, and develop decentralized optimistic actor-critic algorithms that implement these updates. Empirical results on cooperative benchmarks demonstrate that risk-seeking optimism consistently improves coordination over both risk-neutral baselines and heuristic optimistic methods. Our framework thus unifies risk-sensitive learning and optimism, offering a theoretically grounded and practically effective approach to cooperation in MARL.
<div id='section'>Paperid: <span id='pid'>199, <a href='https://arxiv.org/pdf/2508.12480.pdf' target='_blank'>https://arxiv.org/pdf/2508.12480.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Constantin Ruhdorfer, Matteo Bortoletto, Andreas Bulling
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.12480">The Yokai Learning Environment: Tracking Beliefs Over Space and Time</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Developing collaborative AI hinges on Theory of Mind (ToM) - the ability to reason about the beliefs of others to build and maintain common ground. Existing ToM benchmarks, however, are restricted to passive observer settings or lack an assessment of how agents establish and maintain common ground over time. To address these gaps, we introduce the Yokai Learning Environment (YLE) - a multi-agent reinforcement learning (RL) environment based on the cooperative card game Yokai. In the YLE, agents take turns peeking at hidden cards and moving them to form clusters based on colour. Success requires tracking evolving beliefs, remembering past observations, using hints as grounded communication, and maintaining common ground with teammates. Our evaluation yields two key findings: First, current RL agents struggle to solve the YLE, even when given access to perfect memory. Second, while belief modelling improves performance, agents are still unable to effectively generalise to unseen partners or form accurate beliefs over longer games, exposing a reliance on brittle conventions rather than robust belief tracking. We use the YLE to investigate research questions in belief modelling, memory, partner generalisation, and scaling to higher-order ToM.
<div id='section'>Paperid: <span id='pid'>200, <a href='https://arxiv.org/pdf/2508.06336.pdf' target='_blank'>https://arxiv.org/pdf/2508.06336.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Constantin Ruhdorfer, Matteo Bortoletto, Victor Oei, Anna Penzkofer, Andreas Bulling
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.06336">Unsupervised Partner Design Enables Robust Ad-hoc Teamwork</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce Unsupervised Partner Design (UPD) - a population-free, multi-agent reinforcement learning framework for robust ad-hoc teamwork that adaptively generates training partners without requiring pretrained partners or manual parameter tuning. UPD constructs diverse partners by stochastically mixing an ego agent's policy with biased random behaviours and scores them using a variance-based learnability metric that prioritises partners near the ego agent's current learning frontier. We show that UPD can be integrated with unsupervised environment design, resulting in the first method enabling fully unsupervised curricula over both level and partner distributions in a cooperative setting. Through extensive evaluations on Overcooked-AI and the Overcooked Generalisation Challenge, we demonstrate that this dynamic partner curriculum is highly effective: UPD consistently outperforms both population-based and population-free baselines as well as ablations. In a user study, we further show that UPD achieves higher returns than all baselines and was perceived as significantly more adaptive, more human-like, a better collaborator, and less frustrating.
<div id='section'>Paperid: <span id='pid'>201, <a href='https://arxiv.org/pdf/2506.13755.pdf' target='_blank'>https://arxiv.org/pdf/2506.13755.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Arya Fayyazi, Mehdi Kamal, Massoud Pedram
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.13755">MARCO: Hardware-Aware Neural Architecture Search for Edge Devices with Multi-Agent Reinforcement Learning and Conformal Prediction Filtering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces MARCO (Multi-Agent Reinforcement learning with Conformal Optimization), a novel hardware-aware framework for efficient neural architecture search (NAS) targeting resource-constrained edge devices. By significantly reducing search time and maintaining accuracy under strict hardware constraints, MARCO bridges the gap between automated DNN design and CAD for edge AI deployment. MARCO's core technical contribution lies in its unique combination of multi-agent reinforcement learning (MARL) with Conformal Prediction (CP) to accelerate the hardware/software co-design process for deploying deep neural networks. Unlike conventional once-for-all (OFA) supernet approaches that require extensive pretraining, MARCO decomposes the NAS task into a hardware configuration agent (HCA) and a Quantization Agent (QA). The HCA optimizes high-level design parameters, while the QA determines per-layer bit-widths under strict memory and latency budgets using a shared reward signal within a centralized-critic, decentralized-execution (CTDE) paradigm. A key innovation is the integration of a calibrated CP surrogate model that provides statistical guarantees (with a user-defined miscoverage rate) to prune unpromising candidate architectures before incurring the high costs of partial training or hardware simulation. This early filtering drastically reduces the search space while ensuring that high-quality designs are retained with a high probability. Extensive experiments on MNIST, CIFAR-10, and CIFAR-100 demonstrate that MARCO achieves a 3-4x reduction in total search time compared to an OFA baseline while maintaining near-baseline accuracy (within 0.3%). Furthermore, MARCO also reduces inference latency. Validation on a MAX78000 evaluation board confirms that simulator trends hold in practice, with simulator estimates deviating from measured values by less than 5%.
<div id='section'>Paperid: <span id='pid'>202, <a href='https://arxiv.org/pdf/2506.05431.pdf' target='_blank'>https://arxiv.org/pdf/2506.05431.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ashwin Ramesh Babu, Sajad Mousavi, Vineet Gundecha, Sahand Ghorbanpour, Avisek Naug, Antonio Guillen, Ricardo Luna Gutierrez, Soumyendu Sarkar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.05431">Robustness Evaluation for Video Models with Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Evaluating the robustness of Video classification models is very challenging, specifically when compared to image-based models. With their increased temporal dimension, there is a significant increase in complexity and computational cost. One of the key challenges is to keep the perturbations to a minimum to induce misclassification. In this work, we propose a multi-agent reinforcement learning approach (spatial and temporal) that cooperatively learns to identify the given video's sensitive spatial and temporal regions. The agents consider temporal coherence in generating fine perturbations, leading to a more effective and visually imperceptible attack. Our method outperforms the state-of-the-art solutions on the Lp metric and the average queries. Our method enables custom distortion types, making the robustness evaluation more relevant to the use case. We extensively evaluate 4 popular models for video action recognition on two popular datasets, HMDB-51 and UCF-101.
<div id='section'>Paperid: <span id='pid'>203, <a href='https://arxiv.org/pdf/2411.01184.pdf' target='_blank'>https://arxiv.org/pdf/2411.01184.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chanjuan Liu, Jinmiao Cong, Bingcai Chen, Yaochu Jin, Enqiang Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.01184">Guiding Multi-agent Multi-task Reinforcement Learning by a Hierarchical Framework with Logical Reward Shaping</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent hierarchical reinforcement learning (MAHRL) has been studied as an effective means to solve intelligent decision problems in complex and large-scale environments. However, most current MAHRL algorithms follow the traditional way of using reward functions in reinforcement learning, which limits their use to a single task. This study aims to design a multi-agent cooperative algorithm with logic reward shaping (LRS), which uses a more flexible way of setting the rewards, allowing for the effective completion of multi-tasks. LRS uses Linear Temporal Logic (LTL) to express the internal logic relation of subtasks within a complex task. Then, it evaluates whether the subformulae of the LTL expressions are satisfied based on a designed reward structure. This helps agents to learn to effectively complete tasks by adhering to the LTL expressions, thus enhancing the interpretability and credibility of their decisions. To enhance coordination and cooperation among multiple agents, a value iteration technique is designed to evaluate the actions taken by each agent. Based on this evaluation, a reward function is shaped for coordination, which enables each agent to evaluate its status and complete the remaining subtasks through experiential learning. Experiments have been conducted on various types of tasks in the Minecraft-like environment. The results demonstrate that the proposed algorithm can improve the performance of multi-agents when learning to complete multi-tasks.
<div id='section'>Paperid: <span id='pid'>204, <a href='https://arxiv.org/pdf/2409.20067.pdf' target='_blank'>https://arxiv.org/pdf/2409.20067.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Laixi Shi, Jingchu Gai, Eric Mazumdar, Yuejie Chi, Adam Wierman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.20067">Breaking the Curse of Multiagency in Robust Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Standard multi-agent reinforcement learning (MARL) algorithms are vulnerable to sim-to-real gaps. To address this, distributionally robust Markov games (RMGs) have been proposed to enhance robustness in MARL by optimizing the worst-case performance when game dynamics shift within a prescribed uncertainty set. RMGs remains under-explored, from reasonable problem formulation to the development of sample-efficient algorithms. Two notorious and open challenges are the formulation of the uncertainty set and whether the corresponding RMGs can overcome the curse of multiagency, where the sample complexity scales exponentially with the number of agents. In this work, we propose a natural class of RMGs inspired by behavioral economics, where each agent's uncertainty set is shaped by both the environment and the integrated behavior of other agents. We first establish the well-posedness of this class of RMGs by proving the existence of game-theoretic solutions such as robust Nash equilibria and coarse correlated equilibria (CCE). Assuming access to a generative model, we then introduce a sample-efficient algorithm for learning the CCE whose sample complexity scales polynomially with all relevant parameters. To the best of our knowledge, this is the first algorithm to break the curse of multiagency for RMGs, regardless of the uncertainty set formulation.
<div id='section'>Paperid: <span id='pid'>205, <a href='https://arxiv.org/pdf/2511.12876.pdf' target='_blank'>https://arxiv.org/pdf/2511.12876.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Heyang Ma, Qirui Mi, Qipeng Yang, Zijun Fan, Bo Li, Haifeng Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.12876">Think, Speak, Decide: Language-Augmented Multi-Agent Reinforcement Learning for Economic Decision-Making</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Economic decision-making depends not only on structured signals such as prices and taxes, but also on unstructured language, including peer dialogue and media narratives. While multi-agent reinforcement learning (MARL) has shown promise in optimizing economic decisions, it struggles with the semantic ambiguity and contextual richness of language. We propose LAMP (Language-Augmented Multi-Agent Policy), a framework that integrates language into economic decision-making and narrows the gap to real-world settings. LAMP follows a Think-Speak-Decide pipeline: (1) Think interprets numerical observations to extract short-term shocks and long-term trends, caching high-value reasoning trajectories; (2) Speak crafts and exchanges strategic messages based on reasoning, updating beliefs by parsing peer communications; and (3) Decide fuses numerical data, reasoning, and reflections into a MARL policy to optimize language-augmented decision-making. Experiments in economic simulation show that LAMP outperforms both MARL and LLM-only baselines in cumulative return (+63.5%, +34.0%), robustness (+18.8%, +59.4%), and interpretability. These results demonstrate the potential of language-augmented policies to deliver more effective and robust economic strategies.
<div id='section'>Paperid: <span id='pid'>206, <a href='https://arxiv.org/pdf/2511.10291.pdf' target='_blank'>https://arxiv.org/pdf/2511.10291.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aswin Arun, Christo Kurisummoottil Thomas, Rimalpudi Sarvendranath, Walid Saad
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.10291">Causal Model-Based Reinforcement Learning for Sample-Efficient IoT Channel Access</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite the advantages of multi-agent reinforcement learning (MARL) for wireless use case such as medium access control (MAC), their real-world deployment in Internet of Things (IoT) is hindered by their sample inefficiency. To alleviate this challenge, one can leverage model-based reinforcement learning (MBRL) solutions, however, conventional MBRL approaches rely on black-box models that are not interpretable and cannot reason. In contrast, in this paper, a novel causal model-based MARL framework is developed by leveraging tools from causal learn- ing. In particular, the proposed model can explicitly represent causal dependencies between network variables using structural causal models (SCMs) and attention-based inference networks. Interpretable causal models are then developed to capture how MAC control messages influence observations, how transmission actions determine outcomes, and how channel observations affect rewards. Data augmentation techniques are then used to generate synthetic rollouts using the learned causal model for policy optimization via proximal policy optimization (PPO). Analytical results demonstrate exponential sample complexity gains of causal MBRL over black-box approaches. Extensive simulations demonstrate that, on average, the proposed approach can reduce environment interactions by 58%, and yield faster convergence compared to model-free baselines. The proposed approach inherently is also shown to provide interpretable scheduling decisions via attention-based causal attribution, revealing which network conditions drive the policy. The resulting combination of sample efficiency and interpretability establishes causal MBRL as a practical approach for resource-constrained wireless systems.
<div id='section'>Paperid: <span id='pid'>207, <a href='https://arxiv.org/pdf/2510.18183.pdf' target='_blank'>https://arxiv.org/pdf/2510.18183.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Eason Yu, Tzu Hao Liu, Yunke Wang, Clément L. Canonne, Nguyen H. Tran, Chang Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.18183">Nash Policy Gradient: A Policy Gradient Method with Iteratively Refined Regularization for Finding Nash Equilibria</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Finding Nash equilibria in imperfect-information games remains a central challenge in multi-agent reinforcement learning. While regularization-based methods have recently achieved last-iteration convergence to a regularized equilibrium, they require the regularization strength to shrink toward zero to approximate a Nash equilibrium, often leading to unstable learning in practice. Instead, we fix the regularization strength at a large value for robustness and achieve convergence by iteratively refining the reference policy. Our main theoretical result shows that this procedure guarantees strictly monotonic improvement and convergence to an exact Nash equilibrium in two-player zero-sum games, without requiring a uniqueness assumption. Building on this framework, we develop a practical algorithm, Nash Policy Gradient (NashPG), which preserves the generalizability of policy gradient methods while relying solely on the current and reference policies. Empirically, NashPG achieves comparable or lower exploitability than prior model-free methods on classic benchmark games and scales to large domains such as Battleship and No-Limit Texas Hold'em, where NashPG consistently attains higher Elo ratings.
<div id='section'>Paperid: <span id='pid'>208, <a href='https://arxiv.org/pdf/2510.11474.pdf' target='_blank'>https://arxiv.org/pdf/2510.11474.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ardian Selmonaj, Giacomo Del Rio, Adrian Schneider, Alessandro Antonucci
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.11474">Coordinated Strategies in Realistic Air Combat by Hierarchical Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Achieving mission objectives in a realistic simulation of aerial combat is highly challenging due to imperfect situational awareness and nonlinear flight dynamics. In this work, we introduce a novel 3D multi-agent air combat environment and a Hierarchical Multi-Agent Reinforcement Learning framework to tackle these challenges. Our approach combines heterogeneous agent dynamics, curriculum learning, league-play, and a newly adapted training algorithm. To this end, the decision-making process is organized into two abstraction levels: low-level policies learn precise control maneuvers, while high-level policies issue tactical commands based on mission objectives. Empirical results show that our hierarchical approach improves both learning efficiency and combat performance in complex dogfight scenarios.
<div id='section'>Paperid: <span id='pid'>209, <a href='https://arxiv.org/pdf/2510.07363.pdf' target='_blank'>https://arxiv.org/pdf/2510.07363.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianxiang Xu, Zhichao Wen, Xinyu Zhao, Jun Wang, Yan Li, Chang Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.07363">L2M-AID: Autonomous Cyber-Physical Defense by Fusing Semantic Reasoning of Large Language Models with Multi-Agent Reinforcement Learning (Preprint)</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The increasing integration of Industrial IoT (IIoT) exposes critical cyber-physical systems to sophisticated, multi-stage attacks that elude traditional defenses lacking contextual awareness. This paper introduces L2M-AID, a novel framework for Autonomous Industrial Defense using LLM-empowered, Multi-agent reinforcement learning. L2M-AID orchestrates a team of collaborative agents, each driven by a Large Language Model (LLM), to achieve adaptive and resilient security. The core innovation lies in the deep fusion of two AI paradigms: we leverage an LLM as a semantic bridge to translate vast, unstructured telemetry into a rich, contextual state representation, enabling agents to reason about adversary intent rather than merely matching patterns. This semantically-aware state empowers a Multi-Agent Reinforcement Learning (MARL) algorithm, MAPPO, to learn complex cooperative strategies. The MARL reward function is uniquely engineered to balance security objectives (threat neutralization) with operational imperatives, explicitly penalizing actions that disrupt physical process stability. To validate our approach, we conduct extensive experiments on the benchmark SWaT dataset and a novel synthetic dataset generated based on the MITRE ATT&CK for ICS framework. Results demonstrate that L2M-AID significantly outperforms traditional IDS, deep learning anomaly detectors, and single-agent RL baselines across key metrics, achieving a 97.2% detection rate while reducing false positives by over 80% and improving response times by a factor of four. Crucially, it demonstrates superior performance in maintaining physical process stability, presenting a robust new paradigm for securing critical national infrastructure.
<div id='section'>Paperid: <span id='pid'>210, <a href='https://arxiv.org/pdf/2509.26002.pdf' target='_blank'>https://arxiv.org/pdf/2509.26002.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ardian Selmonaj, Giacomo Del Rio, Adrian Schneider, Alessandro Antonucci
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.26002">Towards Human Engagement with Realistic AI Combat Pilots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a system that enables real-time interaction between human users and agents trained to control fighter jets in simulated 3D air combat scenarios. The agents are trained in a dedicated environment using Multi-Agent Reinforcement Learning. A communication link is developed to allow seamless deployment of trained agents into VR-Forces, a widely used defense simulation tool for realistic tactical scenarios. This integration allows mixed simulations where human-controlled entities engage with intelligent agents exhibiting distinct combat behaviors. Our interaction model creates new opportunities for human-agent teaming, immersive training, and the exploration of innovative tactics in defense contexts.
<div id='section'>Paperid: <span id='pid'>211, <a href='https://arxiv.org/pdf/2509.23462.pdf' target='_blank'>https://arxiv.org/pdf/2509.23462.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alakh Sharma, Gaurish Trivedi, Kartikey Bhandari, Yash Sinha, Dhruv Kumar, Pratik Narang, Jagat Sesh Challa
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.23462">Generative Evolutionary Meta-Solver (GEMS): Scalable Surrogate-Free Multi-Agent Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Scalable multi-agent reinforcement learning (MARL) remains a central challenge for AI. Existing population-based methods, like Policy-Space Response Oracles, PSRO, require storing explicit policy populations and constructing full payoff matrices, incurring quadratic computation and linear memory costs. We present Generative Evolutionary Meta-Solver (GEMS), a surrogate-free framework that replaces explicit populations with a compact set of latent anchors and a single amortized generator. Instead of exhaustively constructing the payoff matrix, GEMS relies on unbiased Monte Carlo rollouts, multiplicative-weights meta-dynamics, and a model-free empirical-Bernstein UCB oracle to adaptively expand the policy set. Best responses are trained within the generator using an advantage-based trust-region objective, eliminating the need to store and train separate actors. We evaluated GEMS in a variety of Two-player and Multi-Player games such as the Deceptive Messages Game, Kuhn Poker and Multi-Particle environment. We find that GEMS is up to ~6x faster, has 1.3x less memory usage than PSRO, while also reaps higher rewards simultaneously. These results demonstrate that GEMS retains the game theoretic guarantees of PSRO, while overcoming its fundamental inefficiencies, hence enabling scalable multi-agent learning in multiple domains.
<div id='section'>Paperid: <span id='pid'>212, <a href='https://arxiv.org/pdf/2508.15652.pdf' target='_blank'>https://arxiv.org/pdf/2508.15652.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ardian Selmonaj, Miroslav Strupl, Oleg Szehr, Alessandro Antonucci
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.15652">Understanding Action Effects through Instrumental Empowerment in Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To reliably deploy Multi-Agent Reinforcement Learning (MARL) systems, it is crucial to understand individual agent behaviors. While prior work typically evaluates overall team performance based on explicit reward signals, it is unclear how to infer agent contributions in the absence of any value feedback. In this work, we investigate whether meaningful insights into agent behaviors can be extracted solely by analyzing the policy distribution. Inspired by the phenomenon that intelligent agents tend to pursue convergent instrumental values, we introduce Intended Cooperation Values (ICVs), a method based on information-theoretic Shapley values for quantifying each agent's causal influence on their co-players' instrumental empowerment. Specifically, ICVs measure an agent's action effect on its teammates' policies by assessing their decision (un)certainty and preference alignment. By analyzing action effects on policies and value functions across cooperative and competitive MARL tasks, our method identifies which agent behaviors are beneficial to team success, either by fostering deterministic decisions or by preserving flexibility for future action choices, while also revealing the extent to which agents adopt similar or diverse strategies. Our proposed method offers novel insights into cooperation dynamics and enhances explainability in MARL systems.
<div id='section'>Paperid: <span id='pid'>213, <a href='https://arxiv.org/pdf/2505.11311.pdf' target='_blank'>https://arxiv.org/pdf/2505.11311.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ardian Selmonaj, Alessandro Antonucci, Adrian Schneider, Michael RÃ¼egsegger, Matthias Sommer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.11311">Explaining Strategic Decisions in Multi-Agent Reinforcement Learning for Aerial Combat Tactics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Artificial intelligence (AI) is reshaping strategic planning, with Multi-Agent Reinforcement Learning (MARL) enabling coordination among autonomous agents in complex scenarios. However, its practical deployment in sensitive military contexts is constrained by the lack of explainability, which is an essential factor for trust, safety, and alignment with human strategies. This work reviews and assesses current advances in explainability methods for MARL with a focus on simulated air combat scenarios. We proceed by adapting various explainability techniques to different aerial combat scenarios to gain explanatory insights about the model behavior. By linking AI-generated tactics with human-understandable reasoning, we emphasize the need for transparency to ensure reliable deployment and meaningful human-machine interaction. By illuminating the crucial importance of explainability in advancing MARL for operational defense, our work supports not only strategic planning but also the training of military personnel with insightful and comprehensible analyses.
<div id='section'>Paperid: <span id='pid'>214, <a href='https://arxiv.org/pdf/2505.08995.pdf' target='_blank'>https://arxiv.org/pdf/2505.08995.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ardian Selmonaj, Oleg Szehr, Giacomo Del Rio, Alessandro Antonucci, Adrian Schneider, Michael RÃ¼egsegger
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.08995">Enhancing Aerial Combat Tactics through Hierarchical Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work presents a Hierarchical Multi-Agent Reinforcement Learning framework for analyzing simulated air combat scenarios involving heterogeneous agents. The objective is to identify effective Courses of Action that lead to mission success within preset simulations, thereby enabling the exploration of real-world defense scenarios at low cost and in a safe-to-fail setting. Applying deep Reinforcement Learning in this context poses specific challenges, such as complex flight dynamics, the exponential size of the state and action spaces in multi-agent systems, and the capability to integrate real-time control of individual units with look-ahead planning. To address these challenges, the decision-making process is split into two levels of abstraction: low-level policies control individual units, while a high-level commander policy issues macro commands aligned with the overall mission targets. This hierarchical structure facilitates the training process by exploiting policy symmetries of individual agents and by separating control from command tasks. The low-level policies are trained for individual combat control in a curriculum of increasing complexity. The high-level commander is then trained on mission targets given pre-trained control policies. The empirical validation confirms the advantages of the proposed framework.
<div id='section'>Paperid: <span id='pid'>215, <a href='https://arxiv.org/pdf/2503.11488.pdf' target='_blank'>https://arxiv.org/pdf/2503.11488.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yifeng Zhang, Yilin Liu, Ping Gong, Peizhuo Li, Mingfeng Fan, Guillaume Sartoretti
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.11488">Unicorn: A Universal and Collaborative Reinforcement Learning Approach Towards Generalizable Network-Wide Traffic Signal Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Adaptive traffic signal control (ATSC) is crucial in reducing congestion, maximizing throughput, and improving mobility in rapidly growing urban areas. Recent advancements in parameter-sharing multi-agent reinforcement learning (MARL) have greatly enhanced the scalable and adaptive optimization of complex, dynamic flows in large-scale homogeneous networks. However, the inherent heterogeneity of real-world traffic networks, with their varied intersection topologies and interaction dynamics, poses substantial challenges to achieving scalable and effective ATSC across different traffic scenarios. To address these challenges, we present Unicorn, a universal and collaborative MARL framework designed for efficient and adaptable network-wide ATSC. Specifically, we first propose a unified approach to map the states and actions of intersections with varying topologies into a common structure based on traffic movements. Next, we design a Universal Traffic Representation (UTR) module with a decoder-only network for general feature extraction, enhancing the model's adaptability to diverse traffic scenarios. Additionally, we incorporate an Intersection Specifics Representation (ISR) module, designed to identify key latent vectors that represent the unique intersection's topology and traffic dynamics through variational inference techniques. To further refine these latent representations, we employ a contrastive learning approach in a self-supervised manner, which enables better differentiation of intersection-specific features. Moreover, we integrate the state-action dependencies of neighboring agents into policy optimization, which effectively captures dynamic agent interactions and facilitates efficient regional collaboration. Our results show that Unicorn outperforms other methods across various evaluation metrics, highlighting its potential in complex, dynamic traffic networks.
<div id='section'>Paperid: <span id='pid'>216, <a href='https://arxiv.org/pdf/2512.11179.pdf' target='_blank'>https://arxiv.org/pdf/2512.11179.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wei Duan, Jie Lu, En Yu, Junyu Xuan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.11179">Bandwidth-constrained Variational Message Encoding for Cooperative Multi-agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Graph-based multi-agent reinforcement learning (MARL) enables coordinated behavior under partial observability by modeling agents as nodes and communication links as edges. While recent methods excel at learning sparse coordination graphs-determining who communicates with whom-they do not address what information should be transmitted under hard bandwidth constraints. We study this bandwidth-limited regime and show that naive dimensionality reduction consistently degrades coordination performance. Hard bandwidth constraints force selective encoding, but deterministic projections lack mechanisms to control how compression occurs. We introduce Bandwidth-constrained Variational Message Encoding (BVME), a lightweight module that treats messages as samples from learned Gaussian posteriors regularized via KL divergence to an uninformative prior. BVME's variational framework provides principled, tunable control over compression strength through interpretable hyperparameters, directly constraining the representations used for decision-making. Across SMACv1, SMACv2, and MPE benchmarks, BVME achieves comparable or superior performance while using 67--83% fewer message dimensions, with gains most pronounced on sparse graphs where message quality critically impacts coordination. Ablations reveal U-shaped sensitivity to bandwidth, with BVME excelling at extreme ratios while adding minimal overhead.
<div id='section'>Paperid: <span id='pid'>217, <a href='https://arxiv.org/pdf/2511.12792.pdf' target='_blank'>https://arxiv.org/pdf/2511.12792.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mohamad A. Hady, Siyi Hu, Mahardhika Pratama, Zehong Cao, Ryszard Kowalczyk
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.12792">Multi-Agent Reinforcement Learning for Heterogeneous Satellite Cluster Resources Optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work investigates resource optimization in heterogeneous satellite clusters performing autonomous Earth Observation (EO) missions using Reinforcement Learning (RL). In the proposed setting, two optical satellites and one Synthetic Aperture Radar (SAR) satellite operate cooperatively in low Earth orbit to capture ground targets and manage their limited onboard resources efficiently. Traditional optimization methods struggle to handle the real-time, uncertain, and decentralized nature of EO operations, motivating the use of RL and Multi-Agent Reinforcement Learning (MARL) for adaptive decision-making. This study systematically formulates the optimization problem from single-satellite to multi-satellite scenarios, addressing key challenges including energy and memory constraints, partial observability, and agent heterogeneity arising from diverse payload capabilities. Using a near-realistic simulation environment built on the Basilisk and BSK-RL frameworks, we evaluate the performance and stability of state-of-the-art MARL algorithms such as MAPPO, HAPPO, and HATRPO. Results show that MARL enables effective coordination across heterogeneous satellites, balancing imaging performance and resource utilization while mitigating non-stationarity and inter-agent reward coupling. The findings provide practical insights into scalable, autonomous satellite operations and contribute a foundation for future research on intelligent EO mission planning under heterogeneous and dynamic conditions.
<div id='section'>Paperid: <span id='pid'>218, <a href='https://arxiv.org/pdf/2509.24226.pdf' target='_blank'>https://arxiv.org/pdf/2509.24226.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jingqi Li, Gechen Qu, Jason J. Choi, Somayeh Sojoudi, Claire Tomlin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.24226">Multi-Agent Guided Policy Search for Non-Cooperative Dynamic Games</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent reinforcement learning (MARL) optimizes strategic interactions in non-cooperative dynamic games, where agents have misaligned objectives. However, data-driven methods such as multi-agent policy gradients (MA-PG) often suffer from instability and limit-cycle behaviors. Prior stabilization techniques typically rely on entropy-based exploration, which slows learning and increases variance. We propose a model-based approach that incorporates approximate priors into the reward function as regularization. In linear quadratic (LQ) games, we prove that such priors stabilize policy gradients and guarantee local exponential convergence to an approximate Nash equilibrium. We then extend this idea to infinite-horizon nonlinear games by introducing Multi-agent Guided Policy Search (MA-GPS), which constructs short-horizon local LQ approximations from trajectories of current policies to guide training. Experiments on nonlinear vehicle platooning and a six-player strategic basketball formation show that MA-GPS achieves faster convergence and more stable learning than existing MARL methods.
<div id='section'>Paperid: <span id='pid'>219, <a href='https://arxiv.org/pdf/2509.16606.pdf' target='_blank'>https://arxiv.org/pdf/2509.16606.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wei Duan, Jie Lu, Junyu Xuan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.16606">Bayesian Ego-graph inference for Networked Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In networked multi-agent reinforcement learning (Networked-MARL), decentralized agents must act under local observability and constrained communication over fixed physical graphs. Existing methods often assume static neighborhoods, limiting adaptability to dynamic or heterogeneous environments. While centralized frameworks can learn dynamic graphs, their reliance on global state access and centralized infrastructure is impractical in real-world decentralized systems. We propose a stochastic graph-based policy for Networked-MARL, where each agent conditions its decision on a sampled subgraph over its local physical neighborhood. Building on this formulation, we introduce BayesG, a decentralized actor-framework that learns sparse, context-aware interaction structures via Bayesian variational inference. Each agent operates over an ego-graph and samples a latent communication mask to guide message passing and policy computation. The variational distribution is trained end-to-end alongside the policy using an evidence lower bound (ELBO) objective, enabling agents to jointly learn both interaction topology and decision-making strategies. BayesG outperforms strong MARL baselines on large-scale traffic control tasks with up to 167 agents, demonstrating superior scalability, efficiency, and performance.
<div id='section'>Paperid: <span id='pid'>220, <a href='https://arxiv.org/pdf/2509.15099.pdf' target='_blank'>https://arxiv.org/pdf/2509.15099.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Taoyuan Yu, Kui Wang, Zongdian Li, Tao Yu, Kei Sakaguchi, Walid Saad
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.15099">Digital Twin-based Cooperative Autonomous Driving in Smart Intersections: A Multi-Agent Reinforcement Learning Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Unsignalized intersections pose safety and efficiency challenges due to complex traffic flows and blind spots. In this paper, a digital twin (DT)-based cooperative driving system with roadside unit (RSU)-centric architecture is proposed for enhancing safety and efficiency at unsignalized intersections. The system leverages comprehensive bird-eye-view (BEV) perception to eliminate blind spots and employs a hybrid reinforcement learning (RL) framework combining offline pre-training with online fine-tuning. Specifically, driving policies are initially trained using conservative Q-learning (CQL) with behavior cloning (BC) on real datasets, then fine-tuned using multi-agent proximal policy optimization (MAPPO) with self-attention mechanisms to handle dynamic multi-agent coordination. The RSU implements real-time commands via vehicle-to-infrastructure (V2I) communications. Experimental results show that the proposed method yields failure rates below 0.03\% coordinating up to three connected autonomous vehicles (CAVs), significantly outperforming traditional methods. In addition, the system exhibits sub-linear computational scaling with inference times under 40 ms. Furthermore, it demonstrates robust generalization across diverse unsignalized intersection scenarios, indicating its practicality and readiness for real-world deployment.
<div id='section'>Paperid: <span id='pid'>221, <a href='https://arxiv.org/pdf/2508.21066.pdf' target='_blank'>https://arxiv.org/pdf/2508.21066.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuan Gong, Xionghui Wang, Jie Wu, Shiyin Wang, Yitong Wang, Xinglong Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.21066">OneReward: Unified Mask-Guided Image Generation via Multi-Task Human Preference Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we introduce OneReward, a unified reinforcement learning framework that enhances the model's generative capabilities across multiple tasks under different evaluation criteria using only \textit{One Reward} model. By employing a single vision-language model (VLM) as the generative reward model, which can distinguish the winner and loser for a given task and a given evaluation criterion, it can be effectively applied to multi-task generation models, particularly in contexts with varied data and diverse task objectives. We utilize OneReward for mask-guided image generation, which can be further divided into several sub-tasks such as image fill, image extend, object removal, and text rendering, involving a binary mask as the edit area. Although these domain-specific tasks share same conditioning paradigm, they differ significantly in underlying data distributions and evaluation metrics. Existing methods often rely on task-specific supervised fine-tuning (SFT), which limits generalization and training efficiency. Building on OneReward, we develop Seedream 3.0 Fill, a mask-guided generation model trained via multi-task reinforcement learning directly on a pre-trained base model, eliminating the need for task-specific SFT. Experimental results demonstrate that our unified edit model consistently outperforms both commercial and open-source competitors, such as Ideogram, Adobe Photoshop, and FLUX Fill [Pro], across multiple evaluation dimensions. Code and model are available at: https://one-reward.github.io
<div id='section'>Paperid: <span id='pid'>222, <a href='https://arxiv.org/pdf/2508.10340.pdf' target='_blank'>https://arxiv.org/pdf/2508.10340.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chak Lam Shek, Guangyao Shi, Pratap Tokekar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.10340">Multi-Agent Trust Region Policy Optimisation: A Joint Constraint Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent reinforcement learning (MARL) requires coordinated and stable policy updates among interacting agents. Heterogeneous-Agent Trust Region Policy Optimization (HATRPO) enforces per-agent trust region constraints using Kullback-Leibler (KL) divergence to stabilize training. However, assigning each agent the same KL threshold can lead to slow and locally optimal updates, especially in heterogeneous settings. To address this limitation, we propose two approaches for allocating the KL divergence threshold across agents: HATRPO-W, a Karush-Kuhn-Tucker-based (KKT-based) method that optimizes threshold assignment under global KL constraints, and HATRPO-G, a greedy algorithm that prioritizes agents based on improvement-to-divergence ratio. By connecting sequential policy optimization with constrained threshold scheduling, our approach enables more flexible and effective learning in heterogeneous-agent settings. Experimental results demonstrate that our methods significantly boost the performance of HATRPO, achieving faster convergence and higher final rewards across diverse MARL benchmarks. Specifically, HATRPO-W and HATRPO-G achieve comparable improvements in final performance, each exceeding 22.5%. Notably, HATRPO-W also demonstrates more stable learning dynamics, as reflected by its lower variance.
<div id='section'>Paperid: <span id='pid'>223, <a href='https://arxiv.org/pdf/2507.10142.pdf' target='_blank'>https://arxiv.org/pdf/2507.10142.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Siyi Hu, Mohamad A Hady, Jianglin Qiao, Jimmy Cao, Mahardhika Pratama, Ryszard Kowalczyk
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.10142">Adaptability in Multi-Agent Reinforcement Learning: A Framework and Unified Review</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-Agent Reinforcement Learning (MARL) has shown clear effectiveness in coordinating multiple agents across simulated benchmarks and constrained scenarios. However, its deployment in real-world multi-agent systems (MAS) remains limited, primarily due to the complex and dynamic nature of such environments. These challenges arise from multiple interacting sources of variability, including fluctuating agent populations, evolving task goals, and inconsistent execution conditions. Together, these factors demand that MARL algorithms remain effective under continuously changing system configurations and operational demands. To better capture and assess this capacity for adjustment, we introduce the concept of \textit{adaptability} as a unified and practically grounded lens through which to evaluate the reliability of MARL algorithms under shifting conditions, broadly referring to any changes in the environment dynamics that may occur during learning or execution. Centred on the notion of adaptability, we propose a structured framework comprising three key dimensions: learning adaptability, policy adaptability, and scenario-driven adaptability. By adopting this adaptability perspective, we aim to support more principled assessments of MARL performance beyond narrowly defined benchmarks. Ultimately, this survey contributes to the development of algorithms that are better suited for deployment in dynamic, real-world multi-agent systems.
<div id='section'>Paperid: <span id='pid'>224, <a href='https://arxiv.org/pdf/2506.15207.pdf' target='_blank'>https://arxiv.org/pdf/2506.15207.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mohamad A. Hady, Siyi Hu, Mahardhika Pratama, Jimmy Cao, Ryszard Kowalczyk
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.15207">Multi-Agent Reinforcement Learning for Autonomous Multi-Satellite Earth Observation: A Realistic Case Study</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The exponential growth of Low Earth Orbit (LEO) satellites has revolutionised Earth Observation (EO) missions, addressing challenges in climate monitoring, disaster management, and more. However, autonomous coordination in multi-satellite systems remains a fundamental challenge. Traditional optimisation approaches struggle to handle the real-time decision-making demands of dynamic EO missions, necessitating the use of Reinforcement Learning (RL) and Multi-Agent Reinforcement Learning (MARL). In this paper, we investigate RL-based autonomous EO mission planning by modelling single-satellite operations and extending to multi-satellite constellations using MARL frameworks. We address key challenges, including energy and data storage limitations, uncertainties in satellite observations, and the complexities of decentralised coordination under partial observability. By leveraging a near-realistic satellite simulation environment, we evaluate the training stability and performance of state-of-the-art MARL algorithms, including PPO, IPPO, MAPPO, and HAPPO. Our results demonstrate that MARL can effectively balance imaging and resource management while addressing non-stationarity and reward interdependency in multi-satellite coordination. The insights gained from this study provide a foundation for autonomous satellite operations, offering practical guidelines for improving policy learning in decentralised EO missions.
<div id='section'>Paperid: <span id='pid'>225, <a href='https://arxiv.org/pdf/2506.12600.pdf' target='_blank'>https://arxiv.org/pdf/2506.12600.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jie Pan, Tianyi Wang, Christian Claudel, Jing Shi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.12600">Trust-MARL: Trust-Based Multi-Agent Reinforcement Learning Framework for Cooperative On-Ramp Merging Control in Heterogeneous Traffic Flow</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Intelligent transportation systems require connected and automated vehicles (CAVs) to conduct safe and efficient cooperation with human-driven vehicles (HVs) in complex real-world traffic environments. However, the inherent unpredictability of human behaviour, especially at bottlenecks such as highway on-ramp merging areas, often disrupts traffic flow and compromises system performance. To address the challenge of cooperative on-ramp merging in heterogeneous traffic environments, this study proposes a trust-based multi-agent reinforcement learning (Trust-MARL) framework. At the macro level, Trust-MARL enhances global traffic efficiency by leveraging inter-agent trust to improve bottleneck throughput and mitigate traffic shockwave through emergent group-level coordination. At the micro level, a dynamic trust mechanism is designed to enable CAVs to adjust their cooperative strategies in response to real-time behaviors and historical interactions with both HVs and other CAVs. Furthermore, a trust-triggered game-theoretic decision-making module is integrated to guide each CAV in adapting its cooperation factor and executing context-aware lane-changing decisions under safety, comfort, and efficiency constraints. An extensive set of ablation studies and comparative experiments validates the effectiveness of the proposed Trust-MARL approach, demonstrating significant improvements in safety, efficiency, comfort, and adaptability across varying CAV penetration rates and traffic densities.
<div id='section'>Paperid: <span id='pid'>226, <a href='https://arxiv.org/pdf/2505.04231.pdf' target='_blank'>https://arxiv.org/pdf/2505.04231.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Taoyuan Yu, Kui Wang, Zongdian Li, Tao Yu, Kei Sakaguchi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.04231">Multi-Agent Reinforcement Learning-based Cooperative Autonomous Driving in Smart Intersections</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Unsignalized intersections pose significant safety and efficiency challenges due to complex traffic flows. This paper proposes a novel roadside unit (RSU)-centric cooperative driving system leveraging global perception and vehicle-to-infrastructure (V2I) communication. The core of the system is an RSU-based decision-making module using a two-stage hybrid reinforcement learning (RL) framework. At first, policies are pre-trained offline using conservative Q-learning (CQL) combined with behavior cloning (BC) on collected dataset. Subsequently, these policies are fine-tuned in the simulation using multi-agent proximal policy optimization (MAPPO), aligned with a self-attention mechanism to effectively solve inter-agent dependencies. RSUs perform real-time inference based on the trained models to realize vehicle control via V2I communications. Extensive experiments in CARLA environment demonstrate high effectiveness of the proposed system, by: \textit{(i)} achieving failure rates below 0.03\% in coordinating three connected and autonomous vehicles (CAVs) through complex intersection scenarios, significantly outperforming the traditional Autoware control method, and \textit{(ii)} exhibiting strong robustness across varying numbers of controlled agents and shows promising generalization capabilities on other maps.
<div id='section'>Paperid: <span id='pid'>227, <a href='https://arxiv.org/pdf/2504.21048.pdf' target='_blank'>https://arxiv.org/pdf/2504.21048.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mohamad A. Hady, Siyi Hu, Mahardhika Pratama, Jimmy Cao, Ryszard Kowalczyk
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.21048">Multi-Agent Reinforcement Learning for Resources Allocation Optimization: A Survey</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-Agent Reinforcement Learning (MARL) has become a powerful framework for numerous real-world applications, modeling distributed decision-making and learning from interactions with complex environments. Resource Allocation Optimization (RAO) benefits significantly from MARL's ability to tackle dynamic and decentralized contexts. MARL-based approaches are increasingly applied to RAO challenges across sectors playing pivotal roles to Industry 4.0 developments. This survey provides a comprehensive review of recent MARL algorithms for RAO, encompassing core concepts, classifications, and a structured taxonomy. By outlining the current research landscape and identifying primary challenges and future directions, this survey aims to support researchers and practitioners in leveraging MARL's potential to advance resource allocation solutions.
<div id='section'>Paperid: <span id='pid'>228, <a href='https://arxiv.org/pdf/2409.02645.pdf' target='_blank'>https://arxiv.org/pdf/2409.02645.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jannik Peters, Constantin Waubert de Puiseau, Hasan Tercan, Arya Gopikrishnan, Gustavo Adolpho Lucas De Carvalho, Christian Bitter, Tobias Meisen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.02645">Emergent Language: A Survey and Taxonomy</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The field of emergent language represents a novel area of research within the domain of artificial intelligence, particularly within the context of multi-agent reinforcement learning. Although the concept of studying language emergence is not new, early approaches were primarily concerned with explaining human language formation, with little consideration given to its potential utility for artificial agents. In contrast, studies based on reinforcement learning aim to develop communicative capabilities in agents that are comparable to or even superior to human language. Thus, they extend beyond the learned statistical representations that are common in natural language processing research. This gives rise to a number of fundamental questions, from the prerequisites for language emergence to the criteria for measuring its success. This paper addresses these questions by providing a comprehensive review of 181 scientific publications on emergent language in artificial intelligence. Its objective is to serve as a reference for researchers interested in or proficient in the field. Consequently, the main contributions are the definition and overview of the prevailing terminology, the analysis of existing evaluation methods and metrics, and the description of the identified research gaps.
<div id='section'>Paperid: <span id='pid'>229, <a href='https://arxiv.org/pdf/2601.12662.pdf' target='_blank'>https://arxiv.org/pdf/2601.12662.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xingran Chen, Navid NaderiAlizadeh, Alejandro Ribeiro, Shirin Saeedi Bidokhti
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.12662">Decentralized Learning Strategies for Estimation Error Minimization with Graph Neural Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We address real-time sampling and estimation of autoregressive Markovian sources in dynamic yet structurally similar multi-hop wireless networks. Each node caches samples from others and communicates over wireless collision channels, aiming to minimize time-average estimation error via decentralized policies. Due to the high dimensionality of action spaces and complexity of network topologies, deriving optimal policies analytically is intractable. To address this, we propose a graphical multi-agent reinforcement learning framework for policy optimization. Theoretically, we demonstrate that our proposed policies are transferable, allowing a policy trained on one graph to be effectively applied to structurally similar graphs. Numerical experiments demonstrate that (i) our proposed policy outperforms state-of-the-art baselines; (ii) the trained policies are transferable to larger networks, with performance gains increasing with the number of agents; (iii) the graphical training procedure withstands non-stationarity, even when using independent learning techniques; and (iv) recurrence is pivotal in both independent learning and centralized training and decentralized execution, and improves the resilience to non-stationarity.
<div id='section'>Paperid: <span id='pid'>230, <a href='https://arxiv.org/pdf/2601.00321.pdf' target='_blank'>https://arxiv.org/pdf/2601.00321.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Eslam Eldeeb, Hirley Alves
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.00321">Offline Multi-Agent Reinforcement Learning for 6G Communications: Fundamentals, Applications and Future Directions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The next-generation wireless technologies, including beyond 5G and 6G networks, are paving the way for transformative applications such as vehicle platooning, smart cities, and remote surgery. These innovations are driven by a vast array of interconnected wireless entities, including IoT devices, access points, UAVs, and CAVs, which increase network complexity and demand more advanced decision-making algorithms. Artificial intelligence (AI) and machine learning (ML), especially reinforcement learning (RL), are key enablers for such networks, providing solutions to high-dimensional and complex challenges. However, as networks expand to multi-agent environments, traditional online RL approaches face cost, safety, and scalability limitations. Offline multi-agent reinforcement learning (MARL) offers a promising solution by utilizing pre-collected data, reducing the need for real-time interaction. This article introduces a novel offline MARL algorithm based on conservative Q-learning (CQL), ensuring safe and efficient training. We extend this with meta-learning to address dynamic environments and validate the approach through use cases in radio resource management and UAV networks. Our work highlights offline MARL's advantages, limitations, and future directions in wireless applications.
<div id='section'>Paperid: <span id='pid'>231, <a href='https://arxiv.org/pdf/2509.20338.pdf' target='_blank'>https://arxiv.org/pdf/2509.20338.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Umer Siddique, Abhinav Sinha, Yongcan Cao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.20338">Adaptive Event-Triggered Policy Gradient for Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Conventional multi-agent reinforcement learning (MARL) methods rely on time-triggered execution, where agents sample and communicate actions at fixed intervals. This approach is often computationally expensive and communication-intensive. To address this limitation, we propose ET-MAPG (Event-Triggered Multi-Agent Policy Gradient reinforcement learning), a framework that jointly learns an agent's control policy and its event-triggering policy. Unlike prior work that decouples these mechanisms, ET-MAPG integrates them into a unified learning process, enabling agents to learn not only what action to take but also when to execute it. For scenarios with inter-agent communication, we introduce AET-MAPG, an attention-based variant that leverages a self-attention mechanism to learn selective communication patterns. AET-MAPG empowers agents to determine not only when to trigger an action but also with whom to communicate and what information to exchange, thereby optimizing coordination. Both methods can be integrated with any policy gradient MARL algorithm. Extensive experiments across diverse MARL benchmarks demonstrate that our approaches achieve performance comparable to state-of-the-art, time-triggered baselines while significantly reducing both computational load and communication overhead.
<div id='section'>Paperid: <span id='pid'>232, <a href='https://arxiv.org/pdf/2507.16941.pdf' target='_blank'>https://arxiv.org/pdf/2507.16941.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Daniel Correa, Tero Kaarlela, Jose Fuentes, Paulo Padrao, Alain Duran, Leonardo Bobadilla
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.16941">Multi-agent Reinforcement Learning for Robotized Coral Reef Sample Collection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a reinforcement learning (RL) environment for developing an autonomous underwater robotic coral sampling agent, a crucial coral reef conservation and research task. Using software-in-the-loop (SIL) and hardware-in-the-loop (HIL), an RL-trained artificial intelligence (AI) controller is developed using a digital twin (DT) in simulation and subsequently verified in physical experiments. An underwater motion capture (MOCAP) system provides real-time 3D position and orientation feedback during verification testing for precise synchronization between the digital and physical domains. A key novelty of this approach is the combined use of a general-purpose game engine for simulation, deep RL, and real-time underwater motion capture for an effective zero-shot sim-to-real strategy.
<div id='section'>Paperid: <span id='pid'>233, <a href='https://arxiv.org/pdf/2505.24618.pdf' target='_blank'>https://arxiv.org/pdf/2505.24618.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Victor Casamayor Pujol, Boris Sedlak, Tommaso Salvatori, Karl Friston, Schahram Dustdar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.24618">Distributed Intelligence in the Computing Continuum with Active Inference</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The Computing Continuum (CC) is an emerging Internet-based computing paradigm that spans from local Internet of Things sensors and constrained edge devices to large-scale cloud data centers. Its goal is to orchestrate a vast array of diverse and distributed computing resources to support the next generation of Internet-based applications. However, the distributed, heterogeneous, and dynamic nature of CC platforms demands distributed intelligence for adaptive and resilient service management. This article introduces a distributed stream processing pipeline as a CC use case, where each service is managed by an Active Inference (AIF) agent. These agents collaborate to fulfill service needs specified by SLOiDs, a term we introduce to denote Service Level Objectives that are aware of its deployed devices, meaning that non-functional requirements must consider the characteristics of the hosting device. We demonstrate how AIF agents can be modeled and deployed alongside distributed services to manage them autonomously. Our experiments show that AIF agents achieve over 90% SLOiD fulfillment when using tested transition models, and around 80% when learning the models during deployment. We compare their performance to a multi-agent reinforcement learning algorithm, finding that while both approaches yield similar results, MARL requires extensive training, whereas AIF agents can operate effectively from the start. Additionally, we evaluate the behavior of AIF agents in offloading scenarios, observing a strong capacity for adaptation. Finally, we outline key research directions to advance AIF integration in CC platforms.
<div id='section'>Paperid: <span id='pid'>234, <a href='https://arxiv.org/pdf/2505.24265.pdf' target='_blank'>https://arxiv.org/pdf/2505.24265.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Harsh Goel, Mohammad Omama, Behdad Chalaki, Vaishnav Tadiparthi, Ehsan Moradi Pari, Sandeep Chinchali
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.24265">R3DM: Enabling Role Discovery and Diversity Through Dynamics Models in Multi-agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent reinforcement learning (MARL) has achieved significant progress in large-scale traffic control, autonomous vehicles, and robotics. Drawing inspiration from biological systems where roles naturally emerge to enable coordination, role-based MARL methods have been proposed to enhance cooperation learning for complex tasks. However, existing methods exclusively derive roles from an agent's past experience during training, neglecting their influence on its future trajectories. This paper introduces a key insight: an agent's role should shape its future behavior to enable effective coordination. Hence, we propose Role Discovery and Diversity through Dynamics Models (R3DM), a novel role-based MARL framework that learns emergent roles by maximizing the mutual information between agents' roles, observed trajectories, and expected future behaviors. R3DM optimizes the proposed objective through contrastive learning on past trajectories to first derive intermediate roles that shape intrinsic rewards to promote diversity in future behaviors across different roles through a learned dynamics model. Benchmarking on SMAC and SMACv2 environments demonstrates that R3DM outperforms state-of-the-art MARL approaches, improving multi-agent coordination to increase win rates by up to 20%.
<div id='section'>Paperid: <span id='pid'>235, <a href='https://arxiv.org/pdf/2505.03771.pdf' target='_blank'>https://arxiv.org/pdf/2505.03771.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ritik Raj, Akshat Ramachandran, Jeff Nye, Shashank Nemawarkar, Tushar Krishna
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.03771">OneDSE: A Unified Microprocessor Metric Prediction and Design Space Exploration Framework</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the slowing of Moores Law and increasing impact of power constraints, processor designs rely on architectural innovation to achieve differentiating performance. However, the innovation complexity has simultaneously increased the design space of modern high performance processors. Specifically, we identify two key challenges in prior Design Space Exploration (DSE) approaches for modern CPU design - (a) cost model (prediction method) is either slow or microarchitecture-specific or workload-specific and single model is inefficient to learn the whole design space (b) optimization (exploration method) is slow and inaccurate in the large CPU parameter space. This work presents a novel solution called OneDSE to address these emerging challenges in modern CPU design. OneDSE is a unified cost model (metric predictor) and optimizer (CPU parameter explorer) with three key techniques - 1. Transformer-based workload-Aware CPU Estimation (TrACE) framework to predict metrics in the parameter space (TrACE-p) and parameters in the in the metric space (TrACE-m). TrACE-p outperforms State of The Art (SOTA) IPC prediction methods by 5.71x and 28x for single and multiple workloads respectively while being two orders of magnitude faster. 2. We also propose a novel Metric spAce Search opTimizer (MAST) that leverages TrACE-m and outperforms SoTA metaheuristics by 1.19x while being an order of magnitude faster. 3. We propose Subsystem-based Multi-Agent Reinforcement-learning based fine-Tuning (SMART)-TrACE that achieves a 10.6% reduction in prediction error compared to TrACE, enabling more accurate and efficient exploration of the CPU design space.
<div id='section'>Paperid: <span id='pid'>236, <a href='https://arxiv.org/pdf/2501.16098.pdf' target='_blank'>https://arxiv.org/pdf/2501.16098.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Eslam Eldeeb, Hirley Alves
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.16098">Multi-Agent Meta-Offline Reinforcement Learning for Timely UAV Path Planning and Data Collection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent reinforcement learning (MARL) has been widely adopted in high-performance computing and complex data-driven decision-making in the wireless domain. However, conventional MARL schemes face many obstacles in real-world scenarios. First, most MARL algorithms are online, which might be unsafe and impractical. Second, MARL algorithms are environment-specific, meaning network configuration changes require model retraining. This letter proposes a novel meta-offline MARL algorithm that combines conservative Q-learning (CQL) and model agnostic meta-learning (MAML). CQL enables offline training by leveraging pre-collected datasets, while MAML ensures scalability and adaptability to dynamic network configurations and objectives. We propose two algorithm variants: independent training (M-I-MARL) and centralized training decentralized execution (M-CTDE-MARL). Simulation results show that the proposed algorithm outperforms conventional schemes, especially the CTDE approach that achieves 50 % faster convergence in dynamic scenarios than the benchmarks. The proposed framework enhances scalability, robustness, and adaptability in wireless communication systems by optimizing UAV trajectories and scheduling policies.
<div id='section'>Paperid: <span id='pid'>237, <a href='https://arxiv.org/pdf/2501.12991.pdf' target='_blank'>https://arxiv.org/pdf/2501.12991.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Eslam Eldeeb, Hirley Alves
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.12991">An Offline Multi-Agent Reinforcement Learning Framework for Radio Resource Management</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Offline multi-agent reinforcement learning (MARL) addresses key limitations of online MARL, such as safety concerns, expensive data collection, extended training intervals, and high signaling overhead caused by online interactions with the environment. In this work, we propose an offline MARL algorithm for radio resource management (RRM), focusing on optimizing scheduling policies for multiple access points (APs) to jointly maximize the sum and tail rates of user equipment (UEs). We evaluate three training paradigms: centralized, independent, and centralized training with decentralized execution (CTDE). Our simulation results demonstrate that the proposed offline MARL framework outperforms conventional baseline approaches, achieving over a 15\% improvement in a weighted combination of sum and tail rates. Additionally, the CTDE framework strikes an effective balance, reducing the computational complexity of centralized methods while addressing the inefficiencies of independent training. These results underscore the potential of offline MARL to deliver scalable, robust, and efficient solutions for resource management in dynamic wireless networks.
<div id='section'>Paperid: <span id='pid'>238, <a href='https://arxiv.org/pdf/2501.01140.pdf' target='_blank'>https://arxiv.org/pdf/2501.01140.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Min Whoo Lee, Kibeom Kim, Soo Wung Shin, Minsu Lee, Byoung-Tak Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.01140">Communicating Unexpectedness for Out-of-Distribution Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Applying multi-agent reinforcement learning methods to realistic settings is challenging as it may require the agents to quickly adapt to unexpected situations that are rarely or never encountered in training. Recent methods for generalization to such out-of-distribution settings are limited to more specific, restricted instances of distribution shifts. To tackle adaptation to distribution shifts, we propose Unexpected Encoding Scheme, a novel decentralized multi-agent reinforcement learning algorithm where agents communicate "unexpectedness," the aspects of the environment that are surprising. In addition to a message yielded by the original reward-driven communication, each agent predicts the next observation based on previous experience, measures the discrepancy between the prediction and the actually encountered observation, and encodes this discrepancy as a message. Experiments on multi-robot warehouse environment support that our proposed method adapts robustly to dynamically changing training environments as well as out-of-distribution environment.
<div id='section'>Paperid: <span id='pid'>239, <a href='https://arxiv.org/pdf/2410.16600.pdf' target='_blank'>https://arxiv.org/pdf/2410.16600.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ian Gemp, Andreas Haupt, Luke Marris, Siqi Liu, Georgios Piliouras
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.16600">Convex Markov Games: A New Frontier for Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Behavioral diversity, expert imitation, fairness, safety goals and others give rise to preferences in sequential decision making domains that do not decompose additively across time. We introduce the class of convex Markov games that allow general convex preferences over occupancy measures. Despite infinite time horizon and strictly higher generality than Markov games, pure strategy Nash equilibria exist. Furthermore, equilibria can be approximated empirically by performing gradient descent on an upper bound of exploitability. Our experiments reveal novel solutions to classic repeated normal-form games, find fair solutions in a repeated asymmetric coordination game, and prioritize safe long-term behavior in a robot warehouse environment. In the prisoner's dilemma, our algorithm leverages transient imitation to find a policy profile that deviates from observed human play only slightly, yet achieves higher per-player utility while also being three orders of magnitude less exploitable.
<div id='section'>Paperid: <span id='pid'>240, <a href='https://arxiv.org/pdf/2510.27289.pdf' target='_blank'>https://arxiv.org/pdf/2510.27289.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhengchang Hua, Panagiotis Oikonomou, Karim Djemame, Nikos Tziritas, Georgios Theodoropoulos
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.27289">A Digital Twin-based Multi-Agent Reinforcement Learning Framework for Vehicle-to-Grid Coordination</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The coordination of large-scale, decentralised systems, such as a fleet of Electric Vehicles (EVs) in a Vehicle-to-Grid (V2G) network, presents a significant challenge for modern control systems. While collaborative Digital Twins have been proposed as a solution to manage such systems without compromising the privacy of individual agents, deriving globally optimal control policies from the high-level information they share remains an open problem. This paper introduces Digital Twin Assisted Multi-Agent Deep Deterministic Policy Gradient (DT-MADDPG) algorithm, a novel hybrid architecture that integrates a multi-agent reinforcement learning framework with a collaborative DT network. Our core contribution is a simulation-assisted learning algorithm where the centralised critic is enhanced by a predictive global model that is collaboratively built from the privacy-preserving data shared by individual DTs. This approach removes the need for collecting sensitive raw data at a centralised entity, a requirement of traditional multi-agent learning algorithms. Experimental results in a simulated V2G environment demonstrate that DT-MADDPG can achieve coordination performance comparable to the standard MADDPG algorithm while offering significant advantages in terms of data privacy and architectural decentralisation. This work presents a practical and robust framework for deploying intelligent, learning-based coordination in complex, real-world cyber-physical systems.
<div id='section'>Paperid: <span id='pid'>241, <a href='https://arxiv.org/pdf/2509.18088.pdf' target='_blank'>https://arxiv.org/pdf/2509.18088.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chuhao Qin, Evangelos Pournaras
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.18088">Strategic Coordination for Evolving Multi-agent Systems: A Hierarchical Reinforcement and Collective Learning Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Decentralized combinatorial optimization in evolving multi-agent systems poses significant challenges, requiring agents to balance long-term decision-making, short-term optimized collective outcomes, while preserving autonomy of interactive agents under unanticipated changes. Reinforcement learning offers a way to model sequential decision-making through dynamic programming to anticipate future environmental changes. However, applying multi-agent reinforcement learning (MARL) to decentralized combinatorial optimization problems remains an open challenge due to the exponential growth of the joint state-action space, high communication overhead, and privacy concerns in centralized training. To address these limitations, this paper proposes Hierarchical Reinforcement and Collective Learning (HRCL), a novel approach that leverages both MARL and decentralized collective learning based on a hierarchical framework. Agents take high-level strategies using MARL to group possible plans for action space reduction and constrain the agent behavior for Pareto optimality. Meanwhile, the low-level collective learning layer ensures efficient and decentralized coordinated decisions among agents with minimal communication. Extensive experiments in a synthetic scenario and real-world smart city application models, including energy self-management and drone swarm sensing, demonstrate that HRCL significantly improves performance, scalability, and adaptability compared to the standalone MARL and collective learning approaches, achieving a win-win synthesis solution.
<div id='section'>Paperid: <span id='pid'>242, <a href='https://arxiv.org/pdf/2509.14276.pdf' target='_blank'>https://arxiv.org/pdf/2509.14276.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuxiang Mai, Qiyue Yin, Wancheng Ni, Pei Xu, Kaiqi Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.14276">Constructive Conflict-Driven Multi-Agent Reinforcement Learning for Strategic Diversity</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, diversity has emerged as a useful mechanism to enhance the efficiency of multi-agent reinforcement learning (MARL). However, existing methods predominantly focus on designing policies based on individual agent characteristics, often neglecting the interplay and mutual influence among agents during policy formation. To address this gap, we propose Competitive Diversity through Constructive Conflict (CoDiCon), a novel approach that incorporates competitive incentives into cooperative scenarios to encourage policy exchange and foster strategic diversity among agents. Drawing inspiration from sociological research, which highlights the benefits of moderate competition and constructive conflict in group decision-making, we design an intrinsic reward mechanism using ranking features to introduce competitive motivations. A centralized intrinsic reward module generates and distributes varying reward values to agents, ensuring an effective balance between competition and cooperation. By optimizing the parameterized centralized reward module to maximize environmental rewards, we reformulate the constrained bilevel optimization problem to align with the original task objectives. We evaluate our algorithm against state-of-the-art methods in the SMAC and GRF environments. Experimental results demonstrate that CoDiCon achieves superior performance, with competitive intrinsic rewards effectively promoting diverse and adaptive strategies among cooperative agents.
<div id='section'>Paperid: <span id='pid'>243, <a href='https://arxiv.org/pdf/2507.16306.pdf' target='_blank'>https://arxiv.org/pdf/2507.16306.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xingjian Zhang, Yizhuo Wang, Guillaume Sartoretti
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.16306">COMPASS: Cooperative Multi-Agent Persistent Monitoring using Spatio-Temporal Attention Network</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Persistent monitoring of dynamic targets is essential in real-world applications such as disaster response, environmental sensing, and wildlife conservation, where mobile agents must continuously gather information under uncertainty. We propose COMPASS, a multi-agent reinforcement learning (MARL) framework that enables decentralized agents to persistently monitor multiple moving targets efficiently. We model the environment as a graph, where nodes represent spatial locations and edges capture topological proximity, allowing agents to reason over structured layouts and revisit informative regions as needed. Each agent independently selects actions based on a shared spatio-temporal attention network that we design to integrate historical observations and spatial context. We model target dynamics using Gaussian Processes (GPs), which support principled belief updates and enable uncertainty-aware planning. We train COMPASS using centralized value estimation and decentralized policy execution under an adaptive reward setting. Our extensive experiments demonstrate that COMPASS consistently outperforms strong baselines in uncertainty reduction, target coverage, and coordination efficiency across dynamic multi-target scenarios.
<div id='section'>Paperid: <span id='pid'>244, <a href='https://arxiv.org/pdf/2507.09495.pdf' target='_blank'>https://arxiv.org/pdf/2507.09495.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hang Wang, Junshan Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.09495">GenAI-based Multi-Agent Reinforcement Learning towards Distributed Agent Intelligence: A Generative-RL Agent Perspective</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent reinforcement learning faces fundamental challenges that conventional approaches have failed to overcome: exponentially growing joint action spaces, non-stationary environments where simultaneous learning creates moving targets, and partial observability that constrains coordination. Current methods remain reactive, employing stimulus-response mechanisms that fail when facing novel scenarios. We argue for a transformative paradigm shift from reactive to proactive multi-agent intelligence through generative AI-based reinforcement learning. This position advocates reconceptualizing agents not as isolated policy optimizers, but as sophisticated generative models capable of synthesizing complex multi-agent dynamics and making anticipatory decisions based on predictive understanding of future interactions. Rather than responding to immediate observations, generative-RL agents can model environment evolution, predict other agents' behaviors, generate coordinated action sequences, and engage in strategic reasoning accounting for long-term dynamics. This approach leverages pattern recognition and generation capabilities of generative AI to enable proactive decision-making, seamless coordination through enhanced communication, and dynamic adaptation to evolving scenarios. We envision this paradigm shift will unlock unprecedented possibilities for distributed intelligence, moving beyond individual optimization toward emergent collective behaviors representing genuine collaborative intelligence. The implications extend across autonomous systems, robotics, and human-AI collaboration, promising solutions to coordination challenges intractable under traditional reactive frameworks.
<div id='section'>Paperid: <span id='pid'>245, <a href='https://arxiv.org/pdf/2506.08149.pdf' target='_blank'>https://arxiv.org/pdf/2506.08149.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hang Wang, Dechen Gao, Junshan Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.08149">Ego-centric Learning of Communicative World Models for Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We study multi-agent reinforcement learning (MARL) for tasks in complex high-dimensional environments, such as autonomous driving. MARL is known to suffer from the \textit{partial observability} and \textit{non-stationarity} issues. To tackle these challenges, information sharing is often employed, which however faces major hurdles in practice, including overwhelming communication overhead and scalability concerns. By making use of generative AI embodied in world model together with its latent representation, we develop {\it CALL}, \underline{C}ommunic\underline{a}tive Wor\underline{l}d Mode\underline{l}, for MARL, where 1) each agent first learns its world model that encodes its state and intention into low-dimensional latent representation with smaller memory footprint, which can be shared with other agents of interest via lightweight communication; and 2) each agent carries out ego-centric learning while exploiting lightweight information sharing to enrich her world model, and then exploits its generalization capacity to improve prediction for better planning. We characterize the gain on the prediction accuracy from the information sharing and its impact on performance gap. Extensive experiments are carried out on the challenging local trajectory planning tasks in the CARLA platform to demonstrate the performance gains of using \textit{CALL}.
<div id='section'>Paperid: <span id='pid'>246, <a href='https://arxiv.org/pdf/2505.06997.pdf' target='_blank'>https://arxiv.org/pdf/2505.06997.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenhao Lu, Zhengqiu Zhu, Yong Zhao, Yonglin Tian, Junjie Zeng, Jun Zhang, Zhong Liu, Fei-Yue Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.06997">A Multi-Agent Reinforcement Learning Approach for Cooperative Air-Ground-Human Crowdsensing in Emergency Rescue</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Mobile crowdsensing is evolving beyond traditional human-centric models by integrating heterogeneous entities like unmanned aerial vehicles (UAVs) and unmanned ground vehicles (UGVs). Optimizing task allocation among these diverse agents is critical, particularly in challenging emergency rescue scenarios characterized by complex environments, limited communication, and partial observability. This paper tackles the Heterogeneous-Entity Collaborative-Sensing Task Allocation (HECTA) problem specifically for emergency rescue, considering humans, UAVs, and UGVs. We introduce a novel ``Hard-Cooperative'' policy where UGVs prioritize recharging low-battery UAVs, alongside performing their sensing tasks. The primary objective is maximizing the task completion rate (TCR) under strict time constraints. We rigorously formulate this NP-hard problem as a decentralized partially observable Markov decision process (Dec-POMDP) to effectively handle sequential decision-making under uncertainty. To solve this, we propose HECTA4ER, a novel multi-agent reinforcement learning algorithm built upon a Centralized Training with Decentralized Execution architecture. HECTA4ER incorporates tailored designs, including specialized modules for complex feature extraction, utilization of action-observation history via hidden states, and a mixing network integrating global and local information, specifically addressing the challenges of partial observability. Furthermore, theoretical analysis confirms the algorithm's convergence properties. Extensive simulations demonstrate that HECTA4ER significantly outperforms baseline algorithms, achieving an average 18.42% increase in TCR. Crucially, a real-world case study validates the algorithm's effectiveness and robustness in dynamic sensing scenarios, highlighting its strong potential for practical application in emergency response.
<div id='section'>Paperid: <span id='pid'>247, <a href='https://arxiv.org/pdf/2503.13547.pdf' target='_blank'>https://arxiv.org/pdf/2503.13547.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xu Guo, Xiangwang Hou, Minrui Xu, Jianrui Chen, Jingjing Wang, Jun Du, Yong Ren
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.13547">Adaptive AUV Hunting Policy with Covert Communication via Diffusion Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Collaborative underwater target hunting, facilitated by multiple autonomous underwater vehicles (AUVs), plays a significant role in various domains, especially military missions. Existing research predominantly focuses on designing efficient and high-success-rate hunting policy, particularly addressing the target's evasion capabilities. However, in real-world scenarios, the target can not only adjust its evasion policy based on its observations and predictions but also possess eavesdropping capabilities. If communication among hunter AUVs, such as hunting policy exchanges, is intercepted by the target, it can adapt its escape policy accordingly, significantly reducing the success rate of the hunting mission. To address this challenge, we propose a covert communication-guaranteed collaborative target hunting framework, which ensures efficient hunting in complex underwater environments while defending against the target's eavesdropping. To the best of our knowledge, this is the first study to incorporate the confidentiality of inter-agent communication into the design of target hunting policy. Furthermore, given the complexity of coordinating multiple AUVs in dynamic and unpredictable environments, we propose an adaptive multi-agent diffusion policy (AMADP), which incorporates the strong generative ability of diffusion models into the multi-agent reinforcement learning (MARL) algorithm. Experimental results demonstrate that AMADP achieves faster convergence and higher hunting success rates while maintaining covertness constraints.
<div id='section'>Paperid: <span id='pid'>248, <a href='https://arxiv.org/pdf/2502.08365.pdf' target='_blank'>https://arxiv.org/pdf/2502.08365.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Riccardo Zamboni, Mirco Mutti, Marcello Restelli
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.08365">Towards Unsupervised Multi-Agent Reinforcement Learning via Task-Agnostic Exploration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In reinforcement learning, we typically refer to unsupervised pre-training when we aim to pre-train a policy without a priori access to the task specification, i.e. rewards, to be later employed for efficient learning of downstream tasks. In single-agent settings, the problem has been extensively studied and mostly understood. A popular approach, called task-agnostic exploration, casts the unsupervised objective as maximizing the entropy of the state distribution induced by the agent's policy, from which principles and methods follow.
  In contrast, little is known about it in multi-agent settings, which are ubiquitous in the real world. What are the pros and cons of alternative problem formulations in this setting? How hard is the problem in theory, how can we solve it in practice? In this paper, we address these questions by first characterizing those alternative formulations and highlighting how the problem, even when tractable in theory, is non-trivial in practice. Then, we present a scalable, decentralized, trust-region policy search algorithm to address the problem in practical settings. Finally, we provide numerical validations to both corroborate the theoretical findings and pave the way for unsupervised multi-agent reinforcement learning via task-agnostic exploration in challenging domains, showing that optimizing for a specific objective, namely mixture entropy, provides an excellent trade-off between tractability and performances.
<div id='section'>Paperid: <span id='pid'>249, <a href='https://arxiv.org/pdf/2501.17079.pdf' target='_blank'>https://arxiv.org/pdf/2501.17079.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Christian Fabian, Kai Cui, Heinz Koeppl
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.17079">Learning Mean Field Control on Sparse Graphs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large agent networks are abundant in applications and nature and pose difficult challenges in the field of multi-agent reinforcement learning (MARL) due to their computational and theoretical complexity. While graphon mean field games and their extensions provide efficient learning algorithms for dense and moderately sparse agent networks, the case of realistic sparser graphs remains largely unsolved. Thus, we propose a novel mean field control model inspired by local weak convergence to include sparse graphs such as power law networks with coefficients above two. Besides a theoretical analysis, we design scalable learning algorithms which apply to the challenging class of graph sequences with finite first moment. We compare our model and algorithms for various examples on synthetic and real world networks with mean field algorithms based on Lp graphons and graphexes. As it turns out, our approach outperforms existing methods in many examples and on various networks due to the special design aiming at an important, but so far hard to solve class of MARL problems.
<div id='section'>Paperid: <span id='pid'>250, <a href='https://arxiv.org/pdf/2501.10529.pdf' target='_blank'>https://arxiv.org/pdf/2501.10529.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sergio Rozada, Santiago Paternain, Juan Andres Bazerque, Antonio G. Marques
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.10529">A Tensor Low-Rank Approximation for Value Functions in Multi-Task Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In pursuit of reinforcement learning systems that could train in physical environments, we investigate multi-task approaches as a means to alleviate the need for massive data acquisition. In a tabular scenario where the Q-functions are collected across tasks, we model our learning problem as optimizing a higher order tensor structure. Recognizing that close-related tasks may require similar actions, our proposed method imposes a low-rank condition on this aggregated Q-tensor. The rationale behind this approach to multi-task learning is that the low-rank structure enforces the notion of similarity, without the need to explicitly prescribe which tasks are similar, but inferring this information from a reduced amount of data simultaneously with the stochastic optimization of the Q-tensor. The efficiency of our low-rank tensor approach to multi-task learning is demonstrated in two numerical experiments, first in a benchmark environment formed by a collection of inverted pendulums, and then into a practical scenario involving multiple wireless communication devices.
<div id='section'>Paperid: <span id='pid'>251, <a href='https://arxiv.org/pdf/2501.10116.pdf' target='_blank'>https://arxiv.org/pdf/2501.10116.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zifeng Shi, Meiqin Liu, Senlin Zhang, Ronghao Zheng, Shanling Dong, Ping Wei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.10116">GAWM: Global-Aware World Model for Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, Model-based Multi-Agent Reinforcement Learning (MARL) has demonstrated significant advantages over model-free methods in terms of sample efficiency by using independent environment dynamics world models for data sample augmentation. However, without considering the limited sample size, these methods still lag behind model-free methods in terms of final convergence performance and stability. This is primarily due to the world model's insufficient and unstable representation of global states in partially observable environments. This limitation hampers the ability to ensure global consistency in the data samples and results in a time-varying and unstable distribution mismatch between the pseudo data samples generated by the world model and the real samples. This issue becomes particularly pronounced in more complex multi-agent environments. To address this challenge, we propose a model-based MARL method called GAWM, which enhances the centralized world model's ability to achieve globally unified and accurate representation of state information while adhering to the CTDE paradigm. GAWM uniquely leverages an additional Transformer architecture to fuse local observation information from different agents, thereby improving its ability to extract and represent global state information. This enhancement not only improves sample efficiency but also enhances training stability, leading to superior convergence performance, particularly in complex and challenging multi-agent environments. This advancement enables model-based methods to be effectively applied to more complex multi-agent environments. Experimental results demonstrate that GAWM outperforms various model-free and model-based approaches, achieving exceptional performance in the challenging domains of SMAC.
<div id='section'>Paperid: <span id='pid'>252, <a href='https://arxiv.org/pdf/2411.19639.pdf' target='_blank'>https://arxiv.org/pdf/2411.19639.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zifeng Shi, Meiqin Liu, Senlin Zhang, Ronghao Zheng, Shanling Dong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.19639">RMIO: A Model-Based MARL Framework for Scenarios with Observation Loss in Some Agents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, model-based reinforcement learning (MBRL) has emerged as a solution to address sample complexity in multi-agent reinforcement learning (MARL) by modeling agent-environment dynamics to improve sample efficiency. However, most MBRL methods assume complete and continuous observations from each agent during the inference stage, which can be overly idealistic in practical applications. A novel model-based MARL approach called RMIO is introduced to address this limitation, specifically designed for scenarios where observation is lost in some agent. RMIO leverages the world model to reconstruct missing observations, and further reduces reconstruction errors through inter-agent information integration to ensure stable multi-agent decision-making. Secondly, unlike CTCE methods such as MAMBA, RMIO adopts the CTDE paradigm in standard environment, and enabling limited communication only when agents lack observation data, thereby reducing reliance on communication. Additionally, RMIO improves asymptotic performance through strategies such as reward smoothing, a dual-layer experience replay buffer, and an RNN-augmented policy model, surpassing previous work. Our experiments conducted in both the SMAC and MaMuJoCo environments demonstrate that RMIO outperforms current state-of-the-art approaches in terms of asymptotic convergence performance and policy robustness, both in standard mission settings and in scenarios involving observation loss.
<div id='section'>Paperid: <span id='pid'>253, <a href='https://arxiv.org/pdf/2411.10031.pdf' target='_blank'>https://arxiv.org/pdf/2411.10031.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jingyuan Zhou, Longhao Yan, Jinhao Liang, Kaidi Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.10031">Enforcing Cooperative Safety for Reinforcement Learning-based Mixed-Autonomy Platoon Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>It is recognized that the control of mixed-autonomy platoons comprising connected and automated vehicles (CAVs) and human-driven vehicles (HDVs) can enhance traffic flow. Among existing methods, Multi-Agent Reinforcement Learning (MARL) appears to be a promising control strategy because it can manage complex scenarios in real time. However, current research on MARL-based mixed-autonomy platoon control suffers from several limitations. First, existing MARL approaches address safety by penalizing safety violations in the reward function, thus lacking theoretical safety guarantees due to the black-box nature of RL. Second, few studies have explored the cooperative safety of multi-CAV platoons, where CAVs can be coordinated to further enhance the system-level safety involving the safety of both CAVs and HDVs. Third, existing work tends to make an unrealistic assumption that the behavior of HDVs and CAVs is publicly known and rationale. To bridge the research gaps, we propose a safe MARL framework for mixed-autonomy platoons. Specifically, this framework (i) characterizes cooperative safety by designing a cooperative Control Barrier Function (CBF), enabling CAVs to collaboratively improve the safety of the entire platoon, (ii) provides a safety guarantee to the MARL-based controller by integrating the CBF-based safety constraints into MARL through a differentiable quadratic programming (QP) layer, and (iii) incorporates a conformal prediction module that enables each CAV to estimate the unknown behaviors of the surrounding vehicles with uncertainty qualification. Simulation results show that our proposed control strategy can effectively enhance the system-level safety through CAV cooperation of a mixed-autonomy platoon with a minimal impact on control performance.
<div id='section'>Paperid: <span id='pid'>254, <a href='https://arxiv.org/pdf/2410.16024.pdf' target='_blank'>https://arxiv.org/pdf/2410.16024.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yue Deng, Weiyu Ma, Yuxin Fan, Ruyi Song, Yin Zhang, Haifeng Zhang, Jian Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.16024">SMAC-R1: The Emergence of Intelligence in Decision-Making Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>StarCraft Multi-Agent Challenge (SMAC) has been one of the most commonly used experimental environments in multi-agent reinforcement learning (MARL), where the specific task is to control a set number of allied units to defeat enemy forces. Traditional MARL algorithms often require interacting with the environment for millions of steps to train a parametric model, of which the resulting policies are typically non-interpretable with weak transferability. In this paper, we introduce SMAC-R1 which is based on the Qwen2.5-7B-Base LLM distilled from DeepSeek-Coder-v2.5-236B. Similar to online reinforcement learning after behavior cloning in offline learning process, in our pipeline, agents leverage the DeepSeek LLM to generate decision tree code by providing task descriptions, and the agents are further self-reflected using feedback from the rewards provided by the environment. Based on that, we augment the generated scripts to fine-tune a small LLM, Qwen2.5-7B-Base, to distill the decision-making ability via Supervised Fine-Tuning (SFT) and enhance the script generation ability by the Group Relative Policy Optimization (GRPO) algorithm. We conduct experiments in the original 23 SMAC tasks and 10 newly-designed tasks to demonstrate that our method can produce high-quality, interpretable decision trees with minimal environmental exploration. Moreover, these scripts exhibit strong transferability, successfully applying to homogeneous SMAC environments without modification. We believe this approach offers a new direction for solving decision-making tasks and domain-specific LLM training pipelines in the future.
<div id='section'>Paperid: <span id='pid'>255, <a href='https://arxiv.org/pdf/2410.15876.pdf' target='_blank'>https://arxiv.org/pdf/2410.15876.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Woosung Koh, Wonbeen Oh, Siyeol Kim, Suhin Shin, Hyeongjin Kim, Jaein Jang, Junghyun Lee, Se-Young Yun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.15876">FlickerFusion: Intra-trajectory Domain Generalizing Multi-Agent RL</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent reinforcement learning has demonstrated significant potential in addressing complex cooperative tasks across various real-world applications. However, existing MARL approaches often rely on the restrictive assumption that the number of entities (e.g., agents, obstacles) remains constant between training and inference. This overlooks scenarios where entities are dynamically removed or added during the inference trajectory -- a common occurrence in real-world environments like search and rescue missions and dynamic combat situations. In this paper, we tackle the challenge of intra-trajectory dynamic entity composition under zero-shot out-of-domain (OOD) generalization, where such dynamic changes cannot be anticipated beforehand. Our empirical studies reveal that existing MARL methods suffer significant performance degradation and increased uncertainty in these scenarios. In response, we propose FlickerFusion, a novel OOD generalization method that acts as a universally applicable augmentation technique for MARL backbone methods. FlickerFusion stochastically drops out parts of the observation space, emulating being in-domain when inferenced OOD. The results show that FlickerFusion not only achieves superior inference rewards but also uniquely reduces uncertainty vis-Ã -vis the backbone, compared to existing methods. Benchmarks, implementations, and model weights are organized and open-sourced at flickerfusion305.github.io, accompanied by ample demo video renderings.
<div id='section'>Paperid: <span id='pid'>256, <a href='https://arxiv.org/pdf/2409.11561.pdf' target='_blank'>https://arxiv.org/pdf/2409.11561.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Weizheng Wang, Aniket Bera, Byung-Cheol Min
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.11561">Hypergraph-based Coordinated Task Allocation and Socially-aware Navigation for Multi-Robot Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A team of multiple robots seamlessly and safely working in human-filled public environments requires adaptive task allocation and socially-aware navigation that account for dynamic human behavior. Current approaches struggle with highly dynamic pedestrian movement and the need for flexible task allocation. We propose Hyper-SAMARL, a hypergraph-based system for multi-robot task allocation and socially-aware navigation, leveraging multi-agent reinforcement learning (MARL). Hyper-SAMARL models the environmental dynamics between robots, humans, and points of interest (POIs) using a hypergraph, enabling adaptive task assignment and socially-compliant navigation through a hypergraph diffusion mechanism. Our framework, trained with MARL, effectively captures interactions between robots and humans, adapting tasks based on real-time changes in human activity. Experimental results demonstrate that Hyper-SAMARL outperforms baseline models in terms of social navigation, task completion efficiency, and adaptability in various simulated scenarios.
<div id='section'>Paperid: <span id='pid'>257, <a href='https://arxiv.org/pdf/2409.00717.pdf' target='_blank'>https://arxiv.org/pdf/2409.00717.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Natalia Zhang, Xinqi Wang, Qiwen Cui, Runlong Zhou, Sham M. Kakade, Simon S. Du
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.00717">Preference-Based Multi-Agent Reinforcement Learning: Data Coverage and Algorithmic Techniques</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We initiate the study of Preference-Based Multi-Agent Reinforcement Learning (PbMARL), exploring both theoretical foundations and empirical validations. We define the task as identifying the Nash equilibrium from a preference-only offline dataset in general-sum games, a problem marked by the challenge of sparse feedback signals. Our theory establishes the upper complexity bounds for Nash Equilibrium in effective PbMARL, demonstrating that single-policy coverage is inadequate and highlighting the importance of unilateral dataset coverage. These theoretical insights are verified through comprehensive experiments. To enhance the practical performance, we further introduce two algorithmic techniques. (1) We propose a Mean Squared Error (MSE) regularization along the time axis to achieve a more uniform reward distribution and improve reward learning outcomes. (2) We propose an additional penalty based on the distribution of the dataset to incorporate pessimism, improving stability and effectiveness during training. Our findings underscore the multifaceted approach required for PbMARL, paving the way for effective preference-based multi-agent systems.
<div id='section'>Paperid: <span id='pid'>258, <a href='https://arxiv.org/pdf/2511.18181.pdf' target='_blank'>https://arxiv.org/pdf/2511.18181.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Adam Callaghan, Karl Mason, Patrick Mannion
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.18181">MOMA-AC: A preference-driven actor-critic framework for continuous multi-objective multi-agent reinforcement learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper addresses a critical gap in Multi-Objective Multi-Agent Reinforcement Learning (MOMARL) by introducing the first dedicated inner-loop actor-critic framework for continuous state and action spaces: Multi-Objective Multi-Agent Actor-Critic (MOMA-AC). Building on single-objective, single-agent algorithms, we instantiate this framework with Twin Delayed Deep Deterministic Policy Gradient (TD3) and Deep Deterministic Policy Gradient (DDPG), yielding MOMA-TD3 and MOMA-DDPG. The framework combines a multi-headed actor network, a centralised critic, and an objective preference-conditioning architecture, enabling a single neural network to encode the Pareto front of optimal trade-off policies for all agents across conflicting objectives in a continuous MOMARL setting. We also outline a natural test suite for continuous MOMARL by combining a pre-existing multi-agent single-objective physics simulator with its multi-objective single-agent counterpart. Evaluating cooperative locomotion tasks in this suite, we show that our framework achieves statistically significant improvements in expected utility and hypervolume relative to outer-loop and independent training baselines, while demonstrating stable scalability as the number of agents increases. These results establish our framework as a foundational step towards robust, scalable multi-objective policy learning in continuous multi-agent domains.
<div id='section'>Paperid: <span id='pid'>259, <a href='https://arxiv.org/pdf/2511.12892.pdf' target='_blank'>https://arxiv.org/pdf/2511.12892.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Liangshun Wu, Wen Chen, Shunqing Zhang, Yajun Wang, Kunlun Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.12892">Green Emergency Communications in RIS- and MA-Assisted Multi-UAV SAGINs: A Partially Observable Reinforcement Learning Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In post-disaster space-air-ground integrated networks (SAGINs), terrestrial infrastructure is often impaired, and unmanned aerial vehicles (UAVs) must rapidly restore connectivity for mission-critical ground terminals in cluttered non-line-of-sight (NLoS) urban environments. To enhance coverage, UAVs employ movable antennas (MAs), while reconfigurable intelligent surfaces (RISs) on surviving high-rises redirect signals. The key challenge is communication-limited partial observability, leaving each UAV with a narrow, fast-changing neighborhood view that destabilizes value estimation. Existing multi-agent reinforcement learning (MARL) approaches are inadequate--non-communication methods rely on unavailable global critics, heuristic sharing is brittle and redundant, and learnable protocols (e.g., CommNet, DIAL) lose per-neighbor structure and aggravate non-stationarity under tight bandwidth. To address partial observability, we propose a spatiotemporal A2C where each UAV transmits prior-decision messages with local state, a compact policy fingerprint, and a recurrent belief, encoded per neighbor and concatenated. A spatial discount shapes value targets to emphasize local interactions, while analysis under one-hop-per-slot latency explains stable training with delayed views. Experimental results show our policy outperforms IA2C, ConseNet, FPrint, DIAL, and CommNet--achieving faster convergence, higher asymptotic reward, reduced Temporal-Difference(TD)/advantage errors, and a better communication throughput-energy trade-off.
<div id='section'>Paperid: <span id='pid'>260, <a href='https://arxiv.org/pdf/2510.10633.pdf' target='_blank'>https://arxiv.org/pdf/2510.10633.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiabao Shi, Minfeng Qi, Lefeng Zhang, Di Wang, Yingjie Zhao, Ziying Li, Yalong Xing, Ningran Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.10633">Collaborative Text-to-Image Generation via Multi-Agent Reinforcement Learning and Semantic Fusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multimodal text-to-image generation remains constrained by the difficulty of maintaining semantic alignment and professional-level detail across diverse visual domains. We propose a multi-agent reinforcement learning framework that coordinates domain-specialized agents (e.g., focused on architecture, portraiture, and landscape imagery) within two coupled subsystems: a text enhancement module and an image generation module, each augmented with multimodal integration components. Agents are trained using Proximal Policy Optimization (PPO) under a composite reward function that balances semantic similarity, linguistic visual quality, and content diversity. Cross-modal alignment is enforced through contrastive learning, bidirectional attention, and iterative feedback between text and image. Across six experimental settings, our system significantly enriches generated content (word count increased by 1614%) while reducing ROUGE-1 scores by 69.7%. Among fusion methods, Transformer-based strategies achieve the highest composite score (0.521), despite occasional stability issues. Multimodal ensembles yield moderate consistency (ranging from 0.444 to 0.481), reflecting the persistent challenges of cross-modal semantic grounding. These findings underscore the promise of collaborative, specialization-driven architectures for advancing reliable multimodal generative systems.
<div id='section'>Paperid: <span id='pid'>261, <a href='https://arxiv.org/pdf/2508.19488.pdf' target='_blank'>https://arxiv.org/pdf/2508.19488.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xavier Cadet, Simona Boboila, Sie Hendrata Dharmawan, Alina Oprea, Peter Chin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.19488">PoolFlip: A Multi-Agent Reinforcement Learning Security Environment for Cyber Defense</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cyber defense requires automating defensive decision-making under stealthy, deceptive, and continuously evolving adversarial strategies. The FlipIt game provides a foundational framework for modeling interactions between a defender and an advanced adversary that compromises a system without being immediately detected. In FlipIt, the attacker and defender compete to control a shared resource by performing a Flip action and paying a cost. However, the existing FlipIt frameworks rely on a small number of heuristics or specialized learning techniques, which can lead to brittleness and the inability to adapt to new attacks. To address these limitations, we introduce PoolFlip, a multi-agent gym environment that extends the FlipIt game to allow efficient learning for attackers and defenders. Furthermore, we propose Flip-PSRO, a multi-agent reinforcement learning (MARL) approach that leverages population-based training to train defender agents equipped to generalize against a range of unknown, potentially adaptive opponents. Our empirical results suggest that Flip-PSRO defenders are $2\times$ more effective than baselines to generalize to a heuristic attack not exposed in training. In addition, our newly designed ownership-based utility functions ensure that Flip-PSRO defenders maintain a high level of control while optimizing performance.
<div id='section'>Paperid: <span id='pid'>262, <a href='https://arxiv.org/pdf/2508.01522.pdf' target='_blank'>https://arxiv.org/pdf/2508.01522.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jack Zeng, Andreu Matoses Gimenez, Eugene Vinitsky, Javier Alonso-Mora, Sihao Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.01522">Decentralized Aerial Manipulation of a Cable-Suspended Load using Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents the first decentralized method to enable real-world 6-DoF manipulation of a cable-suspended load using a team of Micro-Aerial Vehicles (MAVs). Our method leverages multi-agent reinforcement learning (MARL) to train an outer-loop control policy for each MAV. Unlike state-of-the-art controllers that utilize a centralized scheme, our policy does not require global states, inter-MAV communications, nor neighboring MAV information. Instead, agents communicate implicitly through load pose observations alone, which enables high scalability and flexibility. It also significantly reduces computing costs during inference time, enabling onboard deployment of the policy. In addition, we introduce a new action space design for the MAVs using linear acceleration and body rates. This choice, combined with a robust low-level controller, enables reliable sim-to-real transfer despite significant uncertainties caused by cable tension during dynamic 3D motion. We validate our method in various real-world experiments, including full-pose control under load model uncertainties, showing setpoint tracking performance comparable to the state-of-the-art centralized method. We also demonstrate cooperation amongst agents with heterogeneous control policies, and robustness to the complete in-flight loss of one MAV. Videos of experiments: https://autonomousrobots.nl/paper_websites/aerial-manipulation-marl
<div id='section'>Paperid: <span id='pid'>263, <a href='https://arxiv.org/pdf/2507.23604.pdf' target='_blank'>https://arxiv.org/pdf/2507.23604.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tommaso Marzi, Cesare Alippi, Andrea Cini
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.23604">Hierarchical Message-Passing Policies for Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Decentralized Multi-Agent Reinforcement Learning (MARL) methods allow for learning scalable multi-agent policies, but suffer from partial observability and induced non-stationarity. These challenges can be addressed by introducing mechanisms that facilitate coordination and high-level planning. Specifically, coordination and temporal abstraction can be achieved through communication (e.g., message passing) and Hierarchical Reinforcement Learning (HRL) approaches to decision-making. However, optimization issues limit the applicability of hierarchical policies to multi-agent systems. As such, the combination of these approaches has not been fully explored. To fill this void, we propose a novel and effective methodology for learning multi-agent hierarchies of message-passing policies. We adopt the feudal HRL framework and rely on a hierarchical graph structure for planning and coordination among agents. Agents at lower levels in the hierarchy receive goals from the upper levels and exchange messages with neighboring agents at the same level. To learn hierarchical multi-agent policies, we design a novel reward-assignment method based on training the lower-level policies to maximize the advantage function associated with the upper levels. Results on relevant benchmarks show that our method performs favorably compared to the state of the art.
<div id='section'>Paperid: <span id='pid'>264, <a href='https://arxiv.org/pdf/2506.17029.pdf' target='_blank'>https://arxiv.org/pdf/2506.17029.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Leizhen Wang, Peibo Duan, Cheng Lyu, Zewen Wang, Zhiqiang He, Nan Zheng, Zhenliang Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.17029">Scalable and Reliable Multi-agent Reinforcement Learning for Traffic Assignment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The evolution of metropolitan cities and the increase in travel demands impose stringent requirements on traffic assignment methods. Multi-agent reinforcement learning (MARL) approaches outperform traditional methods in modeling adaptive routing behavior without requiring explicit system dynamics, which is beneficial for real-world deployment. However, MARL frameworks face challenges in scalability and reliability when managing extensive networks with substantial travel demand, which limiting their practical applicability in solving large-scale traffic assignment problems. To address these challenges, this study introduces MARL-OD-DA, a new MARL framework for the traffic assignment problem, which redefines agents as origin-destination (OD) pair routers rather than individual travelers, significantly enhancing scalability. Additionally, a Dirichlet-based action space with action pruning and a reward function based on the local relative gap are designed to enhance solution reliability and improve convergence efficiency. Experiments demonstrate that the proposed MARL framework effectively handles medium-sized networks with extensive and varied city-level OD demand, surpassing existing MARL methods. When implemented in the SiouxFalls network, MARL-OD-DA achieves better assignment solutions in 10 steps, with a relative gap that is 94.99% lower than that of conventional methods.
<div id='section'>Paperid: <span id='pid'>265, <a href='https://arxiv.org/pdf/2505.05968.pdf' target='_blank'>https://arxiv.org/pdf/2505.05968.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dan Qiao, Wenhao Li, Shanchao Yang, Hongyuan Zha, Baoxiang Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.05968">Offline Multi-agent Reinforcement Learning via Score Decomposition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Offline cooperative multi-agent reinforcement learning (MARL) faces unique challenges due to distributional shifts, particularly stemming from the high dimensionality of joint action spaces and the presence of out-of-distribution joint action selections. In this work, we highlight that a fundamental challenge in offline MARL arises from the multi-equilibrium nature of cooperative tasks, which induces a highly multimodal joint behavior policy space coupled with heterogeneous-quality behavior data. This makes it difficult for individual policy regularization to align with a consistent coordination pattern, leading to the policy distribution shift problems. To tackle this challenge, we design a sequential score function decomposition method that distills per-agent regularization signals from the joint behavior policy, which induces coordinated modality selection under decentralized execution constraints. Then we leverage a flexible diffusion-based generative model to learn these score functions from multimodal offline data, and integrate them into joint-action critics to guide policy updates toward high-reward, in-distribution regions under a shared team reward. Our approach achieves state-of-the-art performance across multiple particle environments and Multi-agent MuJoCo benchmarks consistently. To the best of our knowledge, this is the first work to explicitly address the distributional gap between offline and online MARL, paving the way for more generalizable offline policy-based MARL methods.
<div id='section'>Paperid: <span id='pid'>266, <a href='https://arxiv.org/pdf/2410.17351.pdf' target='_blank'>https://arxiv.org/pdf/2410.17351.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aditya Vikram Singh, Ethan Rathbun, Emma Graham, Lisa Oakley, Simona Boboila, Alina Oprea, Peter Chin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.17351">Hierarchical Multi-agent Reinforcement Learning for Cyber Network Defense</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in multi-agent reinforcement learning (MARL) have created opportunities to solve complex real-world tasks. Cybersecurity is a notable application area, where defending networks against sophisticated adversaries remains a challenging task typically performed by teams of security operators. In this work, we explore novel MARL strategies for building autonomous cyber network defenses that address challenges such as large policy spaces, partial observability, and stealthy, deceptive adversarial strategies. To facilitate efficient and generalized learning, we propose a hierarchical Proximal Policy Optimization (PPO) architecture that decomposes the cyber defense task into specific sub-tasks like network investigation and host recovery. Our approach involves training sub-policies for each sub-task using PPO enhanced with cybersecurity domain expertise. These sub-policies are then leveraged by a master defense policy that coordinates their selection to solve complex network defense tasks. Furthermore, the sub-policies can be fine-tuned and transferred with minimal cost to defend against shifts in adversarial behavior or changes in network settings. We conduct extensive experiments using CybORG Cage 4, the state-of-the-art MARL environment for cyber defense. Comparisons with multiple baselines across different adversaries show that our hierarchical learning approach achieves top performance in terms of convergence speed, episodic return, and several interpretable metrics relevant to cybersecurity, including the fraction of clean machines on the network, precision, and false positives.
<div id='section'>Paperid: <span id='pid'>267, <a href='https://arxiv.org/pdf/2511.06142.pdf' target='_blank'>https://arxiv.org/pdf/2511.06142.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sizhe Tang, Jiayu Chen, Tian Lan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.06142">MALinZero: Efficient Low-Dimensional Search for Mastering Complex Multi-Agent Planning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Monte Carlo Tree Search (MCTS), which leverages Upper Confidence Bound for Trees (UCTs) to balance exploration and exploitation through randomized sampling, is instrumental to solving complex planning problems. However, for multi-agent planning, MCTS is confronted with a large combinatorial action space that often grows exponentially with the number of agents. As a result, the branching factor of MCTS during tree expansion also increases exponentially, making it very difficult to efficiently explore and exploit during tree search. To this end, we propose MALinZero, a new approach to leverage low-dimensional representational structures on joint-action returns and enable efficient MCTS in complex multi-agent planning. Our solution can be viewed as projecting the joint-action returns into the low-dimensional space representable using a contextual linear bandit problem formulation. We solve the contextual linear bandit problem with convex and $μ$-smooth loss functions -- in order to place more importance on better joint actions and mitigate potential representational limitations -- and derive a linear Upper Confidence Bound applied to trees (LinUCT) to enable novel multi-agent exploration and exploitation in the low-dimensional space. We analyze the regret of MALinZero for low-dimensional reward functions and propose an $(1-\tfrac1e)$-approximation algorithm for the joint action selection by maximizing a sub-modular objective. MALinZero demonstrates state-of-the-art performance on multi-agent benchmarks such as matrix games, SMAC, and SMACv2, outperforming both model-based and model-free multi-agent reinforcement learning baselines with faster learning speed and better performance.
<div id='section'>Paperid: <span id='pid'>268, <a href='https://arxiv.org/pdf/2510.23053.pdf' target='_blank'>https://arxiv.org/pdf/2510.23053.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiyu Wang, Suman Raj, Rajkumar Buyya
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.23053">AirFed: Federated Graph-Enhanced Multi-Agent Reinforcement Learning for Multi-UAV Cooperative Mobile Edge Computing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multiple Unmanned Aerial Vehicles (UAVs) cooperative Mobile Edge Computing (MEC) systems face critical challenges in coordinating trajectory planning, task offloading, and resource allocation while ensuring Quality of Service (QoS) under dynamic and uncertain environments. Existing approaches suffer from limited scalability, slow convergence, and inefficient knowledge sharing among UAVs, particularly when handling large-scale IoT device deployments with stringent deadline constraints. This paper proposes AirFed, a novel federated graph-enhanced multi-agent reinforcement learning framework that addresses these challenges through three key innovations. First, we design dual-layer dynamic Graph Attention Networks (GATs) that explicitly model spatial-temporal dependencies among UAVs and IoT devices, capturing both service relationships and collaborative interactions within the network topology. Second, we develop a dual-Actor single-Critic architecture that jointly optimizes continuous trajectory control and discrete task offloading decisions. Third, we propose a reputation-based decentralized federated learning mechanism with gradient-sensitive adaptive quantization, enabling efficient and robust knowledge sharing across heterogeneous UAVs. Extensive experiments demonstrate that AirFed achieves 42.9% reduction in weighted cost compared to state-of-the-art baselines, attains over 99% deadline satisfaction and 94.2% IoT device coverage rate, and reduces communication overhead by 54.5%. Scalability analysis confirms robust performance across varying UAV numbers, IoT device densities, and system scales, validating AirFed's practical applicability for large-scale UAV-MEC deployments.
<div id='section'>Paperid: <span id='pid'>269, <a href='https://arxiv.org/pdf/2508.18610.pdf' target='_blank'>https://arxiv.org/pdf/2508.18610.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shrenik Jadhav, Birva Sevak, Srijita Das, Akhtar Hussain, Wencong Su, Van-Hai Bui
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.18610">Scalable Fairness Shaping with LLM-Guided Multi-Agent Reinforcement Learning for Peer-to-Peer Electricity Markets</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Peer-to-peer (P2P) energy trading is becoming central to modern distribution systems as rooftop PV and home energy management systems become pervasive, yet most existing market and reinforcement learning designs emphasize efficiency or private profit and offer little real-time guidance to ensure equitable outcomes under uncertainty. To address this gap, a fairness-aware multiagent reinforcement learning framework, FairMarket-RL, is proposed in which a large language model (LLM) critic shapes bidding policies within a continuous double auction under partial observability and discrete price-quantity actions. After each trading slot, the LLM returns normalized fairness scores Fairness-to-Grid (FTG), Fairness-Between-Sellers (FBS), and Fairness-of-Pricing (FPP) that are integrated into the reward via ramped coefficients and tunable scaling, so that fairness guidance complements, rather than overwhelms, economic incentives. The environment models realistic residential load and PV profiles and enforce hard constraints on prices, physical feasibility, and policy-update stability. Across a progression of experiments from a small pilot to a larger simulated community and a mixed-asset real-world dataset, the framework shifts exchanges toward local P2P trades, lowers consumer costs relative to grid-only procurement, sustains strong fairness across participants, and preserves utility viability. Sensitivity analyses over solar availability and aggregate demand further indicate robust performance, suggesting a scalable, LLM-guided pathway to decentralized electricity markets that are economically efficient, socially equitable, and technically sound.
<div id='section'>Paperid: <span id='pid'>270, <a href='https://arxiv.org/pdf/2507.22278.pdf' target='_blank'>https://arxiv.org/pdf/2507.22278.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sunny Amatya, Yi Ren, Zhe Xu, Wenlong Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.22278">Successor Features for Transfer in Alternating Markov Games</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper explores successor features for knowledge transfer in zero-sum, complete-information, and turn-based games. Prior research in single-agent systems has shown that successor features can provide a ``jump start" for agents when facing new tasks with varying reward structures. However, knowledge transfer in games typically relies on value and equilibrium transfers, which heavily depends on the similarity between tasks. This reliance can lead to failures when the tasks differ significantly. To address this issue, this paper presents an application of successor features to games and presents a novel algorithm called Game Generalized Policy Improvement (GGPI), designed to address Markov games in multi-agent reinforcement learning. The proposed algorithm enables the transfer of learning values and policies across games. An upper bound of the errors for transfer is derived as a function the similarity of the task. Through experiments with a turn-based pursuer-evader game, we demonstrate that the GGPI algorithm can generate high-reward interactions and one-shot policy transfer. When further tested in a wider set of initial conditions, the GGPI algorithm achieves higher success rates with improved path efficiency compared to those of the baseline algorithms.
<div id='section'>Paperid: <span id='pid'>271, <a href='https://arxiv.org/pdf/2507.17433.pdf' target='_blank'>https://arxiv.org/pdf/2507.17433.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hugh Adams, Srijoni Majumdar, Evangelos Pournaras
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.17433">Fair Compromises in Participatory Budgeting: a Multi-Agent Deep Reinforcement Learning Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Participatory budgeting is a method of collectively understanding and addressing spending priorities where citizens vote on how a budget is spent, it is regularly run to improve the fairness of the distribution of public funds. Participatory budgeting requires voters to make decisions on projects which can lead to ``choice overload". A multi-agent reinforcement learning approach to decision support can make decision making easier for voters by identifying voting strategies that increase the winning proportion of their vote. This novel approach can also support policymakers by highlighting aspects of election design that enable fair compromise on projects. This paper presents a novel, ethically aligned approach to decision support using multi-agent deep reinforcement learning modelling. This paper introduces a novel use of a branching neural network architecture to overcome scalability challenges of multi-agent reinforcement learning in a decentralized way. Fair compromises are found through optimising voter actions towards greater representation of voter preferences in the winning set. Experimental evaluation with real-world participatory budgeting data reveals a pattern in fair compromise: that it is achievable through projects with smaller cost.
<div id='section'>Paperid: <span id='pid'>272, <a href='https://arxiv.org/pdf/2506.22708.pdf' target='_blank'>https://arxiv.org/pdf/2506.22708.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shrenik Jadhav, Birva Sevak, Srijita Das, Akhtar Hussain, Wencong Su, Van-Hai Bui
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.22708">FairMarket-RL: LLM-Guided Fairness Shaping for Multi-Agent Reinforcement Learning in Peer-to-Peer Markets</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Peer-to-peer (P2P) trading is increasingly recognized as a key mechanism for decentralized market regulation, yet existing approaches often lack robust frameworks to ensure fairness. This paper presents FairMarket-RL, a novel hybrid framework that combines Large Language Models (LLMs) with Reinforcement Learning (RL) to enable fairness-aware trading agents. In a simulated P2P microgrid with multiple sellers and buyers, the LLM acts as a real-time fairness critic, evaluating each trading episode using two metrics: Fairness-To-Buyer (FTB) and Fairness-Between-Sellers (FBS). These fairness scores are integrated into agent rewards through scheduled Î»-coefficients, forming an adaptive LLM-guided reward shaping loop that replaces brittle, rule-based fairness constraints. Agents are trained using Independent Proximal Policy Optimization (IPPO) and achieve equitable outcomes, fulfilling over 90% of buyer demand, maintaining fair seller margins, and consistently reaching FTB and FBS scores above 0.80. The training process demonstrates that fairness feedback improves convergence, reduces buyer shortfalls, and narrows profit disparities between sellers. With its language-based critic, the framework scales naturally, and its extension to a large power distribution system with household prosumers illustrates its practical applicability. FairMarket-RL thus offers a scalable, equity-driven solution for autonomous trading in decentralized energy systems.
<div id='section'>Paperid: <span id='pid'>273, <a href='https://arxiv.org/pdf/2506.09434.pdf' target='_blank'>https://arxiv.org/pdf/2506.09434.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Michael Amir, Matteo Bettini, Amanda Prorok
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.09434">When Is Diversity Rewarded in Cooperative Multi-Agent Learning?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The success of teams in robotics, nature, and society often depends on the division of labor among diverse specialists; however, a principled explanation for when such diversity surpasses a homogeneous team is still missing. Focusing on multi-agent task allocation problems, our goal is to study this question from the perspective of reward design: what kinds of objectives are best suited for heterogeneous teams? We first consider an instantaneous, non-spatial setting where the global reward is built by two generalized aggregation operators: an inner operator that maps the $N$ agents' effort allocations on individual tasks to a task score, and an outer operator that merges the $M$ task scores into the global team reward. We prove that the curvature of these operators determines whether heterogeneity can increase reward, and that for broad reward families this collapses to a simple convexity test. Next, we ask what incentivizes heterogeneity to emerge when embodied, time-extended agents must learn an effort allocation policy. To study heterogeneity in such settings, we use multi-agent reinforcement learning (MARL) as our computational paradigm, and introduce Heterogeneous Environment Design (HED), a gradient-based algorithm that optimizes the parameter space of underspecified MARL environments to find scenarios where heterogeneity is advantageous. Experiments in matrix games and an embodied Multi-Goal-Capture environment show that, despite the difference in settings, HED rediscovers the reward regimes predicted by our theory to maximize the advantage of heterogeneity, both validating HED and connecting our theoretical insights to reward design in MARL. Together, these results help us understand when behavioral diversity delivers a measurable benefit.
<div id='section'>Paperid: <span id='pid'>274, <a href='https://arxiv.org/pdf/2505.22151.pdf' target='_blank'>https://arxiv.org/pdf/2505.22151.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Claude Formanek, Omayma Mahjoub, Louay Ben Nessir, Sasha Abramowitz, Ruan de Kock, Wiem Khlifi, Simon Du Toit, Felix Chalumeau, Daniel Rajaonarivonivelomanantsoa, Arnol Fokam, Siddarth Singh, Ulrich Mbou Sob, Arnu Pretorius
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.22151">Oryx: a Performant and Scalable Algorithm for Many-Agent Coordination in Offline MARL</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A key challenge in offline multi-agent reinforcement learning (MARL) is achieving effective many-agent multi-step coordination in complex environments. In this work, we propose Oryx, a novel algorithm for offline cooperative MARL to directly address this challenge. Oryx adapts the recently proposed retention-based architecture Sable and combines it with a sequential form of implicit constraint Q-learning (ICQ), to develop a novel offline auto-regressive policy update scheme. This allows Oryx to solve complex coordination challenges while maintaining temporal coherence over lengthy trajectories. We evaluate Oryx across a diverse set of benchmarks from prior works (SMAC, RWARE, and Multi-Agent MuJoCo) covering tasks of both discrete and continuous control, varying in scale and difficulty. Oryx achieves state-of-the-art performance on more than 80% of the 65 tested datasets, outperforming prior offline MARL methods and demonstrating robust generalisation across domains with many agents and long horizons. Finally, we introduce new datasets to push the limits of many-agent coordination in offline MARL, and demonstrate Oryx's superior ability to scale effectively in such settings. We will make all of our datasets, experimental data, and code available upon publication.
<div id='section'>Paperid: <span id='pid'>275, <a href='https://arxiv.org/pdf/2504.21164.pdf' target='_blank'>https://arxiv.org/pdf/2504.21164.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bhavini Jeloka, Yue Guan, Panagiotis Tsiotras
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.21164">Learning Large-Scale Competitive Team Behaviors with Mean-Field Interactions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>State-of-the-art multi-agent reinforcement learning (MARL) algorithms such as MADDPG and MAAC fail to scale in situations where the number of agents becomes large. Mean-field theory has shown encouraging results in modeling macroscopic agent behavior for teams with a large number of agents through a continuum approximation of the agent population and its interaction with the environment. In this work, we extend proximal policy optimization (PPO) to the mean-field domain by introducing the Mean-Field Multi-Agent Proximal Policy Optimization (MF-MAPPO), a novel algorithm that utilizes the effectiveness of the finite-population mean-field approximation in the context of zero-sum competitive multi-agent games between two teams. The proposed algorithm can be easily scaled to hundreds and thousands of agents in each team as shown through numerical experiments. In particular, the algorithm is applied to realistic applications such as large-scale offense-defense battlefield scenarios.
<div id='section'>Paperid: <span id='pid'>276, <a href='https://arxiv.org/pdf/2504.15425.pdf' target='_blank'>https://arxiv.org/pdf/2504.15425.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Songyuan Zhang, Oswin So, Mitchell Black, Zachary Serlin, Chuchu Fan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.15425">Solving Multi-Agent Safe Optimal Control with Distributed Epigraph Form MARL</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Tasks for multi-robot systems often require the robots to collaborate and complete a team goal while maintaining safety. This problem is usually formalized as a constrained Markov decision process (CMDP), which targets minimizing a global cost and bringing the mean of constraint violation below a user-defined threshold. Inspired by real-world robotic applications, we define safety as zero constraint violation. While many safe multi-agent reinforcement learning (MARL) algorithms have been proposed to solve CMDPs, these algorithms suffer from unstable training in this setting. To tackle this, we use the epigraph form for constrained optimization to improve training stability and prove that the centralized epigraph form problem can be solved in a distributed fashion by each agent. This results in a novel centralized training distributed execution MARL algorithm named Def-MARL. Simulation experiments on 8 different tasks across 2 different simulators show that Def-MARL achieves the best overall performance, satisfies safety constraints, and maintains stable training. Real-world hardware experiments on Crazyflie quadcopters demonstrate the ability of Def-MARL to safely coordinate agents to complete complex collaborative tasks compared to other methods.
<div id='section'>Paperid: <span id='pid'>277, <a href='https://arxiv.org/pdf/2501.08941.pdf' target='_blank'>https://arxiv.org/pdf/2501.08941.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Surya Murthy, John-Paul Clarke, Ufuk Topcu, Zhenyu Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.08941">A Reinforcement Learning Approach to Quiet and Safe UAM Traffic Management</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Urban air mobility (UAM) is a transformative system that operates various small aerial vehicles in urban environments to reshape urban transportation. However, integrating UAM into existing urban environments presents a variety of complex challenges. Recent analyses of UAM's operational constraints highlight aircraft noise and system safety as key hurdles to UAM system implementation. Future UAM air traffic management schemes must ensure that the system is both quiet and safe. We propose a multi-agent reinforcement learning approach to manage UAM traffic, aiming at both vertical separation assurance and noise mitigation. Through extensive training, the reinforcement learning agent learns to balance the two primary objectives by employing altitude adjustments in a multi-layer UAM network. The results reveal the tradeoffs among noise impact, traffic congestion, and separation. Overall, our findings demonstrate the potential of reinforcement learning in mitigating UAM's noise impact while maintaining safe separation using altitude adjustments
<div id='section'>Paperid: <span id='pid'>278, <a href='https://arxiv.org/pdf/2501.08600.pdf' target='_blank'>https://arxiv.org/pdf/2501.08600.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tyler Stennett, Myeongsoo Kim, Saurabh Sinha, Alessandro Orso
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.08600">AutoRestTest: A Tool for Automated REST API Testing Using LLMs and MARL</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As REST APIs have become widespread in modern web services, comprehensive testing of these APIs is increasingly crucial. Because of the vast search space of operations, parameters, and parameter values, along with their dependencies and constraints, current testing tools often achieve low code coverage, resulting in suboptimal fault detection. To address this limitation, we present AutoRestTest, a novel tool that integrates the Semantic Property Dependency Graph (SPDG) with Multi-Agent Reinforcement Learning (MARL) and large language models (LLMs) for effective REST API testing. AutoRestTest determines operation-dependent parameters using the SPDG and employs five specialized agents (operation, parameter, value, dependency, and header) to identify dependencies of operations and generate operation sequences, parameter combinations, and values. Through an intuitive command-line interface, users can easily configure and monitor tests with successful operation count, unique server errors detected, and time elapsed. Upon completion, AutoRestTest generates a detailed report highlighting errors detected and operations exercised. In this paper, we introduce our tool and present preliminary findings, with a demonstration video available at https://www.youtube.com/watch?v=VVus2W8rap8.
<div id='section'>Paperid: <span id='pid'>279, <a href='https://arxiv.org/pdf/2412.16244.pdf' target='_blank'>https://arxiv.org/pdf/2412.16244.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Matteo Bettini, Ryan Kortvelesy, Amanda Prorok
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.16244">The impact of behavioral diversity in multi-agent reinforcement learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Many of the world's most pressing issues, such as climate change and global peace, require complex collective problem-solving skills. Recent studies indicate that diversity in individuals' behaviors is key to developing such skills and increasing collective performance. Yet behavioral diversity in collective artificial learning is understudied, with today's machine learning paradigms commonly favoring homogeneous agent strategies over heterogeneous ones, mainly due to computational considerations. In this work, we employ diversity measurement and control paradigms to study the impact of behavioral heterogeneity in several facets of multi-agent reinforcement learning. Through experiments in team play and other cooperative tasks, we show the emergence of unbiased behavioral roles that improve team outcomes; how behavioral diversity synergizes with morphological diversity; how diverse agents are more effective at finding cooperative solutions in sparse reward settings; and how behaviorally heterogeneous teams learn and retain latent skills to overcome repeated disruptions. Overall, our results indicate that, by controlling diversity, we can obtain non-trivial benefits over homogeneous training paradigms, demonstrating that diversity is a fundamental component of collective artificial learning, an insight thus far overlooked.
<div id='section'>Paperid: <span id='pid'>280, <a href='https://arxiv.org/pdf/2412.15461.pdf' target='_blank'>https://arxiv.org/pdf/2412.15461.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Desmond Chan, Bart De Keijzer, Tobias Galla, Stefanos Leonardos, Carmine Ventre
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.15461">Asymptotic Extinction in Large Coordination Games</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We study the exploration-exploitation trade-off for large multiplayer coordination games where players strategise via Q-Learning, a common learning framework in multi-agent reinforcement learning. Q-Learning is known to have two shortcomings, namely non-convergence and potential equilibrium selection problems, when there are multiple fixed points, called Quantal Response Equilibria (QRE). Furthermore, whilst QRE have full support for finite games, it is not clear how Q-Learning behaves as the game becomes large. In this paper, we characterise the critical exploration rate that guarantees convergence to a unique fixed point, addressing the two shortcomings above. Using a generating-functional method, we show that this rate increases with the number of players and the alignment of their payoffs. For many-player coordination games with perfectly aligned payoffs, this exploration rate is roughly twice that of $p$-player zero-sum games. As for large games, we provide a structural result for QRE, which suggests that as the game size increases, Q-Learning converges to a QRE near the boundary of the simplex of the action space, a phenomenon we term asymptotic extinction, where a constant fraction of the actions are played with zero probability at a rate $o(1/N)$ for an $N$-action game.
<div id='section'>Paperid: <span id='pid'>281, <a href='https://arxiv.org/pdf/2411.07098.pdf' target='_blank'>https://arxiv.org/pdf/2411.07098.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Myeongsoo Kim, Tyler Stennett, Saurabh Sinha, Alessandro Orso
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.07098">A Multi-Agent Approach for REST API Testing with Semantic Graphs and LLM-Driven Inputs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As modern web services increasingly rely on REST APIs, their thorough testing has become crucial. Furthermore, the advent of REST API documentation languages, such as the OpenAPI Specification, has led to the emergence of many black-box REST API testing tools. However, these tools often focus on individual test elements in isolation (e.g., APIs, parameters, values), resulting in lower coverage and less effectiveness in fault detection. To address these limitations, we present AutoRestTest, the first black-box tool to adopt a dependency-embedded multi-agent approach for REST API testing that integrates multi-agent reinforcement learning (MARL) with a semantic property dependency graph (SPDG) and Large Language Models (LLMs). Our approach treats REST API testing as a separable problem, where four agents -- API, dependency, parameter, and value agents -- collaborate to optimize API exploration. LLMs handle domain-specific value generation, the SPDG model simplifies the search space for dependencies using a similarity score between API operations, and MARL dynamically optimizes the agents' behavior. Our evaluation of AutoRestTest on 12 real-world REST services shows that it outperforms the four leading black-box REST API testing tools, including those assisted by RESTGPT (which generates realistic test inputs using LLMs), in terms of code coverage, operation coverage, and fault detection. Notably, AutoRestTest is the only tool able to trigger an internal server error in the Spotify service. Our ablation study illustrates that each component of AutoRestTest -- the SPDG, the LLM, and the agent-learning mechanism -- contributes to its overall effectiveness.
<div id='section'>Paperid: <span id='pid'>282, <a href='https://arxiv.org/pdf/2411.01766.pdf' target='_blank'>https://arxiv.org/pdf/2411.01766.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Cheng Zhang, Lan Wei, Ji Fan, Zening Liu, Yongming Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.01766">Lyapunov-guided Multi-Agent Reinforcement Learning for Delay-Sensitive Wireless Scheduling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, a two-stage intelligent scheduler is proposed to minimize the packet-level delay jitter while guaranteeing delay bound. Firstly, Lyapunov technology is employed to transform the delay-violation constraint into a sequential slot-level queue stability problem. Secondly, a hierarchical scheme is proposed to solve the resource allocation between multiple base stations and users, where the multi-agent reinforcement learning (MARL) gives the user priority and the number of scheduled packets, while the underlying scheduler allocates the resource. Our proposed scheme achieves lower delay jitter and delay violation rate than the Round-Robin Earliest Deadline First algorithm and MARL with delay violation penalty.
<div id='section'>Paperid: <span id='pid'>283, <a href='https://arxiv.org/pdf/2410.19382.pdf' target='_blank'>https://arxiv.org/pdf/2410.19382.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jemma Daniel, Ruan de Kock, Louay Ben Nessir, Sasha Abramowitz, Omayma Mahjoub, Wiem Khlifi, Claude Formanek, Arnu Pretorius
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.19382">Multi-Agent Reinforcement Learning with Selective State-Space Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The Transformer model has demonstrated success across a wide range of domains, including in Multi-Agent Reinforcement Learning (MARL) where the Multi-Agent Transformer (MAT) has emerged as a leading algorithm in the field. However, a significant drawback of Transformer models is their quadratic computational complexity relative to input size, making them computationally expensive when scaling to larger inputs. This limitation restricts MAT's scalability in environments with many agents. Recently, State-Space Models (SSMs) have gained attention due to their computational efficiency, but their application in MARL remains unexplored. In this work, we investigate the use of Mamba, a recent SSM, in MARL and assess whether it can match the performance of MAT while providing significant improvements in efficiency. We introduce a modified version of MAT that incorporates standard and bi-directional Mamba blocks, as well as a novel "cross-attention" Mamba block. Extensive testing shows that our Multi-Agent Mamba (MAM) matches the performance of MAT across multiple standard multi-agent environments, while offering superior scalability to larger agent scenarios. This is significant for the MARL community, because it indicates that SSMs could replace Transformers without compromising performance, whilst also supporting more effective scaling to higher numbers of agents. Our project page is available at https://sites.google.com/view/multi-agent-mamba .
<div id='section'>Paperid: <span id='pid'>284, <a href='https://arxiv.org/pdf/2410.01706.pdf' target='_blank'>https://arxiv.org/pdf/2410.01706.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Omayma Mahjoub, Sasha Abramowitz, Ruan de Kock, Wiem Khlifi, Simon du Toit, Jemma Daniel, Louay Ben Nessir, Louise Beyers, Claude Formanek, Liam Clark, Arnu Pretorius
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.01706">Sable: a Performant, Efficient and Scalable Sequence Model for MARL</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As multi-agent reinforcement learning (MARL) progresses towards solving larger and more complex problems, it becomes increasingly important that algorithms exhibit the key properties of (1) strong performance, (2) memory efficiency, and (3) scalability. In this work, we introduce Sable, a performant, memory-efficient, and scalable sequence modeling approach to MARL. Sable works by adapting the retention mechanism in Retentive Networks (Sun et al., 2023) to achieve computationally efficient processing of multi-agent observations with long context memory for temporal reasoning. Through extensive evaluations across six diverse environments, we demonstrate how Sable is able to significantly outperform existing state-of-the-art methods in a large number of diverse tasks (34 out of 45 tested). Furthermore, Sable maintains performance as we scale the number of agents, handling environments with more than a thousand agents while exhibiting a linear increase in memory usage. Finally, we conduct ablation studies to isolate the source of Sable's performance gains and confirm its efficient computational memory usage.
<div id='section'>Paperid: <span id='pid'>285, <a href='https://arxiv.org/pdf/2409.18435.pdf' target='_blank'>https://arxiv.org/pdf/2409.18435.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xian Yeow Lee, Haiyan Wang, Daisuke Katsumata, Takaharu Matsui, Chetan Gupta
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.18435">Multi-agent Reinforcement Learning for Dynamic Dispatching in Material Handling Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper proposes a multi-agent reinforcement learning (MARL) approach to learn dynamic dispatching strategies, which is crucial for optimizing throughput in material handling systems across diverse industries. To benchmark our method, we developed a material handling environment that reflects the complexities of an actual system, such as various activities at different locations, physical constraints, and inherent uncertainties. To enhance exploration during learning, we propose a method to integrate domain knowledge in the form of existing dynamic dispatching heuristics. Our experimental results show that our method can outperform heuristics by up to 7.4 percent in terms of median throughput. Additionally, we analyze the effect of different architectures on MARL performance when training multiple agents with different functions. We also demonstrate that the MARL agents performance can be further improved by using the first iteration of MARL agents as heuristics to train a second iteration of MARL agents. This work demonstrates the potential of applying MARL to learn effective dynamic dispatching strategies that may be deployed in real-world systems to improve business outcomes.
<div id='section'>Paperid: <span id='pid'>286, <a href='https://arxiv.org/pdf/2409.12001.pdf' target='_blank'>https://arxiv.org/pdf/2409.12001.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Claude Formanek, Louise Beyers, Callum Rhys Tilbury, Jonathan P. Shock, Arnu Pretorius
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.12001">Putting Data at the Centre of Offline Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Offline multi-agent reinforcement learning (MARL) is an exciting direction of research that uses static datasets to find optimal control policies for multi-agent systems. Though the field is by definition data-driven, efforts have thus far neglected data in their drive to achieve state-of-the-art results. We first substantiate this claim by surveying the literature, showing how the majority of works generate their own datasets without consistent methodology and provide sparse information about the characteristics of these datasets. We then show why neglecting the nature of the data is problematic, through salient examples of how tightly algorithmic performance is coupled to the dataset used, necessitating a common foundation for experiments in the field. In response, we take a big step towards improving data usage and data awareness in offline MARL, with three key contributions: (1) a clear guideline for generating novel datasets; (2) a standardisation of over 80 existing datasets, hosted in a publicly available repository, using a consistent storage format and easy-to-use API; and (3) a suite of analysis tools that allow us to understand these datasets better, aiding further development.
<div id='section'>Paperid: <span id='pid'>287, <a href='https://arxiv.org/pdf/2409.04224.pdf' target='_blank'>https://arxiv.org/pdf/2409.04224.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Daniel J. Tan, Qianyi Xu, Kay Choong See, Dilruk Perera, Mengling Feng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.04224">Advancing Multi-Organ Disease Care: A Hierarchical Multi-Agent Reinforcement Learning Framework</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In healthcare, multi-organ system diseases pose unique and significant challenges as they impact multiple physiological systems concurrently, demanding complex and coordinated treatment strategies. Despite recent advancements in the AI based clinical decision support systems, these solutions only focus on individual organ systems, failing to account for complex interdependencies between them. This narrow focus greatly hinders their effectiveness in recommending holistic and clinically actionable treatments in the real world setting. To address this critical gap, we propose a novel Hierarchical Multi-Agent Reinforcement Learning (HMARL) framework. Our architecture deploys specialized and dedicated agents for each organ system and facilitates inter-agent communication to enable synergistic decision-making across organ systems. Furthermore, we introduce a dual-layer state representation technique that contextualizes patient conditions at both global and organ-specific levels, improving the accuracy and relevance of treatment decisions. We evaluate our HMARL solution on the task of sepsis management, a common and critical multi-organ disease, using both qualitative and quantitative metrics. Our method learns effective, clinically aligned treatment policies that considerably improve patient survival. We believe this framework represents a significant advancement in clinical decision support systems, introducing the first RL solution explicitly designed for multi-organ treatment recommendations. Our solution moves beyond prevailing simplified, single-organ models that fall short in addressing the complexity of multi-organ diseases.
<div id='section'>Paperid: <span id='pid'>288, <a href='https://arxiv.org/pdf/2601.16578.pdf' target='_blank'>https://arxiv.org/pdf/2601.16578.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Julius Beerwerth, Jianye Xu, Simon Schäfer, Fynn Belderink, Bassam Alrifaee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.16578">Zero-Shot MARL Benchmark in the Cyber-Physical Mobility Lab</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a reproducible benchmark for evaluating sim-to-real transfer of Multi-Agent Reinforcement Learning (MARL) policies for Connected and Automated Vehicles (CAVs). The platform, based on the Cyber-Physical Mobility Lab (CPM Lab) [1], integrates simulation, a high-fidelity digital twin, and a physical testbed, enabling structured zero-shot evaluation of MARL motion-planning policies. We demonstrate its use by deploying a SigmaRL-trained policy [2] across all three domains, revealing two complementary sources of performance degradation: architectural differences between simulation and hardware control stacks, and the sim-to-real gap induced by increasing environmental realism. The open-source setup enables systematic analysis of sim-to-real challenges in MARL under realistic, reproducible conditions.
<div id='section'>Paperid: <span id='pid'>289, <a href='https://arxiv.org/pdf/2512.04405.pdf' target='_blank'>https://arxiv.org/pdf/2512.04405.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chenyuan Feng, Anbang Zhang, Geyong Min, Yongming Huang, Tony Q. S. Quek, Xiaohu You
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.04405">Towards 6G Native-AI Edge Networks: A Semantic-Aware and Agentic Intelligence Paradigm</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The evolution toward sixth-generation wireless systems positions intelligence as a native network capability, fundamentally transforming the design of radio access networks (RANs). Within this vision, Semantic-native communication and agentic intelligence are expected to play central roles. SemCom departs from bit-level fidelity and instead emphasizes task-oriented meaning exchange, enabling compact SC and introducing new performance measures such as semantic fidelity and task success rate. Agentic intelligence endows distributed RAN entities with goal-driven autonomy, reasoning, planning, and multi-agent collaboration, increasingly supported by foundation models and knowledge graphs. In this work, we first introduce the conceptual foundations of SemCom and agentic networking, and discuss why existing AI-driven O-RAN solutions remain largely bit-centric and task-siloed. We then present a unified taxonomy that organizes recent research along three axes: i) semantic abstraction level (symbol/feature/intent/knowledge), ii) agent autonomy and coordination granularity (single-, multi-, and hierarchical-agent), and iii) RAN control placement across PHY/MAC, near-real-time RIC, and non-real-time RIC. Based on this taxonomy, we systematically introduce enabling technologies including task-oriented semantic encoders/decoders, multi-agent reinforcement learning, foundation-model-assisted RAN agents, and knowledge-graph-based reasoning for cross-layer awareness. Representative 6G use cases, such as immersive XR, vehicular V2X, and industrial digital twins, are analyzed to illustrate the semantic-agentic convergence in practice. Finally, we identify open challenges in semantic representation standardization, scalable trustworthy agent coordination, O-RAN interoperability, and energy-efficient AI deployment, and outline research directions toward operational semantic-agentic AI-RAN.
<div id='section'>Paperid: <span id='pid'>290, <a href='https://arxiv.org/pdf/2511.13137.pdf' target='_blank'>https://arxiv.org/pdf/2511.13137.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yanda Zhu, Yuanyang Zhu, Daoyi Dong, Caihua Chen, Chunlin Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.13137">Conditional Diffusion Model for Multi-Agent Dynamic Task Decomposition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Task decomposition has shown promise in complex cooperative multi-agent reinforcement learning (MARL) tasks, which enables efficient hierarchical learning for long-horizon tasks in dynamic and uncertain environments. However, learning dynamic task decomposition from scratch generally requires a large number of training samples, especially exploring the large joint action space under partial observability. In this paper, we present the Conditional Diffusion Model for Dynamic Task Decomposition (C$\text{D}^\text{3}$T), a novel two-level hierarchical MARL framework designed to automatically infer subtask and coordination patterns. The high-level policy learns subtask representation to generate a subtask selection strategy based on subtask effects. To capture the effects of subtasks on the environment, C$\text{D}^\text{3}$T predicts the next observation and reward using a conditional diffusion model. At the low level, agents collaboratively learn and share specialized skills within their assigned subtasks. Moreover, the learned subtask representation is also used as additional semantic information in a multi-head attention mixing network to enhance value decomposition and provide an efficient reasoning bridge between individual and joint value functions. Experimental results on various benchmarks demonstrate that C$\text{D}^\text{3}$T achieves better performance than existing baselines.
<div id='section'>Paperid: <span id='pid'>291, <a href='https://arxiv.org/pdf/2511.05005.pdf' target='_blank'>https://arxiv.org/pdf/2511.05005.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dongsu Lee, Daehee Lee, Amy Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.05005">Multi-agent Coordination via Flow Matching</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work presents MAC-Flow, a simple yet expressive framework for multi-agent coordination. We argue that requirements of effective coordination are twofold: (i) a rich representation of the diverse joint behaviors present in offline data and (ii) the ability to act efficiently in real time. However, prior approaches often sacrifice one for the other, i.e., denoising diffusion-based solutions capture complex coordination but are computationally slow, while Gaussian policy-based solutions are fast but brittle in handling multi-agent interaction. MAC-Flow addresses this trade-off by first learning a flow-based representation of joint behaviors, and then distilling it into decentralized one-step policies that preserve coordination while enabling fast execution. Across four different benchmarks, including $12$ environments and $34$ datasets, MAC-Flow alleviates the trade-off between performance and computational cost, specifically achieving about $\boldsymbol{\times14.5}$ faster inference compared to diffusion-based MARL methods, while maintaining good performance. At the same time, its inference speed is similar to that of prior Gaussian policy-based offline multi-agent reinforcement learning (MARL) methods.
<div id='section'>Paperid: <span id='pid'>292, <a href='https://arxiv.org/pdf/2510.11410.pdf' target='_blank'>https://arxiv.org/pdf/2510.11410.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Anastasia Psarou, Łukasz Gorczyca, Dominik Gaweł, Rafał Kucharski
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.11410">Autonomous vehicles need social awareness to find optima in multi-agent reinforcement learning routing games</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Previous work has shown that when multiple selfish Autonomous Vehicles (AVs) are introduced to future cities and start learning optimal routing strategies using Multi-Agent Reinforcement Learning (MARL), they may destabilize traffic systems, as they would require a significant amount of time to converge to the optimal solution, equivalent to years of real-world commuting. We demonstrate that moving beyond the selfish component in the reward significantly relieves this issue. If each AV, apart from minimizing its own travel time, aims to reduce its impact on the system, this will be beneficial not only for the system-wide performance but also for each individual player in this routing game. By introducing an intrinsic reward signal based on the marginal cost matrix, we significantly reduce training time and achieve convergence more reliably. Marginal cost quantifies the impact of each individual action (route-choice) on the system (total travel time). Including it as one of the components of the reward can reduce the degree of non-stationarity by aligning agents' objectives. Notably, the proposed counterfactual formulation preserves the system's equilibria and avoids oscillations. Our experiments show that training MARL algorithms with our novel reward formulation enables the agents to converge to the optimal solution, whereas the baseline algorithms fail to do so. We show these effects in both a toy network and the real-world network of Saint-Arnoult. Our results optimistically indicate that social awareness (i.e., including marginal costs in routing decisions) improves both the system-wide and individual performance of future urban systems with AVs.
<div id='section'>Paperid: <span id='pid'>293, <a href='https://arxiv.org/pdf/2509.17350.pdf' target='_blank'>https://arxiv.org/pdf/2509.17350.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoran Zhou, Yangwei You, Shuaijun Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.17350">DyDexHandover: Human-like Bimanual Dynamic Dexterous Handover using RGB-only Perception</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Dynamic in air handover is a fundamental challenge for dual-arm robots, requiring accurate perception, precise coordination, and natural motion. Prior methods often rely on dynamics models, strong priors, or depth sensing, limiting generalization and naturalness. We present DyDexHandover, a novel framework that employs multi-agent reinforcement learning to train an end to end RGB based policy for bimanual object throwing and catching. To achieve more human-like behavior, the throwing policy is guided by a human policy regularization scheme, encouraging fluid and natural motion, and enhancing the generalization capability of the policy. A dual arm simulation environment was built in Isaac Sim for experimental evaluation. DyDexHandover achieves nearly 99 percent success on training objects and 75 percent on unseen objects, while generating human-like throwing and catching behaviors. To our knowledge, it is the first method to realize dual-arm in-air handover using only raw RGB perception.
<div id='section'>Paperid: <span id='pid'>294, <a href='https://arxiv.org/pdf/2508.17696.pdf' target='_blank'>https://arxiv.org/pdf/2508.17696.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Woojun Kim, Katia Sycara
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.17696">Fair Cooperation in Mixed-Motive Games via Conflict-Aware Gradient Adjustment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent reinforcement learning in mixed-motive settings presents a fundamental challenge: agents must balance individual interests with collective goals, which are neither fully aligned nor strictly opposed. To address this, reward restructuring methods such as gifting and intrinsic motivation have been proposed. However, these approaches primarily focus on promoting cooperation by managing the trade-off between individual and collective returns, without explicitly addressing fairness with respect to the agents' task-specific rewards. In this paper, we propose an adaptive conflict-aware gradient adjustment method that promotes cooperation while ensuring fairness in individual rewards. The proposed method dynamically balances policy gradients derived from individual and collective objectives in situations where the two objectives are in conflict. By explicitly resolving such conflicts, our method improves collective performance while preserving fairness across agents. We provide theoretical results that guarantee monotonic non-decreasing improvement in both the collective and individual objectives and ensure fairness. Empirical results in sequential social dilemma environments demonstrate that our approach outperforms baselines in terms of social welfare while ensuring fairness among agents.
<div id='section'>Paperid: <span id='pid'>295, <a href='https://arxiv.org/pdf/2508.12845.pdf' target='_blank'>https://arxiv.org/pdf/2508.12845.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Artem Pshenitsyn, Aleksandr Panov, Alexey Skrynnik
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.12845">CAMAR: Continuous Actions Multi-Agent Routing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent reinforcement learning (MARL) is a powerful paradigm for solving cooperative and competitive decision-making problems. While many MARL benchmarks have been proposed, few combine continuous state and action spaces with challenging coordination and planning tasks. We introduce CAMAR, a new MARL benchmark designed explicitly for multi-agent pathfinding in environments with continuous actions. CAMAR supports cooperative and competitive interactions between agents and runs efficiently at up to 100,000 environment steps per second. We also propose a three-tier evaluation protocol to better track algorithmic progress and enable deeper analysis of performance. In addition, CAMAR allows the integration of classical planning methods such as RRT and RRT* into MARL pipelines. We use them as standalone baselines and combine RRT* with popular MARL algorithms to create hybrid approaches. We provide a suite of test scenarios and benchmarking tools to ensure reproducibility and fair comparison. Experiments show that CAMAR presents a challenging and realistic testbed for the MARL community.
<div id='section'>Paperid: <span id='pid'>296, <a href='https://arxiv.org/pdf/2507.19151.pdf' target='_blank'>https://arxiv.org/pdf/2507.19151.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Michael Amir, Guang Yang, Zhan Gao, Keisuke Okumura, Heedo Woo, Amanda Prorok
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.19151">ReCoDe: Reinforcement Learning-based Dynamic Constraint Design for Multi-Agent Coordination</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Constraint-based optimization is a cornerstone of robotics, enabling the design of controllers that reliably encode task and safety requirements such as collision avoidance or formation adherence. However, handcrafted constraints can fail in multi-agent settings that demand complex coordination. We introduce ReCoDe--Reinforcement-based Constraint Design--a decentralized, hybrid framework that merges the reliability of optimization-based controllers with the adaptability of multi-agent reinforcement learning. Rather than discarding expert controllers, ReCoDe improves them by learning additional, dynamic constraints that capture subtler behaviors, for example, by constraining agent movements to prevent congestion in cluttered scenarios. Through local communication, agents collectively constrain their allowed actions to coordinate more effectively under changing conditions. In this work, we focus on applications of ReCoDe to multi-agent navigation tasks requiring intricate, context-based movements and consensus, where we show that it outperforms purely handcrafted controllers, other hybrid approaches, and standard MARL baselines. We give empirical (real robot) and theoretical evidence that retaining a user-defined controller, even when it is imperfect, is more efficient than learning from scratch, especially because ReCoDe can dynamically change the degree to which it relies on this controller.
<div id='section'>Paperid: <span id='pid'>297, <a href='https://arxiv.org/pdf/2507.14850.pdf' target='_blank'>https://arxiv.org/pdf/2507.14850.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>H. M. Sabbir Ahmad, Ehsan Sabouni, Alexander Wasilkoff, Param Budhraja, Zijian Guo, Songyuan Zhang, Chuchu Fan, Christos Cassandras, Wenchao Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.14850">Hierarchical Multi-Agent Reinforcement Learning with Control Barrier Functions for Safety-Critical Autonomous Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We address the problem of safe policy learning in multi-agent safety-critical autonomous systems. In such systems, it is necessary for each agent to meet the safety requirements at all times while also cooperating with other agents to accomplish the task. Toward this end, we propose a safe Hierarchical Multi-Agent Reinforcement Learning (HMARL) approach based on Control Barrier Functions (CBFs). Our proposed hierarchical approach decomposes the overall reinforcement learning problem into two levels learning joint cooperative behavior at the higher level and learning safe individual behavior at the lower or agent level conditioned on the high-level policy. Specifically, we propose a skill-based HMARL-CBF algorithm in which the higher level problem involves learning a joint policy over the skills for all the agents and the lower-level problem involves learning policies to execute the skills safely with CBFs. We validate our approach on challenging environment scenarios whereby a large number of agents have to safely navigate through conflicting road networks. Compared with existing state of the art methods, our approach significantly improves the safety achieving near perfect (within 5%) success/safety rate while also improving performance across all the environments.
<div id='section'>Paperid: <span id='pid'>298, <a href='https://arxiv.org/pdf/2507.06004.pdf' target='_blank'>https://arxiv.org/pdf/2507.06004.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chang Yao, Youfang Lin, Shoucheng Song, Hao Wu, Yuqing Ma, Shang Han, Kai Lv
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.06004">From General Relation Patterns to Task-Specific Decision-Making in Continual Multi-Agent Coordination</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Continual Multi-Agent Reinforcement Learning (Co-MARL) requires agents to address catastrophic forgetting issues while learning new coordination policies with the dynamics team. In this paper, we delve into the core of Co-MARL, namely Relation Patterns, which refer to agents' general understanding of interactions. In addition to generality, relation patterns exhibit task-specificity when mapped to different action spaces. To this end, we propose a novel method called General Relation Patterns-Guided Task-Specific Decision-Maker (RPG). In RPG, agents extract relation patterns from dynamic observation spaces using a relation capturer. These task-agnostic relation patterns are then mapped to different action spaces via a task-specific decision-maker generated by a conditional hypernetwork. To combat forgetting, we further introduce regularization items on both the relation capturer and the conditional hypernetwork. Results on SMAC and LBF demonstrate that RPG effectively prevents catastrophic forgetting when learning new tasks and achieves zero-shot generalization to unseen tasks.
<div id='section'>Paperid: <span id='pid'>299, <a href='https://arxiv.org/pdf/2506.05527.pdf' target='_blank'>https://arxiv.org/pdf/2506.05527.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Caroline Wang, Di Yang Shi, Elad Liebman, Ishan Durugkar, Arrasy Rahman, Peter Stone
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.05527">Sequence Modeling for N-Agent Ad Hoc Teamwork</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>N-agent ad hoc teamwork (NAHT) is a newly introduced challenge in multi-agent reinforcement learning, where controlled subteams of varying sizes must dynamically collaborate with varying numbers and types of unknown teammates without pre-coordination. The existing learning algorithm (POAM) considers only independent learning for its flexibility in dealing with a changing number of agents. However, independent learning fails to fully capture the inter-agent dynamics essential for effective collaboration. Based on our observation that transformers deal effectively with sequences with varying lengths and have been shown to be highly effective for a variety of machine learning problems, this work introduces a centralized, transformer-based method for N-agent ad hoc teamwork. Our proposed approach incorporates historical observations and actions of all controlled agents, enabling optimal responses to diverse and unseen teammates in partially observable environments. Empirical evaluation on a StarCraft II task demonstrates that MAT-NAHT outperforms POAM, achieving superior sample efficiency and generalization, without auxiliary agent-modeling objectives.
<div id='section'>Paperid: <span id='pid'>300, <a href='https://arxiv.org/pdf/2506.00982.pdf' target='_blank'>https://arxiv.org/pdf/2506.00982.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Keshawn Smith, Zhili Zhang, H M Sabbir Ahmad, Ehsan Sabouni, Maniak Mondal, Song Han, Wenchao Li, Fei Miao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.00982">Robust and Safe Multi-Agent Reinforcement Learning Framework with Communication for Autonomous Vehicles</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep multi-agent reinforcement learning (MARL) has been demonstrated effectively in simulations for many multi-robot problems. For autonomous vehicles, the development of vehicle-to-vehicle (V2V) communication technologies provide opportunities to further enhance safety of the system. However, zero-shot transfer of simulator-trained MARL policies to hardware dynamic systems remains challenging, and how to leverage communication and shared information for MARL has limited demonstrations on hardware. This problem is challenged by discrepancies between simulated and physical states, system state and model uncertainties, practical shared information design, and the need for safety guarantees in both simulation and hardware. This paper introduces RSR-RSMARL, a novel Robust and Safe MARL framework that supports Real-Sim-Real (RSR) policy adaptation for multi-agent systems with communication among agents, with both simulation and hardware demonstrations. RSR-RSMARL leverages state (includes shared state information among agents) and action representations considering real system complexities for MARL formulation. The MARL policy is trained with robust MARL algorithm to enable zero-shot transfer to hardware considering the sim-to-real gap. A safety shield module using Control Barrier Functions (CBFs) provides safety guarantee for each individual agent. Experiment results on F1/10th-scale autonomous vehicles with V2V communication demonstrate the ability of RSR-RSMARL framework to enhance driving safety and coordination across multiple configurations. These findings emphasize the importance of jointly designing robust policy representations and modular safety architectures to enable scalable, generalizable RSR transfer in multi-agent autonomy.
<div id='section'>Paperid: <span id='pid'>301, <a href='https://arxiv.org/pdf/2505.11100.pdf' target='_blank'>https://arxiv.org/pdf/2505.11100.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lang Feng, Jiahao Lin, Dong Xing, Li Zhang, De Ma, Gang Pan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.11100">Bidirectional Distillation: A Mixed-Play Framework for Multi-Agent Generalizable Behaviors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Population-population generalization is a challenging problem in multi-agent reinforcement learning (MARL), particularly when agents encounter unseen co-players. However, existing self-play-based methods are constrained by the limitation of inside-space generalization. In this study, we propose Bidirectional Distillation (BiDist), a novel mixed-play framework, to overcome this limitation in MARL. BiDist leverages knowledge distillation in two alternating directions: forward distillation, which emulates the historical policies' space and creates an implicit self-play, and reverse distillation, which systematically drives agents towards novel distributions outside the known policy space in a non-self-play manner. In addition, BiDist operates as a concise and efficient solution without the need for the complex and costly storage of past policies. We provide both theoretical analysis and empirical evidence to support BiDist's effectiveness. Our results highlight its remarkable generalization ability across a variety of cooperative, competitive, and social dilemma tasks, and reveal that BiDist significantly diversifies the policy distribution space. We also present comprehensive ablation studies to reinforce BiDist's effectiveness and key success factors. Source codes are available in the supplementary material.
<div id='section'>Paperid: <span id='pid'>302, <a href='https://arxiv.org/pdf/2504.14422.pdf' target='_blank'>https://arxiv.org/pdf/2504.14422.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Paul Fischer, Sebastian Kaltenbach, Sergey Litvinov, Sauro Succi, Petros Koumoutsakos
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.14422">Optimal Lattice Boltzmann Closures through Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The Lattice Boltzmann method (LBM) offers a powerful and versatile approach to simulating diverse hydrodynamic phenomena, spanning microfluidics to aerodynamics. The vast range of spatiotemporal scales inherent in these systems currently renders full resolution impractical, necessitating the development of effective closure models for under-resolved simulations. Under-resolved LBMs are unstable, and while there is a number of important efforts to stabilize them, they often face limitations in generalizing across scales and physical systems. We present a novel, data-driven, multiagent reinforcement learning (MARL) approach that drastically improves stability and accuracy of coarse-grained LBM simulations. The proposed method uses a convolutional neural network to dynamically control the local relaxation parameter for the LB across the simulation grid. The LB-MARL framework is showcased in turbulent Kolmogorov flows. We find that the MARL closures stabilize the simulations and recover the energy spectra of significantly more expensive fully resolved simulations while maintaining computational efficiency. The learned closure model can be transferred to flow scenarios unseen during training and has improved robustness and spectral accuracy compared to traditional LBM models. We believe that MARL closures open new frontiers for efficient and accurate simulations of a multitude of complex problems not accessible to present-day LB methods alone.
<div id='section'>Paperid: <span id='pid'>303, <a href='https://arxiv.org/pdf/2504.04160.pdf' target='_blank'>https://arxiv.org/pdf/2504.04160.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alexandre Oliveira, Katarina Dyreby, Francisco Caldas, ClÃ¡udia Soares
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.04160">OrbitZoo: Multi-Agent Reinforcement Learning Environment for Orbital Dynamics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The increasing number of satellites and orbital debris has made space congestion a critical issue, threatening satellite safety and sustainability. Challenges such as collision avoidance, station-keeping, and orbital maneuvering require advanced techniques to handle dynamic uncertainties and multi-agent interactions. Reinforcement learning (RL) has shown promise in this domain, enabling adaptive, autonomous policies for space operations; however, many existing RL frameworks rely on custom-built environments developed from scratch, which often use simplified models and require significant time to implement and validate the orbital dynamics, limiting their ability to fully capture real-world complexities. To address this, we introduce OrbitZoo, a versatile multi-agent RL environment built on a high-fidelity industry standard library, that enables realistic data generation, supports scenarios like collision avoidance and cooperative maneuvers, and ensures robust and accurate orbital dynamics. The environment is validated against a real satellite constellation, Starlink, achieving a Mean Absolute Percentage Error (MAPE) of 0.16% compared to real-world data. This validation ensures reliability for generating high-fidelity simulations and enabling autonomous and independent satellite operations.
<div id='section'>Paperid: <span id='pid'>304, <a href='https://arxiv.org/pdf/2503.20078.pdf' target='_blank'>https://arxiv.org/pdf/2503.20078.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Volkan Ustun, Soham Hans, Rajay Kumar, Yunzhe Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.20078">Abstracting Geo-specific Terrains to Scale Up Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent reinforcement learning (MARL) is increasingly ubiquitous in training dynamic and adaptive synthetic characters for interactive simulations on geo-specific terrains. Frameworks such as Unity's ML-Agents help to make such reinforcement learning experiments more accessible to the simulation community. Military training simulations also benefit from advances in MARL, but they have immense computational requirements due to their complex, continuous, stochastic, partially observable, non-stationary, and doctrine-based nature. Furthermore, these simulations require geo-specific terrains, further exacerbating the computational resources problem. In our research, we leverage Unity's waypoints to automatically generate multi-layered representation abstractions of the geo-specific terrains to scale up reinforcement learning while still allowing the transfer of learned policies between different representations. Our early exploratory results on a novel MARL scenario, where each side has differing objectives, indicate that waypoint-based navigation enables faster and more efficient learning while producing trajectories similar to those taken by expert human players in CSGO gaming environments. This research points out the potential of waypoint-based navigation for reducing the computational costs of developing and training MARL models for military training simulations, where geo-specific terrains and differing objectives are crucial.
<div id='section'>Paperid: <span id='pid'>305, <a href='https://arxiv.org/pdf/2502.16608.pdf' target='_blank'>https://arxiv.org/pdf/2502.16608.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuli Zhang, Shangbo Wang, Dongyao Jia, Pengfei Fan, Ruiyuan Jiang, Hankang Gu, Andy H. F. Chow
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.16608">Toward Dependency Dynamics in Multi-Agent Reinforcement Learning for Traffic Signal Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reinforcement learning (RL) emerges as a promising data-driven approach for adaptive traffic signal control (ATSC) in complex urban traffic networks, with deep neural networks substantially augmenting its learning capabilities. However, centralized RL becomes impractical for ATSC involving multiple agents due to the exceedingly high dimensionality of the joint action space. Multi-agent RL (MARL) mitigates this scalability issue by decentralizing control to local RL agents. Nevertheless, this decentralized method introduces new challenges: the environment becomes partially observable from the perspective of each local agent due to constrained inter-agent communication. Both centralized RL and MARL exhibit distinct strengths and weaknesses, particularly under heavy intersectional traffic conditions. In this paper, we justify that MARL can achieve the optimal global Q-value by separating into multiple IRL (Independent Reinforcement Learning) processes when no spill-back congestion occurs (no agent dependency) among agents (intersections). In the presence of spill-back congestion (with agent dependency), the maximum global Q-value can be achieved by using centralized RL. Building upon the conclusions, we propose a novel Dynamic Parameter Update Strategy for Deep Q-Network (DQN-DPUS), which updates the weights and bias based on the dependency dynamics among agents, i.e. updating only the diagonal sub-matrices for the scenario without spill-back congestion. We validate the DQN-DPUS in a simple network with two intersections under varying traffic, and show that the proposed strategy can speed up the convergence rate without sacrificing optimal exploration. The results corroborate our theoretical findings, demonstrating the efficacy of DQN-DPUS in optimizing traffic signal control.
<div id='section'>Paperid: <span id='pid'>306, <a href='https://arxiv.org/pdf/2502.10148.pdf' target='_blank'>https://arxiv.org/pdf/2502.10148.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiyuan Li, Wenshuai Zhao, Joni Pajarinen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.10148">Cooperative Multi-Agent Planning with Adaptive Skill Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite much progress in training distributed artificial intelligence (AI), building cooperative multi-agent systems with multi-agent reinforcement learning (MARL) faces challenges in sample efficiency, interpretability, and transferability. Unlike traditional learning-based methods that require extensive interaction with the environment, large language models (LLMs) demonstrate remarkable capabilities in zero-shot planning and complex reasoning. However, existing LLM-based approaches heavily rely on text-based observations and struggle with the non-Markovian nature of multi-agent interactions under partial observability. We present COMPASS, a novel multi-agent architecture that integrates vision-language models (VLMs) with a dynamic skill library and structured communication for decentralized closed-loop decision-making. The skill library, bootstrapped from demonstrations, evolves via planner-guided tasks to enable adaptive strategies. COMPASS propagates entity information through multi-hop communication under partial observability. Evaluations on the improved StarCraft Multi-Agent Challenge (SMACv2) demonstrate COMPASS's strong performance against state-of-the-art MARL baselines across both symmetric and asymmetric scenarios. Notably, in the symmetric Protoss 5v5 task, COMPASS achieved a 57\% win rate, representing a 30 percentage point advantage over QMIX (27\%). Project page can be found at https://stellar-entremet-1720bb.netlify.app/.
<div id='section'>Paperid: <span id='pid'>307, <a href='https://arxiv.org/pdf/2501.18138.pdf' target='_blank'>https://arxiv.org/pdf/2501.18138.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Woojun Kim, Katia Sycara
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.18138">B3C: A Minimalist Approach to Offline Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Overestimation arising from selecting unseen actions during policy evaluation is a major challenge in offline reinforcement learning (RL). A minimalist approach in the single-agent setting -- adding behavior cloning (BC) regularization to existing online RL algorithms -- has been shown to be effective; however, this approach is understudied in multi-agent settings. In particular, overestimation becomes worse in multi-agent settings due to the presence of multiple actions, resulting in the BC regularization-based approach easily suffering from either over-regularization or critic divergence. To address this, we propose a simple yet effective method, Behavior Cloning regularization with Critic Clipping (B3C), which clips the target critic value in policy evaluation based on the maximum return in the dataset and pushes the limit of the weight on the RL objective over BC regularization, thereby improving performance. Additionally, we leverage existing value factorization techniques, particularly non-linear factorization, which is understudied in offline settings. Integrated with non-linear value factorization, B3C outperforms state-of-the-art algorithms on various offline multi-agent benchmarks.
<div id='section'>Paperid: <span id='pid'>308, <a href='https://arxiv.org/pdf/2501.00211.pdf' target='_blank'>https://arxiv.org/pdf/2501.00211.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Noor Aboueleneen, Yahuza Bello, Abdullatif Albaseer, Ahmed Refaey Hussein, Mohamed Abdallah, Ekram Hossain
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.00211">Distributed Traffic Control in Complex Dynamic Roadblocks: A Multi-Agent Deep RL Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autonomous Vehicles (AVs) represent a transformative advancement in the transportation industry. These vehicles have sophisticated sensors, advanced algorithms, and powerful computing systems that allow them to navigate and operate without direct human intervention. However, AVs' systems still get overwhelmed when they encounter a complex dynamic change in the environment resulting from an accident or a roadblock for maintenance. The advanced features of Sixth Generation (6G) technology are set to offer strong support to AVs, enabling real-time data exchange and management of complex driving maneuvers. This paper proposes a Multi-Agent Reinforcement Learning (MARL) framework to improve AVs' decision-making in dynamic and complex Intelligent Transportation Systems (ITS) utilizing 6G-V2X communication. The primary objective is to enable AVs to avoid roadblocks efficiently by changing lanes while maintaining optimal traffic flow and maximizing the mean harmonic speed. To ensure realistic operations, key constraints such as minimum vehicle speed, roadblock count, and lane change frequency are integrated. We train and test the proposed MARL model with two traffic simulation scenarios using the SUMO and TraCI interface. Through extensive simulations, we demonstrate that the proposed model adapts to various traffic conditions and achieves efficient and robust traffic flow management. The trained model effectively navigates dynamic roadblocks, promoting improved traffic efficiency in AV operations with more than 70% efficiency over other benchmark solutions.
<div id='section'>Paperid: <span id='pid'>309, <a href='https://arxiv.org/pdf/2412.08138.pdf' target='_blank'>https://arxiv.org/pdf/2412.08138.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuchang Sun, Xinran Li, Tao Lin, Jun Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.08138">Learn How to Query from Unlabeled Data Streams in Federated Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Federated learning (FL) enables collaborative learning among decentralized clients while safeguarding the privacy of their local data. Existing studies on FL typically assume offline labeled data available at each client when the training starts. Nevertheless, the training data in practice often arrive at clients in a streaming fashion without ground-truth labels. Given the expensive annotation cost, it is critical to identify a subset of informative samples for labeling on clients. However, selecting samples locally while accommodating the global training objective presents a challenge unique to FL. In this work, we tackle this conundrum by framing the data querying process in FL as a collaborative decentralized decision-making problem and proposing an effective solution named LeaDQ, which leverages multi-agent reinforcement learning algorithms. In particular, under the implicit guidance from global information, LeaDQ effectively learns the local policies for distributed clients and steers them towards selecting samples that can enhance the global model's accuracy. Extensive simulations on image and text tasks show that LeaDQ advances the model performance in various FL scenarios, outperforming the benchmarking algorithms.
<div id='section'>Paperid: <span id='pid'>310, <a href='https://arxiv.org/pdf/2410.01101.pdf' target='_blank'>https://arxiv.org/pdf/2410.01101.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenhao Zhan, Scott Fujimoto, Zheqing Zhu, Jason D. Lee, Daniel R. Jiang, Yonathan Efroni
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.01101">Exploiting Structure in Offline Multi-Agent RL: The Benefits of Low Interaction Rank</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We study the problem of learning an approximate equilibrium in the offline multi-agent reinforcement learning (MARL) setting. We introduce a structural assumption -- the interaction rank -- and establish that functions with low interaction rank are significantly more robust to distribution shift compared to general ones. Leveraging this observation, we demonstrate that utilizing function classes with low interaction rank, when combined with regularization and no-regret learning, admits decentralized, computationally and statistically efficient learning in offline MARL. Our theoretical results are complemented by experiments that showcase the potential of critic architectures with low interaction rank in offline MARL, contrasting with commonly used single-agent value decomposition architectures.
<div id='section'>Paperid: <span id='pid'>311, <a href='https://arxiv.org/pdf/2409.16460.pdf' target='_blank'>https://arxiv.org/pdf/2409.16460.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hang Liu, Yi Cheng, Rankun Li, Xiaowen Hu, Linqi Ye, Houde Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.16460">MBC: Multi-Brain Collaborative Control for Quadruped Robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the field of locomotion task of quadruped robots, Blind Policy and Perceptive Policy each have their own advantages and limitations. The Blind Policy relies on preset sensor information and algorithms, suitable for known and structured environments, but it lacks adaptability in complex or unknown environments. The Perceptive Policy uses visual sensors to obtain detailed environmental information, allowing it to adapt to complex terrains, but its effectiveness is limited under occluded conditions, especially when perception fails. Unlike the Blind Policy, the Perceptive Policy is not as robust under these conditions. To address these challenges, we propose a MBC:Multi-Brain collaborative system that incorporates the concepts of Multi-Agent Reinforcement Learning and introduces collaboration between the Blind Policy and the Perceptive Policy. By applying this multi-policy collaborative model to a quadruped robot, the robot can maintain stable locomotion even when the perceptual system is impaired or observational data is incomplete. Our simulations and real-world experiments demonstrate that this system significantly improves the robot's passability and robustness against perception failures in complex environments, validating the effectiveness of multi-policy collaboration in enhancing robotic motion performance.
<div id='section'>Paperid: <span id='pid'>312, <a href='https://arxiv.org/pdf/2409.11852.pdf' target='_blank'>https://arxiv.org/pdf/2409.11852.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianye Xu, Omar Sobhy, Bassam Alrifaee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.11852">XP-MARL: Auxiliary Prioritization in Multi-Agent Reinforcement Learning to Address Non-Stationarity</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Non-stationarity poses a fundamental challenge in Multi-Agent Reinforcement Learning (MARL), arising from agents simultaneously learning and altering their policies. This creates a non-stationary environment from the perspective of each individual agent, often leading to suboptimal or even unconverged learning outcomes. We propose an open-source framework named XP-MARL, which augments MARL with auxiliary prioritization to address this challenge in cooperative settings. XP-MARL is 1) founded upon our hypothesis that prioritizing agents and letting higher-priority agents establish their actions first would stabilize the learning process and thus mitigate non-stationarity and 2) enabled by our proposed mechanism called action propagation, where higher-priority agents act first and communicate their actions, providing a more stationary environment for others. Moreover, instead of using a predefined or heuristic priority assignment, XP-MARL learns priority-assignment policies with an auxiliary MARL problem, leading to a joint learning scheme. Experiments in a motion-planning scenario involving Connected and Automated Vehicles (CAVs) demonstrate that XP-MARL improves the safety of a baseline model by 84.4% and outperforms a state-of-the-art approach, which improves the baseline by only 12.8%. Code: github.com/cas-lab-munich/sigmarl
<div id='section'>Paperid: <span id='pid'>313, <a href='https://arxiv.org/pdf/2511.13103.pdf' target='_blank'>https://arxiv.org/pdf/2511.13103.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Vidur Sinha, Muhammed Ustaomeroglu, Guannan Qu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.13103">Transformer-Based Scalable Multi-Agent Reinforcement Learning for Networked Systems with Long-Range Interactions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent reinforcement learning (MARL) has shown promise for large-scale network control, yet existing methods face two major limitations. First, they typically rely on assumptions leading to decay properties of local agent interactions, limiting their ability to capture long-range dependencies such as cascading power failures or epidemic outbreaks. Second, most approaches lack generalizability across network topologies, requiring retraining when applied to new graphs. We introduce STACCA (Shared Transformer Actor-Critic with Counterfactual Advantage), a unified transformer-based MARL framework that addresses both challenges. STACCA employs a centralized Graph Transformer Critic to model long-range dependencies and provide system-level feedback, while its shared Graph Transformer Actor learns a generalizable policy capable of adapting across diverse network structures. Further, to improve credit assignment during training, STACCA integrates a novel counterfactual advantage estimator that is compatible with state-value critic estimates. We evaluate STACCA on epidemic containment and rumor-spreading network control tasks, demonstrating improved performance, network generalization, and scalability. These results highlight the potential of transformer-based MARL architectures to achieve scalable and generalizable control in large-scale networked systems.
<div id='section'>Paperid: <span id='pid'>314, <a href='https://arxiv.org/pdf/2511.11992.pdf' target='_blank'>https://arxiv.org/pdf/2511.11992.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hung Du, Hy Nguyen, Srikanth Thudumu, Rajesh Vasa, Kon Mouzakis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.11992">Goal-Oriented Multi-Agent Reinforcement Learning for Decentralized Agent Teams</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Connected and autonomous vehicles across land, water, and air must often operate in dynamic, unpredictable environments with limited communication, no centralized control, and partial observability. These real-world constraints pose significant challenges for coordination, particularly when vehicles pursue individual objectives. To address this, we propose a decentralized Multi-Agent Reinforcement Learning (MARL) framework that enables vehicles, acting as agents, to communicate selectively based on local goals and observations. This goal-aware communication strategy allows agents to share only relevant information, enhancing collaboration while respecting visibility limitations. We validate our approach in complex multi-agent navigation tasks featuring obstacles and dynamic agent populations. Results show that our method significantly improves task success rates and reduces time-to-goal compared to non-cooperative baselines. Moreover, task performance remains stable as the number of agents increases, demonstrating scalability. These findings highlight the potential of decentralized, goal-driven MARL to support effective coordination in realistic multi-vehicle systems operating across diverse domains.
<div id='section'>Paperid: <span id='pid'>315, <a href='https://arxiv.org/pdf/2510.20218.pdf' target='_blank'>https://arxiv.org/pdf/2510.20218.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qinyu Xu, Yuanyang Zhu, Xuefei Wu, Chunlin Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.20218">High-order Interactions Modeling for Interpretable Multi-Agent Q-Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The ability to model interactions among agents is crucial for effective coordination and understanding their cooperation mechanisms in multi-agent reinforcement learning (MARL). However, previous efforts to model high-order interactions have been primarily hindered by the combinatorial explosion or the opaque nature of their black-box network structures. In this paper, we propose a novel value decomposition framework, called Continued Fraction Q-Learning (QCoFr), which can flexibly capture arbitrary-order agent interactions with only linear complexity $\mathcal{O}\left({n}\right)$ in the number of agents, thus avoiding the combinatorial explosion when modeling rich cooperation. Furthermore, we introduce the variational information bottleneck to extract latent information for estimating credits. This latent information helps agents filter out noisy interactions, thereby significantly enhancing both cooperation and interpretability. Extensive experiments demonstrate that QCoFr not only consistently achieves better performance but also provides interpretability that aligns with our theoretical analysis.
<div id='section'>Paperid: <span id='pid'>316, <a href='https://arxiv.org/pdf/2509.00678.pdf' target='_blank'>https://arxiv.org/pdf/2509.00678.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qintong Xie, Edward Koh, Xavier Cadet, Peter Chin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.00678">Nash Q-Network for Multi-Agent Cybersecurity Simulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cybersecurity defense involves interactions between adversarial parties (namely defenders and hackers), making multi-agent reinforcement learning (MARL) an ideal approach for modeling and learning strategies for these scenarios. This paper addresses one of the key challenges to MARL, the complexity of simultaneous training of agents in nontrivial environments, and presents a novel policy-based Nash Q-learning to directly converge onto a steady equilibrium. We demonstrate the successful implementation of this algorithm in a notable complex cyber defense simulation treated as a two-player zero-sum Markov game setting. We propose the Nash Q-Network, which aims to learn Nash-optimal strategies that translate to robust defenses in cybersecurity settings. Our approach incorporates aspects of proximal policy optimization (PPO), deep Q-network (DQN), and the Nash-Q algorithm, addressing common challenges like non-stationarity and instability in multi-agent learning. The training process employs distributed data collection and carefully designed neural architectures for both agents and critics.
<div id='section'>Paperid: <span id='pid'>317, <a href='https://arxiv.org/pdf/2507.20143.pdf' target='_blank'>https://arxiv.org/pdf/2507.20143.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhonghan Ge, Yuanyang Zhu, Chunlin Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.20143">Concept Learning for Cooperative Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite substantial progress in applying neural networks (NN) to multi-agent reinforcement learning (MARL) areas, they still largely suffer from a lack of transparency and interoperability. However, its implicit cooperative mechanism is not yet fully understood due to black-box networks. In this work, we study an interpretable value decomposition framework via concept bottleneck models, which promote trustworthiness by conditioning credit assignment on an intermediate level of human-like cooperation concepts. To address this problem, we propose a novel value-based method, named Concepts learning for Multi-agent Q-learning (CMQ), that goes beyond the current performance-vs-interpretability trade-off by learning interpretable cooperation concepts. CMQ represents each cooperation concept as a supervised vector, as opposed to existing models where the information flowing through their end-to-end mechanism is concept-agnostic. Intuitively, using individual action value conditioning on global state embeddings to represent each concept allows for extra cooperation representation capacity. Empirical evaluations on the StarCraft II micromanagement challenge and level-based foraging (LBF) show that CMQ achieves superior performance compared with the state-of-the-art counterparts. The results also demonstrate that CMQ provides more cooperation concept representation capturing meaningful cooperation modes, and supports test-time concept interventions for detecting potential biases of cooperation mode and identifying spurious artifacts that impact cooperation.
<div id='section'>Paperid: <span id='pid'>318, <a href='https://arxiv.org/pdf/2507.18867.pdf' target='_blank'>https://arxiv.org/pdf/2507.18867.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuefei Wu, Xiao Yin, Yuanyang Zhu, Chunlin Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.18867">Learning Individual Intrinsic Reward in Multi-Agent Reinforcement Learning via Incorporating Generalized Human Expertise</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Efficient exploration in multi-agent reinforcement learning (MARL) is a challenging problem when receiving only a team reward, especially in environments with sparse rewards. A powerful method to mitigate this issue involves crafting dense individual rewards to guide the agents toward efficient exploration. However, individual rewards generally rely on manually engineered shaping-reward functions that lack high-order intelligence, thus it behaves ineffectively than humans regarding learning and generalization in complex problems. To tackle these issues, we combine the above two paradigms and propose a novel framework, LIGHT (Learning Individual Intrinsic reward via Incorporating Generalized Human experTise), which can integrate human knowledge into MARL algorithms in an end-to-end manner. LIGHT guides each agent to avoid unnecessary exploration by considering both individual action distribution and human expertise preference distribution. Then, LIGHT designs individual intrinsic rewards for each agent based on actionable representational transformation relevant to Q-learning so that the agents align their action preferences with the human expertise while maximizing the joint action value. Experimental results demonstrate the superiority of our method over representative baselines regarding performance and better knowledge reusability across different sparse-reward tasks on challenging scenarios.
<div id='section'>Paperid: <span id='pid'>319, <a href='https://arxiv.org/pdf/2507.16249.pdf' target='_blank'>https://arxiv.org/pdf/2507.16249.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Srivatsan Krishnan, Jason Jabbour, Dan Zhang, Natasha Jaques, Aleksandra Faust, Shayegan Omidshafiei, Vijay Janapa Reddi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.16249">Multi-Agent Reinforcement Learning for Sample-Efficient Deep Neural Network Mapping</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Mapping deep neural networks (DNNs) to hardware is critical for optimizing latency, energy consumption, and resource utilization, making it a cornerstone of high-performance accelerator design. Due to the vast and complex mapping space, reinforcement learning (RL) has emerged as a promising approach-but its effectiveness is often limited by sample inefficiency. We present a decentralized multi-agent reinforcement learning (MARL) framework designed to overcome this challenge. By distributing the search across multiple agents, our framework accelerates exploration. To avoid inefficiencies from training multiple agents in parallel, we introduce an agent clustering algorithm that assigns similar mapping parameters to the same agents based on correlation analysis. This enables a decentralized, parallelized learning process that significantly improves sample efficiency. Experimental results show our MARL approach improves sample efficiency by 30-300x over standard single-agent RL, achieving up to 32.61x latency reduction and 16.45x energy-delay product (EDP) reduction under iso-sample conditions.
<div id='section'>Paperid: <span id='pid'>320, <a href='https://arxiv.org/pdf/2507.15587.pdf' target='_blank'>https://arxiv.org/pdf/2507.15587.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yinsong Chen, Kaifeng Wang, Xiaoqiang Meng, Xueyuan Li, Zirui Li, Xin Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.15587">Red-Team Multi-Agent Reinforcement Learning for Emergency Braking Scenario</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current research on decision-making in safety-critical scenarios often relies on inefficient data-driven scenario generation or specific modeling approaches, which fail to capture corner cases in real-world contexts. To address this issue, we propose a Red-Team Multi-Agent Reinforcement Learning framework, where background vehicles with interference capabilities are treated as red-team agents. Through active interference and exploration, red-team vehicles can uncover corner cases outside the data distribution. The framework uses a Constraint Graph Representation Markov Decision Process, ensuring that red-team vehicles comply with safety rules while continuously disrupting the autonomous vehicles (AVs). A policy threat zone model is constructed to quantify the threat posed by red-team vehicles to AVs, inducing more extreme actions to increase the danger level of the scenario. Experimental results show that the proposed framework significantly impacts AVs decision-making safety and generates various corner cases. This method also offers a novel direction for research in safety-critical scenarios.
<div id='section'>Paperid: <span id='pid'>321, <a href='https://arxiv.org/pdf/2507.09989.pdf' target='_blank'>https://arxiv.org/pdf/2507.09989.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaoyang Yu, Youfang Lin, Shuo Wang, Sheng Han
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.09989">Improving monotonic optimization in heterogeneous multi-agent reinforcement learning with optimal marginal deterministic policy gradient</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In heterogeneous multi-agent reinforcement learning (MARL), achieving monotonic improvement plays a pivotal role in enhancing performance. The HAPPO algorithm proposes a feasible solution by introducing a sequential update scheme, which requires independent learning with No Parameter-sharing (NoPS). However, heterogeneous MARL generally requires Partial Parameter-sharing (ParPS) based on agent grouping to achieve high cooperative performance. Our experiments prove that directly combining ParPS with the sequential update scheme leads to the policy updating baseline drift problem, thereby failing to achieve improvement. To solve the conflict between monotonic improvement and ParPS, we propose the Optimal Marginal Deterministic Policy Gradient (OMDPG) algorithm. First, we replace the sequentially computed $Q_Ï^s(s,a_{1:i})$ with the Optimal Marginal Q (OMQ) function $Ï_Ï^*(s,a_{1:i})$ derived from Q-functions. This maintains MAAD's monotonic improvement while eliminating the conflict through optimal joint action sequences instead of sequential policy ratio calculations. Second, we introduce the Generalized Q Critic (GQC) as the critic function, employing pessimistic uncertainty-constrained loss to optimize different Q-value estimations. This provides the required Q-values for OMQ computation and stable baselines for actor updates. Finally, we implement a Centralized Critic Grouped Actor (CCGA) architecture that simultaneously achieves ParPS in local policy networks and accurate global Q-function computation. Experimental results in SMAC and MAMuJoCo environments demonstrate that OMDPG outperforms various state-of-the-art MARL baselines.
<div id='section'>Paperid: <span id='pid'>322, <a href='https://arxiv.org/pdf/2506.09390.pdf' target='_blank'>https://arxiv.org/pdf/2506.09390.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kehan Zheng, Jinfeng Zhou, Hongning Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.09390">Beyond Nash Equilibrium: Bounded Rationality of LLMs and humans in Strategic Decision-making</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large language models are increasingly used in strategic decision-making settings, yet evidence shows that, like humans, they often deviate from full rationality. In this study, we compare LLMs and humans using experimental paradigms directly adapted from behavioral game-theory research. We focus on two well-studied strategic games, Rock-Paper-Scissors and the Prisoner's Dilemma, which are well known for revealing systematic departures from rational play in human subjects. By placing LLMs in identical experimental conditions, we evaluate whether their behaviors exhibit the bounded rationality characteristic of humans. Our findings show that LLMs reproduce familiar human heuristics, such as outcome-based strategy switching and increased cooperation when future interaction is possible, but they apply these rules more rigidly and demonstrate weaker sensitivity to the dynamic changes in the game environment. Model-level analyses reveal distinctive architectural signatures in strategic behavior, and even reasoning models sometimes struggle to find effective strategies in adaptive situations. These results indicate that current LLMs capture only a partial form of human-like bounded rationality and highlight the need for training methods that encourage flexible opponent modeling and stronger context awareness.
<div id='section'>Paperid: <span id='pid'>323, <a href='https://arxiv.org/pdf/2506.06179.pdf' target='_blank'>https://arxiv.org/pdf/2506.06179.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Muhammed Ustaomeroglu, Guannan Qu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.06179">A Theoretical Study of (Hyper) Self-Attention through the Lens of Interactions: Representation, Training, Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Self-attention has emerged as a core component of modern neural architectures, yet its theoretical underpinnings remain elusive. In this paper, we study self-attention through the lens of interacting entities, ranging from agents in multi-agent reinforcement learning to alleles in genetic sequences, and show that a single layer linear self-attention can efficiently represent, learn, and generalize functions capturing pairwise interactions, including out-of-distribution scenarios. Our analysis reveals that self-attention acts as a mutual interaction learner under minimal assumptions on the diversity of interaction patterns observed during training, thereby encompassing a wide variety of real-world domains. In addition, we validate our theoretical insights through experiments demonstrating that self-attention learns interaction functions and generalizes across both population distributions and out-of-distribution scenarios. Building on our theories, we introduce HyperFeatureAttention, a novel neural network module designed to learn couplings of different feature-level interactions between entities. Furthermore, we propose HyperAttention, a new module that extends beyond pairwise interactions to capture multi-entity dependencies, such as three-way, four-way, or general n-way interactions.
<div id='section'>Paperid: <span id='pid'>324, <a href='https://arxiv.org/pdf/2505.08448.pdf' target='_blank'>https://arxiv.org/pdf/2505.08448.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yanggang Xu, Weijie Hong, Jirong Zha, Geng Chen, Jianfeng Zheng, Chen-Chun Hsia, Xinlei Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.08448">Scalable UAV Multi-Hop Networking via Multi-Agent Reinforcement Learning with Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In disaster scenarios, establishing robust emergency communication networks is critical, and unmanned aerial vehicles (UAVs) offer a promising solution to rapidly restore connectivity. However, organizing UAVs to form multi-hop networks in large-scale dynamic environments presents significant challenges, including limitations in algorithmic scalability and the vast exploration space required for coordinated decision-making. To address these issues, we propose MRLMN, a novel framework that integrates multi-agent reinforcement learning (MARL) and large language models (LLMs) to jointly optimize UAV agents toward achieving optimal networking performance. The framework incorporates a grouping strategy with reward decomposition to enhance algorithmic scalability and balance decision-making across UAVs. In addition, behavioral constraints are applied to selected key UAVs to improve the robustness of the network. Furthermore, the framework integrates LLM agents, leveraging knowledge distillation to transfer their high-level decision-making capabilities to MARL agents. This enhances both the efficiency of exploration and the overall training process. In the distillation module, a Hungarian algorithm-based matching scheme is applied to align the decision outputs of the LLM and MARL agents and define the distillation loss. Extensive simulation results validate the effectiveness of our approach, demonstrating significant improvements in network performance, including enhanced coverage and communication quality.
<div id='section'>Paperid: <span id='pid'>325, <a href='https://arxiv.org/pdf/2504.11569.pdf' target='_blank'>https://arxiv.org/pdf/2504.11569.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Heming Fu, Guojun Xiong, Jian Li, Shan Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.11569">Multi-Agent Reinforcement Learning for Decentralized Reservoir Management via Murmuration Intelligence</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Conventional centralized water management systems face critical limitations from computational complexity and uncertainty propagation. We present MurmuRL, a novel decentralized framework inspired by starling murmurations intelligence, integrating bio-inspired alignment, separation, and cohesion rules with multi-agent reinforcement learning. MurmuRL enables individual reservoirs to make autonomous local decisions while achieving emergent global coordination. Experiments on grid networks demonstrate that MurmuRL achieves 8.8% higher final performance while using 27% less computing overhead compared to centralized approaches. Notably, strategic diversity scales super-linearly with system size, exhibiting sophisticated coordination patterns and enhanced resilience during extreme events. MurmuRL offers a scalable solution for managing complex water systems by leveraging principles of natural collective behavior.
<div id='section'>Paperid: <span id='pid'>326, <a href='https://arxiv.org/pdf/2503.21807.pdf' target='_blank'>https://arxiv.org/pdf/2503.21807.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuan Wei, Xiaohan Shan, Jianmin Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.21807">LERO: LLM-driven Evolutionary framework with Hybrid Rewards and Enhanced Observation for Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent reinforcement learning (MARL) faces two critical bottlenecks distinct from single-agent RL: credit assignment in cooperative tasks and partial observability of environmental states. We propose LERO, a framework integrating Large language models (LLMs) with evolutionary optimization to address these MARL-specific challenges. The solution centers on two LLM-generated components: a hybrid reward function that dynamically allocates individual credit through reward decomposition, and an observation enhancement function that augments partial observations with inferred environmental context. An evolutionary algorithm optimizes these components through iterative MARL training cycles, where top-performing candidates guide subsequent LLM generations. Evaluations in Multi-Agent Particle Environments (MPE) demonstrate LERO's superiority over baseline methods, with improved task performance and training efficiency.
<div id='section'>Paperid: <span id='pid'>327, <a href='https://arxiv.org/pdf/2502.09780.pdf' target='_blank'>https://arxiv.org/pdf/2502.09780.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tong Yang, Bo Dai, Lin Xiao, Yuejie Chi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.09780">Incentivize without Bonus: Provably Efficient Model-based Online Multi-agent RL for Markov Games</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent reinforcement learning (MARL) lies at the heart of a plethora of applications involving the interaction of a group of agents in a shared unknown environment. A prominent framework for studying MARL is Markov games, with the goal of finding various notions of equilibria in a sample-efficient manner, such as the Nash equilibrium (NE) and the coarse correlated equilibrium (CCE). However, existing sample-efficient approaches either require tailored uncertainty estimation under function approximation, or careful coordination of the players. In this paper, we propose a novel model-based algorithm, called VMG, that incentivizes exploration via biasing the empirical estimate of the model parameters towards those with a higher collective best-response values of all the players when fixing the other players' policies, thus encouraging the policy to deviate from its current equilibrium for more exploration. VMG is oblivious to different forms of function approximation, and permits simultaneous and uncoupled policy updates of all players. Theoretically, we also establish that VMG achieves a near-optimal regret for finding both the NEs of two-player zero-sum Markov games and CCEs of multi-player general-sum Markov games under linear function approximation in an online environment, which nearly match their counterparts with sophisticated uncertainty quantification.
<div id='section'>Paperid: <span id='pid'>328, <a href='https://arxiv.org/pdf/2501.15695.pdf' target='_blank'>https://arxiv.org/pdf/2501.15695.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hung Du, Srikanth Thudumu, Hy Nguyen, Rajesh Vasa, Kon Mouzakis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.15695">Contextual Knowledge Sharing in Multi-Agent Reinforcement Learning with Decentralized Communication and Coordination</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Decentralized Multi-Agent Reinforcement Learning (Dec-MARL) has emerged as a pivotal approach for addressing complex tasks in dynamic environments. Existing Multi-Agent Reinforcement Learning (MARL) methodologies typically assume a shared objective among agents and rely on centralized control. However, many real-world scenarios feature agents with individual goals and limited observability of other agents, complicating coordination and hindering adaptability. Existing Dec-MARL strategies prioritize either communication or coordination, lacking an integrated approach that leverages both. This paper presents a novel Dec-MARL framework that integrates peer-to-peer communication and coordination, incorporating goal-awareness and time-awareness into the agents' knowledge-sharing processes. Our framework equips agents with the ability to (i) share contextually relevant knowledge to assist other agents, and (ii) reason based on information acquired from multiple agents, while considering their own goals and the temporal context of prior knowledge. We evaluate our approach through several complex multi-agent tasks in environments with dynamically appearing obstacles. Our work demonstrates that incorporating goal-aware and time-aware knowledge sharing significantly enhances overall performance.
<div id='section'>Paperid: <span id='pid'>329, <a href='https://arxiv.org/pdf/2410.15221.pdf' target='_blank'>https://arxiv.org/pdf/2410.15221.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Vindula Jayawardana, Baptiste Freydt, Ao Qu, Cameron Hickert, Zhongxia Yan, Cathy Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.15221">IntersectionZoo: Eco-driving for Benchmarking Multi-Agent Contextual Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite the popularity of multi-agent reinforcement learning (RL) in simulated and two-player applications, its success in messy real-world applications has been limited. A key challenge lies in its generalizability across problem variations, a common necessity for many real-world problems. Contextual reinforcement learning (CRL) formalizes learning policies that generalize across problem variations. However, the lack of standardized benchmarks for multi-agent CRL has hindered progress in the field. Such benchmarks are desired to be based on real-world applications to naturally capture the many open challenges of real-world problems that affect generalization. To bridge this gap, we propose IntersectionZoo, a comprehensive benchmark suite for multi-agent CRL through the real-world application of cooperative eco-driving in urban road networks. The task of cooperative eco-driving is to control a fleet of vehicles to reduce fleet-level vehicular emissions. By grounding IntersectionZoo in a real-world application, we naturally capture real-world problem characteristics, such as partial observability and multiple competing objectives. IntersectionZoo is built on data-informed simulations of 16,334 signalized intersections derived from 10 major US cities, modeled in an open-source industry-grade microscopic traffic simulator. By modeling factors affecting vehicular exhaust emissions (e.g., temperature, road conditions, travel demand), IntersectionZoo provides one million data-driven traffic scenarios. Using these traffic scenarios, we benchmark popular multi-agent RL and human-like driving algorithms and demonstrate that the popular multi-agent RL algorithms struggle to generalize in CRL settings.
<div id='section'>Paperid: <span id='pid'>330, <a href='https://arxiv.org/pdf/2410.03997.pdf' target='_blank'>https://arxiv.org/pdf/2410.03997.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuan Zhuang, Yi Shen, Zhili Zhang, Yuxiao Chen, Fei Miao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.03997">YOLO-MARL: You Only LLM Once for Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Advancements in deep multi-agent reinforcement learning (MARL) have positioned it as a promising approach for decision-making in cooperative games. However, it still remains challenging for MARL agents to learn cooperative strategies for some game environments. Recently, large language models (LLMs) have demonstrated emergent reasoning capabilities, making them promising candidates for enhancing coordination among the agents. However, due to the model size of LLMs, it can be expensive to frequently infer LLMs for actions that agents can take. In this work, we propose You Only LLM Once for MARL (YOLO-MARL), a novel framework that leverages the high-level task planning capabilities of LLMs to improve the policy learning process of multi-agents in cooperative games. Notably, for each game environment, YOLO-MARL only requires one time interaction with LLMs in the proposed strategy generation, state interpretation and planning function generation modules, before the MARL policy training process. This avoids the ongoing costs and computational time associated with frequent LLMs API calls during training. Moreover, trained decentralized policies based on normal-sized neural networks operate independently of the LLM. We evaluate our method across two different environments and demonstrate that YOLO-MARL outperforms traditional MARL algorithms.
<div id='section'>Paperid: <span id='pid'>331, <a href='https://arxiv.org/pdf/2409.17348.pdf' target='_blank'>https://arxiv.org/pdf/2409.17348.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Huao Li, Hossein Nourkhiz Mahjoub, Behdad Chalaki, Vaishnav Tadiparthi, Kwonjoon Lee, Ehsan Moradi-Pari, Charles Michael Lewis, Katia P Sycara
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.17348">Language Grounded Multi-agent Reinforcement Learning with Human-interpretable Communication</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-Agent Reinforcement Learning (MARL) methods have shown promise in enabling agents to learn a shared communication protocol from scratch and accomplish challenging team tasks. However, the learned language is usually not interpretable to humans or other agents not co-trained together, limiting its applicability in ad-hoc teamwork scenarios. In this work, we propose a novel computational pipeline that aligns the communication space between MARL agents with an embedding space of human natural language by grounding agent communications on synthetic data generated by embodied Large Language Models (LLMs) in interactive teamwork scenarios. Our results demonstrate that introducing language grounding not only maintains task performance but also accelerates the emergence of communication. Furthermore, the learned communication protocols exhibit zero-shot generalization capabilities in ad-hoc teamwork scenarios with unseen teammates and novel task states. This work presents a significant step toward enabling effective communication and collaboration between artificial agents and humans in real-world teamwork settings.
<div id='section'>Paperid: <span id='pid'>332, <a href='https://arxiv.org/pdf/2409.07127.pdf' target='_blank'>https://arxiv.org/pdf/2409.07127.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dongkun Huo, Huateng Zhang, Yixue Hao, Yuanlin Ye, Long Hu, Rui Wang, Min Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.07127">DCMAC: Demand-aware Customized Multi-Agent Communication via Upper Bound Training</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Efficient communication can enhance the overall performance of collaborative multi-agent reinforcement learning. A common approach is to share observations through full communication, leading to significant communication overhead. Existing work attempts to perceive the global state by conducting teammate model based on local information. However, they ignore that the uncertainty generated by prediction may lead to difficult training. To address this problem, we propose a Demand-aware Customized Multi-Agent Communication (DCMAC) protocol, which use an upper bound training to obtain the ideal policy. By utilizing the demand parsing module, agent can interpret the gain of sending local message on teammate, and generate customized messages via compute the correlation between demands and local observation using cross-attention mechanism. Moreover, our method can adapt to the communication resources of agents and accelerate the training progress by appropriating the ideal policy which is trained with joint observation. Experimental results reveal that DCMAC significantly outperforms the baseline algorithms in both unconstrained and communication constrained scenarios.
<div id='section'>Paperid: <span id='pid'>333, <a href='https://arxiv.org/pdf/2601.11401.pdf' target='_blank'>https://arxiv.org/pdf/2601.11401.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ahmed Rashwan, Keith Briggs, Chris Budd, Lisa Kreusser
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.11401">Factored Value Functions for Graph-Based Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Credit assignment is a core challenge in multi-agent reinforcement learning (MARL), especially in large-scale systems with structured, local interactions. Graph-based Markov decision processes (GMDPs) capture such settings via an influence graph, but standard critics are poorly aligned with this structure: global value functions provide weak per-agent learning signals, while existing local constructions can be difficult to estimate and ill-behaved in infinite-horizon settings. We introduce the Diffusion Value Function (DVF), a factored value function for GMDPs that assigns to each agent a value component by diffusing rewards over the influence graph with temporal discounting and spatial attenuation. We show that DVF is well-defined, admits a Bellman fixed point, and decomposes the global discounted value via an averaging property. DVF can be used as a drop-in critic in standard RL algorithms and estimated scalably with graph neural networks. Building on DVF, we propose Diffusion A2C (DA2C) and a sparse message-passing actor, Learned DropEdge GNN (LD-GNN), for learning decentralised algorithms under communication costs. Across the firefighting benchmark and three distributed computation tasks (vector graph colouring and two transmit power optimisation problems), DA2C consistently outperforms local and global critic baselines, improving average reward by up to 11%.
<div id='section'>Paperid: <span id='pid'>334, <a href='https://arxiv.org/pdf/2601.07463.pdf' target='_blank'>https://arxiv.org/pdf/2601.07463.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sijia li, Xinran Li, Shibo Chen, Jun Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.07463">Puzzle it Out: Local-to-Global World Model for Offline Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Offline multi-agent reinforcement learning (MARL) aims to solve cooperative decision-making problems in multi-agent systems using pre-collected datasets. Existing offline MARL methods primarily constrain training within the dataset distribution, resulting in overly conservative policies that struggle to generalize beyond the support of the data. While model-based approaches offer a promising solution by expanding the original dataset with synthetic data generated from a learned world model, the high dimensionality, non-stationarity, and complexity of multi-agent systems make it challenging to accurately estimate the transitions and reward functions in offline MARL. Given the difficulty of directly modeling joint dynamics, we propose a local-to-global (LOGO) world model, a novel framework that leverages local predictions-which are easier to estimate-to infer global state dynamics, thus improving prediction accuracy while implicitly capturing agent-wise dependencies. Using the trained world model, we generate synthetic data to augment the original dataset, expanding the effective state-action space. To ensure reliable policy learning, we further introduce an uncertainty-aware sampling mechanism that adaptively weights synthetic data by prediction uncertainty, reducing approximation error propagation to policies. In contrast to conventional ensemble-based methods, our approach requires only an additional encoder for uncertainty estimation, significantly reducing computational overhead while maintaining accuracy. Extensive experiments across 8 scenarios against 8 baselines demonstrate that our method surpasses state-of-the-art baselines on standard offline MARL benchmarks, establishing a new model-based baseline for generalizable offline multi-agent learning.
<div id='section'>Paperid: <span id='pid'>335, <a href='https://arxiv.org/pdf/2601.07122.pdf' target='_blank'>https://arxiv.org/pdf/2601.07122.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yixiao Peng, Hao Hu, Feiyang Li, Xinye Cao, Yingchang Jiang, Jipeng Tang, Guoshun Nan, Yuling Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.07122">Enhancing Cloud Network Resilience via a Robust LLM-Empowered Multi-Agent Reinforcement Learning Framework</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While virtualization and resource pooling empower cloud networks with structural flexibility and elastic scalability, they inevitably expand the attack surface and challenge cyber resilience. Reinforcement Learning (RL)-based defense strategies have been developed to optimize resource deployment and isolation policies under adversarial conditions, aiming to enhance system resilience by maintaining and restoring network availability. However, existing approaches lack robustness as they require retraining to adapt to dynamic changes in network structure, node scale, attack strategies, and attack intensity. Furthermore, the lack of Human-in-the-Loop (HITL) support limits interpretability and flexibility. To address these limitations, we propose CyberOps-Bots, a hierarchical multi-agent reinforcement learning framework empowered by Large Language Models (LLMs). Inspired by MITRE ATT&CK's Tactics-Techniques model, CyberOps-Bots features a two-layer architecture: (1) An upper-level LLM agent with four modules--ReAct planning, IPDRR-based perception, long-short term memory, and action/tool integration--performs global awareness, human intent recognition, and tactical planning; (2) Lower-level RL agents, developed via heterogeneous separated pre-training, execute atomic defense actions within localized network regions. This synergy preserves LLM adaptability and interpretability while ensuring reliable RL execution. Experiments on real cloud datasets show that, compared to state-of-the-art algorithms, CyberOps-Bots maintains network availability 68.5% higher and achieves a 34.7% jumpstart performance gain when shifting the scenarios without retraining. To our knowledge, this is the first study to establish a robust LLM-RL framework with HITL support for cloud defense. We will release our framework to the community, facilitating the advancement of robust and autonomous defense in cloud networks.
<div id='section'>Paperid: <span id='pid'>336, <a href='https://arxiv.org/pdf/2511.18671.pdf' target='_blank'>https://arxiv.org/pdf/2511.18671.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yan Wang, Ke Deng, Yongli Ren
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.18671">Multi-Agent Cross-Entropy Method with Monotonic Nonlinear Critic Decomposition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cooperative multi-agent reinforcement learning (MARL) commonly adopts centralized training with decentralized execution (CTDE), where centralized critics leverage global information to guide decentralized actors. However, centralized-decentralized mismatch (CDM) arises when the suboptimal behavior of one agent degrades others' learning. Prior approaches mitigate CDM through value decomposition, but linear decompositions allow per-agent gradients at the cost of limited expressiveness, while nonlinear decompositions improve representation but require centralized gradients, reintroducing CDM. To overcome this trade-off, we propose the multi-agent cross-entropy method (MCEM), combined with monotonic nonlinear critic decomposition (NCD). MCEM updates policies by increasing the probability of high-value joint actions, thereby excluding suboptimal behaviors. For sample efficiency, we extend off-policy learning with a modified k-step return and Retrace. Analysis and experiments demonstrate that MCEM outperforms state-of-the-art methods across both continuous and discrete action benchmarks.
<div id='section'>Paperid: <span id='pid'>337, <a href='https://arxiv.org/pdf/2511.10501.pdf' target='_blank'>https://arxiv.org/pdf/2511.10501.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Georgios Chalkiadakis, Charilaos Akasiadis, Gerasimos Koresis, Stergios Plataniotis, Leonidas Bakopoulos
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.10501">Strategic Opponent Modeling with Graph Neural Networks, Deep Reinforcement Learning and Probabilistic Topic Modeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper provides a comprehensive review of mainly Graph Neural Networks, Deep Reinforcement Learning, and Probabilistic Topic Modeling methods with a focus on their potential incorporation in strategic multiagent settings. We draw interest in (i) Machine Learning methods currently utilized for uncovering unknown model structures adaptable to the task of strategic opponent modeling, and (ii) the integration of these methods with Game Theoretic concepts that avoid relying on assumptions often invalid in real-world scenarios, such as the Common Prior Assumption (CPA) and the Self-Interest Hypothesis (SIH). We analyze the ability to handle uncertainty and heterogeneity, two characteristics that are very common in real-world application cases, as well as scalability. As a potential answer to effectively modeling relationships and interactions in multiagent settings, we champion the use of Graph Neural Networks (GNN). Such approaches are designed to operate upon graph-structured data, and have been shown to be a very powerful tool for performing tasks such as node classification and link prediction. Next, we review the domain of Reinforcement Learning (RL), and in particular that of Multiagent Deep Reinforcement Learning (MADRL). Following, we describe existing relevant game theoretic solution concepts and consider properties such as fairness and stability. Our review comes complete with a note on the literature that utilizes PTM in domains other than that of document analysis and classification. The capability of PTM to estimate unknown underlying distributions can help with tackling heterogeneity and unknown agent beliefs. Finally, we identify certain open challenges specifically, the need to (i) fit non-stationary environments, (ii) balance the degrees of stability and adaptation, (iii) tackle uncertainty and heterogeneity, (iv) guarantee scalability and solution tractability.
<div id='section'>Paperid: <span id='pid'>338, <a href='https://arxiv.org/pdf/2509.22216.pdf' target='_blank'>https://arxiv.org/pdf/2509.22216.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ahmet Onur Akman, Anastasia Psarou, ZoltÃ¡n GyÃ¶rgy Varga, Grzegorz JamrÃ³z, RafaÅ Kucharski
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.22216">Impact of Collective Behaviors of Autonomous Vehicles on Urban Traffic Dynamics: A Multi-Agent Reinforcement Learning Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This study examines the potential impact of reinforcement learning (RL)-enabled autonomous vehicles (AV) on urban traffic flow in a mixed traffic environment. We focus on a simplified day-to-day route choice problem in a multi-agent setting. We consider a city network where human drivers travel through their chosen routes to reach their destinations in minimum travel time. Then, we convert one-third of the population into AVs, which are RL agents employing Deep Q-learning algorithm. We define a set of optimization targets, or as we call them behaviors, namely selfish, collaborative, competitive, social, altruistic, and malicious. We impose a selected behavior on AVs through their rewards. We run our simulations using our in-house developed RL framework PARCOUR. Our simulations reveal that AVs optimize their travel times by up to 5\%, with varying impacts on human drivers' travel times depending on the AV behavior. In all cases where AVs adopt a self-serving behavior, they achieve shorter travel times than human drivers. Our findings highlight the complexity differences in learning tasks of each target behavior. We demonstrate that the multi-agent RL setting is applicable for collective routing on traffic networks, though their impact on coexisting parties greatly varies with the behaviors adopted.
<div id='section'>Paperid: <span id='pid'>339, <a href='https://arxiv.org/pdf/2509.20766.pdf' target='_blank'>https://arxiv.org/pdf/2509.20766.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gawon Lee, Daesol Cho, H. Jin Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.20766">Leveraging Temporally Extended Behavior Sharing for Multi-task Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-task reinforcement learning (MTRL) offers a promising approach to improve sample efficiency and generalization by training agents across multiple tasks, enabling knowledge sharing between them. However, applying MTRL to robotics remains challenging due to the high cost of collecting diverse task data. To address this, we propose MT-LÃ©vy, a novel exploration strategy that enhances sample efficiency in MTRL environments by combining behavior sharing across tasks with temporally extended exploration inspired by LÃ©vy flight. MT-LÃ©vy leverages policies trained on related tasks to guide exploration towards key states, while dynamically adjusting exploration levels based on task success ratios. This approach enables more efficient state-space coverage, even in complex robotics environments. Empirical results demonstrate that MT-LÃ©vy significantly improves exploration and sample efficiency, supported by quantitative and qualitative analyses. Ablation studies further highlight the contribution of each component, showing that combining behavior sharing with adaptive exploration strategies can significantly improve the practicality of MTRL in robotics applications.
<div id='section'>Paperid: <span id='pid'>340, <a href='https://arxiv.org/pdf/2508.20018.pdf' target='_blank'>https://arxiv.org/pdf/2508.20018.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Quanfeng Lu, Zhantao Ma, Shuai Zhong, Jin Wang, Dahai Yu, Michael K. Ng, Ping Luo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.20018">SWIRL: A Staged Workflow for Interleaved Reinforcement Learning in Mobile GUI Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid advancement of large vision language models (LVLMs) and agent systems has heightened interest in mobile GUI agents that can reliably translate natural language into interface operations. Existing single-agent approaches, however, remain limited by structural constraints. Although multi-agent systems naturally decouple different competencies, recent progress in multi-agent reinforcement learning (MARL) has often been hindered by inefficiency and remains incompatible with current LVLM architectures. To address these challenges, we introduce SWIRL, a staged workflow for interleaved reinforcement learning designed for multi-agent systems. SWIRL reformulates MARL into a sequence of single-agent reinforcement learning tasks, updating one agent at a time while keeping the others fixed. This formulation enables stable training and promotes efficient coordination across agents. Theoretically, we provide a stepwise safety bound, a cross-round monotonic improvement theorem, and convergence guarantees on return, ensuring robust and principled optimization. In application to mobile GUI control, SWIRL instantiates a Navigator that converts language and screen context into structured plans, and an Interactor that grounds these plans into executable atomic actions. Extensive experiments demonstrate superior performance on both high-level and low-level GUI benchmarks. Beyond GUI tasks, SWIRL also demonstrates strong capability in multi-agent mathematical reasoning, underscoring its potential as a general framework for developing efficient and robust multi-agent systems.
<div id='section'>Paperid: <span id='pid'>341, <a href='https://arxiv.org/pdf/2508.10253.pdf' target='_blank'>https://arxiv.org/pdf/2508.10253.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Guanzi Yao, Heyao Liu, Linyan Dai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.10253">Multi-Agent Reinforcement Learning for Adaptive Resource Orchestration in Cloud-Native Clusters</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper addresses the challenges of high resource dynamism and scheduling complexity in cloud-native database systems. It proposes an adaptive resource orchestration method based on multi-agent reinforcement learning. The method introduces a heterogeneous role-based agent modeling mechanism. This allows different resource entities, such as compute nodes, storage nodes, and schedulers, to adopt distinct policy representations. These agents are better able to reflect diverse functional responsibilities and local environmental characteristics within the system. A reward-shaping mechanism is designed to integrate local observations with global feedback. This helps mitigate policy learning bias caused by incomplete state observations. By combining real-time local performance signals with global system value estimation, the mechanism improves coordination among agents and enhances policy convergence stability. A unified multi-agent training framework is developed and evaluated on a representative production scheduling dataset. Experimental results show that the proposed method outperforms traditional approaches across multiple key metrics. These include resource utilization, scheduling latency, policy convergence speed, system stability, and fairness. The results demonstrate strong generalization and practical utility. Across various experimental scenarios, the method proves effective in handling orchestration tasks with high concurrency, high-dimensional state spaces, and complex dependency relationships. This confirms its advantages in real-world, large-scale scheduling environments.
<div id='section'>Paperid: <span id='pid'>342, <a href='https://arxiv.org/pdf/2508.04811.pdf' target='_blank'>https://arxiv.org/pdf/2508.04811.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lin Jiang, Yu Yang, Guang Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.04811">HCRide: Harmonizing Passenger Fairness and Driver Preference for Human-Centered Ride-Hailing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Order dispatch systems play a vital role in ride-hailing services, which directly influence operator revenue, driver profit, and passenger experience. Most existing work focuses on improving system efficiency in terms of operator revenue, which may cause a bad experience for both passengers and drivers. Hence, in this work, we aim to design a human-centered ride-hailing system by considering both passenger fairness and driver preference without compromising the overall system efficiency. However, it is nontrivial to achieve this target due to the potential conflicts between passenger fairness and driver preference since optimizing one may sacrifice the other. To address this challenge, we design HCRide, a Human-Centered Ride-hailing system based on a novel multi-agent reinforcement learning algorithm called Harmonization-oriented Actor-Bi-Critic (Habic), which includes three major components (i.e., a multi-agent competition mechanism, a dynamic Actor network, and a Bi-Critic network) to optimize system efficiency and passenger fairness with driver preference consideration. We extensively evaluate our HCRide using two real-world ride-hailing datasets from Shenzhen and New York City. Experimental results show our HCRide effectively improves system efficiency by 2.02%, fairness by 5.39%, and driver preference by 10.21% compared to state-of-the-art baselines.
<div id='section'>Paperid: <span id='pid'>343, <a href='https://arxiv.org/pdf/2505.12811.pdf' target='_blank'>https://arxiv.org/pdf/2505.12811.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wei-Chen Liao, Ti-Rong Wu, I-Chen Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.12811">Dynamic Sight Range Selection in Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent reinforcement Learning (MARL) is often challenged by the sight range dilemma, where agents either receive insufficient or excessive information from their environment. In this paper, we propose a novel method, called Dynamic Sight Range Selection (DSR), to address this issue. DSR utilizes an Upper Confidence Bound (UCB) algorithm and dynamically adjusts the sight range during training. Experiment results show several advantages of using DSR. First, we demonstrate using DSR achieves better performance in three common MARL environments, including Level-Based Foraging (LBF), Multi-Robot Warehouse (RWARE), and StarCraft Multi-Agent Challenge (SMAC). Second, our results show that DSR consistently improves performance across multiple MARL algorithms, including QMIX and MAPPO. Third, DSR offers suitable sight ranges for different training steps, thereby accelerating the training process. Finally, DSR provides additional interpretability by indicating the optimal sight range used during training. Unlike existing methods that rely on global information or communication mechanisms, our approach operates solely based on the individual sight ranges of agents. This approach offers a practical and efficient solution to the sight range dilemma, making it broadly applicable to real-world complex environments.
<div id='section'>Paperid: <span id='pid'>344, <a href='https://arxiv.org/pdf/2505.05262.pdf' target='_blank'>https://arxiv.org/pdf/2505.05262.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Andreas Kontogiannis, Konstantinos Papathanasiou, Yi Shen, Giorgos Stamou, Michael M. Zavlanos, George Vouros
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.05262">Enhancing Cooperative Multi-Agent Reinforcement Learning with State Modelling and Adversarial Exploration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Learning to cooperate in distributed partially observable environments with no communication abilities poses significant challenges for multi-agent deep reinforcement learning (MARL). This paper addresses key concerns in this domain, focusing on inferring state representations from individual agent observations and leveraging these representations to enhance agents' exploration and collaborative task execution policies. To this end, we propose a novel state modelling framework for cooperative MARL, where agents infer meaningful belief representations of the non-observable state, with respect to optimizing their own policies, while filtering redundant and less informative joint state information. Building upon this framework, we propose the MARL SMPE algorithm. In SMPE, agents enhance their own policy's discriminative abilities under partial observability, explicitly by incorporating their beliefs into the policy network, and implicitly by adopting an adversarial type of exploration policies which encourages agents to discover novel, high-value states while improving the discriminative abilities of others. Experimentally, we show that SMPE outperforms state-of-the-art MARL algorithms in complex fully cooperative tasks from the MPE, LBF, and RWARE benchmarks.
<div id='section'>Paperid: <span id='pid'>345, <a href='https://arxiv.org/pdf/2503.11829.pdf' target='_blank'>https://arxiv.org/pdf/2503.11829.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jushan Chen, Santiago Paternain
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.11829">Learning Closed-Loop Parametric Nash Equilibria of Multi-Agent Collaborative Field Coverage</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent reinforcement learning is a challenging and active field of research due to the inherent nonstationary property and coupling between agents. A popular approach to modeling the multi-agent interactions underlying the multi-agent RL problem is the Markov Game. There is a special type of Markov Game, termed Markov Potential Game, which allows us to reduce the Markov Game to a single-objective optimal control problem where the objective function is a potential function. In this work, we prove that a multi-agent collaborative field coverage problem, which is found in many engineering applications, can be formulated as a Markov Potential Game, and we can learn a parameterized closed-loop Nash Equilibrium by solving an equivalent single-objective optimal control problem. As a result, our algorithm is 10x faster during training compared to a game-theoretic baseline and converges faster during policy execution.
<div id='section'>Paperid: <span id='pid'>346, <a href='https://arxiv.org/pdf/2503.07129.pdf' target='_blank'>https://arxiv.org/pdf/2503.07129.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Deuksin Kwon, Jiwon Hae, Emma Clift, Daniel Shamsoddini, Jonathan Gratch, Gale M. Lucas
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.07129">ASTRA: A Negotiation Agent with Adaptive and Strategic Reasoning via Tool-integrated Action for Dynamic Offer Optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Negotiation requires dynamically balancing self-interest and cooperation within the flow of conversation to maximize one's own utility. Yet, existing agents struggle due to bounded rationality in human data, low adaptability to counterpart behavior, and limited strategic reasoning. To address this, we introduce principle-driven negotiation agents, powered by ASTRA, a novel framework for turn-level offer optimization grounded in two core principles: opponent modeling and Tit-for-Tat reciprocity. ASTRA operates in three stages: (1) interpreting counterpart behavior, (2) optimizing counteroffers via a tool-integrated action with a linear programming (LP) solver, and (3) selecting offers based on strategy assessment and the partner's acceptance probability. Through simulations and human evaluations, our agent effectively adapts to an opponent's shifting stance and achieves favorable outcomes through enhanced adaptability and strategic reasoning. Beyond enhancing negotiation performance, it also serves as a powerful coaching tool, offering interpretable strategic feedback and optimal offer recommendations beyond human bounded rationality, with its potential further validated through human evaluation.
<div id='section'>Paperid: <span id='pid'>347, <a href='https://arxiv.org/pdf/2503.02077.pdf' target='_blank'>https://arxiv.org/pdf/2503.02077.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziyan Wang, Zhicheng Zhang, Fei Fang, Yali Du
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.02077">M3HF: Multi-agent Reinforcement Learning from Multi-phase Human Feedback of Mixed Quality</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Designing effective reward functions in multi-agent reinforcement learning (MARL) is a significant challenge, often leading to suboptimal or misaligned behaviors in complex, coordinated environments. We introduce Multi-agent Reinforcement Learning from Multi-phase Human Feedback of Mixed Quality ($\text{M}^3\text{HF}$), a novel framework that integrates multi-phase human feedback of mixed quality into the MARL training process. By involving humans with diverse expertise levels to provide iterative guidance, $\text{M}^3\text{HF}$ leverages both expert and non-expert feedback to continuously refine agents' policies. During training, we strategically pause agent learning for human evaluation, parse feedback using large language models to assign it appropriately and update reward functions through predefined templates and adaptive weights by using weight decay and performance-based adjustments. Our approach enables the integration of nuanced human insights across various levels of quality, enhancing the interpretability and robustness of multi-agent cooperation. Empirical results in challenging environments demonstrate that $\text{M}^3\text{HF}$ significantly outperforms state-of-the-art methods, effectively addressing the complexities of reward design in MARL and enabling broader human participation in the training process.
<div id='section'>Paperid: <span id='pid'>348, <a href='https://arxiv.org/pdf/2502.20065.pdf' target='_blank'>https://arxiv.org/pdf/2502.20065.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ahmet Onur Akman, Anastasia Psarou, Åukasz Gorczyca, ZoltÃ¡n GyÃ¶rgy Varga, Grzegorz JamrÃ³z, RafaÅ Kucharski
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.20065">RouteRL: Multi-agent reinforcement learning framework for urban route choice with autonomous vehicles</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>RouteRL is a novel framework that integrates multi-agent reinforcement learning (MARL) with a microscopic traffic simulation, facilitating the testing and development of efficient route choice strategies for autonomous vehicles (AVs). The proposed framework simulates the daily route choices of driver agents in a city, including two types: human drivers, emulated using behavioral route choice models, and AVs, modeled as MARL agents optimizing their policies for a predefined objective. RouteRL aims to advance research in MARL, transport modeling, and human-AI interaction for transportation applications. This study presents a technical report on RouteRL, outlines its potential research contributions, and showcases its impact via illustrative examples.
<div id='section'>Paperid: <span id='pid'>349, <a href='https://arxiv.org/pdf/2502.13188.pdf' target='_blank'>https://arxiv.org/pdf/2502.13188.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Anastasia Psarou, Ahmet Onur Akman, Åukasz Gorczyca, MichaÅ Hoffmann, Grzegorz JamrÃ³z, RafaÅ Kucharski
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.13188">Collaboration Between the City and Machine Learning Community is Crucial to Efficient Autonomous Vehicles Routing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autonomous vehicles (AVs), possibly using Multi-Agent Reinforcement Learning (MARL) for simultaneous route optimization, may destabilize traffic networks, with human drivers potentially experiencing longer travel times. We study this interaction by simulating human drivers and AVs. Our experiments with standard MARL algorithms reveal that, both in simplified and complex networks, policies often fail to converge to an optimal solution or require long training periods. This problem is amplified by the fact that we cannot rely entirely on simulated training, as there are no accurate models of human routing behavior. At the same time, real-world training in cities risks destabilizing urban traffic systems, increasing externalities, such as $CO_2$ emissions, and introducing non-stationarity as human drivers will adapt unpredictably to AV behaviors. In this position paper, we argue that city authorities must collaborate with the ML community to monitor and critically evaluate the routing algorithms proposed by car companies toward fair and system-efficient routing algorithms and regulatory standards.
<div id='section'>Paperid: <span id='pid'>350, <a href='https://arxiv.org/pdf/2502.07635.pdf' target='_blank'>https://arxiv.org/pdf/2502.07635.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Guilherme S. Varela, Alberto Sardinha, Francisco S. Melo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.07635">Distributed Value Decomposition Networks with Networked Agents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We investigate the problem of distributed training under partial observability, whereby cooperative multi-agent reinforcement learning agents (MARL) maximize the expected cumulative joint reward. We propose distributed value decomposition networks (DVDN) that generate a joint Q-function that factorizes into agent-wise Q-functions. Whereas the original value decomposition networks rely on centralized training, our approach is suitable for domains where centralized training is not possible and agents must learn by interacting with the physical environment in a decentralized manner while communicating with their peers. DVDN overcomes the need for centralized training by locally estimating the shared objective. We contribute with two innovative algorithms, DVDN and DVDN (GT), for the heterogeneous and homogeneous agents settings respectively. Empirically, both algorithms approximate the performance of value decomposition networks, in spite of the information loss during communication, as demonstrated in ten MARL tasks in three standard environments.
<div id='section'>Paperid: <span id='pid'>351, <a href='https://arxiv.org/pdf/2501.14451.pdf' target='_blank'>https://arxiv.org/pdf/2501.14451.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Linfeng Liang, Xi Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.14451">MARL-OT: Multi-Agent Reinforcement Learning Guided Online Fuzzing to Detect Safety Violation in Autonomous Driving Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autonomous Driving Systems (ADSs) are safety-critical, as real-world safety violations can result in significant losses. Rigorous testing is essential before deployment, with simulation testing playing a key role. However, ADSs are typically complex, consisting of multiple modules such as perception and planning, or well-trained end-to-end autonomous driving systems. Offline methods, such as the Genetic Algorithm (GA), can only generate predefined trajectories for dynamics, which struggle to cause safety violations for ADSs rapidly and efficiently in different scenarios due to their evolutionary nature. Online methods, such as single-agent reinforcement learning (RL), can quickly adjust the dynamics' trajectory online to adapt to different scenarios, but they struggle to capture complex corner cases of ADS arising from the intricate interplay among multiple vehicles. Multi-agent reinforcement learning (MARL) has a strong ability in cooperative tasks. On the other hand, it faces its own challenges, particularly with convergence. This paper introduces MARL-OT, a scalable framework that leverages MARL to detect safety violations of ADS resulting from surrounding vehicles' cooperation. MARL-OT employs MARL for high-level guidance, triggering various dangerous scenarios for the rule-based online fuzzer to explore potential safety violations of ADS, thereby generating dynamic, realistic safety violation scenarios. Our approach improves the detected safety violation rate by up to 136.2% compared to the state-of-the-art (SOTA) testing technique.
<div id='section'>Paperid: <span id='pid'>352, <a href='https://arxiv.org/pdf/2501.08778.pdf' target='_blank'>https://arxiv.org/pdf/2501.08778.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Guilherme S. Varela, Alberto Sardinha, Francisco S. Melo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.08778">Networked Agents in the Dark: Team Value Learning under Partial Observability</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a novel cooperative multi-agent reinforcement learning (MARL) approach for networked agents. In contrast to previous methods that rely on complete state information or joint observations, our agents must learn how to reach shared objectives under partial observability. During training, they collect individual rewards and approximate a team value function through local communication, resulting in cooperative behavior. To describe our problem, we introduce the networked dynamic partially observable Markov game framework, where agents communicate over a switching topology communication network. Our distributed method, DNA-MARL, uses a consensus mechanism for local communication and gradient descent for local computation. DNA-MARL increases the range of the possible applications of networked agents, being well-suited for real world domains that impose privacy and where the messages may not reach their recipients. We evaluate DNA-MARL across benchmark MARL scenarios. Our results highlight the superior performance of DNA-MARL over previous methods.
<div id='section'>Paperid: <span id='pid'>353, <a href='https://arxiv.org/pdf/2501.08234.pdf' target='_blank'>https://arxiv.org/pdf/2501.08234.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Enrique Adrian Villarrubia-Martin, Luis Rodriguez-Benitez, David MuÃ±oz-Valero, Giovanni Montana, Luis Jimenez-Linares
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.08234">Dynamic Pricing in High-Speed Railways Using Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper addresses a critical challenge in the high-speed passenger railway industry: designing effective dynamic pricing strategies in the context of competing and cooperating operators. To address this, a multi-agent reinforcement learning (MARL) framework based on a non-zero-sum Markov game is proposed, incorporating random utility models to capture passenger decision making. Unlike prior studies in areas such as energy, airlines, and mobile networks, dynamic pricing for railway systems using deep reinforcement learning has received limited attention. A key contribution of this paper is a parametrisable and versatile reinforcement learning simulator designed to model a variety of railway network configurations and demand patterns while enabling realistic, microscopic modelling of user behaviour, called RailPricing-RL. This environment supports the proposed MARL framework, which models heterogeneous agents competing to maximise individual profits while fostering cooperative behaviour to synchronise connecting services. Experimental results validate the framework, demonstrating how user preferences affect MARL performance and how pricing policies influence passenger choices, utility, and overall system dynamics. This study provides a foundation for advancing dynamic pricing strategies in railway systems, aligning profitability with system-wide efficiency, and supporting future research on optimising pricing policies.
<div id='section'>Paperid: <span id='pid'>354, <a href='https://arxiv.org/pdf/2411.13934.pdf' target='_blank'>https://arxiv.org/pdf/2411.13934.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yancheng Liang, Daphne Chen, Abhishek Gupta, Simon S. Du, Natasha Jaques
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.13934">Learning to Cooperate with Humans using Generative Agents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Training agents that can coordinate zero-shot with humans is a key mission in multi-agent reinforcement learning (MARL). Current algorithms focus on training simulated human partner policies which are then used to train a Cooperator agent. The simulated human is produced either through behavior cloning over a dataset of human cooperation behavior, or by using MARL to create a population of simulated agents. However, these approaches often struggle to produce a Cooperator that can coordinate well with real humans, since the simulated humans fail to cover the diverse strategies and styles employed by people in the real world. We show \emph{learning a generative model of human partners} can effectively address this issue. Our model learns a latent variable representation of the human that can be regarded as encoding the human's unique strategy, intention, experience, or style. This generative model can be flexibly trained from any (human or neural policy) agent interaction data. By sampling from the latent space, we can use the generative model to produce different partners to train Cooperator agents. We evaluate our method -- \textbf{G}enerative \textbf{A}gent \textbf{M}odeling for \textbf{M}ulti-agent \textbf{A}daptation (GAMMA) -- on Overcooked, a challenging cooperative cooking game that has become a standard benchmark for zero-shot coordination. We conduct an evaluation with real human teammates, and the results show that GAMMA consistently improves performance, whether the generative model is trained on simulated populations or human datasets. Further, we propose a method for posterior sampling from the generative model that is biased towards the human data, enabling us to efficiently improve performance with only a small amount of expensive human interaction data.
<div id='section'>Paperid: <span id='pid'>355, <a href='https://arxiv.org/pdf/2410.17466.pdf' target='_blank'>https://arxiv.org/pdf/2410.17466.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yann Bouteiller, Karthik Soma, Giovanni Beltrame
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.17466">Evolution of Societies via Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The universe involves many independent co-learning agents as an ever-evolving part of our observed environment. Yet, in practice, Multi-Agent Reinforcement Learning (MARL) applications are typically constrained to small, homogeneous populations and remain computationally intensive. We propose a methodology that enables simulating populations of Reinforcement Learning agents at evolutionary scale. More specifically, we derive a fast, parallelizable implementation of Policy Gradient (PG) and Opponent-Learning Awareness (LOLA), tailored for evolutionary simulations where agents undergo random pairwise interactions in stateless normal-form games. We demonstrate our approach by simulating the evolution of very large populations made of heterogeneous co-learning agents, under both naive and advanced learning strategies. In our experiments, 200,000 PG or LOLA agents evolve in the classic games of Hawk-Dove, Stag-Hunt, and Rock-Paper-Scissors. Each game provides distinct insights into how populations evolve under both naive and advanced MARL rules, including compelling ways in which Opponent-Learning Awareness affects social evolution.
<div id='section'>Paperid: <span id='pid'>356, <a href='https://arxiv.org/pdf/2410.05673.pdf' target='_blank'>https://arxiv.org/pdf/2410.05673.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fivos Kalogiannis, Jingming Yan, Ioannis Panageas
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.05673">Learning Equilibria in Adversarial Team Markov Games: A Nonconvex-Hidden-Concave Min-Max Optimization Problem</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We study the problem of learning a Nash equilibrium (NE) in Markov games which is a cornerstone in multi-agent reinforcement learning (MARL). In particular, we focus on infinite-horizon adversarial team Markov games (ATMGs) in which agents that share a common reward function compete against a single opponent, the adversary. These games unify two-player zero-sum Markov games and Markov potential games, resulting in a setting that encompasses both collaboration and competition. Kalogiannis et al. (2023a) provided an efficient equilibrium computation algorithm for ATMGs which presumes knowledge of the reward and transition functions and has no sample complexity guarantees. We contribute a learning algorithm that utilizes MARL policy gradient methods with iteration and sample complexity that is polynomial in the approximation error $Îµ$ and the natural parameters of the ATMG, resolving the main caveats of the solution by (Kalogiannis et al., 2023a). It is worth noting that previously, the existence of learning algorithms for NE was known for Markov two-player zero-sum and potential games but not for ATMGs.
  Seen through the lens of min-max optimization, computing a NE in these games consists a nonconvex-nonconcave saddle-point problem. Min-max optimization has received extensive study. Nevertheless, the case of nonconvex-nonconcave landscapes remains elusive: in full generality, finding saddle-points is computationally intractable (Daskalakis et al., 2021). We circumvent the aforementioned intractability by developing techniques that exploit the hidden structure of the objective function via a nonconvex-concave reformulation. However, this introduces the challenge of a feasibility set with coupled constraints. We tackle these challenges by establishing novel techniques for optimizing weakly-smooth nonconvex functions, extending the framework of (Devolder et al., 2014).
<div id='section'>Paperid: <span id='pid'>357, <a href='https://arxiv.org/pdf/2410.02664.pdf' target='_blank'>https://arxiv.org/pdf/2410.02664.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zeyang Liu, Xinrui Yang, Shiguang Sun, Long Qian, Lipeng Wan, Xingyu Chen, Xuguang Lan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.02664">Grounded Answers for Multi-agent Decision-making Problem through Generative World Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent progress in generative models has stimulated significant innovations in many fields, such as image generation and chatbots. Despite their success, these models often produce sketchy and misleading solutions for complex multi-agent decision-making problems because they miss the trial-and-error experience and reasoning as humans. To address this limitation, we explore a paradigm that integrates a language-guided simulator into the multi-agent reinforcement learning pipeline to enhance the generated answer. The simulator is a world model that separately learns dynamics and reward, where the dynamics model comprises an image tokenizer as well as a causal transformer to generate interaction transitions autoregressively, and the reward model is a bidirectional transformer learned by maximizing the likelihood of trajectories in the expert demonstrations under language guidance. Given an image of the current state and the task description, we use the world model to train the joint policy and produce the image sequence as the answer by running the converged policy on the dynamics model. The empirical results demonstrate that this framework can improve the answers for multi-agent decision-making problems by showing superior performance on the training and unseen tasks of the StarCraft Multi-Agent Challenge benchmark. In particular, it can generate consistent interaction sequences and explainable reward functions at interaction states, opening the path for training generative models of the future.
<div id='section'>Paperid: <span id='pid'>358, <a href='https://arxiv.org/pdf/2409.20326.pdf' target='_blank'>https://arxiv.org/pdf/2409.20326.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zichong Li, Filip Bjelonic, Victor Klemm, Marco Hutter
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.20326">MARLadona -- Towards Cooperative Team Play Using Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robot soccer, in its full complexity, poses an unsolved research challenge. Current solutions heavily rely on engineered heuristic strategies, which lack robustness and adaptability. Deep reinforcement learning has gained significant traction in various complex robotics tasks such as locomotion, manipulation, and competitive games (e.g., AlphaZero, OpenAI Five), making it a promising solution to the robot soccer problem. This paper introduces MARLadona. A decentralized multi-agent reinforcement learning (MARL) training pipeline capable of producing agents with sophisticated team play behavior, bridging the shortcomings of heuristic methods. Furthermore, we created an open-source multi-agent soccer environment. Utilizing our MARL framework and a modified global entity encoder (GEE) as our core architecture, our approach achieves a 66.8% win rate against HELIOS agent, which employs a state-of-the-art heuristic strategy. In addition, we provided an in-depth analysis of the policy behavior and interpreted the agent's intention using the critic network.
<div id='section'>Paperid: <span id='pid'>359, <a href='https://arxiv.org/pdf/2409.12882.pdf' target='_blank'>https://arxiv.org/pdf/2409.12882.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hairi, Minghong Fang, Zifan Zhang, Alvaro Velasquez, Jia Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.12882">On the Hardness of Decentralized Multi-Agent Policy Evaluation under Byzantine Attacks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we study a fully-decentralized multi-agent policy evaluation problem, which is an important sub-problem in cooperative multi-agent reinforcement learning, in the presence of up to $f$ faulty agents. In particular, we focus on the so-called Byzantine faulty model with model poisoning setting. In general, policy evaluation is to evaluate the value function of any given policy. In cooperative multi-agent system, the system-wide rewards are usually modeled as the uniform average of rewards from all agents. We investigate the multi-agent policy evaluation problem in the presence of Byzantine agents, particularly in the setting of heterogeneous local rewards. Ideally, the goal of the agents is to evaluate the accumulated system-wide rewards, which are uniform average of rewards of the normal agents for a given policy. It means that all agents agree upon common values (the consensus part) and furthermore, the consensus values are the value functions (the convergence part). However, we prove that this goal is not achievable. Instead, we consider a relaxed version of the problem, where the goal of the agents is to evaluate accumulated system-wide reward, which is an appropriately weighted average reward of the normal agents. We further prove that there is no correct algorithm that can guarantee that the total number of positive weights exceeds $|\mathcal{N}|-f $, where $|\mathcal{N}|$ is the number of normal agents. Towards the end, we propose a Byzantine-tolerant decentralized temporal difference algorithm that can guarantee asymptotic consensus under scalar function approximation. We then empirically test the effective of the proposed algorithm.
<div id='section'>Paperid: <span id='pid'>360, <a href='https://arxiv.org/pdf/2512.22941.pdf' target='_blank'>https://arxiv.org/pdf/2512.22941.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianyi Hu, Zhiqiang Pu, Yuan Wang, Tenghai Qiu, Min Chen, Xin Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.22941">Heterogeneity in Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Heterogeneity is a fundamental property in multi-agent reinforcement learning (MARL), which is closely related not only to the functional differences of agents, but also to policy diversity and environmental interactions. However, the MARL field currently lacks a rigorous definition and deeper understanding of heterogeneity. This paper systematically discusses heterogeneity in MARL from the perspectives of definition, quantification, and utilization. First, based on an agent-level modeling of MARL, we categorize heterogeneity into five types and provide mathematical definitions. Second, we define the concept of heterogeneity distance and propose a practical quantification method. Third, we design a heterogeneity-based multi-agent dynamic parameter sharing algorithm as an example of the application of our methodology. Case studies demonstrate that our method can effectively identify and quantify various types of agent heterogeneity. Experimental results show that the proposed algorithm, compared to other parameter sharing baselines, has better interpretability and stronger adaptability. The proposed methodology will help the MARL community gain a more comprehensive and profound understanding of heterogeneity, and further promote the development of practical algorithms.
<div id='section'>Paperid: <span id='pid'>361, <a href='https://arxiv.org/pdf/2511.15002.pdf' target='_blank'>https://arxiv.org/pdf/2511.15002.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fatemeh Lotfi, Hossein Rajoli, Fatemeh Afghah
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.15002">Task Specific Sharpness Aware O-RAN Resource Management using Multi Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Next-generation networks utilize the Open Radio Access Network (O-RAN) architecture to enable dynamic resource management, facilitated by the RAN Intelligent Controller (RIC). While deep reinforcement learning (DRL) models show promise in optimizing network resources, they often struggle with robustness and generalizability in dynamic environments. This paper introduces a novel resource management approach that enhances the Soft Actor Critic (SAC) algorithm with Sharpness-Aware Minimization (SAM) in a distributed Multi-Agent RL (MARL) framework. Our method introduces an adaptive and selective SAM mechanism, where regularization is explicitly driven by temporal-difference (TD)-error variance, ensuring that only agents facing high environmental complexity are regularized. This targeted strategy reduces unnecessary overhead, improves training stability, and enhances generalization without sacrificing learning efficiency. We further incorporate a dynamic $ρ$ scheduling scheme to refine the exploration-exploitation trade-off across agents. Experimental results show our method significantly outperforms conventional DRL approaches, yielding up to a $22\%$ improvement in resource allocation efficiency and ensuring superior QoS satisfaction across diverse O-RAN slices.
<div id='section'>Paperid: <span id='pid'>362, <a href='https://arxiv.org/pdf/2511.12123.pdf' target='_blank'>https://arxiv.org/pdf/2511.12123.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zejiao Liu, Junqi Tu, Yitian Hong, Luolin Xiong, Yaochu Jin, Yang Tang, Fangfei Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.12123">HCPO: Hierarchical Conductor-Based Policy Optimization in Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In cooperative Multi-Agent Reinforcement Learning (MARL), efficient exploration is crucial for optimizing the performance of joint policy. However, existing methods often update joint policies via independent agent exploration, without coordination among agents, which inherently constrains the expressive capacity and exploration of joint policies. To address this issue, we propose a conductor-based joint policy framework that directly enhances the expressive capacity of joint policies and coordinates exploration. In addition, we develop a Hierarchical Conductor-based Policy Optimization (HCPO) algorithm that instructs policy updates for the conductor and agents in a direction aligned with performance improvement. A rigorous theoretical guarantee further establishes the monotonicity of the joint policy optimization process. By deploying local conductors, HCPO retains centralized training benefits while eliminating inter-agent communication during execution. Finally, we evaluate HCPO on three challenging benchmarks: StarCraftII Multi-agent Challenge, Multi-agent MuJoCo, and Multi-agent Particle Environment. The results indicate that HCPO outperforms competitive MARL baselines regarding cooperative efficiency and stability.
<div id='section'>Paperid: <span id='pid'>363, <a href='https://arxiv.org/pdf/2511.11393.pdf' target='_blank'>https://arxiv.org/pdf/2511.11393.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zejiao Liu, Yi Li, Jiali Wang, Junqi Tu, Yitian Hong, Fangfei Li, Yang Liu, Toshiharu Sugawara, Yang Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.11393">Robust and Efficient Communication in Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent reinforcement learning (MARL) has made significant strides in enabling coordinated behaviors among autonomous agents. However, most existing approaches assume that communication is instantaneous, reliable, and has unlimited bandwidth; these conditions are rarely met in real-world deployments. This survey systematically reviews recent advances in robust and efficient communication strategies for MARL under realistic constraints, including message perturbations, transmission delays, and limited bandwidth. Furthermore, because the challenges of low-latency reliability, bandwidth-intensive data sharing, and communication-privacy trade-offs are central to practical MARL systems, we focus on three applications involving cooperative autonomous driving, distributed simultaneous localization and mapping, and federated learning. Finally, we identify key open challenges and future research directions, advocating a unified approach that co-designs communication, learning, and robustness to bridge the gap between theoretical MARL models and practical implementations.
<div id='section'>Paperid: <span id='pid'>364, <a href='https://arxiv.org/pdf/2511.02304.pdf' target='_blank'>https://arxiv.org/pdf/2511.02304.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Beyazit Yalcinkaya, Marcell Vazquez-Chanlatte, Ameesh Shah, Hanna Krasowski, Sanjit A. Seshia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.02304">Automata-Conditioned Cooperative Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We study the problem of learning multi-task, multi-agent policies for cooperative, temporal objectives, under centralized training, decentralized execution. In this setting, using automata to represent tasks enables the decomposition of complex tasks into simpler sub-tasks that can be assigned to agents. However, existing approaches remain sample-inefficient and are limited to the single-task case. In this work, we present Automata-Conditioned Cooperative Multi-Agent Reinforcement Learning (ACC-MARL), a framework for learning task-conditioned, decentralized team policies. We identify the main challenges to ACC-MARL's feasibility in practice, propose solutions, and prove the correctness of our approach. We further show that the value functions of learned policies can be used to assign tasks optimally at test time. Experiments show emergent task-aware, multi-step coordination among agents, e.g., pressing a button to unlock a door, holding the door, and short-circuiting tasks.
<div id='section'>Paperid: <span id='pid'>365, <a href='https://arxiv.org/pdf/2510.09937.pdf' target='_blank'>https://arxiv.org/pdf/2510.09937.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shahbaz P Qadri Syed, He Bai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.09937">Structured Cooperative Multi-Agent Reinforcement Learning: a Bayesian Network Perspective</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The empirical success of multi-agent reinforcement learning (MARL) has motivated the search for more efficient and scalable algorithms for large scale multi-agent systems. However, existing state-of-the-art algorithms do not fully exploit inter-agent coupling information to develop MARL algorithms. In this paper, we propose a systematic approach to leverage structures in the inter-agent couplings for efficient model-free reinforcement learning. We model the cooperative MARL problem via a Bayesian network and characterize the subset of agents, termed as the value dependency set, whose information is required by each agent to estimate its local action value function exactly. Moreover, we propose a partially decentralized training decentralized execution (P-DTDE) paradigm based on the value dependency set. We theoretically establish that the total variance of our P-DTDE policy gradient estimator is less than the centralized training decentralized execution (CTDE) policy gradient estimator. We derive a multi-agent policy gradient theorem based on the P-DTDE scheme and develop a scalable actor-critic algorithm. We demonstrate the efficiency and scalability of the proposed algorithm on multi-warehouse resource allocation and multi-zone temperature control examples. For dense value dependency sets, we propose an approximation scheme based on truncation of the Bayesian network and empirically show that it achieves a faster convergence than the exact value dependence set for applications with a large number of agents.
<div id='section'>Paperid: <span id='pid'>366, <a href='https://arxiv.org/pdf/2509.25034.pdf' target='_blank'>https://arxiv.org/pdf/2509.25034.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Heming Fu, Guojun Xiong, Shan Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.25034">MARLIN: Multi-Agent Reinforcement Learning with Murmuration Intelligence and LLM Guidance for Reservoir Management</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As climate change intensifies extreme weather events, water disasters pose growing threats to global communities, making adaptive reservoir management critical for protecting vulnerable populations and ensuring water security. Modern water resource management faces unprecedented challenges from cascading uncertainties propagating through interconnected reservoir networks. These uncertainties, rooted in physical water transfer losses and environmental variability, make precise control difficult. For example, sending 10 tons downstream may yield only 8-12 tons due to evaporation and seepage. Traditional centralized optimization approaches suffer from exponential computational complexity and cannot effectively handle such real-world uncertainties, while existing multi-agent reinforcement learning (MARL) methods fail to achieve effective coordination under uncertainty. To address these challenges, we present MARLIN, a decentralized reservoir management framework inspired by starling murmurations intelligence. Integrating bio-inspired alignment, separation, and cohesion rules with MARL, MARLIN enables individual reservoirs to make local decisions while achieving emergent global coordination. In addition, a LLM provides real-time reward shaping signals, guiding agents to adapt to environmental changes and human-defined preferences. Experiments on real-world USGS data show that MARLIN improves uncertainty handling by 23\%, cuts computation by 35\%, and accelerates flood response by 68\%, exhibiting super-linear coordination, with complexity scaling 5.4x from 400 to 10,000 nodes. These results demonstrate MARLIN's potential for disaster prevention and protecting communities through intelligent, scalable water resource management.
<div id='section'>Paperid: <span id='pid'>367, <a href='https://arxiv.org/pdf/2509.21828.pdf' target='_blank'>https://arxiv.org/pdf/2509.21828.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>The Viet Bui, Tien Mai, Hong Thanh Nguyen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.21828">Preference-Guided Learning for Sparse-Reward Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We study the problem of online multi-agent reinforcement learning (MARL) in environments with sparse rewards, where reward feedback is not provided at each interaction but only revealed at the end of a trajectory. This setting, though realistic, presents a fundamental challenge: the lack of intermediate rewards hinders standard MARL algorithms from effectively guiding policy learning. To address this issue, we propose a novel framework that integrates online inverse preference learning with multi-agent on-policy optimization into a unified architecture. At its core, our approach introduces an implicit multi-agent reward learning model, built upon a preference-based value-decomposition network, which produces both global and local reward signals. These signals are further used to construct dual advantage streams, enabling differentiated learning targets for the centralized critic and decentralized actors. In addition, we demonstrate how large language models (LLMs) can be leveraged to provide preference labels that enhance the quality of the learned reward model. Empirical evaluations on state-of-the-art benchmarks, including MAMuJoCo and SMACv2, show that our method achieves superior performance compared to existing baselines, highlighting its effectiveness in addressing sparse-reward challenges in online MARL.
<div id='section'>Paperid: <span id='pid'>368, <a href='https://arxiv.org/pdf/2509.14380.pdf' target='_blank'>https://arxiv.org/pdf/2509.14380.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Seoyeon Choi, Kanghyun Ryu, Jonghoon Ock, Negar Mehr
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.14380">CRAFT: Coaching Reinforcement Learning Autonomously using Foundation Models for Multi-Robot Coordination Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-Agent Reinforcement Learning (MARL) provides a powerful framework for learning coordination in multi-agent systems. However, applying MARL to robotics still remains challenging due to high-dimensional continuous joint action spaces, complex reward design, and non-stationary transitions inherent to decentralized settings. On the other hand, humans learn complex coordination through staged curricula, where long-horizon behaviors are progressively built upon simpler skills. Motivated by this, we propose CRAFT: Coaching Reinforcement learning Autonomously using Foundation models for multi-robot coordination Tasks, a framework that leverages the reasoning capabilities of foundation models to act as a "coach" for multi-robot coordination. CRAFT automatically decomposes long-horizon coordination tasks into sequences of subtasks using the planning capability of Large Language Models (LLMs). In what follows, CRAFT trains each subtask using reward functions generated by LLM, and refines them through a Vision Language Model (VLM)-guided reward-refinement loop. We evaluate CRAFT on multi-quadruped navigation and bimanual manipulation tasks, demonstrating its capability to learn complex coordination behaviors. In addition, we validate the multi-quadruped navigation policy in real hardware experiments.
<div id='section'>Paperid: <span id='pid'>369, <a href='https://arxiv.org/pdf/2509.08310.pdf' target='_blank'>https://arxiv.org/pdf/2509.08310.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>S Krishna Niketh, Sagar Babu Mitikiri, V Vignesh, Vedantham Lakshmi Srinivas, Mayukha Pal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.08310">Game-Theoretic Resilience Framework for Cyber-Physical Microgrids using Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The increasing reliance on cyber physical infrastructure in modern power systems has amplified the risk of targeted cyber attacks, necessitating robust and adaptive resilience strategies. This paper presents a mathematically rigorous game theoretic framework to evaluate and enhance microgrid resilience using a combination of quantitative resilience metrics Load Served Ratio LSR, Critical Load Resilience CLR, Topological Survivability Score TSS, and DER Resilience Score DRS. These are integrated into a unified payoff matrix using the Analytic Hierarchy Process AHP to assess attack defense interactions. The framework is formalized as a finite horizon Markov Decision Process MDP with formal convergence guarantees and computational complexity bounds. Three case studies are developed 1. static attacks analyzed via Nash equilibrium, 2. severe attacks incorporating high impact strategies, and 3. adaptive attacks using Stackelberg games, regret matching, softmax heuristics, and Multi Agent Q Learning. Rigorous theoretical analysis provides convergence proofs with explicit rates , PAC learning sample complexity bounds, and computational complexity analysis. The framework is tested on an enhanced IEEE 33bus distribution system with DERs and control switches, demonstrating the effectiveness of adaptive and strategic defenses in improving cyber physical resilience with statistically significant improvements of 18.7% 2.1% over static approaches.
<div id='section'>Paperid: <span id='pid'>370, <a href='https://arxiv.org/pdf/2509.06053.pdf' target='_blank'>https://arxiv.org/pdf/2509.06053.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mingrui Lv, Hangzhi Liu, Zhi Luo, Hongjie Zhang, Jie Ou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.06053">PolicyEvolve: Evolving Programmatic Policies by LLMs for multi-player games via Population-Based Training</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent reinforcement learning (MARL) has achieved significant progress in solving complex multi-player games through self-play. However, training effective adversarial policies requires millions of experience samples and substantial computational resources. Moreover, these policies lack interpretability, hindering their practical deployment. Recently, researchers have successfully leveraged Large Language Models (LLMs) to generate programmatic policies for single-agent tasks, transforming neural network-based policies into interpretable rule-based code with high execution efficiency. Inspired by this, we propose PolicyEvolve, a general framework for generating programmatic policies in multi-player games. PolicyEvolve significantly reduces reliance on manually crafted policy code, achieving high-performance policies with minimal environmental interactions. The framework comprises four modules: Global Pool, Local Pool, Policy Planner, and Trajectory Critic. The Global Pool preserves elite policies accumulated during iterative training. The Local Pool stores temporary policies for the current iteration; only sufficiently high-performing policies from this pool are promoted to the Global Pool. The Policy Planner serves as the core policy generation module. It samples the top three policies from the Global Pool, generates an initial policy for the current iteration based on environmental information, and refines this policy using feedback from the Trajectory Critic. Refined policies are then deposited into the Local Pool. This iterative process continues until the policy achieves a sufficiently high average win rate against the Global Pool, at which point it is integrated into the Global Pool. The Trajectory Critic analyzes interaction data from the current policy, identifies vulnerabilities, and proposes directional improvements to guide the Policy Planner
<div id='section'>Paperid: <span id='pid'>371, <a href='https://arxiv.org/pdf/2509.05446.pdf' target='_blank'>https://arxiv.org/pdf/2509.05446.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Iftekhar Haider Chowdhury, Zaed Ikbal Syed, Ahmed Faizul Haque Dhrubo, Mohammad Abdul Qayum
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.05446">Dynamic Sensitivity Filter Pruning using Multi-Agent Reinforcement Learning For DCNN's</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep Convolutional Neural Networks have achieved state of the art performance across various computer vision tasks, however their practical deployment is limited by computational and memory overhead. This paper introduces Differential Sensitivity Fusion Pruning, a novel single shot filter pruning framework that focuses on evaluating the stability and redundancy of filter importance scores across multiple criteria. Differential Sensitivity Fusion Pruning computes a differential sensitivity score for each filter by fusing the discrepancies among gradient based sensitivity, first order Taylor expansion, and KL divergence of activation distributions. An exponential scaling mechanism is applied to emphasize filters with inconsistent importance across metrics, identifying candidates that are structurally unstable or less critical to the model performance. Unlike iterative or reinforcement learning based pruning strategies, Differential Sensitivity Fusion Pruning is efficient and deterministic, requiring only a single forward-backward pass for scoring and pruning. Extensive experiments across varying pruning rates between 50 to 70 percent demonstrate that Differential Sensitivity Fusion Pruning significantly reduces model complexity, achieving over 80 percent Floating point Operations Per Seconds reduction while maintaining high accuracy. For instance, at 70 percent pruning, our approach retains up to 98.23 percent of baseline accuracy, surpassing traditional heuristics in both compression and generalization. The proposed method presents an effective solution for scalable and adaptive Deep Convolutional Neural Networks compression, paving the way for efficient deployment on edge and mobile platforms.
<div id='section'>Paperid: <span id='pid'>372, <a href='https://arxiv.org/pdf/2509.02579.pdf' target='_blank'>https://arxiv.org/pdf/2509.02579.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mazyar Taghavi, Rahman Farnoosh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.02579">Latent Variable Modeling in Multi-Agent Reinforcement Learning via Expectation-Maximization for UAV-Based Wildlife Protection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Protecting endangered wildlife from illegal poaching presents a critical challenge, particularly in vast and partially observable environments where real-time response is essential. This paper introduces a novel Expectation-Maximization (EM) based latent variable modeling approach in the context of Multi-Agent Reinforcement Learning (MARL) for Unmanned Aerial Vehicle (UAV) coordination in wildlife protection. By modeling hidden environmental factors and inter-agent dynamics through latent variables, our method enhances exploration and coordination under uncertainty.We implement and evaluate our EM-MARL framework using a custom simulation involving 10 UAVs tasked with patrolling protected habitats of the endangered Iranian leopard. Extensive experimental results demonstrate superior performance in detection accuracy, adaptability, and policy convergence when compared to standard algorithms such as Proximal Policy Optimization (PPO) and Deep Deterministic Policy Gradient (DDPG). Our findings underscore the potential of combining EM inference with MARL to improve decentralized decisionmaking in complex, high-stakes conservation scenarios. The full implementation, simulation environment, and training scripts are publicly available on GitHub.
<div id='section'>Paperid: <span id='pid'>373, <a href='https://arxiv.org/pdf/2508.04652.pdf' target='_blank'>https://arxiv.org/pdf/2508.04652.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuo Liu, Zeyu Liang, Xueguang Lyu, Christopher Amato
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.04652">LLM Collaboration With Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A large amount of work has been done in Multi-Agent Systems (MAS) for modeling and solving problems with multiple interacting agents. However, most LLMs are pretrained independently and not specifically optimized for coordination. Existing LLM fine-tuning frameworks rely on individual rewards, which require complex reward designs for each agent to encourage collaboration. To address these challenges, we model LLM collaboration as a cooperative Multi-Agent Reinforcement Learning (MARL) problem. We develop a multi-agent, multi-turn algorithm, Multi-Agent Group Relative Policy Optimization (MAGRPO), to solve it, building on current RL approaches for LLMs as well as MARL techniques. Our experiments on LLM writing and coding collaboration demonstrate that fine-tuning MAS with MAGRPO enables agents to generate high-quality responses efficiently through effective cooperation. Our approach opens the door to using other MARL methods for LLMs and highlights the associated challenges.
<div id='section'>Paperid: <span id='pid'>374, <a href='https://arxiv.org/pdf/2508.01561.pdf' target='_blank'>https://arxiv.org/pdf/2508.01561.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zijian Guo, Ä°lker IÅÄ±k, H. M. Sabbir Ahmad, Wenchao Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.01561">One Subgoal at a Time: Zero-Shot Generalization to Arbitrary Linear Temporal Logic Requirements in Multi-Task Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generalizing to complex and temporally extended task objectives and safety constraints remains a critical challenge in reinforcement learning (RL). Linear temporal logic (LTL) offers a unified formalism to specify such requirements, yet existing methods are limited in their abilities to handle nested long-horizon tasks and safety constraints, and cannot identify situations when a subgoal is not satisfiable and an alternative should be sought. In this paper, we introduce GenZ-LTL, a method that enables zero-shot generalization to arbitrary LTL specifications. GenZ-LTL leverages the structure of BÃ¼chi automata to decompose an LTL task specification into sequences of reach-avoid subgoals. Contrary to the current state-of-the-art method that conditions on subgoal sequences, we show that it is more effective to achieve zero-shot generalization by solving these reach-avoid problems \textit{one subgoal at a time} through proper safe RL formulations. In addition, we introduce a novel subgoal-induced observation reduction technique that can mitigate the exponential complexity of subgoal-state combinations under realistic assumptions. Empirical results show that GenZ-LTL substantially outperforms existing methods in zero-shot generalization to unseen LTL specifications.
<div id='section'>Paperid: <span id='pid'>375, <a href='https://arxiv.org/pdf/2505.18750.pdf' target='_blank'>https://arxiv.org/pdf/2505.18750.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiarong Fan, Chenghao Huang, Hao Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.18750">Agent-Based Decentralized Energy Management of EV Charging Station with Solar Photovoltaics via Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the pursuit of energy net zero within smart cities, transportation electrification plays a pivotal role. The adoption of Electric Vehicles (EVs) keeps increasing, making energy management of EV charging stations critically important. While previous studies have managed to reduce energy cost of EV charging while maintaining grid stability, they often overlook the robustness of EV charging management against uncertainties of various forms, such as varying charging behaviors and possible faults in faults in some chargers. To address the gap, a novel Multi-Agent Reinforcement Learning (MARL) approach is proposed treating each charger to be an agent and coordinate all the agents in the EV charging station with solar photovoltaics in a more realistic scenario, where system faults may occur. A Long Short-Term Memory (LSTM) network is incorporated in the MARL algorithm to extract temporal features from time-series. Additionally, a dense reward mechanism is designed for training the agents in the MARL algorithm to improve EV charging experience. Through validation on a real-world dataset, we show that our approach is robust against system uncertainties and faults and also effective in minimizing EV charging costs and maximizing charging service satisfaction.
<div id='section'>Paperid: <span id='pid'>376, <a href='https://arxiv.org/pdf/2505.10484.pdf' target='_blank'>https://arxiv.org/pdf/2505.10484.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Andrea Baisero, Rupali Bhati, Shuo Liu, Aathira Pillai, Christopher Amato
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.10484">Fixing Incomplete Value Function Decomposition for Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Value function decomposition methods for cooperative multi-agent reinforcement learning compose joint values from individual per-agent utilities, and train them using a joint objective. To ensure that the action selection process between individual utilities and joint values remains consistent, it is imperative for the composition to satisfy the individual-global max (IGM) property. Although satisfying IGM itself is straightforward, most existing methods (e.g., VDN, QMIX) have limited representation capabilities and are unable to represent the full class of IGM values, and the one exception that has no such limitation (QPLEX) is unnecessarily complex. In this work, we present a simple formulation of the full class of IGM values that naturally leads to the derivation of QFIX, a novel family of value function decomposition models that expand the representation capabilities of prior models by means of a thin "fixing" layer. We derive multiple variants of QFIX, and implement three variants in two well-known multi-agent frameworks. We perform an empirical evaluation on multiple SMACv2 and Overcooked environments, which confirms that QFIX (i) succeeds in enhancing the performance of prior methods, (ii) learns more stably and performs better than its main competitor QPLEX, and (iii) achieves this while employing the simplest and smallest mixing models.
<div id='section'>Paperid: <span id='pid'>377, <a href='https://arxiv.org/pdf/2505.08825.pdf' target='_blank'>https://arxiv.org/pdf/2505.08825.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pedro Antonio Alarcon Granadeno, Theodore Chambers, Jane Cleland-Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.08825">Multi-source Plume Tracing via Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Industrial catastrophes like the Bhopal disaster (1984) and the Aliso Canyon gas leak (2015) demonstrate the urgent need for rapid and reliable plume tracing algorithms to protect public health and the environment. Traditional methods, such as gradient-based or biologically inspired approaches, often fail in realistic, turbulent conditions. To address these challenges, we present a Multi-Agent Reinforcement Learning (MARL) algorithm designed for localizing multiple airborne pollution sources using a swarm of small uncrewed aerial systems (sUAS). Our method models the problem as a Partially Observable Markov Game (POMG), employing a Long Short-Term Memory (LSTM)-based Action-specific Double Deep Recurrent Q-Network (ADDRQN) that uses full sequences of historical action-observation pairs, effectively approximating latent states. Unlike prior work, we use a general-purpose simulation environment based on the Gaussian Plume Model (GPM), incorporating realistic elements such as a three-dimensional environment, sensor noise, multiple interacting agents, and multiple plume sources. The incorporation of action histories as part of the inputs further enhances the adaptability of our model in complex, partially observable environments. Extensive simulations show that our algorithm significantly outperforms conventional approaches. Specifically, our model allows agents to explore only 1.29\% of the environment to successfully locate pollution sources.
<div id='section'>Paperid: <span id='pid'>378, <a href='https://arxiv.org/pdf/2504.05045.pdf' target='_blank'>https://arxiv.org/pdf/2504.05045.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Huilin Yin, Zhikun Yang, Linchuan Zhang, Daniel Watzenig
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.05045">Attention-Augmented Inverse Reinforcement Learning with Graph Convolutions for Multi-Agent Task Allocation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.
  Multi-agent task allocation (MATA) plays a vital role in cooperative multi-agent systems, with significant implications for applications such as logistics, search and rescue, and robotic coordination. Although traditional deep reinforcement learning (DRL) methods have been shown to be promising, their effectiveness is hindered by a reliance on manually designed reward functions and inefficiencies in dynamic environments. In this paper, an inverse reinforcement learning (IRL)-based framework is proposed, in which multi-head self-attention (MHSA) and graph attention mechanisms are incorporated to enhance reward function learning and task execution efficiency. Expert demonstrations are utilized to infer optimal reward densities, allowing dependence on handcrafted designs to be reduced and adaptability to be improved. Extensive experiments validate the superiority of the proposed method over widely used multi-agent reinforcement learning (MARL) algorithms in terms of both cumulative rewards and task execution efficiency.
<div id='section'>Paperid: <span id='pid'>379, <a href='https://arxiv.org/pdf/2504.04675.pdf' target='_blank'>https://arxiv.org/pdf/2504.04675.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tzu-Han Hsu, Arshia Rafieioskouei, Borzoo Bonakdarpour
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.04675">HypRL: Reinforcement Learning of Control Policies for Hyperproperties</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reward shaping in multi-agent reinforcement learning (MARL) for complex tasks remains a significant challenge. Existing approaches often fail to find optimal solutions or cannot efficiently handle such tasks. We propose HYPRL, a specification-guided reinforcement learning framework that learns control policies w.r.t. hyperproperties expressed in HyperLTL. Hyperproperties constitute a powerful formalism for specifying objectives and constraints over sets of execution traces across agents. To learn policies that maximize the satisfaction of a HyperLTL formula $Ï$, we apply Skolemization to manage quantifier alternations and define quantitative robustness functions to shape rewards over execution traces of a Markov decision process with unknown transitions. A suitable RL algorithm is then used to learn policies that collectively maximize the expected reward and, consequently, increase the probability of satisfying $Ï$. We evaluate HYPRL on a diverse set of benchmarks, including safety-aware planning, Deep Sea Treasure, and the Post Correspondence Problem. We also compare with specification-driven baselines to demonstrate the effectiveness and efficiency of HYPRL.
<div id='section'>Paperid: <span id='pid'>380, <a href='https://arxiv.org/pdf/2503.22958.pdf' target='_blank'>https://arxiv.org/pdf/2503.22958.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Supriyo Maji, Linran Zhao, Souradip Poddar, David Z. Pan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.22958">Late Breaking Results: Breaking Symmetry- Unconventional Placement of Analog Circuits using Multi-Level Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Layout-dependent effects (LDEs) significantly impact analog circuit performance. Traditionally, designers have relied on symmetric placement of circuit components to mitigate variations caused by LDEs. However, due to non-linear nature of these effects, conventional methods often fall short. We propose an objective-driven, multi-level, multi-agent Q-learning framework to explore unconventional design space of analog layout, opening new avenues for optimizing analog circuit performance. Our approach achieves better variation performance than the state-of-the-art layout techniques. Notably, this is the first application of multi-agent RL in analog layout automation. The proposed approach is compared with non-ML approach based on simulated annealing.
<div id='section'>Paperid: <span id='pid'>381, <a href='https://arxiv.org/pdf/2503.15947.pdf' target='_blank'>https://arxiv.org/pdf/2503.15947.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianyi Hu, Qingxu Fu, Zhiqiang Pu, Yuan Wang, Tenghai Qiu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.15947">Unreal-MAP: Unreal-Engine-Based General Platform for Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we propose Unreal Multi-Agent Playground (Unreal-MAP), an MARL general platform based on the Unreal-Engine (UE). Unreal-MAP allows users to freely create multi-agent tasks using the vast visual and physical resources available in the UE community, and deploy state-of-the-art (SOTA) MARL algorithms within them. Unreal-MAP is user-friendly in terms of deployment, modification, and visualization, and all its components are open-source. We also develop an experimental framework compatible with algorithms ranging from rule-based to learning-based provided by third-party frameworks. Lastly, we deploy several SOTA algorithms in example tasks developed via Unreal-MAP, and conduct corresponding experimental analyses. We believe Unreal-MAP can play an important role in the MARL field by closely integrating existing algorithms with user-customized tasks, thus advancing the field of MARL.
<div id='section'>Paperid: <span id='pid'>382, <a href='https://arxiv.org/pdf/2503.13077.pdf' target='_blank'>https://arxiv.org/pdf/2503.13077.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Amir Baghi, Jens SjÃ¶lund, Joakim Bergdahl, Linus GisslÃ©n, Alessandro Sestini
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.13077">Towards Better Sample Efficiency in Multi-Agent Reinforcement Learning via Exploration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent reinforcement learning has shown promise in learning cooperative behaviors in team-based environments. However, such methods often demand extensive training time. For instance, the state-of-the-art method TiZero takes 40 days to train high-quality policies for a football environment. In this paper, we hypothesize that better exploration mechanisms can improve the sample efficiency of multi-agent methods. We propose two different approaches for better exploration in TiZero: a self-supervised intrinsic reward and a random network distillation bonus. Additionally, we introduce architectural modifications to the original algorithm to enhance TiZero's computational efficiency. We evaluate the sample efficiency of these approaches through extensive experiments. Our results show that random network distillation improves training sample efficiency by 18.8% compared to the original TiZero. Furthermore, we evaluate the qualitative behavior of the models produced by both variants against a heuristic AI, with the self-supervised reward encouraging possession and random network distillation leading to a more offensive performance. Our results highlights the applicability of our random network distillation variant in practical settings. Lastly, due to the nature of the proposed method, we acknowledge its use beyond football simulation, especially in environments with strong multi-agent and strategic aspects.
<div id='section'>Paperid: <span id='pid'>383, <a href='https://arxiv.org/pdf/2503.03796.pdf' target='_blank'>https://arxiv.org/pdf/2503.03796.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hyeonjun Kim, Kanghoon Lee, Junho Park, Jiachen Li, Jinkyoo Park
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.03796">Human Implicit Preference-Based Policy Fine-tuning for Multi-Agent Reinforcement Learning in USV Swarm</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-Agent Reinforcement Learning (MARL) has shown promise in solving complex problems involving cooperation and competition among agents, such as an Unmanned Surface Vehicle (USV) swarm used in search and rescue, surveillance, and vessel protection. However, aligning system behavior with user preferences is challenging due to the difficulty of encoding expert intuition into reward functions. To address the issue, we propose a Reinforcement Learning with Human Feedback (RLHF) approach for MARL that resolves credit-assignment challenges through an Agent-Level Feedback system categorizing feedback into intra-agent, inter-agent, and intra-team types. To overcome the challenges of direct human feedback, we employ a Large Language Model (LLM) evaluator to validate our approach using feedback scenarios such as region constraints, collision avoidance, and task allocation. Our method effectively refines USV swarm policies, addressing key challenges in multi-agent systems while maintaining fairness and performance consistency.
<div id='section'>Paperid: <span id='pid'>384, <a href='https://arxiv.org/pdf/2503.02030.pdf' target='_blank'>https://arxiv.org/pdf/2503.02030.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yitao Bai, Sihan Zeng, Justin Romberg, Thinh T. Doan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.02030">Accelerating Multi-Task Temporal Difference Learning under Low-Rank Representation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We study policy evaluation problems in multi-task reinforcement learning (RL) under a low-rank representation setting. In this setting, we are given $N$ learning tasks where the corresponding value function of these tasks lie in an $r$-dimensional subspace, with $r<N$. One can apply the classic temporal-difference (TD) learning method for solving these problems where this method learns the value function of each task independently. In this paper, we are interested in understanding whether one can exploit the low-rank structure of the multi-task setting to accelerate the performance of TD learning. To answer this question, we propose a new variant of TD learning method, where we integrate the so-called truncated singular value decomposition step into the update of TD learning. This additional step will enable TD learning to exploit the dominant directions due to the low rank structure to update the iterates, therefore, improving its performance. Our empirical results show that the proposed method significantly outperforms the classic TD learning, where the performance gap increases as the rank $r$ decreases.
  From the theoretical point of view, introducing the truncated singular value decomposition step into TD learning might cause an instability on the updates. We provide a theoretical result showing that the instability does not happen. Specifically, we prove that the proposed method converges at a rate $\mathcal{O}(\frac{\ln(t)}{t})$, where $t$ is the number of iterations. This rate matches that of the standard TD learning.
<div id='section'>Paperid: <span id='pid'>385, <a href='https://arxiv.org/pdf/2502.06835.pdf' target='_blank'>https://arxiv.org/pdf/2502.06835.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziping Xu, Hinal Jajal, Sung Won Choi, Inbal Nahum-Shani, Guy Shani, Alexandra M. Psihogios, Pei-Yao Hung, Susan Murphy
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.06835">Reinforcement Learning on Dyads to Enhance Medication Adherence</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Medication adherence is critical for the recovery of adolescents and young adults (AYAs) who have undergone hematopoietic cell transplantation (HCT). However, maintaining adherence is challenging for AYAs after hospital discharge, who experience both individual (e.g. physical and emotional symptoms) and interpersonal barriers (e.g., relational difficulties with their care partner, who is often involved in medication management). To optimize the effectiveness of a three-component digital intervention targeting both members of the dyad as well as their relationship, we propose a novel Multi-Agent Reinforcement Learning (MARL) approach to personalize the delivery of interventions. By incorporating the domain knowledge, the MARL framework, where each agent is responsible for the delivery of one intervention component, allows for faster learning compared with a flattened agent. Evaluation using a dyadic simulator environment, based on real clinical data, shows a significant improvement in medication adherence (approximately 3%) compared to purely random intervention delivery. The effectiveness of this approach will be further evaluated in an upcoming trial.
<div id='section'>Paperid: <span id='pid'>386, <a href='https://arxiv.org/pdf/2412.17838.pdf' target='_blank'>https://arxiv.org/pdf/2412.17838.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuyi Wang, Huan Zhao, Yuji Cao, Zibin Pan, Guolong Liu, Gaoqi Liang, Junhua Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.17838">Coordinated Power Smoothing Control for Wind Storage Integrated System with Physics-informed Deep Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The Wind Storage Integrated System with Power Smoothing Control (PSC) has emerged as a promising solution to ensure both efficient and reliable wind energy generation. However, existing PSC strategies overlook the intricate interplay and distinct control frequencies between batteries and wind turbines, and lack consideration of wake effect and battery degradation cost. In this paper, a novel coordinated control framework with hierarchical levels is devised to address these challenges effectively, which integrates the wake model and battery degradation model. In addition, after reformulating the problem as a Markov decision process, the multi-agent reinforcement learning method is introduced to overcome the bi-level characteristic of the problem. Moreover, a Physics-informed Neural Network-assisted Multi-agent Deep Deterministic Policy Gradient (PAMA-DDPG) algorithm is proposed to incorporate the power fluctuation differential equation and expedite the learning process. The effectiveness of the proposed methodology is evaluated through simulations conducted in four distinct scenarios using WindFarmSimulator (WFSim). The results demonstrate that the proposed algorithm facilitates approximately an 11% increase in total profit and a 19% decrease in power fluctuation compared to the traditional methods, thereby addressing the dual objectives of economic efficiency and grid-connected energy reliability.
<div id='section'>Paperid: <span id='pid'>387, <a href='https://arxiv.org/pdf/2411.19526.pdf' target='_blank'>https://arxiv.org/pdf/2411.19526.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yang Lv, Jinlong Lei, Peng Yi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.19526">A Local Information Aggregation based Multi-Agent Reinforcement Learning for Robot Swarm Dynamic Task Allocation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we explore how to optimize task allocation for robot swarms in dynamic environments, emphasizing the necessity of formulating robust, flexible, and scalable strategies for robot cooperation. We introduce a novel framework using a decentralized partially observable Markov decision process (Dec_POMDP), specifically designed for distributed robot swarm networks. At the core of our methodology is the Local Information Aggregation Multi-Agent Deep Deterministic Policy Gradient (LIA_MADDPG) algorithm, which merges centralized training with distributed execution (CTDE). During the centralized training phase, a local information aggregation (LIA) module is meticulously designed to gather critical data from neighboring robots, enhancing decision-making efficiency. In the distributed execution phase, a strategy improvement method is proposed to dynamically adjust task allocation based on changing and partially observable environmental conditions. Our empirical evaluations show that the LIA module can be seamlessly integrated into various CTDE-based MARL methods, significantly enhancing their performance. Additionally, by comparing LIA_MADDPG with six conventional reinforcement learning algorithms and a heuristic algorithm, we demonstrate its superior scalability, rapid adaptation to environmental changes, and ability to maintain both stability and convergence speed. These results underscore LIA_MADDPG's outstanding performance and its potential to significantly improve dynamic task allocation in robot swarms through enhanced local collaboration and adaptive strategy execution.
<div id='section'>Paperid: <span id='pid'>388, <a href='https://arxiv.org/pdf/2410.01954.pdf' target='_blank'>https://arxiv.org/pdf/2410.01954.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>The Viet Bui, Thanh Hong Nguyen, Tien Mai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.01954">ComaDICE: Offline Cooperative Multi-Agent Reinforcement Learning with Stationary Distribution Shift Regularization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Offline reinforcement learning (RL) has garnered significant attention for its ability to learn effective policies from pre-collected datasets without the need for further environmental interactions. While promising results have been demonstrated in single-agent settings, offline multi-agent reinforcement learning (MARL) presents additional challenges due to the large joint state-action space and the complexity of multi-agent behaviors. A key issue in offline RL is the distributional shift, which arises when the target policy being optimized deviates from the behavior policy that generated the data. This problem is exacerbated in MARL due to the interdependence between agents' local policies and the expansive joint state-action space. Prior approaches have primarily addressed this challenge by incorporating regularization in the space of either Q-functions or policies. In this work, we introduce a regularizer in the space of stationary distributions to better handle distributional shift. Our algorithm, ComaDICE, offers a principled framework for offline cooperative MARL by incorporating stationary distribution regularization for the global learning policy, complemented by a carefully structured multi-agent value decomposition strategy to facilitate multi-agent training. Through extensive experiments on the multi-agent MuJoCo and StarCraft II benchmarks, we demonstrate that ComaDICE achieves superior performance compared to state-of-the-art offline MARL methods across nearly all tasks.
<div id='section'>Paperid: <span id='pid'>389, <a href='https://arxiv.org/pdf/2601.21523.pdf' target='_blank'>https://arxiv.org/pdf/2601.21523.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bang Giang Le, Viet Cuong Ta
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.21523">Explicit Credit Assignment through Local Rewards and Dependence Graphs in Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To promote cooperation in Multi-Agent Reinforcement Learning, the reward signals of all agents can be aggregated together, forming global rewards that are commonly known as the fully cooperative setting. However, global rewards are usually noisy because they contain the contributions of all agents, which have to be resolved in the credit assignment process. On the other hand, using local reward benefits from faster learning due to the separation of agents' contributions, but can be suboptimal as agents myopically optimize their own reward while disregarding the global optimality. In this work, we propose a method that combines the merits of both approaches. By using a graph of interaction between agents, our method discerns the individual agent contribution in a more fine-grained manner than a global reward, while alleviating the cooperation problem with agents' local reward. We also introduce a practical approach for approximating such a graph. Our experiments demonstrate the flexibility of the approach, enabling improvements over the traditional local and global reward settings.
<div id='section'>Paperid: <span id='pid'>390, <a href='https://arxiv.org/pdf/2601.08166.pdf' target='_blank'>https://arxiv.org/pdf/2601.08166.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mohammad Pivezhandi, Mahdi Banisharif, Abusayeed Saifullah, Ali Jannesari
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.08166">ZeroDVFS: Zero-Shot LLM-Guided Core and Frequency Allocation for Embedded Platforms</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Dynamic voltage and frequency scaling (DVFS) and task-to-core allocation are critical for thermal management and balancing energy and performance in embedded systems. Existing approaches either rely on utilization-based heuristics that overlook stall times, or require extensive offline profiling for table generation, preventing runtime adaptation. We propose a model-based hierarchical multi-agent reinforcement learning (MARL) framework for thermal- and energy-aware scheduling on multi-core platforms. Two collaborative agents decompose the exponential action space, achieving 358ms latency for subsequent decisions. First decisions require 3.5 to 8.0s including one-time LLM feature extraction. An accurate environment model leverages regression techniques to predict thermal dynamics and performance states. When combined with LLM-extracted semantic features, the environment model enables zero-shot deployment for new workloads on trained platforms by generating synthetic training data without requiring workload-specific profiling samples. We introduce LLM-based semantic feature extraction that characterizes OpenMP programs through 13 code-level features without execution. The Dyna-Q-inspired framework integrates direct reinforcement learning with model-based planning, achieving 20x faster convergence than model-free methods. Experiments on BOTS and PolybenchC benchmarks across NVIDIA Jetson TX2, Jetson Orin NX, RubikPi, and Intel Core i7 demonstrate 7.09x better energy efficiency and 4.0x better makespan than Linux ondemand governor. First-decision latency is 8,300x faster than table-based profiling, enabling practical deployment in dynamic embedded systems.
<div id='section'>Paperid: <span id='pid'>391, <a href='https://arxiv.org/pdf/2512.08341.pdf' target='_blank'>https://arxiv.org/pdf/2512.08341.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Thai Duong Nguyen, Ngoc-Tan Nguyen, Thanh-Dao Nguyen, Nguyen Van Huynh, Dinh-Hieu Tran, Symeon Chatzinotas
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.08341">Multi-Agent Deep Reinforcement Learning for Collaborative UAV Relay Networks under Jamming Atatcks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The deployment of Unmanned Aerial Vehicle (UAV) swarms as dynamic communication relays is critical for next-generation tactical networks. However, operating in contested environments requires solving a complex trade-off, including maximizing system throughput while ensuring collision avoidance and resilience against adversarial jamming. Existing heuristic-based approaches often struggle to find effective solutions due to the dynamic and multi-objective nature of this problem. This paper formulates this challenge as a cooperative Multi-Agent Reinforcement Learning (MARL) problem, solved using the Centralized Training with Decentralized Execution (CTDE) framework. Our approach employs a centralized critic that uses global state information to guide decentralized actors which operate using only local observations. Simulation results show that our proposed framework significantly outperforms heuristic baselines, increasing the total system throughput by approximately 50% while simultaneously achieving a near-zero collision rate. A key finding is that the agents develop an emergent anti-jamming strategy without explicit programming. They learn to intelligently position themselves to balance the trade-off between mitigating interference from jammers and maintaining effective communication links with ground users.
<div id='section'>Paperid: <span id='pid'>392, <a href='https://arxiv.org/pdf/2512.00352.pdf' target='_blank'>https://arxiv.org/pdf/2512.00352.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Na Li, Zewu Zheng, Wei Ni, Hangguan Shan, Wenjie Zhang, Xinyu Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.00352">Sample-Efficient Tabular Self-Play for Offline Robust Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent reinforcement learning (MARL), as a thriving field, explores how multiple agents independently make decisions in a shared dynamic environment. Due to environmental uncertainties, policies in MARL must remain robust to tackle the sim-to-real gap. We focus on robust two-player zero-sum Markov games (TZMGs) in offline settings, specifically on tabular robust TZMGs (RTZMGs). We propose a model-based algorithm (\textit{RTZ-VI-LCB}) for offline RTZMGs, which is optimistic robust value iteration combined with a data-driven Bernstein-style penalty term for robust value estimation. By accounting for distribution shifts in the historical dataset, the proposed algorithm establishes near-optimal sample complexity guarantees under partial coverage and environmental uncertainty. An information-theoretic lower bound is developed to confirm the tightness of our algorithm's sample complexity, which is optimal regarding both state and action spaces. To the best of our knowledge, RTZ-VI-LCB is the first to attain this optimality, sets a new benchmark for offline RTZMGs, and is validated experimentally.
<div id='section'>Paperid: <span id='pid'>393, <a href='https://arxiv.org/pdf/2511.08436.pdf' target='_blank'>https://arxiv.org/pdf/2511.08436.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Satpreet H. Singh, Sonja Johnson-Yu, Zhouyang Lu, Aaron Walsman, Federico Pedraja, Denis Turcu, Pratyusha Sharma, Naomi Saphra, Nathaniel B. Sawtell, Kanaka Rajan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.08436">Understanding Electro-communication and Electro-sensing in Weakly Electric Fish using Multi-Agent Deep Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Weakly electric fish, like Gnathonemus petersii, use a remarkable electrical modality for active sensing and communication, but studying their rich electrosensing and electrocommunication behavior and associated neural activity in naturalistic settings remains experimentally challenging. Here, we present a novel biologically-inspired computational framework to study these behaviors, where recurrent neural network (RNN) based artificial agents trained via multi-agent reinforcement learning (MARL) learn to modulate their electric organ discharges (EODs) and movement patterns to collectively forage in virtual environments. Trained agents demonstrate several emergent features consistent with real fish collectives, including heavy tailed EOD interval distributions, environmental context dependent shifts in EOD interval distributions, and social interaction patterns like freeloading, where agents reduce their EOD rates while benefiting from neighboring agents' active sensing. A minimal two-fish assay further isolates the role of electro-communication, showing that access to conspecific EODs and relative dominance jointly shape foraging success. Notably, these behaviors emerge through evolution-inspired rewards for individual fitness and emergent inter-agent interactions, rather than through rewarding agents explicitly for social interactions. Our work has broad implications for the neuroethology of weakly electric fish, as well as other social, communicating animals in which extensive recordings from multiple individuals, and thus traditional data-driven modeling, are infeasible.
<div id='section'>Paperid: <span id='pid'>394, <a href='https://arxiv.org/pdf/2509.17353.pdf' target='_blank'>https://arxiv.org/pdf/2509.17353.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ahmed T. Elboardy, Ghada Khoriba, Essam A. Rashed
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.17353">Medical AI Consensus: A Multi-Agent Framework for Radiology Report Generation and Evaluation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Automating radiology report generation poses a dual challenge: building clinically reliable systems and designing rigorous evaluation protocols. We introduce a multi-agent reinforcement learning framework that serves as both a benchmark and evaluation environment for multimodal clinical reasoning in the radiology ecosystem. The proposed framework integrates large language models (LLMs) and large vision models (LVMs) within a modular architecture composed of ten specialized agents responsible for image analysis, feature extraction, report generation, review, and evaluation. This design enables fine-grained assessment at both the agent level (e.g., detection and segmentation accuracy) and the consensus level (e.g., report quality and clinical relevance). We demonstrate an implementation using chatGPT-4o on public radiology datasets, where LLMs act as evaluators alongside medical radiologist feedback. By aligning evaluation protocols with the LLM development lifecycle, including pretraining, finetuning, alignment, and deployment, the proposed benchmark establishes a path toward trustworthy deviance-based radiology report generation.
<div id='section'>Paperid: <span id='pid'>395, <a href='https://arxiv.org/pdf/2509.14431.pdf' target='_blank'>https://arxiv.org/pdf/2509.14431.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Keqin Wang, Tao Zhong, David Chang, Christine Allen-Blanchette
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.14431">Local-Canonicalization Equivariant Graph Neural Networks for Sample-Efficient and Generalizable Swarm Robot Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent reinforcement learning (MARL) has emerged as a powerful paradigm for coordinating swarms of agents in complex decision-making, yet major challenges remain. In competitive settings such as pursuer-evader tasks, simultaneous adaptation can destabilize training; non-kinetic countermeasures often fail under adverse conditions; and policies trained in one configuration rarely generalize to environments with a different number of agents. To address these issues, we propose the Local-Canonicalization Equivariant Graph Neural Networks (LEGO) framework, which integrates seamlessly with popular MARL algorithms such as MAPPO. LEGO employs graph neural networks to capture permutation equivariance and generalization to different agent numbers, canonicalization to enforce E(n)-equivariance, and heterogeneous representations to encode role-specific inductive biases. Experiments on cooperative and competitive swarm benchmarks show that LEGO outperforms strong baselines and improves generalization. In real-world experiments, LEGO demonstrates robustness to varying team sizes and agent failure.
<div id='section'>Paperid: <span id='pid'>396, <a href='https://arxiv.org/pdf/2508.07001.pdf' target='_blank'>https://arxiv.org/pdf/2508.07001.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Myeung Suk Oh, Zhiyao Zhang, FNU Hairi, Alvaro Velasquez, Jia Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.07001">Consensus-based Decentralized Multi-agent Reinforcement Learning for Random Access Network Optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With wireless devices increasingly forming a unified smart network for seamless, user-friendly operations, random access (RA) medium access control (MAC) design is considered a key solution for handling unpredictable data traffic from multiple terminals. However, it remains challenging to design an effective RA-based MAC protocol to minimize collisions and ensure transmission fairness across the devices. While existing multi-agent reinforcement learning (MARL) approaches with centralized training and decentralized execution (CTDE) have been proposed to optimize RA performance, their reliance on centralized training and the significant overhead required for information collection can make real-world applications unrealistic. In this work, we adopt a fully decentralized MARL architecture, where policy learning does not rely on centralized tasks but leverages consensus-based information exchanges across devices. We design our MARL algorithm over an actor-critic (AC) network and propose exchanging only local rewards to minimize communication overhead. Furthermore, we provide a theoretical proof of global convergence for our approach. Numerical experiments show that our proposed MARL algorithm can significantly improve RA network performance compared to other baselines.
<div id='section'>Paperid: <span id='pid'>397, <a href='https://arxiv.org/pdf/2507.15351.pdf' target='_blank'>https://arxiv.org/pdf/2507.15351.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zijian Zhao, Sen Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.15351">One Step is Enough: Multi-Agent Reinforcement Learning based on One-Step Policy Optimization for Order Dispatch on Ride-Sharing Platforms</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>On-demand ride-sharing platforms face the fundamental challenge of dynamically bundling passengers with diverse origins and destinations and matching them with vehicles in real time, all under significant uncertainty. Recently, MARL has emerged as a promising solution for this problem, leveraging decentralized learning to address the curse of dimensionality caused by the large number of agents in the ride-hailing market and the resulting expansive state and action spaces. However, conventional MARL-based ride-sharing approaches heavily rely on the accurate estimation of Q-values or V-values, which becomes problematic in large-scale, highly uncertain environments. Specifically, most of these approaches adopt an independent paradigm, exacerbating this issue, as each agent treats others as part of the environment, leading to unstable training and substantial estimation bias in value functions. To address these challenges, we propose two novel alternative methods that bypass value function estimation. First, we adapt GRPO to ride-sharing, replacing the PPO baseline with the group average reward to eliminate critic estimation errors and reduce training bias. Second, inspired by GRPO's full utilization of group reward information, we customize the PPO framework for ride-sharing platforms and show that, under a homogeneous fleet, the optimal policy can be trained using only one-step rewards - a method we term One-Step Policy Optimization (OSPO). Experiments on a real-world Manhattan ride-hailing dataset demonstrate that both GRPO and OSPO achieve superior performance across most scenarios, efficiently optimizing pickup times and the number of served orders using simple MLP networks.
<div id='section'>Paperid: <span id='pid'>398, <a href='https://arxiv.org/pdf/2507.09094.pdf' target='_blank'>https://arxiv.org/pdf/2507.09094.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaoren Xu, Hao Xu, Dongyu Wei, Walid Saad, Mehdi Bennis, Mingzhe Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.09094">Transformer based Collaborative Reinforcement Learning for Fluid Antenna System (FAS)-enabled 3D UAV Positioning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, a novel Three dimensional (3D) positioning framework of fluid antenna system (FAS)-enabled unmanned aerial vehicles (UAVs) is developed. In the proposed framework, a set of controlled UAVs cooperatively estimate the real-time 3D position of a target UAV. Here, the active UAV transmits a measurement signal to the passive UAVs via the reflection from the target UAV. Each passive UAV estimates the distance of the active-target-passive UAV link and selects an antenna port to share the distance information with the base station (BS) that calculates the real-time position of the target UAV. As the target UAV is moving due to its task operation, the controlled UAVs must optimize their trajectories and select optimal antenna port, aiming to estimate the real-time position of the target UAV. We formulate this problem as an optimization problem to minimize the target UAV positioning error via optimizing the trajectories of all controlled UAVs and antenna port selection of passive UAVs. Here, an attention-based recurrent multi-agent reinforcement learning (AR-MARL) scheme is proposed, which enables each controlled UAV to use the local Q function to determine its trajectory and antenna port while optimizing the target UAV positioning performance without knowing the trajectories and antenna port selections of other controlled UAVs. Different from current MARL methods, the proposed method uses a recurrent neural network (RNN) that incorporates historical state-action pairs of each controlled UAV, and an attention mechanism to analyze the importance of these historical state-action pairs, thus improving the global Q function approximation accuracy and the target UAV positioning accuracy. Simulation results show that the proposed AR-MARL scheme can reduce the average positioning error by up to 17.5% and 58.5% compared to the VD-MARL scheme and the proposed method without FAS.
<div id='section'>Paperid: <span id='pid'>399, <a href='https://arxiv.org/pdf/2506.20664.pdf' target='_blank'>https://arxiv.org/pdf/2506.20664.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Andrei Lupu, Timon Willi, Jakob Foerster
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.20664">The Decrypto Benchmark for Multi-Agent Reasoning and Theory of Mind</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As Large Language Models (LLMs) gain agentic abilities, they will have to navigate complex multi-agent scenarios, interacting with human users and other agents in cooperative and competitive settings. This will require new reasoning skills, chief amongst them being theory of mind (ToM), or the ability to reason about the "mental" states of other agents. However, ToM and other multi-agent abilities in LLMs are poorly understood, since existing benchmarks suffer from narrow scope, data leakage, saturation, and lack of interactivity. We thus propose Decrypto, a game-based benchmark for multi-agent reasoning and ToM drawing inspiration from cognitive science, computational pragmatics and multi-agent reinforcement learning. It is designed to be as easy as possible in all other dimensions, eliminating confounding factors commonly found in other benchmarks. To our knowledge, it is also the first platform for designing interactive ToM experiments.
  We validate the benchmark design through comprehensive empirical evaluations of frontier LLMs, robustness studies, and human-AI cross-play experiments. We find that LLM game-playing abilities lag behind humans and simple word-embedding baselines. We then create variants of two classic cognitive science experiments within Decrypto to evaluate three key ToM abilities. Surprisingly, we find that state-of-the-art reasoning models are significantly worse at those tasks than their older counterparts. This demonstrates that Decrypto addresses a crucial gap in current reasoning and ToM evaluations, and paves the path towards better artificial agents.
<div id='section'>Paperid: <span id='pid'>400, <a href='https://arxiv.org/pdf/2506.11445.pdf' target='_blank'>https://arxiv.org/pdf/2506.11445.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuan Duy Ta, Bang Giang Le, Thanh Ha Le, Viet Cuong Ta
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.11445">Resolve Highway Conflict in Multi-Autonomous Vehicle Controls with Local State Attention</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In mixed-traffic environments, autonomous vehicles must adapt to human-controlled vehicles and other unusual driving situations. This setting can be framed as a multi-agent reinforcement learning (MARL) environment with full cooperative reward among the autonomous vehicles. While methods such as Multi-agent Proximal Policy Optimization can be effective in training MARL tasks, they often fail to resolve local conflict between agents and are unable to generalize to stochastic events. In this paper, we propose a Local State Attention module to assist the input state representation. By relying on the self-attention operator, the module is expected to compress the essential information of nearby agents to resolve the conflict in traffic situations. Utilizing a simulated highway merging scenario with the priority vehicle as the unexpected event, our approach is able to prioritize other vehicles' information to manage the merging process. The results demonstrate significant improvements in merging efficiency compared to popular baselines, especially in high-density traffic settings.
<div id='section'>Paperid: <span id='pid'>401, <a href='https://arxiv.org/pdf/2505.18433.pdf' target='_blank'>https://arxiv.org/pdf/2505.18433.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiyao Zhang, Myeung Suk Oh, FNU Hairi, Ziyue Luo, Alvaro Velasquez, Jia Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.18433">Finite-Time Global Optimality Convergence in Deep Neural Actor-Critic Methods for Decentralized Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Actor-critic methods for decentralized multi-agent reinforcement learning (MARL) facilitate collaborative optimal decision making without centralized coordination, thus enabling a wide range of applications in practice. To date, however, most theoretical convergence studies for existing actor-critic decentralized MARL methods are limited to the guarantee of a stationary solution under the linear function approximation. This leaves a significant gap between the highly successful use of deep neural actor-critic for decentralized MARL in practice and the current theoretical understanding. To bridge this gap, in this paper, we make the first attempt to develop a deep neural actor-critic method for decentralized MARL, where both the actor and critic components are inherently non-linear. We show that our proposed method enjoys a global optimality guarantee with a finite-time convergence rate of O(1/T), where T is the total iteration times. This marks the first global convergence result for deep neural actor-critic methods in the MARL literature. We also conduct extensive numerical experiments, which verify our theoretical results.
<div id='section'>Paperid: <span id='pid'>402, <a href='https://arxiv.org/pdf/2504.17590.pdf' target='_blank'>https://arxiv.org/pdf/2504.17590.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sihem Bakri, Indrakshi Dey, Harun Siljak, Marco Ruffini, Nicola Marchetti
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.17590">Mitigating xApp conflicts for efficient network slicing in 6G O-RAN: a graph convolutional-based attention network approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>O-RAN (Open-Radio Access Network) offers a flexible, open architecture for next-generation wireless networks. Network slicing within O-RAN allows network operators to create customized virtual networks, each tailored to meet the specific needs of a particular application or service. Efficiently managing these slices is crucial for future 6G networks. O-RAN introduces specialized software applications called xApps that manage different network functions. In network slicing, an xApp can be responsible for managing a separate network slice. To optimize resource allocation across numerous network slices, these xApps must coordinate. Traditional methods where all xApps communicate freely can lead to excessive overhead, hindering network performance. In this paper, we address the issue of xApp conflict mitigation by proposing an innovative Zero-Touch Management (ZTM) solution for radio resource management in O-RAN. Our approach leverages Multi-Agent Reinforcement Learning (MARL) to enable xApps to learn and optimize resource allocation without the need for constant manual intervention. We introduce a Graph Convolutional Network (GCN)-based attention mechanism to streamline communication among xApps, reducing overhead and improving overall system efficiency. Our results compare traditional MARL, where all xApps communicate, against our MARL GCN-based attention method. The findings demonstrate the superiority of our approach, especially as the number of xApps increases, ultimately providing a scalable and efficient solution for optimal network slicing management in O-RAN.
<div id='section'>Paperid: <span id='pid'>403, <a href='https://arxiv.org/pdf/2504.10677.pdf' target='_blank'>https://arxiv.org/pdf/2504.10677.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Muhammad Al-Zafar Khan, Jamal Al-Karaki
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.10677">Achieving Optimal Tissue Repair Through MARL with Reward Shaping and Curriculum Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we present a multi-agent reinforcement learning (MARL) framework for optimizing tissue repair processes using engineered biological agents. Our approach integrates: (1) stochastic reaction-diffusion systems modeling molecular signaling, (2) neural-like electrochemical communication with Hebbian plasticity, and (3) a biologically informed reward function combining chemical gradient tracking, neural synchronization, and robust penalties. A curriculum learning scheme guides the agent through progressively complex repair scenarios. In silico experiments demonstrate emergent repair strategies, including dynamic secretion control and spatial coordination.
<div id='section'>Paperid: <span id='pid'>404, <a href='https://arxiv.org/pdf/2503.19699.pdf' target='_blank'>https://arxiv.org/pdf/2503.19699.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Muhammad Al-Zafar Khan, Jamal Al-Karaki
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.19699">Optimal Path Planning and Cost Minimization for a Drone Delivery System Via Model Predictive Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this study, we formulate the drone delivery problem as a control problem and solve it using Model Predictive Control. Two experiments are performed: The first is on a less challenging grid world environment with lower dimensionality, and the second is with a higher dimensionality and added complexity. The MPC method was benchmarked against three popular Multi-Agent Reinforcement Learning (MARL): Independent $Q$-Learning (IQL), Joint Action Learners (JAL), and Value-Decomposition Networks (VDN). It was shown that the MPC method solved the problem quicker and required fewer optimal numbers of drones to achieve a minimized cost and navigate the optimal path.
<div id='section'>Paperid: <span id='pid'>405, <a href='https://arxiv.org/pdf/2502.04864.pdf' target='_blank'>https://arxiv.org/pdf/2502.04864.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aditya Kapoor, Kale-ab Tessera, Mayank Baranwal, Harshad Khadilkar, Stefano Albrecht, Mingfei Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.04864">$TAR^2$: Temporal-Agent Reward Redistribution for Optimal Policy Preservation in Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In cooperative multi-agent reinforcement learning (MARL), learning effective policies is challenging when global rewards are sparse and delayed. This difficulty arises from the need to assign credit across both agents and time steps, a problem that existing methods often fail to address in episodic, long-horizon tasks. We propose Temporal-Agent Reward Redistribution $TAR^2$, a novel approach that decomposes sparse global rewards into agent-specific, time-step-specific components, thereby providing more frequent and accurate feedback for policy learning. Theoretically, we show that $TAR^2$ (i) aligns with potential-based reward shaping, preserving the same optimal policies as the original environment, and (ii) maintains policy gradient update directions identical to those under the original sparse reward, ensuring unbiased credit signals. Empirical results on two challenging benchmarks, SMACLite and Google Research Football, demonstrate that $TAR^2$ significantly stabilizes and accelerates convergence, outperforming strong baselines like AREL and STAS in both learning speed and final performance. These findings establish $TAR^2$ as a principled and practical solution for agent-temporal credit assignment in sparse-reward multi-agent systems.
<div id='section'>Paperid: <span id='pid'>406, <a href='https://arxiv.org/pdf/2502.04038.pdf' target='_blank'>https://arxiv.org/pdf/2502.04038.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuchen Lian, Arianna Bisazza, Tessa Verhoef
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.04038">Simulating the Emergence of Differential Case Marking with Communicating Neural-Network Agents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Differential Case Marking (DCM) refers to the phenomenon where grammatical case marking is applied selectively based on semantic, pragmatic, or other factors. The emergence of DCM has been studied in artificial language learning experiments with human participants, which were specifically aimed at disentangling the effects of learning from those of communication (Smith & Culbertson, 2020). Multi-agent reinforcement learning frameworks based on neural networks have gained significant interest to simulate the emergence of human-like linguistic phenomena. In this study, we employ such a framework in which agents first acquire an artificial language before engaging in communicative interactions, enabling direct comparisons to human result. Using a very generic communication optimization algorithm and neural-network learners that have no prior experience with language or semantic preferences, our results demonstrate that learning alone does not lead to DCM, but when agents communicate, differential use of markers arises. This supports Smith and Culbertson (2020)'s findings that highlight the critical role of communication in shaping DCM and showcases the potential of neural-agent models to complement experimental research on language evolution.
<div id='section'>Paperid: <span id='pid'>407, <a href='https://arxiv.org/pdf/2501.02888.pdf' target='_blank'>https://arxiv.org/pdf/2501.02888.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chuxiong Sun, Peng He, Rui Wang, Changwen Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.02888">Revisiting Communication Efficiency in Multi-Agent Reinforcement Learning from the Dimensional Analysis Perspective</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this work, we introduce a novel perspective, i.e., dimensional analysis, to address the challenge of communication efficiency in Multi-Agent Reinforcement Learning (MARL). Our findings reveal that simply optimizing the content and timing of communication at sending end is insufficient to fully resolve communication efficiency issues. Even after applying optimized and gated messages, dimensional redundancy and confounders still persist in the integrated message embeddings at receiving end, which negatively impact communication quality and decision-making. To address these challenges, we propose Dimensional Rational Multi-Agent Communication (DRMAC), designed to mitigate both dimensional redundancy and confounders in MARL. DRMAC incorporates a redundancy-reduction regularization term to encourage the decoupling of information across dimensions within the learned representations of integrated messages. Additionally, we introduce a dimensional mask that dynamically adjusts gradient weights during training to eliminate the influence of decision-irrelevant dimensions. We evaluate DRMAC across a diverse set of multi-agent tasks, demonstrating its superior performance over existing state-of-the-art methods in complex scenarios. Furthermore, the plug-and-play nature of DRMAC's key modules highlights its generalizable performance, serving as a valuable complement rather than a replacement for existing multi-agent communication strategies.
<div id='section'>Paperid: <span id='pid'>408, <a href='https://arxiv.org/pdf/2501.02174.pdf' target='_blank'>https://arxiv.org/pdf/2501.02174.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Peihong Yu, Manav Mishra, Syed Zaidi, Pratap Tokekar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.02174">TACTIC: Task-Agnostic Contrastive pre-Training for Inter-Agent Communication</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The "sight range dilemma" in cooperative Multi-Agent Reinforcement Learning (MARL) presents a significant challenge: limited observability hinders team coordination, while extensive sight ranges lead to distracted attention and reduced performance. While communication can potentially address this issue, existing methods often struggle to generalize across different sight ranges, limiting their effectiveness. We propose TACTIC, Task-Agnostic Contrastive pre-Training strategy Inter-Agent Communication. TACTIC is an adaptive communication mechanism that enhances agent coordination even when the sight range during execution is vastly different from that during training. The communication mechanism encodes messages and integrates them with local observations, generating representations grounded in the global state using contrastive learning. By learning to generate and interpret messages that capture important information about the whole environment, TACTIC enables agents to effectively "see" more through communication, regardless of their sight ranges. We comprehensively evaluate TACTIC on the SMACv2 benchmark across various scenarios with broad sight ranges. The results demonstrate that TACTIC consistently outperforms traditional state-of-the-art MARL techniques with and without communication, in terms of generalizing to sight ranges different from those seen in training, particularly in cases of extremely limited or extensive observability.
<div id='section'>Paperid: <span id='pid'>409, <a href='https://arxiv.org/pdf/2412.14779.pdf' target='_blank'>https://arxiv.org/pdf/2412.14779.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aditya Kapoor, Sushant Swamy, Kale-ab Tessera, Mayank Baranwal, Mingfei Sun, Harshad Khadilkar, Stefano V. Albrecht
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.14779">Agent-Temporal Credit Assignment for Optimal Policy Preservation in Sparse Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In multi-agent environments, agents often struggle to learn optimal policies due to sparse or delayed global rewards, particularly in long-horizon tasks where it is challenging to evaluate actions at intermediate time steps. We introduce Temporal-Agent Reward Redistribution (TAR$^2$), a novel approach designed to address the agent-temporal credit assignment problem by redistributing sparse rewards both temporally and across agents. TAR$^2$ decomposes sparse global rewards into time-step-specific rewards and calculates agent-specific contributions to these rewards. We theoretically prove that TAR$^2$ is equivalent to potential-based reward shaping, ensuring that the optimal policy remains unchanged. Empirical results demonstrate that TAR$^2$ stabilizes and accelerates the learning process. Additionally, we show that when TAR$^2$ is integrated with single-agent reinforcement learning algorithms, it performs as well as or better than traditional multi-agent reinforcement learning methods.
<div id='section'>Paperid: <span id='pid'>410, <a href='https://arxiv.org/pdf/2409.19831.pdf' target='_blank'>https://arxiv.org/pdf/2409.19831.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhengran Ji, Lingyu Zhang, Paul Sajda, Boyuan Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.19831">Enabling Multi-Robot Collaboration from Single-Human Guidance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Learning collaborative behaviors is essential for multi-agent systems. Traditionally, multi-agent reinforcement learning solves this implicitly through a joint reward and centralized observations, assuming collaborative behavior will emerge. Other studies propose to learn from demonstrations of a group of collaborative experts. Instead, we propose an efficient and explicit way of learning collaborative behaviors in multi-agent systems by leveraging expertise from only a single human. Our insight is that humans can naturally take on various roles in a team. We show that agents can effectively learn to collaborate by allowing a human operator to dynamically switch between controlling agents for a short period and incorporating a human-like theory-of-mind model of teammates. Our experiments showed that our method improves the success rate of a challenging collaborative hide-and-seek task by up to 58% with only 40 minutes of human guidance. We further demonstrate our findings transfer to the real world by conducting multi-robot experiments.
<div id='section'>Paperid: <span id='pid'>411, <a href='https://arxiv.org/pdf/2601.18419.pdf' target='_blank'>https://arxiv.org/pdf/2601.18419.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Michael Kölle, Christian Reff, Leo Sünkel, Julian Hager, Gerhard Stenzel, Claudia Linnhoff-Popien
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.18419">Emergent Cooperation in Quantum Multi-Agent Reinforcement Learning Using Communication</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Emergent cooperation in classical Multi-Agent Reinforcement Learning has gained significant attention, particularly in the context of Sequential Social Dilemmas (SSDs). While classical reinforcement learning approaches have demonstrated capability for emergent cooperation, research on extending these methods to Quantum Multi-Agent Reinforcement Learning remains limited, particularly through communication. In this paper, we apply communication approaches to quantum Q-Learning agents: the Mutual Acknowledgment Token Exchange (MATE) protocol, its extension Mutually Endorsed Distributed Incentive Acknowledgment Token Exchange (MEDIATE), the peer rewarding mechanism Gifting, and Reinforced Inter-Agent Learning (RIAL). We evaluate these approaches in three SSDs: the Iterated Prisoner's Dilemma, Iterated Stag Hunt, and Iterated Game of Chicken. Our experimental results show that approaches using MATE with temporal-difference measure (MATE\textsubscript{TD}), AutoMATE, MEDIATE-I, and MEDIATE-S achieved high cooperation levels across all dilemmas, demonstrating that communication is a viable mechanism for fostering emergent cooperation in Quantum Multi-Agent Reinforcement Learning.
<div id='section'>Paperid: <span id='pid'>412, <a href='https://arxiv.org/pdf/2601.16276.pdf' target='_blank'>https://arxiv.org/pdf/2601.16276.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Victor Conchello Vendrell, Max Ruiz Luyten, Mihaela van der Schaar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.16276">GameTalk: Training LLMs for Strategic Conversation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Strategic decision-making in multi-agent settings is a key challenge for large language models (LLMs), particularly when coordination and negotiation must unfold over extended conversations. While recent work has explored the use of LLMs in isolated decision tasks, little attention has been given to optimizing long-term objectives through dialogue. We introduce \textbf{GameTalk}, a framework for training LLMs to make strategic decisions via multi-turn interactions. Unlike prior work that focuses on single-turn objectives or static action prediction, we train LLMs to optimize a global objective across full conversations. We achieve this by adapting fine-tuning methods like GRPO, DPO, and STaR to incorporate reward signals that depend on the entire interaction. We evaluate this approach on a suite of increasingly complex games, designed to stress different aspects of reasoning, coordination, and opponent modeling. Our results show that GameTalk significantly outperforms untrained models, especially under reward shaping, with DPO consistently yielding the strongest gains. These findings position conversational fine-tuning as a promising path for LLMs to reason, negotiate, and act in interactive environments.
<div id='section'>Paperid: <span id='pid'>413, <a href='https://arxiv.org/pdf/2512.24325.pdf' target='_blank'>https://arxiv.org/pdf/2512.24325.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wan Jiang, Xinyi Zang, Yudong Zhao, Yusi Zou, Yunfei Lu, Junbo Tong, Yang Liu, Ming Li, Jiani Shi, Xin Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.24325">MaRCA: Multi-Agent Reinforcement Learning for Dynamic Computation Allocation in Large-Scale Recommender Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Modern recommender systems face significant computational challenges due to growing model complexity and traffic scale, making efficient computation allocation critical for maximizing business revenue. Existing approaches typically simplify multi-stage computation resource allocation, neglecting inter-stage dependencies, thus limiting global optimality. In this paper, we propose MaRCA, a multi-agent reinforcement learning framework for end-to-end computation resource allocation in large-scale recommender systems. MaRCA models the stages of a recommender system as cooperative agents, using Centralized Training with Decentralized Execution (CTDE) to optimize revenue under computation resource constraints. We introduce an AutoBucket TestBench for accurate computation cost estimation, and a Model Predictive Control (MPC)-based Revenue-Cost Balancer to proactively forecast traffic loads and adjust the revenue-cost trade-off accordingly. Since its end-to-end deployment in the advertising pipeline of a leading global e-commerce platform in November 2024, MaRCA has consistently handled hundreds of billions of ad requests per day and has delivered a 16.67% revenue uplift using existing computation resources.
<div id='section'>Paperid: <span id='pid'>414, <a href='https://arxiv.org/pdf/2511.16916.pdf' target='_blank'>https://arxiv.org/pdf/2511.16916.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ye Han, Lijun Zhang, Dejian Meng, Zhuang Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.16916">Hybrid Differential Reward: Combining Temporal Difference and Action Gradients for Efficient Multi-Agent Reinforcement Learning in Cooperative Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In multi-vehicle cooperative driving tasks involving high-frequency continuous control, traditional state-based reward functions suffer from the issue of vanishing reward differences. This phenomenon results in a low signal-to-noise ratio (SNR) for policy gradients, significantly hindering algorithm convergence and performance improvement. To address this challenge, this paper proposes a novel Hybrid Differential Reward (HDR) mechanism. We first theoretically elucidate how the temporal quasi-steady nature of traffic states and the physical proximity of actions lead to the failure of traditional reward signals. Building on this analysis, the HDR framework innovatively integrates two complementary components: (1) a Temporal Difference Reward (TRD) based on a global potential function, which utilizes the evolutionary trend of potential energy to ensure optimal policy invariance and consistency with long-term objectives; and (2) an Action Gradient Reward (ARG), which directly measures the marginal utility of actions to provide a local guidance signal with a high SNR. Furthermore, we formulate the cooperative driving problem as a Multi-Agent Partially Observable Markov Game (POMDPG) with a time-varying agent set and provide a complete instantiation scheme for HDR within this framework. Extensive experiments conducted using both online planning (MCTS) and Multi-Agent Reinforcement Learning (QMIX, MAPPO, MADDPG) algorithms demonstrate that the HDR mechanism significantly improves convergence speed and policy stability. The results confirm that HDR guides agents to learn high-quality cooperative policies that effectively balance traffic efficiency and safety.
<div id='section'>Paperid: <span id='pid'>415, <a href='https://arxiv.org/pdf/2511.01078.pdf' target='_blank'>https://arxiv.org/pdf/2511.01078.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qinwei Huang, Stefan Wang, Simon Khan, Garrett Katz, Qinru Qiu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.01078">Predictive Auxiliary Learning for Belief-based Multi-Agent Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The performance of multi-agent reinforcement learning (MARL) in partially observable environments depends on effectively aggregating information from observations, communications, and reward signals. While most existing multi-agent systems primarily rely on rewards as the only feedback for policy training, our research shows that introducing auxiliary predictive tasks can significantly enhance learning efficiency and stability. We propose Belief-based Predictive Auxiliary Learning (BEPAL), a framework that incorporates auxiliary training objectives to support policy optimization. BEPAL follows the centralized training with decentralized execution paradigm. Each agent learns a belief model that predicts unobservable state information, such as other agents' rewards or motion directions, alongside its policy model. By enriching hidden state representations with information that does not directly contribute to immediate reward maximization, this auxiliary learning process stabilizes MARL training and improves overall performance. We evaluate BEPAL in the predator-prey environment and Google Research Football, where it achieves an average improvement of about 16 percent in performance metrics and demonstrates more stable convergence compared to baseline methods.
<div id='section'>Paperid: <span id='pid'>416, <a href='https://arxiv.org/pdf/2510.08607.pdf' target='_blank'>https://arxiv.org/pdf/2510.08607.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhaoqilin Yang, Chanchan Li, Tianqi Liu, Hongxin Zhao, Youliang Tian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.08607">GRPO-GCC: Enhancing Cooperation in Spatial Public Goods Games via Group Relative Policy Optimization with Global Cooperation Constraint</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Inspired by the principle of self-regulating cooperation in collective institutions, we propose the Group Relative Policy Optimization with Global Cooperation Constraint (GRPO-GCC) framework. This work is the first to introduce GRPO into spatial public goods games, establishing a new deep reinforcement learning baseline for structured populations. GRPO-GCC integrates group relative policy optimization with a global cooperation constraint that strengthens incentives at intermediate cooperation levels while weakening them at extremes. This mechanism aligns local decision making with sustainable collective outcomes and prevents collapse into either universal defection or unconditional cooperation. The framework advances beyond existing approaches by combining group-normalized advantage estimation, a reference-anchored KL penalty, and a global incentive term that dynamically adjusts cooperative payoffs. As a result, it achieves accelerated cooperation onset, stabilized policy adaptation, and long-term sustainability. GRPO-GCC demonstrates how a simple yet global signal can reshape incentives toward resilient cooperation, and provides a new paradigm for multi-agent reinforcement learning in socio-technical systems.
<div id='section'>Paperid: <span id='pid'>417, <a href='https://arxiv.org/pdf/2510.07813.pdf' target='_blank'>https://arxiv.org/pdf/2510.07813.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Valerio La Gatta, Dolev Mutzari, Sarit Kraus, VS Subrahmanian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.07813">Strategic Communication under Threat: Learning Information Trade-offs in Pursuit-Evasion Games</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Adversarial environments require agents to navigate a key strategic trade-off: acquiring information enhances situational awareness, but may simultaneously expose them to threats. To investigate this tension, we formulate a PursuitEvasion-Exposure-Concealment Game (PEEC) in which a pursuer agent must decide when to communicate in order to obtain the evader's position. Each communication reveals the pursuer's location, increasing the risk of being targeted. Both agents learn their movement policies via reinforcement learning, while the pursuer additionally learns a communication policy that balances observability and risk. We propose SHADOW (Strategic-communication Hybrid Action Decision-making under partial Observation for Warfare), a multi-headed sequential reinforcement learning framework that integrates continuous navigation control, discrete communication actions, and opponent modeling for behavior prediction. Empirical evaluations show that SHADOW pursuers achieve higher success rates than six competitive baselines. Our ablation study confirms that temporal sequence modeling and opponent modeling are critical for effective decision-making. Finally, our sensitivity analysis reveals that the learned policies generalize well across varying communication risks and physical asymmetries between agents.
<div id='section'>Paperid: <span id='pid'>418, <a href='https://arxiv.org/pdf/2510.03534.pdf' target='_blank'>https://arxiv.org/pdf/2510.03534.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nicolò Dal Fabbro, Milad Mesbahi, Renato Mendes, João Borges de Sousa, George J. Pappas
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.03534">Long-Term Mapping of the Douro River Plume with Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We study the problem of long-term (multiple days) mapping of a river plume using multiple autonomous underwater vehicles (AUVs), focusing on the Douro river representative use-case. We propose an energy - and communication - efficient multi-agent reinforcement learning approach in which a central coordinator intermittently communicates with the AUVs, collecting measurements and issuing commands. Our approach integrates spatiotemporal Gaussian process regression (GPR) with a multi-head Q-network controller that regulates direction and speed for each AUV. Simulations using the Delft3D ocean model demonstrate that our method consistently outperforms both single- and multi-agent benchmarks, with scaling the number of agents both improving mean squared error (MSE) and operational endurance. In some instances, our algorithm demonstrates that doubling the number of AUVs can more than double endurance while maintaining or improving accuracy, underscoring the benefits of multi-agent coordination. Our learned policies generalize across unseen seasonal regimes over different months and years, demonstrating promise for future developments of data-driven long-term monitoring of dynamic plume environments.
<div id='section'>Paperid: <span id='pid'>419, <a href='https://arxiv.org/pdf/2509.23157.pdf' target='_blank'>https://arxiv.org/pdf/2509.23157.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yanqing Fu, Chao Huang, Chenrun Wang, Zhuping Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.23157">Grouped Satisficing Paths in Pure Strategy Games: a Topological Perspective</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In game theory and multi-agent reinforcement learning (MARL), each agent selects a strategy, interacts with the environment and other agents, and subsequently updates its strategy based on the received payoff. This process generates a sequence of joint strategies $(s^t)_{t \geq 0}$, where $s^t$ represents the strategy profile of all agents at time step $t$. A widely adopted principle in MARL algorithms is "win-stay, lose-shift", which dictates that an agent retains its current strategy if it achieves the best response. This principle exhibits a fixed-point property when the joint strategy has become an equilibrium. The sequence of joint strategies under this principle is referred to as a satisficing path, a concept first introduced in [40] and explored in the context of $N$-player games in [39]. A fundamental question arises regarding this principle: Under what conditions does every initial joint strategy $s$ admit a finite-length satisficing path $(s^t)_{0 \leq t \leq T}$ where $s^0=s$ and $s^T$ is an equilibrium? This paper establishes a sufficient condition for such a property, and demonstrates that any finite-state Markov game, as well as any $N$-player game, guarantees the existence of a finite-length satisficing path from an arbitrary initial strategy to some equilibrium. These results provide a stronger theoretical foundation for the design of MARL algorithms.
<div id='section'>Paperid: <span id='pid'>420, <a href='https://arxiv.org/pdf/2509.17676.pdf' target='_blank'>https://arxiv.org/pdf/2509.17676.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Abdullahi Isa Ahmed, Jamal Bentahar, El Mehdi Amhoud
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.17676">GLo-MAPPO: A Multi-Agent Proximal Policy Optimization for Energy Efficiency in UAV-Assisted LoRa Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Long Range (LoRa) based low-power wide area networks (LPWANs) are crucial for enabling next-generation IoT (NG-IoT) applications in 5G/6G ecosystems due to their long-range, low-power, and low-cost characteristics. However, achieving high energy efficiency in such networks remains a critical challenge, particularly in large-scale or dynamically changing environments. Traditional terrestrial LoRa deployments often suffer from coverage gaps and non-line-of-sight (NLoS) propagation losses, while satellite-based IoT solutions consume excessive energy and introduce high latency, limiting their suitability for energy-constrained and delay-sensitive applications. To address these limitations, we propose a novel architecture using multiple unmanned aerial vehicles (UAVs) as flying LoRa gateways to dynamically collect data from ground-based LoRa end devices. Our approach aims to maximize the system's weighted global energy efficiency by jointly optimizing spreading factors, transmission powers, UAV trajectories, and end-device associations. Additionally, we formulate this complex optimization problem as a partially observable Markov decision process (POMDP) and propose green LoRa multi-agent proximal policy optimization (GLo-MAPPO), a multi-agent reinforcement learning (MARL) framework based on centralized training with decentralized execution (CTDE). Simulation results show that GLo-MAPPO significantly outperforms benchmark algorithms, achieving energy efficiency improvements of 71.25%, 18.56%, 67.00%, 59.73%, and 49.95% for networks with 10, 20, 30, 40, and 50 LoRa end devices, respectively.
<div id='section'>Paperid: <span id='pid'>421, <a href='https://arxiv.org/pdf/2509.16709.pdf' target='_blank'>https://arxiv.org/pdf/2509.16709.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>NicolÃ² Botteghi, Matteo Tomasetto, Urban Fasel, Francesco Braghin, Andrea Manzoni
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.16709">HypeMARL: Multi-Agent Reinforcement Learning For High-Dimensional, Parametric, and Distributed Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep reinforcement learning has recently emerged as a promising feedback control strategy for complex dynamical systems governed by partial differential equations (PDEs). When dealing with distributed, high-dimensional problems in state and control variables, multi-agent reinforcement learning (MARL) has been proposed as a scalable approach for breaking the curse of dimensionality. In particular, through decentralized training and execution, multiple agents cooperate to steer the system towards a target configuration, relying solely on local state and reward information. However, the principle of locality may become a limiting factor whenever a collective, nonlocal behavior of the agents is crucial to maximize the reward function, as typically happens in PDE-constrained optimal control problems. In this work, we propose HypeMARL: a decentralized MARL algorithm tailored to the control of high-dimensional, parametric, and distributed systems. HypeMARL employs hypernetworks to effectively parametrize the agents' policies and value functions with respect to the system parameters and the agents' relative positions, encoded by sinusoidal positional encoding. Through the application on challenging control problems, such as density and flow control, we show that HypeMARL (i) can effectively control systems through a collective behavior of the agents, outperforming state-of-the-art decentralized MARL, (ii) can efficiently deal with parametric dependencies, (iii) requires minimal hyperparameter tuning and (iv) can reduce the amount of expensive environment interactions by a factor of ~10 thanks to its model-based extension, MB-HypeMARL, which relies on computationally efficient deep learning-based surrogate models approximating the dynamics locally, with minimal deterioration of the policy performance.
<div id='section'>Paperid: <span id='pid'>422, <a href='https://arxiv.org/pdf/2509.03817.pdf' target='_blank'>https://arxiv.org/pdf/2509.03817.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wei Yang, Jesse Thomason
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.03817">Learning to Deliberate: Meta-policy Collaboration for Agentic LLMs with Multi-agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent systems of large language models (LLMs) show promise for complex reasoning, but their effectiveness is often limited by fixed collaboration protocols. These frameworks typically focus on macro-level orchestration while overlooking agents' internal deliberative capabilities. This critical meta-cognitive blindspot treats agents as passive executors unable to adapt their strategy based on internal cognitive states like uncertainty or confidence. We introduce the Meta-Policy Deliberation Framework (MPDF), where agents learn a decentralized policy over a set of high-level meta-cognitive actions: Persist, Refine, and Concede. To overcome the instability of traditional policy gradients in this setting, we develop SoftRankPO, a novel reinforcement learning algorithm. SoftRankPO stabilizes training by shaping advantages based on the rank of rewards mapped through smooth normal quantiles, making the learning process robust to reward variance. Experiments show that MPDF with SoftRankPO achieves a a 4-5% absolute gain in average accuracy across five mathematical and general reasoning benchmarks compared to six state-of-the-art heuristic and learning-based multi-agent reasoning algorithms. Our work presents a paradigm for learning adaptive, meta-cognitive policies for multi-agent LLM systems, shifting the focus from designing fixed protocols to learning dynamic, deliberative strategies.
<div id='section'>Paperid: <span id='pid'>423, <a href='https://arxiv.org/pdf/2509.01257.pdf' target='_blank'>https://arxiv.org/pdf/2509.01257.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Andrea Fox, Francesco De Pellegrini, Eitan Altman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.01257">Multi-Agent Reinforcement Learning for Task Offloading in Wireless Edge Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In edge computing systems, autonomous agents must make fast local decisions while competing for shared resources. Existing MARL methods often resume to centralized critics or frequent communication, which fail under limited observability and communication constraints. We propose a decentralized framework in which each agent solves a constrained Markov decision process (CMDP), coordinating implicitly through a shared constraint vector. For the specific case of offloading, e.g., constraints prevent overloading shared server resources. Coordination constraints are updated infrequently and act as a lightweight coordination mechanism. They enable agents to align with global resource usage objectives but require little direct communication. Using safe reinforcement learning, agents learn policies that meet both local and global goals. We establish theoretical guarantees under mild assumptions and validate our approach experimentally, showing improved performance over centralized and independent baselines, especially in large-scale settings.
<div id='section'>Paperid: <span id='pid'>424, <a href='https://arxiv.org/pdf/2508.17341.pdf' target='_blank'>https://arxiv.org/pdf/2508.17341.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Muhammet Anil Yagiz, Zeynep Sude Cengiz, Polat Goktas
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.17341">MetaFed: Advancing Privacy, Performance, and Sustainability in Federated Metaverse Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid expansion of immersive Metaverse applications introduces complex challenges at the intersection of performance, privacy, and environmental sustainability. Centralized architectures fall short in addressing these demands, often resulting in elevated energy consumption, latency, and privacy concerns. This paper proposes MetaFed, a decentralized federated learning (FL) framework that enables sustainable and intelligent resource orchestration for Metaverse environments. MetaFed integrates (i) multi-agent reinforcement learning for dynamic client selection, (ii) privacy-preserving FL using homomorphic encryption, and (iii) carbon-aware scheduling aligned with renewable energy availability. Evaluations on MNIST and CIFAR-10 using lightweight ResNet architectures demonstrate that MetaFed achieves up to 25% reduction in carbon emissions compared to conventional approaches, while maintaining high accuracy and minimal communication overhead. These results highlight MetaFed as a scalable solution for building environmentally responsible and privacy-compliant Metaverse infrastructures.
<div id='section'>Paperid: <span id='pid'>425, <a href='https://arxiv.org/pdf/2508.08800.pdf' target='_blank'>https://arxiv.org/pdf/2508.08800.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>David Mguni, Yaqi Sun, Haojun Chen, Amir Darabi, Larry Olanrewaju Orimoloye, Yaodong Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.08800">Fault Tolerant Multi-Agent Learning with Adversarial Budget Constraints</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In multi-agent systems, the safe and reliable execution of tasks often depends on agents correctly coordinating their actions. However, in real-world deployments, failures of computational components are inevitable, presenting a critical challenge: ensuring that multi-agent reinforcement learning (MARL) policies remain effective even when some agents malfunction. We propose the Multi-Agent Robust Training Algorithm (MARTA), a plug-and-play framework for training MARL agents to be resilient to potentially severe faults. MARTA operates in cooperative multi-agent settings where agents may lose the ability to execute their intended actions. It learns to identify failure scenarios that are especially detrimental to system performance and equips agents with strategies to mitigate their impact. At the heart of MARTA is a novel adversarial Markov game in which an adversary -- modelled via \emph{Markov switching controls} -- learns to disable agents in high-risk state regions, while the remaining agents are trained to \emph{jointly} best-respond to such targeted malfunctions. To ensure practicality, MARTA enforces a malfunction budget, constraining the adversary to a fixed number of failures and learning robust policies accordingly. We provide theoretical guarantees that MARTA converges to a Markov perfect equilibrium, ensuring agents optimally counteract worst-case faults. Empirically, we show that MARTA achieves state-of-the-art fault-tolerant performance across benchmark environments, including Multi-Agent Particle World and Level-Based Foraging.
<div id='section'>Paperid: <span id='pid'>426, <a href='https://arxiv.org/pdf/2508.02912.pdf' target='_blank'>https://arxiv.org/pdf/2508.02912.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Brennen A. Hill, Mant Koh En Wei, Thangavel Jishnuanandh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.02912">Engineered over Emergent Communication in MARL for Scalable and Sample-Efficient Cooperative Task Allocation in a Partially Observable Grid</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We compare the efficacy of learned versus engineered communication strategies in a cooperative multi-agent reinforcement learning (MARL) environment. For the learned approach, we introduce Learned Direct Communication (LDC), where agents generate messages and actions concurrently via a neural network. Our engineered approach, Intention Communication, employs an Imagined Trajectory Generation Module (ITGM) and a Message Generation Network (MGN) to formulate messages based on predicted future states. Both strategies are evaluated on their success rates in cooperative tasks under fully and partially observable conditions. Our findings indicate that while emergent communication is viable, the engineered approach demonstrates superior performance and scalability, particularly as environmental complexity increases.
<div id='section'>Paperid: <span id='pid'>427, <a href='https://arxiv.org/pdf/2508.02027.pdf' target='_blank'>https://arxiv.org/pdf/2508.02027.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinzheng Wu, Junyi Chen, Shaolingfeng Ye, Wei Jiang, Yong Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.02027">An Evolving Scenario Generation Method based on Dual-modal Driver Model Trained by Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the autonomous driving testing methods based on evolving scenarios, the construction method of the driver model, which determines the driving maneuvers of background vehicles (BVs) in the scenario, plays a critical role in generating safety-critical scenarios. In particular, the cooperative adversarial driving characteristics between BVs can contribute to the efficient generation of safety-critical scenarios with high testing value. In this paper, a multi-agent reinforcement learning (MARL) method is used to train and generate a dual-modal driver model (Dual-DM) with non-adversarial and adversarial driving modalities. The model is then connected to a continuous simulated traffic environment to generate complex, diverse and strong interactive safety-critical scenarios through evolving scenario generation method. After that, the generated evolving scenarios are evaluated in terms of fidelity, test efficiency, complexity and diversity. Results show that without performance degradation in scenario fidelity (>85% similarity to real-world scenarios) and complexity (complexity metric: 0.45, +32.35% and +12.5% over two baselines), Dual-DM achieves a substantial enhancement in the efficiency of generating safety-critical scenarios (efficiency metric: 0.86, +195% over two baselines). Furthermore, statistical analysis and case studies demonstrate the diversity of safety-critical evolving scenarios generated by Dual-DM in terms of the adversarial interaction patterns. Therefore, Dual-DM can greatly improve the performance of the generation of safety-critical scenarios through evolving scenario generation method.
<div id='section'>Paperid: <span id='pid'>428, <a href='https://arxiv.org/pdf/2508.01049.pdf' target='_blank'>https://arxiv.org/pdf/2508.01049.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nicholas E. Corrado, Josiah P. Hanna
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.01049">Centralized Adaptive Sampling for Reliable Co-Training of Independent Multi-Agent Policies</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Independent on-policy policy gradient algorithms are widely used for multi-agent reinforcement learning (MARL) in cooperative and no-conflict games, but they are known to converge suboptimally when each agent's policy gradient points toward a suboptimal equilibrium. In this work, we identify a subtler failure mode that arises \textit{even when the expected policy gradients of all agents point toward an optimal solution.} After collecting a finite set of trajectories, stochasticity in independent action sampling can cause the joint data distribution to deviate from the expected joint on-policy distribution. This \textit{sampling error} w.r.t. the joint on-policy distribution produces inaccurate gradient estimates that can lead agents to converge suboptimally. In this paper, we investigate if joint sampling error can be reduced through coordinated action selection and whether doing so improves the reliability of policy gradient learning in MARL. Toward this end, we introduce an adaptive action sampling approach to reduce joint sampling error. Our method, Multi-Agent Proximal Robust On-Policy Sampling (MA-PROPS), uses a centralized behavior policy that we continually adapt to place larger probability on joint actions that are currently under-sampled w.r.t. the current joint policy. We empirically evaluate MA-PROPS in a diverse range of multi-agent games and demonstrate that (1) MA-PROPS reduces joint sampling error more efficiently than standard on-policy sampling and (2) improves the reliability of independent policy gradient algorithms, increasing the fraction of training runs that converge to an optimal joint policy.
<div id='section'>Paperid: <span id='pid'>429, <a href='https://arxiv.org/pdf/2507.18095.pdf' target='_blank'>https://arxiv.org/pdf/2507.18095.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yi Wang, Dawei Qiu, Fei Teng, Goran Strbac
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.18095">Towards Microgrid Resilience Enhancement via Mobile Power Sources and Repair Crews: A Multi-Agent Reinforcement Learning Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Mobile power sources (MPSs) have been gradually deployed in microgrids as critical resources to coordinate with repair crews (RCs) towards resilience enhancement owing to their flexibility and mobility in handling the complex coupled power-transport systems. However, previous work solves the coordinated dispatch problem of MPSs and RCs in a centralized manner with the assumption that the communication network is still fully functioning after the event. However, there is growing evidence that certain extreme events will damage or degrade communication infrastructure, which makes centralized decision making impractical. To fill this gap, this paper formulates the resilience-driven dispatch problem of MPSs and RCs in a decentralized framework. To solve this problem, a hierarchical multi-agent reinforcement learning method featuring a two-level framework is proposed, where the high-level action is used to switch decision-making between power and transport networks, and the low-level action constructed via a hybrid policy is used to compute continuous scheduling and discrete routing decisions in power and transport networks, respectively. The proposed method also uses an embedded function encapsulating system dynamics to enhance learning stability and scalability. Case studies based on IEEE 33-bus and 69-bus power networks are conducted to validate the effectiveness of the proposed method in load restoration.
<div id='section'>Paperid: <span id='pid'>430, <a href='https://arxiv.org/pdf/2507.04140.pdf' target='_blank'>https://arxiv.org/pdf/2507.04140.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ho Jae Lee, Se Hwan Jeon, Sangbae Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.04140">Learning Humanoid Arm Motion via Centroidal Momentum Regularized Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humans naturally swing their arms during locomotion to regulate whole-body dynamics, reduce angular momentum, and help maintain balance. Inspired by this principle, we present a limb-level multi-agent reinforcement learning (RL) framework that enables coordinated whole-body control of humanoid robots through emergent arm motion. Our approach employs separate actor-critic structures for the arms and legs, trained with centralized critics but decentralized actors that share only base states and centroidal angular momentum (CAM) observations, allowing each agent to specialize in task-relevant behaviors through modular reward design. The arm agent guided by CAM tracking and damping rewards promotes arm motions that reduce overall angular momentum and vertical ground reaction moments, contributing to improved balance during locomotion or under external perturbations. Comparative studies with single-agent and alternative multi-agent baselines further validate the effectiveness of our approach. Finally, we deploy the learned policy on a humanoid platform, achieving robust performance across diverse locomotion tasks, including flat-ground walking, rough terrain traversal, and stair climbing.
<div id='section'>Paperid: <span id='pid'>431, <a href='https://arxiv.org/pdf/2506.19417.pdf' target='_blank'>https://arxiv.org/pdf/2506.19417.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yisak Park, Sunwoo Lee, Seungyul Han
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.19417">Center of Gravity-Guided Focusing Influence Mechanism for Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cooperative multi-agent reinforcement learning (MARL) under sparse rewards presents a fundamental challenge due to limited exploration and insufficient coordinated attention among agents. In this work, we propose the Focusing Influence Mechanism (FIM), a novel framework that enhances cooperation by directing agent influence toward task-critical elements, referred to as Center of Gravity (CoG) state dimensions, inspired by Clausewitz's military theory. FIM consists of three core components: (1) identifying CoG state dimensions based on their stability under agent behavior, (2) designing counterfactual intrinsic rewards to promote meaningful influence on these dimensions, and (3) encouraging persistent and synchronized focus through eligibility-trace-based credit accumulation. These mechanisms enable agents to induce more targeted and effective state transitions, facilitating robust cooperation even in extremely sparse reward settings. Empirical evaluations across diverse MARL benchmarks demonstrate that the proposed FIM significantly improves cooperative performance compared to baselines.
<div id='section'>Paperid: <span id='pid'>432, <a href='https://arxiv.org/pdf/2506.18679.pdf' target='_blank'>https://arxiv.org/pdf/2506.18679.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruicheng Zhang, Yu Sun, Zeyu Zhang, Jinai Li, Xiaofan Liu, Au Hoi Fan, Haowei Guo, Puxin Yan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.18679">MARL-MambaContour: Unleashing Multi-Agent Deep Reinforcement Learning for Active Contour Optimization in Medical Image Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce MARL-MambaContour, the first contour-based medical image segmentation framework based on Multi-Agent Reinforcement Learning (MARL). Our approach reframes segmentation as a multi-agent cooperation task focused on generate topologically consistent object-level contours, addressing the limitations of traditional pixel-based methods which could lack topological constraints and holistic structural awareness of anatomical regions. Each contour point is modeled as an autonomous agent that iteratively adjusts its position to align precisely with the target boundary, enabling adaptation to blurred edges and intricate morphologies common in medical images. This iterative adjustment process is optimized by a contour-specific Soft Actor-Critic (SAC) algorithm, further enhanced with the Entropy Regularization Adjustment Mechanism (ERAM) which dynamically balance agent exploration with contour smoothness. Furthermore, the framework incorporates a Mamba-based policy network featuring a novel Bidirectional Cross-attention Hidden-state Fusion Mechanism (BCHFM). This mechanism mitigates potential memory confusion limitations associated with long-range modeling in state space models, thereby facilitating more accurate inter-agent information exchange and informed decision-making. Extensive experiments on five diverse medical imaging datasets demonstrate the state-of-the-art performance of MARL-MambaContour, highlighting its potential as an accurate and robust clinical application.
<div id='section'>Paperid: <span id='pid'>433, <a href='https://arxiv.org/pdf/2506.18651.pdf' target='_blank'>https://arxiv.org/pdf/2506.18651.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuocun Yang, Huawen Hu, Enze Shi, Shu Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.18651">Dual-level Behavioral Consistency for Inter-group and Intra-group Coordination in Multi-Agent Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Behavioral diversity in Multi-agent reinforcement learning(MARL) represents an emerging and promising research area. Prior work has largely centered on intra-group behavioral consistency in multi-agent systems, with limited attention given to behavioral consistency in multi-agent grouping scenarios. In this paper, we introduce Dual-Level Behavioral Consistency (DLBC), a novel MARL control method designed to explicitly regulate agent behaviors at both intra-group and inter-group levels. DLBC partitions agents into distinct groups and dynamically modulates behavioral diversity both within and between these groups. By dynamically modulating behavioral diversity within and between these groups, DLBC achieves enhanced division of labor through inter-group consistency, which constrains behavioral strategies across different groups. Simultaneously, intra-group consistency, achieved by aligning behavioral strategies within each group, fosters stronger intra-group cooperation. Crucially, DLBC's direct constraint of agent policy functions ensures its broad applicability across various algorithmic frameworks. Experimental results in various grouping cooperation scenarios demonstrate that DLBC significantly enhances both intra-group cooperative performance and inter-group task specialization, yielding substantial performance improvements. DLBC provides new ideas for behavioral consistency control of multi-intelligent body systems, and its potential for application in more complex tasks and dynamic environments can be further explored in the future.
<div id='section'>Paperid: <span id='pid'>434, <a href='https://arxiv.org/pdf/2506.13113.pdf' target='_blank'>https://arxiv.org/pdf/2506.13113.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Stella C. Dong, James R. Finlay
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.13113">Dynamic Reinsurance Treaty Bidding via Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper develops a novel multi-agent reinforcement learning (MARL) framework for reinsurance treaty bidding, addressing long-standing inefficiencies in traditional broker-mediated placement processes. We pose the core research question: Can autonomous, learning-based bidding systems improve risk transfer efficiency and outperform conventional pricing approaches in reinsurance markets?
  In our model, each reinsurer is represented by an adaptive agent that iteratively refines its bidding strategy within a competitive, partially observable environment. The simulation explicitly incorporates institutional frictions including broker intermediation, incumbent advantages, last-look privileges, and asymmetric access to underwriting information.
  Empirical analysis demonstrates that MARL agents achieve up to 15% higher underwriting profit, 20% lower tail risk (CVaR), and over 25% improvement in Sharpe ratios relative to actuarial and heuristic baselines. Sensitivity tests confirm robustness across hyperparameter settings, and stress testing reveals strong resilience under simulated catastrophe shocks and capital constraints.
  These findings suggest that MARL offers a viable path toward more transparent, adaptive, and risk-sensitive reinsurance markets. The proposed framework contributes to emerging literature at the intersection of algorithmic market design, strategic bidding, and AI-enabled financial decision-making.
<div id='section'>Paperid: <span id='pid'>435, <a href='https://arxiv.org/pdf/2505.19837.pdf' target='_blank'>https://arxiv.org/pdf/2505.19837.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Christoph R. Landolt, Christoph WÃ¼rsch, Roland Meier, Alain Mermoud, Julian Jang-Jaccard
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.19837">Multi-Agent Reinforcement Learning in Cybersecurity: From Fundamentals to Applications</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-Agent Reinforcement Learning (MARL) has shown great potential as an adaptive solution for addressing modern cybersecurity challenges. MARL enables decentralized, adaptive, and collaborative defense strategies and provides an automated mechanism to combat dynamic, coordinated, and sophisticated threats. This survey investigates the current state of research in MARL applications for automated cyber defense (ACD), focusing on intruder detection and lateral movement containment. Additionally, it examines the role of Autonomous Intelligent Cyber-defense Agents (AICA) and Cyber Gyms in training and validating MARL agents. Finally, the paper outlines existing challenges, such as scalability and adversarial robustness, and proposes future research directions. This also discusses how MARL integrates in AICA to provide adaptive, scalable, and dynamic solutions to counter the increasingly sophisticated landscape of cyber threats. It highlights the transformative potential of MARL in areas like intrusion detection and lateral movement containment, and underscores the value of Cyber Gyms for training and validation of AICA.
<div id='section'>Paperid: <span id='pid'>436, <a href='https://arxiv.org/pdf/2505.08222.pdf' target='_blank'>https://arxiv.org/pdf/2505.08222.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Matteo Gallici, Ivan Masmitja, Mario MartÃ­n
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.08222">Scaling Multi Agent Reinforcement Learning for Underwater Acoustic Tracking via Autonomous Vehicles</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autonomous vehicles (AV) offer a cost-effective solution for scientific missions such as underwater tracking. Recently, reinforcement learning (RL) has emerged as a powerful method for controlling AVs in complex marine environments. However, scaling these techniques to a fleet--essential for multi-target tracking or targets with rapid, unpredictable motion--presents significant computational challenges. Multi-Agent Reinforcement Learning (MARL) is notoriously sample-inefficient, and while high-fidelity simulators like Gazebo's LRAUV provide 100x faster-than-real-time single-robot simulations, they offer no significant speedup for multi-vehicle scenarios, making MARL training impractical. To address these limitations, we propose an iterative distillation method that transfers high-fidelity simulations into a simplified, GPU-accelerated environment while preserving high-level dynamics. This approach achieves up to a 30,000x speedup over Gazebo through parallelization, enabling efficient training via end-to-end GPU acceleration. Additionally, we introduce a novel Transformer-based architecture (TransfMAPPO) that learns multi-agent policies invariant to the number of agents and targets, significantly improving sample efficiency. Following large-scale curriculum learning conducted entirely on GPU, we perform extensive evaluations in Gazebo, demonstrating that our method maintains tracking errors below 5 meters over extended durations, even in the presence of multiple fast-moving targets. This work bridges the gap between large-scale MARL training and high-fidelity deployment, providing a scalable framework for autonomous fleet control in real-world sea missions.
<div id='section'>Paperid: <span id='pid'>437, <a href='https://arxiv.org/pdf/2505.05967.pdf' target='_blank'>https://arxiv.org/pdf/2505.05967.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Uyoata E. Uyoata, Gilberto Berardinelli, Ramoni Adeogun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.05967">Learning Power Control Protocol for In-Factory 6G Subnetworks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In-X Subnetworks are envisioned to meet the stringent demands of short-range communication in diverse 6G use cases. In the context of In-Factory scenarios, effective power control is critical to mitigating the impact of interference resulting from potentially high subnetwork density. Existing approaches to power control in this domain have predominantly emphasized the data plane, often overlooking the impact of signaling overhead. Furthermore, prior work has typically adopted a network-centric perspective, relying on the assumption of complete and up-to-date channel state information (CSI) being readily available at the central controller. This paper introduces a novel multi-agent reinforcement learning (MARL) framework designed to enable access points to autonomously learn both signaling and power control protocols in an In-Factory Subnetwork environment. By formulating the problem as a partially observable Markov decision process (POMDP) and leveraging multi-agent proximal policy optimization (MAPPO), the proposed approach achieves significant advantages. The simulation results demonstrate that the learning-based method reduces signaling overhead by a factor of 8 while maintaining a buffer flush rate that lags the ideal "Genie" approach by only 5%.
<div id='section'>Paperid: <span id='pid'>438, <a href='https://arxiv.org/pdf/2504.05553.pdf' target='_blank'>https://arxiv.org/pdf/2504.05553.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yongjie Fu, Lingyun Zhong, Zifan Li, Xuan Di
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.05553">Federated Hierarchical Reinforcement Learning for Adaptive Traffic Signal Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent reinforcement learning (MARL) has shown promise for adaptive traffic signal control (ATSC), enabling multiple intersections to coordinate signal timings in real time. However, in large-scale settings, MARL faces constraints due to extensive data sharing and communication requirements. Federated learning (FL) mitigates these challenges by training shared models without directly exchanging raw data, yet traditional FL methods such as FedAvg struggle with highly heterogeneous intersections. Different intersections exhibit varying traffic patterns, demands, and road structures, so performing FedAvg across all agents is inefficient. To address this gap, we propose Hierarchical Federated Reinforcement Learning (HFRL) for ATSC. HFRL employs clustering-based or optimization-based techniques to dynamically group intersections and perform FedAvg independently within groups of intersections with similar characteristics, enabling more effective coordination and scalability than standard FedAvg. Our experiments on synthetic and real-world traffic networks demonstrate that HFRL not only outperforms both decentralized and standard federated RL approaches but also identifies suitable grouping patterns based on network structure or traffic demand, resulting in a more robust framework for distributed, heterogeneous systems.
<div id='section'>Paperid: <span id='pid'>439, <a href='https://arxiv.org/pdf/2503.05126.pdf' target='_blank'>https://arxiv.org/pdf/2503.05126.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Reginald McLean, Evangelos Chatzaroulas, Jordan Terry, Isaac Woungang, Nariman Farsad, Pablo Samuel Castro
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.05126">Multi-Task Reinforcement Learning Enables Parameter Scaling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-task reinforcement learning (MTRL) aims to endow a single agent with the ability to perform well on multiple tasks. Recent works have focused on developing novel sophisticated architectures to improve performance, often resulting in larger models; it is unclear, however, whether the performance gains are a consequence of the architecture design itself or the extra parameters. We argue that gains are mostly due to scale by demonstrating that naively scaling up a simple MTRL baseline to match parameter counts outperforms the more sophisticated architectures, and these gains benefit most from scaling the critic over the actor. Additionally, we explore the training stability advantages that come with task diversity, demonstrating that increasing the number of tasks can help mitigate plasticity loss. Our findings suggest that MTRL's simultaneous training across multiple tasks provides a natural framework for beneficial parameter scaling in reinforcement learning, challenging the need for complex architectural innovations.
<div id='section'>Paperid: <span id='pid'>440, <a href='https://arxiv.org/pdf/2503.04262.pdf' target='_blank'>https://arxiv.org/pdf/2503.04262.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Vasilios Mavroudis, Gregory Palmer, Sara Farmer, Kez Smithson Whitehead, David Foster, Adam Price, Ian Miles, Alberto Caron, Stephen Pasteris
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.04262">Guidelines for Applying RL and MARL in Cybersecurity Applications</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reinforcement Learning (RL) and Multi-Agent Reinforcement Learning (MARL) have emerged as promising methodologies for addressing challenges in automated cyber defence (ACD). These techniques offer adaptive decision-making capabilities in high-dimensional, adversarial environments. This report provides a structured set of guidelines for cybersecurity professionals and researchers to assess the suitability of RL and MARL for specific use cases, considering factors such as explainability, exploration needs, and the complexity of multi-agent coordination. It also discusses key algorithmic approaches, implementation challenges, and real-world constraints, such as data scarcity and adversarial interference. The report further outlines open research questions, including policy optimality, agent cooperation levels, and the integration of MARL systems into operational cybersecurity frameworks. By bridging theoretical advancements and practical deployment, these guidelines aim to enhance the effectiveness of AI-driven cyber defence strategies.
<div id='section'>Paperid: <span id='pid'>441, <a href='https://arxiv.org/pdf/2502.13430.pdf' target='_blank'>https://arxiv.org/pdf/2502.13430.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao Ma, Shijie Wang, Zhiqiang Pu, Siyao Zhao, Xiaolin Ai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.13430">Vision-Based Generic Potential Function for Policy Alignment in Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Guiding the policy of multi-agent reinforcement learning to align with human common sense is a difficult problem, largely due to the complexity of modeling common sense as a reward, especially in complex and long-horizon multi-agent tasks. Recent works have shown the effectiveness of reward shaping, such as potential-based rewards, to enhance policy alignment. The existing works, however, primarily rely on experts to design rule-based rewards, which are often labor-intensive and lack a high-level semantic understanding of common sense. To solve this problem, we propose a hierarchical vision-based reward shaping method. At the bottom layer, a visual-language model (VLM) serves as a generic potential function, guiding the policy to align with human common sense through its intrinsic semantic understanding. To help the policy adapts to uncertainty and changes in long-horizon tasks, the top layer features an adaptive skill selection module based on a visual large language model (vLLM). The module uses instructions, video replays, and training records to dynamically select suitable potential function from a pre-designed pool. Besides, our method is theoretically proven to preserve the optimal policy. Extensive experiments conducted in the Google Research Football environment demonstrate that our method not only achieves a higher win rate but also effectively aligns the policy with human common sense.
<div id='section'>Paperid: <span id='pid'>442, <a href='https://arxiv.org/pdf/2502.03377.pdf' target='_blank'>https://arxiv.org/pdf/2502.03377.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Abdullahi Isa Ahmed, Jamal Bentahar, El Mehdi Amhoud
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.03377">Energy-Efficient Flying LoRa Gateways: A Multi-Agent Reinforcement Learning Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As next-generation Internet of Things (NG-IoT) networks continue to grow, the number of connected devices is rapidly increasing, along with their energy demands. This creates challenges for resource management and sustainability. Energy-efficient communication, particularly for power-limited IoT devices, is therefore a key research focus. In this paper, we deployed flying LoRa gateways mounted on unmanned aerial vehicles (UAVs) to collect data from LoRa end devices and transmit it to a central server. Our primary objective is to maximize the global system energy efficiency of wireless LoRa networks by joint optimization of transmission power, spreading factor, bandwidth, and user association. To solve this challenging problem, we model the problem as a partially observable Markov decision process (POMDP), where each flying LoRa GW acts as a learning agent using a cooperative multi-agent reinforcement learning (MARL). Simulation results demonstrate that our proposed method, based on the multi-agent proximal policy optimization algorithm, significantly improves the global system energy efficiency and surpasses the popular MARL and other conventional schemes.
<div id='section'>Paperid: <span id='pid'>443, <a href='https://arxiv.org/pdf/2502.02984.pdf' target='_blank'>https://arxiv.org/pdf/2502.02984.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dengyu Zhang, Chenghao, Feng Xue, Qingrui Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.02984">Learning Efficient Flocking Control based on Gibbs Random Fields</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Flocking control is essential for multi-robot systems in diverse applications, yet achieving efficient flocking in congested environments poses challenges regarding computation burdens, performance optimality, and motion safety. This paper addresses these challenges through a multi-agent reinforcement learning (MARL) framework built on Gibbs Random Fields (GRFs). With GRFs, a multi-robot system is represented by a set of random variables conforming to a joint probability distribution, thus offering a fresh perspective on flocking reward design. A decentralized training and execution mechanism, which enhances the scalability of MARL concerning robot quantity, is realized using a GRF-based credit assignment method. An action attention module is introduced to implicitly anticipate the motion intentions of neighboring robots, consequently mitigating potential non-stationarity issues in MARL. The proposed framework enables learning an efficient distributed control policy for multi-robot systems in challenging environments with success rate around $99\%$, as demonstrated through thorough comparisons with state-of-the-art solutions in simulations and experiments. Ablation studies are also performed to validate the efficiency of different framework modules.
<div id='section'>Paperid: <span id='pid'>444, <a href='https://arxiv.org/pdf/2502.00558.pdf' target='_blank'>https://arxiv.org/pdf/2502.00558.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sydney Dolan, Siddharth Nayak, Jasmine Jerry Aloor, Hamsa Balakrishnan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.00558">Asynchronous Cooperative Multi-Agent Reinforcement Learning with Limited Communication</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We consider the problem setting in which multiple autonomous agents must cooperatively navigate and perform tasks in an unknown, communication-constrained environment. Traditional multi-agent reinforcement learning (MARL) approaches assume synchronous communications and perform poorly in such environments. We propose AsynCoMARL, an asynchronous MARL approach that uses graph transformers to learn communication protocols from dynamic graphs. AsynCoMARL can accommodate infrequent and asynchronous communications between agents, with edges of the graph only forming when agents communicate with each other. We show that AsynCoMARL achieves similar success and collision rates as leading baselines, despite 26\% fewer messages being passed between agents.
<div id='section'>Paperid: <span id='pid'>445, <a href='https://arxiv.org/pdf/2410.14916.pdf' target='_blank'>https://arxiv.org/pdf/2410.14916.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jasmine Jerry Aloor, Siddharth Nayak, Sydney Dolan, Hamsa Balakrishnan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.14916">Cooperation and Fairness in Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent systems are trained to maximize shared cost objectives, which typically reflect system-level efficiency. However, in the resource-constrained environments of mobility and transportation systems, efficiency may be achieved at the expense of fairness -- certain agents may incur significantly greater costs or lower rewards compared to others. Tasks could be distributed inequitably, leading to some agents receiving an unfair advantage while others incur disproportionately high costs. It is important to consider the tradeoffs between efficiency and fairness. We consider the problem of fair multi-agent navigation for a group of decentralized agents using multi-agent reinforcement learning (MARL). We consider the reciprocal of the coefficient of variation of the distances traveled by different agents as a measure of fairness and investigate whether agents can learn to be fair without significantly sacrificing efficiency (i.e., increasing the total distance traveled). We find that by training agents using min-max fair distance goal assignments along with a reward term that incentivizes fairness as they move towards their goals, the agents (1) learn a fair assignment of goals and (2) achieve almost perfect goal coverage in navigation scenarios using only local observations. For goal coverage scenarios, we find that, on average, our model yields a 14% improvement in efficiency and a 5% improvement in fairness over a baseline trained using random assignments. Furthermore, an average of 21% improvement in fairness can be achieved compared to a model trained on optimally efficient assignments; this increase in fairness comes at the expense of only a 7% decrease in efficiency. Finally, we extend our method to environments in which agents must complete coverage tasks in prescribed formations and show that it is possible to do so without tailoring the models to specific formation shapes.
<div id='section'>Paperid: <span id='pid'>446, <a href='https://arxiv.org/pdf/2410.14383.pdf' target='_blank'>https://arxiv.org/pdf/2410.14383.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Toby Godfrey, William Hunt, Mohammad D. Soorati
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.14383">MARLIN: Multi-Agent Reinforcement Learning Guided by Language-Based Inter-Robot Negotiation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent reinforcement learning is a key method for training multi-robot systems over a series of episodes in which robots are rewarded or punished according to their performance; only once the system is trained to a suitable standard is it deployed in the real world. If the system is not trained enough, the task will likely not be completed and could pose a risk to the surrounding environment. We introduce Multi-Agent Reinforcement Learning guided by Language-based Inter-Robot Negotiation (MARLIN), in which the training process requires fewer training episodes to reach peak performance. Robots are equipped with large language models that negotiate and debate a task, producing plans used to guide the policy during training. The approach dynamically switches between using reinforcement learning and large language model-based action negotiation throughout training. This reduces the number of training episodes required, compared to standard multi-agent reinforcement learning, and hence allows the system to be deployed to physical hardware earlier. The performance of this approach is evaluated against multi-agent reinforcement learning, showing that our hybrid method achieves comparable results with significantly reduced training time.
<div id='section'>Paperid: <span id='pid'>447, <a href='https://arxiv.org/pdf/2410.06101.pdf' target='_blank'>https://arxiv.org/pdf/2410.06101.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao Ma, Tianyi Hu, Zhiqiang Pu, Boyin Liu, Xiaolin Ai, Yanyan Liang, Min Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.06101">Coevolving with the Other You: Fine-Tuning LLM with Sequential Cooperative Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reinforcement learning (RL) has emerged as a pivotal technique for fine-tuning large language models (LLMs) on specific tasks. However, prevailing RL fine-tuning methods predominantly rely on PPO and its variants. Though these algorithms are effective in general RL settings, they often exhibit suboptimal performance and vulnerability to distribution collapse when applied to the fine-tuning of LLMs. In this paper, we propose CORY, extending the RL fine-tuning of LLMs to a sequential cooperative multi-agent reinforcement learning framework, to leverage the inherent coevolution and emergent capabilities of multi-agent systems. In CORY, the LLM to be fine-tuned is initially duplicated into two autonomous agents: a pioneer and an observer. The pioneer generates responses based on queries, while the observer generates responses using both the queries and the pioneer's responses. The two agents are trained together. During training, the agents exchange roles periodically, fostering cooperation and coevolution between them. Experiments evaluate CORY's performance by fine-tuning GPT-2 and Llama-2 under subjective and objective reward functions on the IMDB Review and GSM8K datasets, respectively. Results show that CORY outperforms PPO in terms of policy optimality, resistance to distribution collapse, and training robustness, thereby underscoring its potential as a superior methodology for refining LLMs in real-world applications.
<div id='section'>Paperid: <span id='pid'>448, <a href='https://arxiv.org/pdf/2410.02516.pdf' target='_blank'>https://arxiv.org/pdf/2410.02516.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Vasanth Reddy Baddam, Suat Gumussoy, Almuatazbellah Boker, Hoda Eldardiry
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.02516">Learning Emergence of Interaction Patterns across Independent RL Agents in Multi-Agent Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Many real-world problems, such as controlling swarms of drones and urban traffic, naturally lend themselves to modeling as multi-agent reinforcement learning (RL) problems. However, existing multi-agent RL methods often suffer from scalability challenges, primarily due to the introduction of communication among agents. Consequently, a key challenge lies in adapting the success of deep learning in single-agent RL to the multi-agent setting. In response to this challenge, we propose an approach that fundamentally reimagines multi-agent environments. Unlike conventional methods that model each agent individually with separate networks, our approach, the Bottom Up Network (BUN), adopts a unique perspective. BUN treats the collective of multi-agents as a unified entity while employing a specialized weight initialization strategy that promotes independent learning. Furthermore, we dynamically establish connections among agents using gradient information, enabling coordination when necessary while maintaining these connections as limited and sparse to effectively manage the computational budget. Our extensive empirical evaluations across a variety of cooperative multi-agent scenarios, including tasks such as cooperative navigation and traffic control, consistently demonstrate BUN's superiority over baseline methods with substantially reduced computational costs.
<div id='section'>Paperid: <span id='pid'>449, <a href='https://arxiv.org/pdf/2409.05480.pdf' target='_blank'>https://arxiv.org/pdf/2409.05480.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yihong Tao, Bo Lei, Haoyang Shi, Jingkai Chen, Xing Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.05480">Adaptive Multi-Layer Deployment for A Digital Twin Empowered Satellite-Terrestrial Integrated Network</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the development of satellite communication technology, satellite-terrestrial integrated networks (STIN), which integrate satellite networks and ground networks, can realize seamless global coverage of communication services. Confronting the intricacies of network dynamics, the diversity of resource heterogeneity, and the unpredictability of user mobility, dynamic resource allocation within networks faces formidable challenges. Digital twin (DT), as a new technique, can reflect a physical network to a virtual network to monitor, analyze, and optimize the physical network. Nevertheless, in the process of constructing the DT model, the deployment location and resource allocation of DTs may adversely affect its performance. Therefore, we propose a STIN model, which alleviates the problem of insufficient single-layer deployment flexibility of the traditional edge network by deploying DTs in multi-layer nodes in a STIN. To address the challenge of deploying DTs in the network, we propose multi-layer DT deployment in a STIN to reduce system delay. Then we adopt a multi-agent reinforcement learning (MARL) scheme to explore the optimal strategy of the DT multi-layer deployment problem. The implemented scheme demonstrates a notable reduction in system delay, as evidenced by simulation outcomes.
<div id='section'>Paperid: <span id='pid'>450, <a href='https://arxiv.org/pdf/2601.05407.pdf' target='_blank'>https://arxiv.org/pdf/2601.05407.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Minwoo Cho, Batuhan Altundas, Matthew Gombolay
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.05407">Interactive Distillation for Cooperative Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Knowledge distillation (KD) has the potential to accelerate MARL by employing a centralized teacher for decentralized students but faces key bottlenecks. Specifically, there are (1) challenges in synthesizing high-performing teaching policies in complex domains, (2) difficulties when teachers must reason in out-of-distribution (OOD) states, and (3) mismatches between the decentralized students' and the centralized teacher's observation spaces. To address these limitations, we propose HINT (Hierarchical INteractive Teacher-based transfer), a novel KD framework for MARL in a centralized training, decentralized execution setup. By leveraging hierarchical RL, HINT provides a scalable, high-performing teacher. Our key innovation, pseudo off-policy RL, enables the teacher policy to be updated using both teacher and student experience, thereby improving OOD adaptation. HINT also applies performance-based filtering to retain only outcome-relevant guidance, reducing observation mismatches. We evaluate HINT on challenging cooperative domains (e.g., FireCommander for resource allocation, MARINE for tactical combat). Across these benchmarks, HINT outperforms baselines, achieving improvements of 60% to 165% in success rate.
<div id='section'>Paperid: <span id='pid'>451, <a href='https://arxiv.org/pdf/2601.03413.pdf' target='_blank'>https://arxiv.org/pdf/2601.03413.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yigal Koifman, Eran Iceland, Erez Koifman, Ariel Barel, Alfred M. Bruckstein
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.03413">Sensor to Pixels: Decentralized Swarm Gathering via Image-Based Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This study highlights the potential of image-based reinforcement learning methods for addressing swarm-related tasks. In multi-agent reinforcement learning, effective policy learning depends on how agents sense, interpret, and process inputs. Traditional approaches often rely on handcrafted feature extraction or raw vector-based representations, which limit the scalability and efficiency of learned policies concerning input order and size. In this work we propose an image-based reinforcement learning method for decentralized control of a multi-agent system, where observations are encoded as structured visual inputs that can be processed by Neural Networks, extracting its spatial features and producing novel decentralized motion control rules. We evaluate our approach on a multi-agent convergence task of agents with limited-range and bearing-only sensing that aim to keep the swarm cohesive during the aggregation. The algorithm's performance is evaluated against two benchmarks: an analytical solution proposed by Bellaiche and Bruckstein, which ensures convergence but progresses slowly, and VariAntNet, a neural network-based framework that converges much faster but shows medium success rates in hard constellations. Our method achieves high convergence, with a pace nearly matching that of VariAntNet. In some scenarios, it serves as the only practical alternative.
<div id='section'>Paperid: <span id='pid'>452, <a href='https://arxiv.org/pdf/2512.05447.pdf' target='_blank'>https://arxiv.org/pdf/2512.05447.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pengcheng Dai, Dongming Wang, Wenwu Yu, Wei Ren
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.05447">Distributed scalable coupled policy algorithm for networked multi-agent reinforcement learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper studies networked multi-agent reinforcement learning (NMARL) with interdependent rewards and coupled policies. In this setting, each agent's reward depends on its own state-action pair as well as those of its direct neighbors, and each agent's policy is parameterized by its local parameters together with those of its $κ_{p}$-hop neighbors, with $κ_{p}\geq 1$ denoting the coupled radius. The objective of the agents is to collaboratively optimize their policies to maximize the discounted average cumulative reward. To address the challenge of interdependent policies in collaborative optimization, we introduce a novel concept termed the neighbors' averaged $Q$-function and derive a new expression for the coupled policy gradient. Based on these theoretical foundations, we develop a distributed scalable coupled policy (DSCP) algorithm, where each agent relies only on the state-action pairs of its $κ_{p}$-hop neighbors and the rewards its their $(κ_{p}+1)$-hop neighbors. Specially, in the DSCP algorithm, we employ a geometric 2-horizon sampling method that does not require storing a full $Q$-table to obtain an unbiased estimate of the coupled policy gradient. Moreover, each agent interacts exclusively with its direct neighbors to obtain accurate policy parameters, while maintaining local estimates of other agents' parameters to execute its local policy and collect samples for optimization. These estimates and policy parameters are updated via a push-sum protocol, enabling distributed coordination of policy updates across the network. We prove that the joint policy produced by the proposed algorithm converges to a first-order stationary point of the objective function. Finally, the effectiveness of DSCP algorithm is demonstrated through simulations in a robot path planning environment, showing clear improvement over state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>453, <a href='https://arxiv.org/pdf/2511.14135.pdf' target='_blank'>https://arxiv.org/pdf/2511.14135.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Promise Ekpo, Saesha Agarwal, Felix Grimm, Lekan Molu, Angelique Taylor
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.14135">Fair-GNE : Generalized Nash Equilibrium-Seeking Fairness in Multiagent Healthcare Automation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Enforcing a fair workload allocation among multiple agents tasked to achieve an objective in learning enabled demand side healthcare worker settings is crucial for consistent and reliable performance at runtime. Existing multi-agent reinforcement learning (MARL) approaches steer fairness by shaping reward through post hoc orchestrations, leaving no certifiable self-enforceable fairness that is immutable by individual agents at runtime. Contextualized within a setting where each agent shares resources with others, we address this shortcoming with a learning enabled optimization scheme among self-interested decision makers whose individual actions affect those of other agents. This extends the problem to a generalized Nash equilibrium (GNE) game-theoretic framework where we steer group policy to a safe and locally efficient equilibrium, so that no agent can improve its utility function by unilaterally changing its decisions. Fair-GNE models MARL as a constrained generalized Nash equilibrium-seeking (GNE) game, prescribing an ideal equitable collective equilibrium within the problem's natural fabric. Our hypothesis is rigorously evaluated in our custom-designed high-fidelity resuscitation simulator. Across all our numerical experiments, Fair-GNE achieves significant improvement in workload balance over fixed-penalty baselines (0.89 vs.\ 0.33 JFI, $p < 0.01$) while maintaining 86\% task success, demonstrating statistically significant fairness gains through adaptive constraint enforcement. Our results communicate our formulations, evaluation metrics, and equilibrium-seeking innovations in large multi-agent learning-based healthcare systems with clarity and principled fairness enforcement.
<div id='section'>Paperid: <span id='pid'>454, <a href='https://arxiv.org/pdf/2511.09171.pdf' target='_blank'>https://arxiv.org/pdf/2511.09171.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinren Zhang, Jiadong Yu, Zixin Zhong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.09171">Learning Efficient Communication Protocols for Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-Agent Systems (MAS) have emerged as a powerful paradigm for modeling complex interactions among autonomous entities in distributed environments. In Multi-Agent Reinforcement Learning (MARL), communication enables coordination but can lead to inefficient information exchange, since agents may generate redundant or non-essential messages. While prior work has focused on boosting task performance with information exchange, the existing research lacks a thorough investigation of both the appropriate definition and the optimization of communication protocols (communication topology and message). To fill this gap, we introduce a generalized framework for learning multi-round communication protocols that are both effective and efficient. Within this framework, we propose three novel Communication Efficiency Metrics (CEMs) to guide and evaluate the learning process: the Information Entropy Efficiency Index (IEI) and Specialization Efficiency Index (SEI) for efficiency-augmented optimization, and the Topology Efficiency Index (TEI) for explicit evaluation. We integrate IEI and SEI as the adjusted loss functions to promote informative messaging and role specialization, while using TEI to quantify the trade-off between communication volume and task performance. Through comprehensive experiments, we demonstrate that our learned communication protocol can significantly enhance communication efficiency and achieves better cooperation performance with improved success rates.
<div id='section'>Paperid: <span id='pid'>455, <a href='https://arxiv.org/pdf/2511.07629.pdf' target='_blank'>https://arxiv.org/pdf/2511.07629.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yue Jin, Giovanni Montana
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.07629">Partial Action Replacement: Tackling Distribution Shift in Offline MARL</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Offline multi-agent reinforcement learning (MARL) is severely hampered by the challenge of evaluating out-of-distribution (OOD) joint actions. Our core finding is that when the behavior policy is factorized - a common scenario where agents act fully or partially independently during data collection - a strategy of partial action replacement (PAR) can significantly mitigate this challenge. PAR updates a single or part of agents' actions while the others remain fixed to the behavioral data, reducing distribution shift compared to full joint-action updates. Based on this insight, we develop Soft-Partial Conservative Q-Learning (SPaCQL), using PAR to mitigate OOD issue and dynamically weighting different PAR strategies based on the uncertainty of value estimation. We provide a rigorous theoretical foundation for this approach, proving that under factorized behavior policies, the induced distribution shift scales linearly with the number of deviating agents rather than exponentially with the joint-action space. This yields a provably tighter value error bound for this important class of offline MARL problems. Our theoretical results also indicate that SPaCQL adaptively addresses distribution shift using uncertainty-informed weights. Our empirical results demonstrate SPaCQL enables more effective policy learning, and manifest its remarkable superiority over baseline algorithms when the offline dataset exhibits the independence structure.
<div id='section'>Paperid: <span id='pid'>456, <a href='https://arxiv.org/pdf/2511.05207.pdf' target='_blank'>https://arxiv.org/pdf/2511.05207.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ryuji Hashimoto, Ryosuke Takata, Masahiro Suzuki, Yuki Tanaka, Kiyoshi Izumi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.05207">Emergence from Emergence: Financial Market Simulation via Learning with Heterogeneous Preferences</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Agent-based models help explain stock price dynamics as emergent phenomena driven by interacting investors. In this modeling tradition, investor behavior has typically been captured by two distinct mechanisms -- learning and heterogeneous preferences -- which have been explored as separate paradigms in prior studies. However, the impact of their joint modeling on the resulting collective dynamics remains largely unexplored. We develop a multi-agent reinforcement learning framework in which agents endowed with heterogeneous risk aversion, time discounting, and information access collectively learn trading strategies within a unified shared-policy framework. The experiment reveals that (i) learning with heterogeneous preferences drives agents to develop strategies aligned with their individual traits, fostering behavioral differentiation and niche specialization within the market, and (ii) the interactions by the differentiated agents are essential for the emergence of realistic market dynamics such as fat-tailed price fluctuations and volatility clustering. This study presents a constructive paradigm for financial market modeling in which the joint design of heterogeneous preferences and learning mechanisms enables two-stage emergence: individual behavior and the collective market dynamics.
<div id='section'>Paperid: <span id='pid'>457, <a href='https://arxiv.org/pdf/2511.01554.pdf' target='_blank'>https://arxiv.org/pdf/2511.01554.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aditya Kapoor, Yash Bhisikar, Benjamin Freed, Jan Peters, Mingfei Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.01554">Learning what to say and how precisely: Efficient Communication via Differentiable Discrete Communication Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Effective communication in multi-agent reinforcement learning (MARL) is critical for success but constrained by bandwidth, yet past approaches have been limited to complex gating mechanisms that only decide \textit{whether} to communicate, not \textit{how precisely}. Learning to optimize message precision at the bit-level is fundamentally harder, as the required discretization step breaks gradient flow. We address this by generalizing Differentiable Discrete Communication Learning (DDCL), a framework for end-to-end optimization of discrete messages. Our primary contribution is an extension of DDCL to support unbounded signals, transforming it into a universal, plug-and-play layer for any MARL architecture. We verify our approach with three key results. First, through a qualitative analysis in a controlled environment, we demonstrate \textit{how} agents learn to dynamically modulate message precision according to the informational needs of the task. Second, we integrate our variant of DDCL into four state-of-the-art MARL algorithms, showing it reduces bandwidth by over an order of magnitude while matching or exceeding task performance. Finally, we provide direct evidence for the \enquote{Bitter Lesson} in MARL communication: a simple Transformer-based policy leveraging DDCL matches the performance of complex, specialized architectures, questioning the necessity of bespoke communication designs.
<div id='section'>Paperid: <span id='pid'>458, <a href='https://arxiv.org/pdf/2510.27383.pdf' target='_blank'>https://arxiv.org/pdf/2510.27383.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yueyang Wang, Mehmet Dogar, Gustav Markkula
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.27383">Realistic pedestrian-driver interaction modelling using multi-agent RL with human perceptual-motor constraints</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Modelling pedestrian-driver interactions is critical for understanding human road user behaviour and developing safe autonomous vehicle systems. Existing approaches often rely on rule-based logic, game-theoretic models, or 'black-box' machine learning methods. However, these models typically lack flexibility or overlook the underlying mechanisms, such as sensory and motor constraints, which shape how pedestrians and drivers perceive and act in interactive scenarios. In this study, we propose a multi-agent reinforcement learning (RL) framework that integrates both visual and motor constraints of pedestrian and driver agents. Using a real-world dataset from an unsignalised pedestrian crossing, we evaluate four model variants, one without constraints, two with either motor or visual constraints, and one with both, across behavioural metrics of interaction realism. Results show that the combined model with both visual and motor constraints performs best. Motor constraints lead to smoother movements that resemble human speed adjustments during crossing interactions. The addition of visual constraints introduces perceptual uncertainty and field-of-view limitations, leading the agents to exhibit more cautious and variable behaviour, such as less abrupt deceleration. In this data-limited setting, our model outperforms a supervised behavioural cloning model, demonstrating that our approach can be effective without large training datasets. Finally, our framework accounts for individual differences by modelling parameters controlling the human constraints as population-level distributions, a perspective that has not been explored in previous work on pedestrian-vehicle interaction modelling. Overall, our work demonstrates that multi-agent RL with human constraints is a promising modelling approach for simulating realistic road user interactions.
<div id='section'>Paperid: <span id='pid'>459, <a href='https://arxiv.org/pdf/2510.20436.pdf' target='_blank'>https://arxiv.org/pdf/2510.20436.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Federico Lozano-Cuadra, Beatriz Soret, Marc Sanchez Net, Abhishek Cauligi, Federico Rossi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.20436">Learning Decentralized Routing Policies via Graph Attention-based Multi-Agent Reinforcement Learning in Lunar Delay-Tolerant Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a fully decentralized routing framework for multi-robot exploration missions operating under the constraints of a Lunar Delay-Tolerant Network (LDTN). In this setting, autonomous rovers must relay collected data to a lander under intermittent connectivity and unknown mobility patterns. We formulate the problem as a Partially Observable Markov Decision Problem (POMDP) and propose a Graph Attention-based Multi-Agent Reinforcement Learning (GAT-MARL) policy that performs Centralized Training, Decentralized Execution (CTDE). Our method relies only on local observations and does not require global topology updates or packet replication, unlike classical approaches such as shortest path and controlled flooding-based algorithms. Through Monte Carlo simulations in randomized exploration environments, GAT-MARL provides higher delivery rates, no duplications, and fewer packet losses, and is able to leverage short-term mobility forecasts; offering a scalable solution for future space robotic systems for planetary exploration, as demonstrated by successful generalization to larger rover teams.
<div id='section'>Paperid: <span id='pid'>460, <a href='https://arxiv.org/pdf/2510.20408.pdf' target='_blank'>https://arxiv.org/pdf/2510.20408.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tom Maus, Asma Atamna, Tobias Glasmachers
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.20408">Balancing Specialization and Centralization: A Multi-Agent Reinforcement Learning Benchmark for Sequential Industrial Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autonomous control of multi-stage industrial processes requires both local specialization and global coordination. Reinforcement learning (RL) offers a promising approach, but its industrial adoption remains limited due to challenges such as reward design, modularity, and action space management. Many academic benchmarks differ markedly from industrial control problems, limiting their transferability to real-world applications. This study introduces an enhanced industry-inspired benchmark environment that combines tasks from two existing benchmarks, SortingEnv and ContainerGym, into a sequential recycling scenario with sorting and pressing operations. We evaluate two control strategies: a modular architecture with specialized agents and a monolithic agent governing the full system, while also analyzing the impact of action masking. Our experiments show that without action masking, agents struggle to learn effective policies, with the modular architecture performing better. When action masking is applied, both architectures improve substantially, and the performance gap narrows considerably. These results highlight the decisive role of action space constraints and suggest that the advantages of specialization diminish as action complexity is reduced. The proposed benchmark thus provides a valuable testbed for exploring practical and robust multi-agent RL solutions in industrial automation, while contributing to the ongoing debate on centralization versus specialization.
<div id='section'>Paperid: <span id='pid'>461, <a href='https://arxiv.org/pdf/2510.13227.pdf' target='_blank'>https://arxiv.org/pdf/2510.13227.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Divyanshu Singh, Ashman Mehra, Snehanshu Saha, Santonu Sarkar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.13227">Altruistic Ride Sharing: A Community-Driven Approach to Short-Distance Mobility</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Urban mobility faces persistent challenges of congestion and fuel consumption, specifically when people choose a private, point-to-point commute option. Profit-driven ride-sharing platforms prioritize revenue over fairness and sustainability. This paper introduces Altruistic Ride-Sharing (ARS), a decentralized, peer-to-peer mobility framework where participants alternate between driver and rider roles based on altruism points rather than monetary incentives. The system integrates multi-agent reinforcement learning (MADDPG) for dynamic ride-matching, game-theoretic equilibrium guarantees for fairness, and a population model to sustain long-term balance. Using real-world New York City taxi data, we demonstrate that ARS reduces travel distance and emissions, increases vehicle utilization, and promotes equitable participation compared to both no-sharing and optimization-based baselines. These results establish ARS as a scalable, community-driven alternative to conventional ride-sharing, aligning individual behavior with collective urban sustainability goals.
<div id='section'>Paperid: <span id='pid'>462, <a href='https://arxiv.org/pdf/2510.07971.pdf' target='_blank'>https://arxiv.org/pdf/2510.07971.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Oskar Bohn Lassen, Serio Angelo Maria Agriesti, Filipe Rodrigues, Francisco Camara Pereira
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.07971">Climate Surrogates for Scalable Multi-Agent Reinforcement Learning: A Case Study with CICERO-SCM</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Climate policy studies require models that capture the combined effects of multiple greenhouse gases on global temperature, but these models are computationally expensive and difficult to embed in reinforcement learning. We present a multi-agent reinforcement learning (MARL) framework that integrates a high-fidelity, highly efficient climate surrogate directly in the environment loop, enabling regional agents to learn climate policies under multi-gas dynamics. As a proof of concept, we introduce a recurrent neural network architecture pretrained on ($20{,}000$) multi-gas emission pathways to surrogate the climate model CICERO-SCM. The surrogate model attains near-simulator accuracy with global-mean temperature RMSE $\approx 0.0004 \mathrm{K}$ and approximately $1000\times$ faster one-step inference. When substituted for the original simulator in a climate-policy MARL setting, it accelerates end-to-end training by $>\!100\times$. We show that the surrogate and simulator converge to the same optimal policies and propose a methodology to assess this property in cases where using the simulator is intractable. Our work allows to bypass the core computational bottleneck without sacrificing policy fidelity, enabling large-scale multi-agent experiments across alternative climate-policy regimes with multi-gas dynamics and high-fidelity climate response.
<div id='section'>Paperid: <span id='pid'>463, <a href='https://arxiv.org/pdf/2510.07888.pdf' target='_blank'>https://arxiv.org/pdf/2510.07888.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinren Zhang, Sixi Cheng, Zixin Zhong, Jiadong Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.07888">Network Topology and Information Efficiency of Multi-Agent Systems: Study based on MARL</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent systems (MAS) solve complex problems through coordinated autonomous entities with individual decision-making capabilities. While Multi-Agent Reinforcement Learning (MARL) enables these agents to learn intelligent strategies, it faces challenges of non-stationarity and partial observability. Communications among agents offer a solution, but questions remain about its optimal structure and evaluation. This paper explores two underexamined aspects: communication topology and information efficiency. We demonstrate that directed and sequential topologies improve performance while reducing communication overhead across both homogeneous and heterogeneous tasks. Additionally, we introduce two metrics -- Information Entropy Efficiency Index (IEI) and Specialization Efficiency Index (SEI) -- to evaluate message compactness and role differentiation. Incorporating these metrics into training objectives improves success rates and convergence speed. Our findings highlight that designing adaptive communication topologies with information-efficient messaging is essential for effective coordination in complex MAS.
<div id='section'>Paperid: <span id='pid'>464, <a href='https://arxiv.org/pdf/2509.18526.pdf' target='_blank'>https://arxiv.org/pdf/2509.18526.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Han Zeng, Haibo Wang, Luhao Fan, Bingcheng Zhu, Xiaohu You, Zaichen Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.18526">AI Agent Access (A\^3) Network: An Embodied, Communication-Aware Multi-Agent Framework for 6G Coverage</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The vision of 6G communication demands autonomous and resilient networking in environments without fixed infrastructure. Yet most multi-agent reinforcement learning (MARL) approaches focus on isolated stages - exploration, relay formation, or access - under static deployments and centralized control, limiting adaptability. We propose the AI Agent Access (A\^3) Network, a unified, embodied intelligence-driven framework that transforms multi-agent networking into a dynamic, decentralized, and end-to-end system. Unlike prior schemes, the A\^3 Network integrates exploration, target user access, and backhaul maintenance within a single learning process, while supporting on-demand agent addition during runtime. Its decentralized policies ensure that even a single agent can operate independently with limited observations, while coordinated agents achieve scalable, communication-optimized coverage. By embedding link-level communication metrics into actor-critic learning, the A\^3 Network couples topology formation with robust decision-making. Numerical simulations demonstrate that the A\^3 Network not only balances exploration and communication efficiency but also delivers system-level adaptability absent in existing MARL frameworks, offering a new paradigm for 6G multi-agent networks.
<div id='section'>Paperid: <span id='pid'>465, <a href='https://arxiv.org/pdf/2509.11508.pdf' target='_blank'>https://arxiv.org/pdf/2509.11508.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tinglong Deng, Hang Tao, Xinxiang Wang, Yinyan Wang, Hanjiang Luo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.11508">SafeDiver: Cooperative AUV-USV Assisted Diver Communication via Multi-agent Reinforcement Learning Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As underwater human activities are increasing, the demand for underwater communication service presents a significant challenge. Existing underwater diver communication methods face hurdles due to inherent disadvantages and complex underwater environments. To address this issue, we propose a scheme that utilizes maritime unmanned systems to assist divers with reliable and high-speed communication. Multiple AUVs are equipped with optical and acoustic multimodal communication devices as relay nodes, providing adaptive communication services based on changes in the diver's activity area. By using a multi-agent reinforcement learning (MARL) approach to control the cooperative movement of AUVs, high-speed and reliable data transmission between divers can be achieved. At the same time, utilizing the advantages of on-demand deployment and wide coverage of unmanned surface vehicles (USVs) as surface relay nodes to coordinate and forward information from AUVs, and controlling AUVs to adaptively select relay USV nodes for data transmission, high-quality communication between divers and surface platform can be achieved. Through simulation verification, the proposed scheme can effectively achieve reliable and high-speed communication for divers.
<div id='section'>Paperid: <span id='pid'>466, <a href='https://arxiv.org/pdf/2509.05051.pdf' target='_blank'>https://arxiv.org/pdf/2509.05051.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aaron Mark Thomas, Yu-Cheng Chen, Hubert Okadome Valencia, Sharu Theresa Jose, Ronin Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.05051">QCA-MolGAN: Quantum Circuit Associative Molecular GAN with Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Navigating the vast chemical space of molecular structures to design novel drug molecules with desired target properties remains a central challenge in drug discovery. Recent advances in generative models offer promising solutions. This work presents a novel quantum circuit Born machine (QCBM)-enabled Generative Adversarial Network (GAN), called QCA-MolGAN, for generating drug-like molecules. The QCBM serves as a learnable prior distribution, which is associatively trained to define a latent space aligning with high-level features captured by the GANs discriminator. Additionally, we integrate a novel multi-agent reinforcement learning network to guide molecular generation with desired targeted properties, optimising key metrics such as quantitative estimate of drug-likeness (QED), octanol-water partition coefficient (LogP) and synthetic accessibility (SA) scores in conjunction with one another. Experimental results demonstrate that our approach enhances the property alignment of generated molecules with the multi-agent reinforcement learning agents effectively balancing chemical properties.
<div id='section'>Paperid: <span id='pid'>467, <a href='https://arxiv.org/pdf/2508.18708.pdf' target='_blank'>https://arxiv.org/pdf/2508.18708.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Promise Osaine Ekpo, Brian La, Thomas Wiener, Saesha Agarwal, Arshia Agrawal, Gonzalo Gonzalez-Pumariega, Lekan P. Molu, Angelique Taylor
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.18708">Skill-Aligned Fairness in Multi-Agent Learning for Collaboration in Healthcare</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Fairness in multi-agent reinforcement learning (MARL) is often framed as a workload balance problem, overlooking agent expertise and the structured coordination required in real-world domains. In healthcare, equitable task allocation requires workload balance or expertise alignment to prevent burnout and overuse of highly skilled agents. Workload balance refers to distributing an approximately equal number of subtasks or equalised effort across healthcare workers, regardless of their expertise. We make two contributions to address this problem. First, we propose FairSkillMARL, a framework that defines fairness as the dual objective of workload balance and skill-task alignment. Second, we introduce MARLHospital, a customizable healthcare-inspired environment for modeling team compositions and energy-constrained scheduling impacts on fairness, as no existing simulators are well-suited for this problem. We conducted experiments to compare FairSkillMARL in conjunction with four standard MARL methods, and against two state-of-the-art fairness metrics. Our results suggest that fairness based solely on equal workload might lead to task-skill mismatches and highlight the need for more robust metrics that capture skill-task misalignment. Our work provides tools and a foundation for studying fairness in heterogeneous multi-agent systems where aligning effort with expertise is critical.
<div id='section'>Paperid: <span id='pid'>468, <a href='https://arxiv.org/pdf/2508.12524.pdf' target='_blank'>https://arxiv.org/pdf/2508.12524.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Joseph SuÃ¡rez, Kyoung Whan Choe, David Bloomin, Jianming Gao, Yunkun Li, Yao Feng, Saidinesh Pola, Kun Zhang, Yonghui Zhu, Nikhil Pinnaparaju, Hao Xiang Li, Nishaanth Kanna, Daniel Scott, Ryan Sullivan, Rose S. Shuman, Lucas de AlcÃ¢ntara, Herbie Bradley, Kirsty You, Bo Wu, Yuhao Jiang, Qimai Li, Jiaxin Chen, Louis Castricato, Xiaolong Zhu, Phillip Isola
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.12524">Results of the NeurIPS 2023 Neural MMO Competition on Multi-task Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present the results of the NeurIPS 2023 Neural MMO Competition, which attracted over 200 participants and submissions. Participants trained goal-conditional policies that generalize to tasks, maps, and opponents never seen during training. The top solution achieved a score 4x higher than our baseline within 8 hours of training on a single 4090 GPU. We open-source everything relating to Neural MMO and the competition under the MIT license, including the policy weights and training code for our baseline and for the top submissions.
<div id='section'>Paperid: <span id='pid'>469, <a href='https://arxiv.org/pdf/2508.11706.pdf' target='_blank'>https://arxiv.org/pdf/2508.11706.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhuofan Xu, Benedikt Bollig, Matthias FÃ¼gger, Thomas Nowak, Vincent Le DrÃ©au
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.11706">Centralized Permutation Equivariant Policy for Cooperative Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The Centralized Training with Decentralized Execution (CTDE) paradigm has gained significant attention in multi-agent reinforcement learning (MARL) and is the foundation of many recent algorithms. However, decentralized policies operate under partial observability and often yield suboptimal performance compared to centralized policies, while fully centralized approaches typically face scalability challenges as the number of agents increases.
  We propose Centralized Permutation Equivariant (CPE) learning, a centralized training and execution framework that employs a fully centralized policy to overcome these limitations. Our approach leverages a novel permutation equivariant architecture, Global-Local Permutation Equivariant (GLPE) networks, that is lightweight, scalable, and easy to implement. Experiments show that CPE integrates seamlessly with both value decomposition and actor-critic methods, substantially improving the performance of standard CTDE algorithms across cooperative benchmarks including MPE, SMAC, and RWARE, and matching the performance of state-of-the-art RWARE implementations.
<div id='section'>Paperid: <span id='pid'>470, <a href='https://arxiv.org/pdf/2508.10423.pdf' target='_blank'>https://arxiv.org/pdf/2508.10423.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qi Liu, Xiaopeng Zhang, Mingshan Tan, Shuaikang Ma, Jinliang Ding, Yanjie Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.10423">MASH: Cooperative-Heterogeneous Multi-Agent Reinforcement Learning for Single Humanoid Robot Locomotion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper proposes a novel method to enhance locomotion for a single humanoid robot through cooperative-heterogeneous multi-agent deep reinforcement learning (MARL). While most existing methods typically employ single-agent reinforcement learning algorithms for a single humanoid robot or MARL algorithms for multi-robot system tasks, we propose a distinct paradigm: applying cooperative-heterogeneous MARL to optimize locomotion for a single humanoid robot. The proposed method, multi-agent reinforcement learning for single humanoid locomotion (MASH), treats each limb (legs and arms) as an independent agent that explores the robot's action space while sharing a global critic for cooperative learning. Experiments demonstrate that MASH accelerates training convergence and improves whole-body cooperation ability, outperforming conventional single-agent reinforcement learning methods. This work advances the integration of MARL into single-humanoid-robot control, offering new insights into efficient locomotion strategies.
<div id='section'>Paperid: <span id='pid'>471, <a href='https://arxiv.org/pdf/2506.04195.pdf' target='_blank'>https://arxiv.org/pdf/2506.04195.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Elena Zamaraeva, Christopher M. Collins, George R. Darling, Matthew S. Dyer, Bei Peng, Rahul Savani, Dmytro Antypov, Vladimir V. Gusev, Judith Clymo, Paul G. Spirakis, Matthew J. Rosseinsky
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.04195">MACS: Multi-Agent Reinforcement Learning for Optimization of Crystal Structures</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Geometry optimization of atomic structures is a common and crucial task in computational chemistry and materials design. Following the learning to optimize paradigm, we propose a new multi-agent reinforcement learning method called Multi-Agent Crystal Structure optimization (MACS) to address periodic crystal structure optimization. MACS treats geometry optimization as a partially observable Markov game in which atoms are agents that adjust their positions to collectively discover a stable configuration. We train MACS across various compositions of reported crystalline materials to obtain a policy that successfully optimizes structures from the training compositions as well as structures of larger sizes and unseen compositions, confirming its excellent scalability and zero-shot transferability. We benchmark our approach against a broad range of state-of-the-art optimization methods and demonstrate that MACS optimizes periodic crystal structures significantly faster, with fewer energy calculations, and the lowest failure rate.
<div id='section'>Paperid: <span id='pid'>472, <a href='https://arxiv.org/pdf/2505.22979.pdf' target='_blank'>https://arxiv.org/pdf/2505.22979.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bengisu Guresti, Chongjie Zhang, Yevgeniy Vorobeychik
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.22979">Learning Recommender Mechanisms for Bayesian Stochastic Games</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>An important challenge in non-cooperative game theory is coordinating on a single (approximate) equilibrium from many possibilities - a challenge that becomes even more complex when players hold private information. Recommender mechanisms tackle this problem by recommending strategies to players based on their reported type profiles. A key consideration in such mechanisms is to ensure that players are incentivized to participate, report their private information truthfully, and follow the recommendations. While previous work has focused on designing recommender mechanisms for one-shot and extensive-form games, these approaches cannot be effectively applied to stochastic games, particularly if we constrain recommendations to be Markov stationary policies. To bridge this gap, we introduce a novel bi-level reinforcement learning approach for automatically designing recommender mechanisms in Bayesian stochastic games. Our method produces a mechanism represented by a parametric function (such as a neural network), and is therefore highly efficient at execution time. Experimental results on two repeated and two stochastic games demonstrate that our approach achieves social welfare levels competitive with cooperative multi-agent reinforcement learning baselines, while also providing significantly improved incentive properties.
<div id='section'>Paperid: <span id='pid'>473, <a href='https://arxiv.org/pdf/2505.08630.pdf' target='_blank'>https://arxiv.org/pdf/2505.08630.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuai Han, Mehdi Dastani, Shihan Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.08630">Credit Assignment and Efficient Exploration based on Influence Scope in Multi-agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Training cooperative agents in sparse-reward scenarios poses significant challenges for multi-agent reinforcement learning (MARL). Without clear feedback on actions at each step in sparse-reward setting, previous methods struggle with precise credit assignment among agents and effective exploration. In this paper, we introduce a novel method to deal with both credit assignment and exploration problems in reward-sparse domains. Accordingly, we propose an algorithm that calculates the Influence Scope of Agents (ISA) on states by taking specific value of the dimensions/attributes of states that can be influenced by individual agents. The mutual dependence between agents' actions and state attributes are then used to calculate the credit assignment and to delimit the exploration space for each individual agent. We then evaluate ISA in a variety of sparse-reward multi-agent scenarios. The results show that our method significantly outperforms the state-of-art baselines.
<div id='section'>Paperid: <span id='pid'>474, <a href='https://arxiv.org/pdf/2505.03288.pdf' target='_blank'>https://arxiv.org/pdf/2505.03288.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Francesco Morri, HÃ©lÃ¨ne Le Cadre, Pierre Gruet, Luce Brotcorne
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.03288">Game Theory and Multi-Agent Reinforcement Learning for Zonal Ancillary Markets</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We characterize zonal ancillary market coupling relying on noncooperative game theory. To that purpose, we formulate the ancillary market as a multi-leader single follower bilevel problem, that we subsequently cast as a generalized Nash game with side constraints and nonconvex feasibility sets. We determine conditions for equilibrium existence and show that the game has a generalized potential game structure. To compute market equilibrium, we rely on two exact approaches: an integrated optimization approach and Gauss-Seidel best-response, that we compare against multi-agent deep reinforcement learning. On real data from Germany and Austria, simulations indicate that multi-agent deep reinforcement learning achieves the smallest convergence rate but requires pretraining, while best-response is the slowest. On the economics side, multi-agent deep reinforcement learning results in smaller market costs compared to the exact methods, but at the cost of higher variability in the profit allocation among stakeholders. Further, stronger coupling between zones tends to reduce costs for larger zones.
<div id='section'>Paperid: <span id='pid'>475, <a href='https://arxiv.org/pdf/2505.01115.pdf' target='_blank'>https://arxiv.org/pdf/2505.01115.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Palok Biswas, Zuzanna Osika, Isidoro Tamassia, Adit Whorra, Jazmin Zatarain-Salazar, Jan Kwakkel, Frans A. Oliehoek, Pradeep K. Murukannaiah
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.01115">Exploring Equity of Climate Policies using Multi-Agent Multi-Objective Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Addressing climate change requires coordinated policy efforts of nations worldwide. These efforts are informed by scientific reports, which rely in part on Integrated Assessment Models (IAMs), prominent tools used to assess the economic impacts of climate policies. However, traditional IAMs optimize policies based on a single objective, limiting their ability to capture the trade-offs among economic growth, temperature goals, and climate justice. As a result, policy recommendations have been criticized for perpetuating inequalities, fueling disagreements during policy negotiations. We introduce Justice, the first framework integrating IAM with Multi-Objective Multi-Agent Reinforcement Learning (MOMARL). By incorporating multiple objectives, Justice generates policy recommendations that shed light on equity while balancing climate and economic goals. Further, using multiple agents can provide a realistic representation of the interactions among the diverse policy actors. We identify equitable Pareto-optimal policies using our framework, which facilitates deliberative decision-making by presenting policymakers with the inherent trade-offs in climate and economic policy.
<div id='section'>Paperid: <span id='pid'>476, <a href='https://arxiv.org/pdf/2503.22867.pdf' target='_blank'>https://arxiv.org/pdf/2503.22867.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Huiwen Yan, Mushuang Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.22867">Markov Potential Game Construction and Multi-Agent Reinforcement Learning with Applications to Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Markov games (MGs) serve as the mathematical foundation for multi-agent reinforcement learning (MARL), enabling self-interested agents to learn their optimal policies while interacting with others in a shared environment. However, due to the complexities of an MG problem, seeking (Markov perfect) Nash equilibrium (NE) is often very challenging for a general-sum MG. Markov potential games (MPGs), which are a special class of MGs, have appealing properties such as guaranteed existence of pure NEs and guaranteed convergence of gradient play algorithms, thereby leading to desirable properties for many MARL algorithms in their NE-seeking processes. However, the question of how to construct MPGs has been open. This paper provides sufficient conditions on the reward design and on the Markov decision process (MDP), under which an MG is an MPG. Numerical results on autonomous driving applications are reported.
<div id='section'>Paperid: <span id='pid'>477, <a href='https://arxiv.org/pdf/2503.14576.pdf' target='_blank'>https://arxiv.org/pdf/2503.14576.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zihao Guo, Shuqing Shi, Richard Willis, Tristan Tomilin, Joel Z. Leibo, Yali Du
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.14576">SocialJax: An Evaluation Suite for Multi-agent Reinforcement Learning in Sequential Social Dilemmas</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Sequential social dilemmas pose a significant challenge in the field of multi-agent reinforcement learning (MARL), requiring environments that accurately reflect the tension between individual and collective interests. Previous benchmarks and environments, such as Melting Pot, provide an evaluation protocol that measures generalization to new social partners in various test scenarios. However, running reinforcement learning algorithms in traditional environments requires substantial computational resources. In this paper, we introduce SocialJax, a suite of sequential social dilemma environments and algorithms implemented in JAX. JAX is a high-performance numerical computing library for Python that enables significant improvements in operational efficiency. Our experiments demonstrate that the SocialJax training pipeline achieves at least 50\texttimes{} speed-up in real-time performance compared to Melting Pot RLlib baselines. Additionally, we validate the effectiveness of baseline algorithms within SocialJax environments. Finally, we use Schelling diagrams to verify the social dilemma properties of these environments, ensuring that they accurately capture the dynamics of social dilemmas.
<div id='section'>Paperid: <span id='pid'>478, <a href='https://arxiv.org/pdf/2502.05573.pdf' target='_blank'>https://arxiv.org/pdf/2502.05573.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Beining Zhang, Aditya Kapoor, Mingfei Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.05573">Low-Rank Agent-Specific Adaptation (LoRASA) for Multi-Agent Policy Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent reinforcement learning (MARL) often relies on \emph{parameter sharing (PS)} to scale efficiently. However, purely shared policies can stifle each agent's unique specialization, reducing overall performance in heterogeneous environments. We propose \textbf{Low-Rank Agent-Specific Adaptation (LoRASA)}, a novel approach that treats each agent's policy as a specialized ``task'' fine-tuned from a shared backbone. Drawing inspiration from parameter-efficient transfer methods, LoRASA appends small, low-rank adaptation matrices to each layer of the shared policy, naturally inducing \emph{parameter-space sparsity} that promotes both specialization and scalability. We evaluate LoRASA on challenging benchmarks including the StarCraft Multi-Agent Challenge (SMAC) and Multi-Agent MuJoCo (MAMuJoCo), implementing it atop widely used algorithms such as MAPPO and A2PO. Across diverse tasks, LoRASA matches or outperforms existing baselines \emph{while reducing memory and computational overhead}. Ablation studies on adapter rank, placement, and timing validate the method's flexibility and efficiency. Our results suggest LoRASA's potential to establish a new norm for MARL policy parameterization: combining a shared foundation for coordination with low-rank agent-specific refinements for individual specialization.
<div id='section'>Paperid: <span id='pid'>479, <a href='https://arxiv.org/pdf/2501.15802.pdf' target='_blank'>https://arxiv.org/pdf/2501.15802.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lanpei Li, Jack Bell, Massimo Coppola, Vincenzo Lomonaco
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.15802">Adaptive AI-based Decentralized Resource Management in the Cloud-Edge Continuum</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The increasing complexity of application requirements and the dynamic nature of the Cloud-Edge Continuum present significant challenges for efficient resource management. These challenges stem from the ever-changing infrastructure, which is characterized by additions, removals, and reconfigurations of nodes and links, as well as the variability of application workloads. Traditional centralized approaches struggle to adapt to these changes due to their static nature, while decentralized solutions face challenges such as limited global visibility and coordination overhead. This paper proposes a hybrid decentralized framework for dynamic application placement and resource management. The framework utilizes Graph Neural Networks (GNNs) to embed resource and application states, enabling comprehensive representation and efficient decision-making. It employs a collaborative multi-agent reinforcement learning (MARL) approach, where local agents optimize resource management in their neighborhoods and a global orchestrator ensures system-wide coordination. By combining decentralized application placement with centralized oversight, our framework addresses the scalability, adaptability, and accuracy challenges inherent in the Cloud-Edge Continuum. This work contributes to the development of decentralized application placement strategies, the integration of GNN embeddings, and collaborative MARL systems, providing a foundation for efficient, adaptive and scalable resource management.
<div id='section'>Paperid: <span id='pid'>480, <a href='https://arxiv.org/pdf/2501.05113.pdf' target='_blank'>https://arxiv.org/pdf/2501.05113.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tobias Kortus, Ralf Keidel, Nicolas R. Gauger, Jan Kieseler
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.05113">Constrained Optimization of Charged Particle Tracking with Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reinforcement learning demonstrated immense success in modelling complex physics-driven systems, providing end-to-end trainable solutions by interacting with a simulated or real environment, maximizing a scalar reward signal. In this work, we propose, building upon previous work, a multi-agent reinforcement learning approach with assignment constraints for reconstructing particle tracks in pixelated particle detectors. Our approach optimizes collaboratively a parametrized policy, functioning as a heuristic to a multidimensional assignment problem, by jointly minimizing the total amount of particle scattering over the reconstructed tracks in a readout frame. To satisfy constraints, guaranteeing a unique assignment of particle hits, we propose a safety layer solving a linear assignment problem for every joint action. Further, to enforce cost margins, increasing the distance of the local policies predictions to the decision boundaries of the optimizer mappings, we recommend the use of an additional component in the blackbox gradient estimation, forcing the policy to solutions with lower total assignment costs. We empirically show on simulated data, generated for a particle detector developed for proton imaging, the effectiveness of our approach, compared to multiple single- and multi-agent baselines. We further demonstrate the effectiveness of constraints with cost margins for both optimization and generalization, introduced by wider regions with high reconstruction performance as well as reduced predictive instabilities. Our results form the basis for further developments in RL-based tracking, offering both enhanced performance with constrained policies and greater flexibility in optimizing tracking algorithms through the option for individual and team rewards.
<div id='section'>Paperid: <span id='pid'>481, <a href='https://arxiv.org/pdf/2412.19538.pdf' target='_blank'>https://arxiv.org/pdf/2412.19538.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuan Zhou, Xiang Shi, Lele Zhang, Chen Chen, Hongbo Li, Lin Ma, Fang Deng, Jie Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.19538">Scalable Hierarchical Reinforcement Learning for Hyper Scale Multi-Robot Task Planning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To improve the efficiency of warehousing system and meet huge customer orders, we aim to solve the challenges of dimension disaster and dynamic properties in hyper scale multi-robot task planning (MRTP) for robotic mobile fulfillment system (RMFS). Existing research indicates that hierarchical reinforcement learning (HRL) is an effective method to reduce these challenges. Based on that, we construct an efficient multi-stage HRL-based multi-robot task planner for hyper scale MRTP in RMFS, and the planning process is represented with a special temporal graph topology. To ensure optimality, the planner is designed with a centralized architecture, but it also brings the challenges of scaling up and generalization that require policies to maintain performance for various unlearned scales and maps. To tackle these difficulties, we first construct a hierarchical temporal attention network (HTAN) to ensure basic ability of handling inputs with unfixed lengths, and then design multi-stage curricula for hierarchical policy learning to further improve the scaling up and generalization ability while avoiding catastrophic forgetting. Additionally, we notice that policies with hierarchical structure suffer from unfair credit assignment that is similar to that in multi-agent reinforcement learning, inspired of which, we propose a hierarchical reinforcement learning algorithm with counterfactual rollout baseline to improve learning performance. Experimental results demonstrate that our planner outperform other state-of-the-art methods on various MRTP instances in both simulated and real-world RMFS. Also, our planner can successfully scale up to hyper scale MRTP instances in RMFS with up to 200 robots and 1000 retrieval racks on unlearned maps while keeping superior performance over other methods.
<div id='section'>Paperid: <span id='pid'>482, <a href='https://arxiv.org/pdf/2412.15517.pdf' target='_blank'>https://arxiv.org/pdf/2412.15517.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yangkun Chen, Kai Yang, Jian Tao, Jiafei Lyu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.15517">Novelty-Guided Data Reuse for Efficient and Diversified Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, deep Multi-Agent Reinforcement Learning (MARL) has demonstrated its potential to tackle complex cooperative tasks, pushing the boundaries of AI in collaborative environments. However, the efficiency of these systems is often compromised by inadequate sample utilization and a lack of diversity in learning strategies. To enhance MARL performance, we introduce a novel sample reuse approach that dynamically adjusts policy updates based on observation novelty. Specifically, we employ a Random Network Distillation (RND) network to gauge the novelty of each agent's current state, assigning additional sample update opportunities based on the uniqueness of the data. We name our method Multi-Agent Novelty-GuidEd sample Reuse (MANGER). This method increases sample efficiency and promotes exploration and diverse agent behaviors. Our evaluations confirm substantial improvements in MARL effectiveness in complex cooperative scenarios such as Google Research Football and super-hard StarCraft II micromanagement tasks.
<div id='section'>Paperid: <span id='pid'>483, <a href='https://arxiv.org/pdf/2412.12326.pdf' target='_blank'>https://arxiv.org/pdf/2412.12326.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yue Jin, Shuangqing Wei, Giovanni Montana
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.12326">Achieving Collective Welfare in Multi-Agent Reinforcement Learning via Suggestion Sharing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In human society, the conflict between self-interest and collective well-being often obstructs efforts to achieve shared welfare. Related concepts like the Tragedy of the Commons and Social Dilemmas frequently manifest in our daily lives. As artificial agents increasingly serve as autonomous proxies for humans, we propose a novel multi-agent reinforcement learning (MARL) method to address this issue - learning policies to maximise collective returns even when individual agents' interests conflict with the collective one. Unlike traditional cooperative MARL solutions that involve sharing rewards, values, and policies or designing intrinsic rewards to encourage agents to learn collectively optimal policies, we propose a novel MARL approach where agents exchange action suggestions. Our method reveals less private information compared to sharing rewards, values, or policies, while enabling effective cooperation without the need to design intrinsic rewards. Our algorithm is supported by our theoretical analysis that establishes a bound on the discrepancy between collective and individual objectives, demonstrating how sharing suggestions can align agents' behaviours with the collective objective. Experimental results demonstrate that our algorithm performs competitively with baselines that rely on value or policy sharing or intrinsic rewards.
<div id='section'>Paperid: <span id='pid'>484, <a href='https://arxiv.org/pdf/2411.13942.pdf' target='_blank'>https://arxiv.org/pdf/2411.13942.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ing-Sheng Bernard-Tiong, Yoshihisa Tsurumine, Ryosuke Sota, Kazuki Shibata, Takamitsu Matsubara
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.13942">Cooperative Grasping and Transportation using Multi-agent Reinforcement Learning with Ternary Force Representation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cooperative grasping and transportation require effective coordination to complete the task. This study focuses on the approach leveraging force-sensing feedback, where robots use sensors to detect forces applied by others on an object to achieve coordination. Unlike explicit communication, it avoids delays and interruptions; however, force-sensing is highly sensitive and prone to interference from variations in grasping environment, such as changes in grasping force, grasping pose, object size and geometry, which can interfere with force signals, subsequently undermining coordination. We propose multi-agent reinforcement learning (MARL) with ternary force representation, a force representation that maintains consistent representation against variations in grasping environment. The simulation and real-world experiments demonstrate the robustness of the proposed method to changes in grasping force, object size and geometry as well as inherent sim2real gap.
<div id='section'>Paperid: <span id='pid'>485, <a href='https://arxiv.org/pdf/2411.11099.pdf' target='_blank'>https://arxiv.org/pdf/2411.11099.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ting Zhu, Yue Jin, Jeremie Houssineau, Giovanni Montana
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.11099">Mitigating Relative Over-Generalization in Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In decentralized multi-agent reinforcement learning, agents learning in isolation can lead to relative over-generalization (RO), where optimal joint actions are undervalued in favor of suboptimal ones. This hinders effective coordination in cooperative tasks, as agents tend to choose actions that are individually rational but collectively suboptimal. To address this issue, we introduce MaxMax Q-Learning (MMQ), which employs an iterative process of sampling and evaluating potential next states, selecting those with maximal Q-values for learning. This approach refines approximations of ideal state transitions, aligning more closely with the optimal joint policy of collaborating agents. We provide theoretical analysis supporting MMQ's potential and present empirical evaluations across various environments susceptible to RO. Our results demonstrate that MMQ frequently outperforms existing baselines, exhibiting enhanced convergence and sample efficiency.
<div id='section'>Paperid: <span id='pid'>486, <a href='https://arxiv.org/pdf/2411.08299.pdf' target='_blank'>https://arxiv.org/pdf/2411.08299.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xin Tang, Qian Chen, Wenjie Weng, Binhan Liao, Jiacheng Wang, Xianbin Cao, Xiaohuan Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.08299">DNN Task Assignment in UAV Networks: A Generative AI Enhanced Multi-Agent Reinforcement Learning Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Unmanned Aerial Vehicles (UAVs) possess high mobility and flexible deployment capabilities, prompting the development of UAVs for various application scenarios within the Internet of Things (IoT). The unique capabilities of UAVs give rise to increasingly critical and complex tasks in uncertain and potentially harsh environments. The substantial amount of data generated from these applications necessitates processing and analysis through deep neural networks (DNNs). However, UAVs encounter challenges due to their limited computing resources when managing DNN models. This paper presents a joint approach that combines multiple-agent reinforcement learning (MARL) and generative diffusion models (GDM) for assigning DNN tasks to a UAV swarm, aimed at reducing latency from task capture to result output. To address these challenges, we first consider the task size of the target area to be inspected and the shortest flying path as optimization constraints, employing a greedy algorithm to resolve the subproblem with a focus on minimizing the UAV's flying path and the overall system cost. In the second stage, we introduce a novel DNN task assignment algorithm, termed GDM-MADDPG, which utilizes the reverse denoising process of GDM to replace the actor network in multi-agent deep deterministic policy gradient (MADDPG). This approach generates specific DNN task assignment actions based on agents' observations in a dynamic environment. Simulation results indicate that our algorithm performs favorably compared to benchmarks in terms of path planning, Age of Information (AoI), energy consumption, and task load balancing.
<div id='section'>Paperid: <span id='pid'>487, <a href='https://arxiv.org/pdf/2409.16720.pdf' target='_blank'>https://arxiv.org/pdf/2409.16720.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xian Wang, Jin Zhou, Yuanli Feng, Jiahao Mei, Jiming Chen, Shuo Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.16720">Dashing for the Golden Snitch: Multi-Drone Time-Optimal Motion Planning with Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent innovations in autonomous drones have facilitated time-optimal flight in single-drone configurations, and enhanced maneuverability in multi-drone systems by applying optimal control and learning-based methods. However, few studies have achieved time-optimal motion planning for multi-drone systems, particularly during highly agile maneuvers or in dynamic scenarios. This paper presents a decentralized policy network using multi-agent reinforcement learning for time-optimal multi-drone flight. To strike a balance between flight efficiency and collision avoidance, we introduce a soft collision-free mechanism inspired by optimization-based methods. By customizing PPO in a centralized training, decentralized execution (CTDE) fashion, we unlock higher efficiency and stability in training while ensuring lightweight implementation. Extensive simulations show that, despite slight performance trade-offs compared to single-drone systems, our multi-drone approach maintains near-time-optimal performance with a low collision rate. Real-world experiments validate our method, with two quadrotors using the same network as in simulation achieving a maximum speed of 13.65 m/s and a maximum body rate of 13.4 rad/s in a 5.5 m * 5.5 m * 2.0 m space across various tracks, relying entirely on onboard computation.
<div id='section'>Paperid: <span id='pid'>488, <a href='https://arxiv.org/pdf/2601.17454.pdf' target='_blank'>https://arxiv.org/pdf/2601.17454.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Muhammad Ahmed Atif, Nehal Naeem Haji, Mohammad Shahid Shaikh, Muhammad Ebad Atif
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.17454">Embodiment-Induced Coordination Regimes in Tabular Multi-Agent Q-Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Centralized value learning is often assumed to improve coordination and stability in multi-agent reinforcement learning, yet this assumption is rarely tested under controlled conditions. We directly evaluate it in a fully tabular predator-prey gridworld by comparing independent and centralized Q-learning under explicit embodiment constraints on agent speed and stamina. Across multiple kinematic regimes and asymmetric agent roles, centralized learning fails to provide a consistent advantage and is frequently outperformed by fully independent learning, even under full observability and exact value estimation. Moreover, asymmetric centralized-independent configurations induce persistent coordination breakdowns rather than transient learning instability. By eliminating confounding effects from function approximation and representation learning, our tabular analysis isolates coordination structure as the primary driver of these effects. The results show that increased coordination can become a liability under embodiment constraints, and that the effectiveness of centralized learning is fundamentally regime and role dependent rather than universal.
<div id='section'>Paperid: <span id='pid'>489, <a href='https://arxiv.org/pdf/2601.05509.pdf' target='_blank'>https://arxiv.org/pdf/2601.05509.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yi-Ning Weng, Hsuan-Wei Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.05509">How Exploration Breaks Cooperation in Shared-Policy Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent reinforcement learning in dynamic social dilemmas commonly relies on parameter sharing to enable scalability. We show that in shared-policy Deep Q-Network learning, standard exploration can induce a robust and systematic collapse of cooperation even in environments where fully cooperative equilibria are stable and payoff dominant. Through controlled experiments, we demonstrate that shared DQN converges to stable but persistently low-cooperation regimes. This collapse is not caused by reward misalignment, noise, or insufficient training, but by a representational failure arising from partial observability combined with parameter coupling across heterogeneous agent states. Exploration-driven updates bias the shared representation toward locally dominant defection responses, which then propagate across agents and suppress cooperative learning. We confirm that the failure persists across network sizes, exploration schedules, and payoff structures, and disappears when parameter sharing is removed or when agents maintain independent representations. These results identify a fundamental failure mode of shared-policy MARL and establish structural conditions under which scalable learning architectures can systematically undermine cooperation. Our findings provide concrete guidance for the design of multi-agent learning systems in social and economic environments where collective behavior is critical.
<div id='section'>Paperid: <span id='pid'>490, <a href='https://arxiv.org/pdf/2512.22832.pdf' target='_blank'>https://arxiv.org/pdf/2512.22832.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Cuiling Wu, Yaozhong Gan, Junliang Xing, Ying Fu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.22832">MARPO: A Reflective Policy Optimization for Multi Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose Multi Agent Reflective Policy Optimization (MARPO) to alleviate the issue of sample inefficiency in multi agent reinforcement learning. MARPO consists of two key components: a reflection mechanism that leverages subsequent trajectories to enhance sample efficiency, and an asymmetric clipping mechanism that is derived from the KL divergence and dynamically adjusts the clipping range to improve training stability. We evaluate MARPO in classic multi agent environments, where it consistently outperforms other methods.
<div id='section'>Paperid: <span id='pid'>491, <a href='https://arxiv.org/pdf/2512.00351.pdf' target='_blank'>https://arxiv.org/pdf/2512.00351.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Na Li, Yuchen Jiao, Hangguan Shan, Shefeng Yan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.00351">Provable Memory Efficient Self-Play Algorithm for Model-free Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The thriving field of multi-agent reinforcement learning (MARL) studies how a group of interacting agents make decisions autonomously in a shared dynamic environment. Existing theoretical studies in this area suffer from at least two of the following obstacles: memory inefficiency, the heavy dependence of sample complexity on the long horizon and the large state space, the high computational complexity, non-Markov policy, non-Nash policy, and high burn-in cost. In this work, we take a step towards settling this problem by designing a model-free self-play algorithm \emph{Memory-Efficient Nash Q-Learning (ME-Nash-QL)} for two-player zero-sum Markov games, which is a specific setting of MARL. ME-Nash-QL is proven to enjoy the following merits. First, it can output an $\varepsilon$-approximate Nash policy with space complexity $O(SABH)$ and sample complexity $\widetilde{O}(H^4SAB/\varepsilon^2)$, where $S$ is the number of states, $\{A, B\}$ is the number of actions for two players, and $H$ is the horizon length. It outperforms existing algorithms in terms of space complexity for tabular cases, and in terms of sample complexity for long horizons, i.e., when $\min\{A, B\}\ll H^2$. Second, ME-Nash-QL achieves the lowest computational complexity $O(T\mathrm{poly}(AB))$ while preserving Markov policies, where $T$ is the number of samples. Third, ME-Nash-QL also achieves the best burn-in cost $O(SAB\,\mathrm{poly}(H))$, whereas previous algorithms have a burn-in cost of at least $O(S^3 AB\,\mathrm{poly}(H))$ to attain the same level of sample complexity with ours.
<div id='section'>Paperid: <span id='pid'>492, <a href='https://arxiv.org/pdf/2511.17506.pdf' target='_blank'>https://arxiv.org/pdf/2511.17506.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Narjes Nourzad, Mingyu Zong, Bhaskar Krishnamachari
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.17506">AURA: Adaptive Unified Reasoning and Automation with LLM-Guided MARL for NextG Cellular Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Next-generation (NextG) cellular networks are expected to manage dynamic traffic while sustaining high performance. Large language models (LLMs) provide strategic reasoning for 6G planning, but their computational cost and latency limit real-time use. Multi-agent reinforcement learning (MARL) supports localized adaptation, yet coordination at scale remains challenging. We present AURA, a framework that integrates cloud-based LLMs for high-level planning with base stations modeled as MARL agents for local decision-making. The LLM generates objectives and subgoals from its understanding of the environment and reasoning capabilities, while agents at base stations execute these objectives autonomously, guided by a trust mechanism that balances local learning with external input. To reduce latency, AURA employs batched communication so that agents update the LLM's view of the environment and receive improved feedback. In a simulated 6G scenario, AURA improves resilience, reducing dropped handoff requests by more than half under normal and high traffic and lowering system failures. Agents use LLM input in fewer than 60\% of cases, showing that guidance augments rather than replaces local adaptability, thereby mitigating latency and hallucination risks. These results highlight the promise of combining LLM reasoning with MARL adaptability for scalable, real-time NextG network management.
<div id='section'>Paperid: <span id='pid'>493, <a href='https://arxiv.org/pdf/2511.02016.pdf' target='_blank'>https://arxiv.org/pdf/2511.02016.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Patrick Cheridito, Jean-Loup Dupret, Zhexin Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.02016">ABIDES-MARL: A Multi-Agent Reinforcement Learning Environment for Endogenous Price Formation and Execution in a Limit Order Book</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present ABIDES-MARL, a framework that combines a new multi-agent reinforcement learning (MARL) methodology with a new realistic limit-order-book (LOB) simulation system to study equilibrium behavior in complex financial market games. The system extends ABIDES-Gym by decoupling state collection from kernel interruption, enabling synchronized learning and decision-making for multiple adaptive agents while maintaining compatibility with standard RL libraries. It preserves key market features such as price-time priority and discrete tick sizes. Methodologically, we use MARL to approximate equilibrium-like behavior in multi-period trading games with a finite number of heterogeneous agents-an informed trader, a liquidity trader, noise traders, and competing market makers-all with individual price impacts. This setting bridges optimal execution and market microstructure by embedding the liquidity trader's optimization problem within a strategic trading environment. We validate the approach by solving an extended Kyle model within the simulation system, recovering the gradual price discovery phenomenon. We then extend the analysis to a liquidity trader's problem where market liquidity arises endogenously and show that, at equilibrium, execution strategies shape market-maker behavior and price dynamics. ABIDES-MARL provides a reproducible foundation for analyzing equilibrium and strategic adaptation in realistic markets and contributes toward building economically interpretable agentic AI systems for finance.
<div id='section'>Paperid: <span id='pid'>494, <a href='https://arxiv.org/pdf/2510.05048.pdf' target='_blank'>https://arxiv.org/pdf/2510.05048.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ondřej Kubíček, Viliam Lisý
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.05048">Look-ahead Reasoning with a Learned Model in Imperfect Information Games</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Test-time reasoning significantly enhances pre-trained AI agents' performance. However, it requires an explicit environment model, often unavailable or overly complex in real-world scenarios. While MuZero enables effective model learning for search in perfect information games, extending this paradigm to imperfect information games presents substantial challenges due to more nuanced look-ahead reasoning techniques and large number of states relevant for individual decisions. This paper introduces an algorithm LAMIR that learns an abstracted model of an imperfect information game directly from the agent-environment interaction. During test time, this trained model is used to perform look-ahead reasoning. The learned abstraction limits the size of each subgame to a manageable size, making theoretically principled look-ahead reasoning tractable even in games where previous methods could not scale. We empirically demonstrate that with sufficient capacity, LAMIR learns the exact underlying game structure, and with limited capacity, it still learns a valuable abstraction, which improves game playing performance of the pre-trained agents even in large games.
<div id='section'>Paperid: <span id='pid'>495, <a href='https://arxiv.org/pdf/2509.23905.pdf' target='_blank'>https://arxiv.org/pdf/2509.23905.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianjiao Sun, Ningyan Guo, Haozhe Gu, Yanyan Peng, Zhiyong Feng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.23905">Integrated Communication and Control for Energy-Efficient UAV Swarms: A Multi-Agent Reinforcement Learning Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The deployment of unmanned aerial vehicle (UAV) swarm-assisted communication networks has become an increasingly vital approach for remediating coverage limitations in infrastructure-deficient environments, with especially pressing applications in temporary scenarios, such as emergency rescue, military and security operations, and remote area coverage. However, complex geographic environments lead to unpredictable and highly dynamic wireless channel conditions, resulting in frequent interruptions of air-to-ground (A2G) links that severely constrain the reliability and quality of service in UAV swarm-assisted mobile communications. To improve the quality of UAV swarm-assisted communications in complex geographic environments, we propose an integrated communication and control co-design mechanism. Given the stringent energy constraints inherent in UAV swarms, our proposed mechanism is designed to optimize energy efficiency while maintaining an equilibrium between equitable communication rates for mobile ground users (GUs) and UAV energy expenditure. We formulate the joint resource allocation and 3D trajectory control problem as a Markov decision process (MDP), and develop a multi-agent reinforcement learning (MARL) framework to enable real-time coordinated actions across the UAV swarm. To optimize the action policy of UAV swarms, we propose a novel multi-agent hybrid proximal policy optimization with action masking (MAHPPO-AM) algorithm, specifically designed to handle complex hybrid action spaces. The algorithm incorporates action masking to enforce hard constraints in high-dimensional action spaces. Experimental results demonstrate that our approach achieves a fairness index of 0.99 while reducing energy consumption by up to 25% compared to baseline methods.
<div id='section'>Paperid: <span id='pid'>496, <a href='https://arxiv.org/pdf/2509.22707.pdf' target='_blank'>https://arxiv.org/pdf/2509.22707.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinqi Yan, Fang He, Qianlong Sang, Bifeng Tong, Peng Sun, Yili Gong, Chuang Hu, Dazhao Cheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.22707">Metadata-Guided Adaptable Frequency Scaling across Heterogeneous Applications and Devices</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Dynamic Voltage and Frequency Scaling is essential for enhancing energy efficiency in mobile platforms. However, traditional heuristic-based governors are increasingly inadequate for managing the complexity of heterogeneous System-on-Chip designs and diverse application workloads. Although reinforcement learning approaches offer improved performance, their poor generalization capability and reliance on extensive retraining for each hardware and application combination leads to significant deployment costs. In this work, we observe that device and application metadata inherently encapsulate valuable knowledge for DVFS, presenting an opportunity to overcome these limitations. We formulate DVFS for heterogeneous devices and applications as a multi-task reinforcement learning problem. We introduce MetaDVFS, which is a metadata-guided framework that systematically leverages metadata to discover and transfer shared knowledge across DVFS tasks. MetaDVFS can output a set of DVFS models with significant generalization capability for various applications of heterogeneous devices. Evaluations on five Google Pixel devices running six applications show that MetaDVFS achieves up to 17% improvement in Performance-Power Ratio and up to 26% improvement in Quality of Experience. Compared to state-of-the-art methods, MetaDVFS delivers 70.8% faster adaptation and 5.8-27.6% higher performance over standalone device-application specific training, while avoiding negative transfer effects. These results establish MetaDVFS as an effective and scalable solution for DVFS deployment in heterogeneous mobile environments.
<div id='section'>Paperid: <span id='pid'>497, <a href='https://arxiv.org/pdf/2509.09135.pdf' target='_blank'>https://arxiv.org/pdf/2509.09135.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuefeng Wang, Lei Zhang, Henglin Pu, Ahmed H. Qureshi, Husheng Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.09135">Continuous-Time Value Iteration for Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing reinforcement learning (RL) methods struggle with complex dynamical systems that demand interactions at high frequencies or irregular time intervals. Continuous-time RL (CTRL) has emerged as a promising alternative by replacing discrete-time Bellman recursion with differential value functions defined as viscosity solutions of the Hamilton--Jacobi--Bellman (HJB) equation. While CTRL has shown promise, its applications have been largely limited to the single-agent domain. This limitation stems from two key challenges: (i) conventional solution methods for HJB equations suffer from the curse of dimensionality (CoD), making them intractable in high-dimensional systems; and (ii) even with HJB-based learning approaches, accurately approximating centralized value functions in multi-agent settings remains difficult, which in turn destabilizes policy training. In this paper, we propose a CT-MARL framework that uses physics-informed neural networks (PINNs) to approximate HJB-based value functions at scale. To ensure the value is consistent with its differential structure, we align value learning with value-gradient learning by introducing a Value Gradient Iteration (VGI) module that iteratively refines value gradients along trajectories. This improves gradient fidelity, in turn yielding more accurate values and stronger policy learning. We evaluate our method using continuous-time variants of standard benchmarks, including multi-agent particle environment (MPE) and multi-agent MuJoCo. Our results demonstrate that our approach consistently outperforms existing continuous-time RL baselines and scales to complex multi-agent dynamics.
<div id='section'>Paperid: <span id='pid'>498, <a href='https://arxiv.org/pdf/2508.14679.pdf' target='_blank'>https://arxiv.org/pdf/2508.14679.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Parham Soltani, Mehrshad Eskandarpour, Amir Ahmadizad, Hossein Soleimani
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.14679">Energy-Efficient Routing Algorithm for Wireless Sensor Networks: A Multi-Agent Reinforcement Learning Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Efficient energy management is essential in Wireless Sensor Networks (WSNs) to extend network lifetime and ensure reliable data transmission. This paper presents a novel method using reinforcement learning-based cluster-head selection and a hybrid multi-hop routing algorithm, which leverages Q-learning within a multi-agent system to dynamically adapt transmission paths based on the energy distribution across sensor nodes. Each sensor node is modeled as an autonomous agent that observes local state parameters, such as residual energy, distance to sink, hop count, and hotspot proximity, and selects routing actions that maximize long-term energy efficiency. After computing the optimal paths, each sensor aggregates sensed data and forwards it through intermediate nodes to a selected transmitter node, chosen based on the highest remaining State of Charge (SoC), thereby avoiding premature node depletion. To promote efficient learning, a carefully designed reward function incentivizes balanced load distribution, hotspot avoidance, and energy-aware forwarding while maintaining signal quality. The learning process occurs either in a decentralized manner or via a cloud-based controller that offloads computation in large-scale deployments. Moreover, the RL-driven routing decisions are fused with classical graph-based methods, Minimum Energy Routing Algorithm (MERA) and Minimum Spanning Tree (MST), to optimize energy consumption and load balancing. Simulations confirm that the proposed approach significantly improves node survival rate, reduces SoC variance, and enhances network resilience, making it a scalable and adaptive solution for energy-constrained WSNs in dynamic sensor deployments and IoT applications.
<div id='section'>Paperid: <span id='pid'>499, <a href='https://arxiv.org/pdf/2508.14676.pdf' target='_blank'>https://arxiv.org/pdf/2508.14676.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Parham Soltani, Mehrshad Eskandarpour, Sina Heidari, Farnaz Alizadeh, Hossein Soleimani
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.14676">Adaptive Vision-Based Coverage Optimization in Mobile Wireless Sensor Networks: A Multi-Agent Deep Reinforcement Learning Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Traditional Wireless Sensor Networks (WSNs) typically rely on pre-analysis of the target area, network size, and sensor coverage to determine initial deployment. This often results in significant overlap to ensure continued network operation despite sensor energy depletion. With the emergence of Mobile Wireless Sensor Networks (MWSNs), issues such as sensor failure and static coverage limitations can be more effectively addressed through mobility. This paper proposes a novel deployment strategy in which mobile sensors autonomously position themselves to maximize area coverage, eliminating the need for predefined policies. A live camera system, combined with deep reinforcement learning (DRL), monitors the network by detecting sensor LED indicators and evaluating real-time coverage. Rewards based on coverage efficiency and sensor movement are computed at each learning step and shared across the network through a Multi-Agent Reinforcement Learning (MARL) framework, enabling decentralized, cooperative sensor control. Key contributions include a vision-based, low-cost coverage evaluation method; a scalable MARL-DRL framework for autonomous deployment; and a self-reconfigurable system that adjusts sensor positioning in response to energy depletion. Compared to traditional distance-based localization, the proposed method achieves a 26.5% improvement in coverage, a 32% reduction in energy consumption, and a 22% decrease in redundancy, extending network lifetime by 45%. This approach significantly enhances adaptability, energy efficiency, and robustness in MWSNs, offering a practical deployment solution within the IoT framework.
<div id='section'>Paperid: <span id='pid'>500, <a href='https://arxiv.org/pdf/2508.13661.pdf' target='_blank'>https://arxiv.org/pdf/2508.13661.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Maciej Wojtala, Bogusz StefaÅczyk, Dominik Bogucki, Åukasz Lepak, Jakub Strykowski, PaweÅ WawrzyÅski
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.13661">MACTAS: Self-Attention-Based Module for Inter-Agent Communication in Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Communication is essential for the collective execution of complex tasks by human agents, motivating interest in communication mechanisms for multi-agent reinforcement learning (MARL). However, existing communication protocols in MARL are often complex and non-differentiable. In this work, we introduce a self-attention-based communication module that exchanges information between the agents in MARL. Our proposed approach is fully differentiable, allowing agents to learn to generate messages in a reward-driven manner. The module can be seamlessly integrated with any action-value function decomposition method and can be viewed as an extension of such decompositions. Notably, it includes a fixed number of trainable parameters, independent of the number of agents. Experimental results on the SMAC benchmark demonstrate the effectiveness of our approach, which achieves state-of-the-art performance on several maps.
<div id='section'>Paperid: <span id='pid'>501, <a href='https://arxiv.org/pdf/2508.02948.pdf' target='_blank'>https://arxiv.org/pdf/2508.02948.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zain Ulabedeen Farhat, Debamita Ghosh, George K. Atia, Yue Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.02948">Online Robust Multi-Agent Reinforcement Learning under Model Uncertainties</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Well-trained multi-agent systems can fail when deployed in real-world environments due to model mismatches between the training and deployment environments, caused by environment uncertainties including noise or adversarial attacks. Distributionally Robust Markov Games (DRMGs) enhance system resilience by optimizing for worst-case performance over a defined set of environmental uncertainties. However, current methods are limited by their dependence on simulators or large offline datasets, which are often unavailable. This paper pioneers the study of online learning in DRMGs, where agents learn directly from environmental interactions without prior data. We introduce the {\it Robust Optimistic Nash Value Iteration (RONAVI)} algorithm and provide the first provable guarantees for this setting. Our theoretical analysis demonstrates that the algorithm achieves low regret and efficiently finds the optimal robust policy for uncertainty sets measured by Total Variation divergence and Kullback-Leibler divergence. These results establish a new, practical path toward developing truly robust multi-agent systems.
<div id='section'>Paperid: <span id='pid'>502, <a href='https://arxiv.org/pdf/2506.18627.pdf' target='_blank'>https://arxiv.org/pdf/2506.18627.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yannik Mahlau, Maximilian Schier, Christoph Reinders, Frederik Schubert, Marco BÃ¼gling, Bodo Rosenhahn
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.18627">Multi-Agent Reinforcement Learning for Inverse Design in Photonic Integrated Circuits</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Inverse design of photonic integrated circuits (PICs) has traditionally relied on gradientbased optimization. However, this approach is prone to end up in local minima, which results in suboptimal design functionality. As interest in PICs increases due to their potential for addressing modern hardware demands through optical computing, more adaptive optimization algorithms are needed. We present a reinforcement learning (RL) environment as well as multi-agent RL algorithms for the design of PICs. By discretizing the design space into a grid, we formulate the design task as an optimization problem with thousands of binary variables. We consider multiple two- and three-dimensional design tasks that represent PIC components for an optical computing system. By decomposing the design space into thousands of individual agents, our algorithms are able to optimize designs with only a few thousand environment samples. They outperform previous state-of-the-art gradient-based optimization in both twoand three-dimensional design tasks. Our work may also serve as a benchmark for further exploration of sample-efficient RL for inverse design in photonics.
<div id='section'>Paperid: <span id='pid'>503, <a href='https://arxiv.org/pdf/2506.06032.pdf' target='_blank'>https://arxiv.org/pdf/2506.06032.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Edward Hughes, Tina O. Zhu, Martin J. Chadwick, Raphael Koster, Antonio GarcÃ­a CastaÃ±eda, Charles Beattie, Thore Graepel, Matthew M. Botvinick, Joel Z. Leibo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.06032">Modeling human reputation-seeking behavior in a spatio-temporally complex public good provision game</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent reinforcement learning algorithms are useful for simulating social behavior in settings that are too complex for other theoretical approaches like game theory. However, they have not yet been empirically supported by laboratory experiments with real human participants. In this work we demonstrate how multi-agent reinforcement learning can model group behavior in a spatially and temporally complex public good provision game called Clean Up. We show that human groups succeed in Clean Up when they can see who is who and track reputations over time but fail under conditions of anonymity. A new multi-agent reinforcement learning model of reputation-based cooperation demonstrates the same difference between identifiable and anonymous conditions. Furthermore, both human groups and artificial agent groups solve the problem via turn-taking despite other options being available. Our results highlight the benefits of using multi-agent reinforcement learning to model human social behavior in complex environments.
<div id='section'>Paperid: <span id='pid'>504, <a href='https://arxiv.org/pdf/2505.19316.pdf' target='_blank'>https://arxiv.org/pdf/2505.19316.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rex Chen, Stephanie Milani, Zhicheng Zhang, Norman Sadeh, Fei Fang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.19316">Making Teams and Influencing Agents: Efficiently Coordinating Decision Trees for Interpretable Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Poor interpretability hinders the practical applicability of multi-agent reinforcement learning (MARL) policies. Deploying interpretable surrogates of uninterpretable policies enhances the safety and verifiability of MARL for real-world applications. However, if these surrogates are to interact directly with the environment within human supervisory frameworks, they must be both performant and computationally efficient. Prior work on interpretable MARL has either sacrificed performance for computational efficiency or computational efficiency for performance. To address this issue, we propose HYDRAVIPER, a decision tree-based interpretable MARL algorithm. HYDRAVIPER coordinates training between agents based on expected team performance, and adaptively allocates budgets for environment interaction to improve computational efficiency. Experiments on standard benchmark environments for multi-agent coordination and traffic signal control show that HYDRAVIPER matches the performance of state-of-the-art methods using a fraction of the runtime, and that it maintains a Pareto frontier of performance for different interaction budgets.
<div id='section'>Paperid: <span id='pid'>505, <a href='https://arxiv.org/pdf/2505.13543.pdf' target='_blank'>https://arxiv.org/pdf/2505.13543.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Muyang Fan, Songyang Liu, Shuai Li, Weizi Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.13543">Origin-Destination Pattern Effects on Large-Scale Mixed Traffic Control via Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Traffic congestion remains a major challenge for modern urban transportation, diminishing both efficiency and quality of life. While autonomous driving technologies and reinforcement learning (RL) have shown promise for improving traffic control, most prior work has focused on small-scale networks or isolated intersections. Large-scale mixed traffic control, involving both human-driven and robotic vehicles, remains underexplored. In this study, we propose a decentralized multi-agent reinforcement learning framework for managing large-scale mixed traffic networks, where intersections are controlled either by traditional traffic signals or by robotic vehicles. We evaluate our approach on a real-world network of 14 intersections in Colorado Springs, Colorado, USA, using average vehicle waiting time as the primary measure of traffic efficiency. We are exploring a problem that has not been sufficiently addressed: Is large-scale Multi-Agent Traffic Control (MTC) still feasible when facing time-varying Origin-Destination (OD) patterns?
<div id='section'>Paperid: <span id='pid'>506, <a href='https://arxiv.org/pdf/2505.07854.pdf' target='_blank'>https://arxiv.org/pdf/2505.07854.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yufei Lin, Chengwei Ye, Huanzhen Zhang, Kangsheng Wang, Linuo Xu, Shuyan Liu, Zeyu Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.07854">CCL: Collaborative Curriculum Learning for Sparse-Reward Multi-Agent Reinforcement Learning via Co-evolutionary Task Evolution</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Sparse reward environments pose significant challenges in reinforcement learning, especially within multi-agent systems (MAS) where feedback is delayed and shared across agents, leading to suboptimal learning. We propose Collaborative Multi-dimensional Course Learning (CCL), a novel curriculum learning framework that addresses this by (1) refining intermediate tasks for individual agents, (2) using a variational evolutionary algorithm to generate informative subtasks, and (3) co-evolving agents with their environment to enhance training stability. Experiments on five cooperative tasks in the MPE and Hide-and-Seek environments show that CCL outperforms existing methods in sparse reward settings.
<div id='section'>Paperid: <span id='pid'>507, <a href='https://arxiv.org/pdf/2504.11258.pdf' target='_blank'>https://arxiv.org/pdf/2504.11258.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Liam Welsh, Udit Grover, Sebastian Jaimungal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.11258">Multi-Agent Reinforcement Learning for Greenhouse Gas Offset Credit Markets</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Climate change is a major threat to the future of humanity, and its impacts are being intensified by excess man-made greenhouse gas emissions. One method governments can employ to control these emissions is to provide firms with emission limits and penalize any excess emissions above the limit. Excess emissions may also be offset by firms who choose to invest in carbon reducing and capturing projects. These projects generate offset credits which can be submitted to a regulating agency to offset a firm's excess emissions, or they can be traded with other firms. In this work, we characterize the finite-agent Nash equilibrium for offset credit markets. As computing Nash equilibria is an NP-hard problem, we utilize the modern reinforcement learning technique Nash-DQN to efficiently estimate the market's Nash equilibria. We demonstrate not only the validity of employing reinforcement learning methods applied to climate themed financial markets, but also the significant financial savings emitting firms may achieve when abiding by the Nash equilibria through numerical experiments.
<div id='section'>Paperid: <span id='pid'>508, <a href='https://arxiv.org/pdf/2504.04691.pdf' target='_blank'>https://arxiv.org/pdf/2504.04691.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Songyang Liu, Muyang Fan, Weizi Li, Jing Du, Shuai Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.04691">Large-Scale Mixed-Traffic and Intersection Control using Multi-agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Traffic congestion remains a significant challenge in modern urban networks. Autonomous driving technologies have emerged as a potential solution. Among traffic control methods, reinforcement learning has shown superior performance over traffic signals in various scenarios. However, prior research has largely focused on small-scale networks or isolated intersections, leaving large-scale mixed traffic control largely unexplored. This study presents the first attempt to use decentralized multi-agent reinforcement learning for large-scale mixed traffic control in which some intersections are managed by traffic signals and others by robot vehicles. Evaluating a real-world network in Colorado Springs, CO, USA with 14 intersections, we measure traffic efficiency via average waiting time of vehicles at intersections and the number of vehicles reaching their destinations within a time window (i.e., throughput). At 80% RV penetration rate, our method reduces waiting time from 6.17s to 5.09s and increases throughput from 454 vehicles per 500 seconds to 493 vehicles per 500 seconds, outperforming the baseline of fully signalized intersections. These findings suggest that integrating reinforcement learning-based control large-scale traffic can improve overall efficiency and may inform future urban planning strategies.
<div id='section'>Paperid: <span id='pid'>509, <a href='https://arxiv.org/pdf/2503.22162.pdf' target='_blank'>https://arxiv.org/pdf/2503.22162.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ning Liu, Sen Shen, Xiangrui Kong, Hongtao Zhang, Thomas BrÃ¤unl
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.22162">Cooperative Hybrid Multi-Agent Pathfinding Based on Shared Exploration Maps</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-Agent Pathfinding is used in areas including multi-robot formations, warehouse logistics, and intelligent vehicles. However, many environments are incomplete or frequently change, making it difficult for standard centralized planning or pure reinforcement learning to maintain both global solution quality and local flexibility. This paper introduces a hybrid framework that integrates D* Lite global search with multi-agent reinforcement learning, using a switching mechanism and a freeze-prevention strategy to handle dynamic conditions and crowded settings. We evaluate the framework in the discrete POGEMA environment and compare it with baseline methods. Experimental outcomes indicate that the proposed framework substantially improves success rate, collision rate, and path efficiency. The model is further tested on the EyeSim platform, where it maintains feasible Pathfinding under frequent changes and large-scale robot deployments.
<div id='section'>Paperid: <span id='pid'>510, <a href='https://arxiv.org/pdf/2503.17803.pdf' target='_blank'>https://arxiv.org/pdf/2503.17803.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Giovanni Briglia, Stefano Mariani, Franco Zambonelli
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.17803">A Roadmap Towards Improving Multi-Agent Reinforcement Learning With Causal Discovery And Inference</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Causal reasoning is increasingly used in Reinforcement Learning (RL) to improve the learning process in several dimensions: efficacy of learned policies, efficiency of convergence, generalisation capabilities, safety and interpretability of behaviour. However, applications of causal reasoning to Multi-Agent RL (MARL) are still mostly unexplored. In this paper, we take the first step in investigating the opportunities and challenges of applying causal reasoning in MARL. We measure the impact of a simple form of causal augmentation in state-of-the-art MARL scenarios increasingly requiring cooperation, and with state-of-the-art MARL algorithms exploiting various degrees of collaboration between agents. Then, we discuss the positive as well as negative results achieved, giving us the chance to outline the areas where further research may help to successfully transfer causal RL to the multi-agent setting.
<div id='section'>Paperid: <span id='pid'>511, <a href='https://arxiv.org/pdf/2503.02437.pdf' target='_blank'>https://arxiv.org/pdf/2503.02437.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Antonio Marino, Esteban Restrepo, Claudio Pacchierotti, Paolo Robuffo Giordano
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.02437">Decentralized Reinforcement Learning for Multi-Agent Multi-Resource Allocation via Dynamic Cluster Agreements</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper addresses the challenge of allocating heterogeneous resources among multiple agents in a decentralized manner. Our proposed method, LGTC-IPPO, builds upon Independent Proximal Policy Optimization (IPPO) by integrating dynamic cluster consensus, a mechanism that allows agents to form and adapt local sub-teams based on resource demands. This decentralized coordination strategy reduces reliance on global information and enhances scalability. We evaluate LGTC-IPPO against standard multi-agent reinforcement learning baselines and a centralized expert solution across a range of team sizes and resource distributions. Experimental results demonstrate that LGTC-IPPO achieves more stable rewards, better coordination, and robust performance even as the number of agents or resource types increases. Additionally, we illustrate how dynamic clustering enables agents to reallocate resources efficiently also for scenarios with discharging resources.
<div id='section'>Paperid: <span id='pid'>512, <a href='https://arxiv.org/pdf/2502.17046.pdf' target='_blank'>https://arxiv.org/pdf/2502.17046.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinyuan Feng, Min Chen, Zhiqiang Pu, Yifan Xu, Yanyan Liang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.17046">MA2RL: Masked Autoencoders for Generalizable Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To develop generalizable models in multi-agent reinforcement learning, recent approaches have been devoted to discovering task-independent skills for each agent, which generalize across tasks and facilitate agents' cooperation. However, particularly in partially observed settings, such approaches struggle with sample efficiency and generalization capabilities due to two primary challenges: (a) How to incorporate global states into coordinating the skills of different agents? (b) How to learn generalizable and consistent skill semantics when each agent only receives partial observations? To address these challenges, we propose a framework called \textbf{M}asked \textbf{A}utoencoders for \textbf{M}ulti-\textbf{A}gent \textbf{R}einforcement \textbf{L}earning (MA2RL), which encourages agents to infer unobserved entities by reconstructing entity-states from the entity perspective. The entity perspective helps MA2RL generalize to diverse tasks with varying agent numbers and action spaces. Specifically, we treat local entity-observations as masked contexts of the global entity-states, and MA2RL can infer the latent representation of dynamically masked entities, facilitating the assignment of task-independent skills and the learning of skill semantics. Extensive experiments demonstrate that MA2RL achieves significant improvements relative to state-of-the-art approaches, demonstrating extraordinary performance, remarkable zero-shot generalization capabilities and advantageous transferability.
<div id='section'>Paperid: <span id='pid'>513, <a href='https://arxiv.org/pdf/2502.16863.pdf' target='_blank'>https://arxiv.org/pdf/2502.16863.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kartik Nagpal, Dayi Dong, Jean-Baptiste Bouvier, Negar Mehr
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.16863">Leveraging Large Language Models for Effective and Explainable Multi-Agent Credit Assignment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent work, spanning from autonomous vehicle coordination to in-space assembly, has shown the importance of learning collaborative behavior for enabling robots to achieve shared goals. A common approach for learning this cooperative behavior is to utilize the centralized-training decentralized-execution paradigm. However, this approach also introduces a new challenge: how do we evaluate the contributions of each agent's actions to the overall success or failure of the team. This credit assignment problem has remained open, and has been extensively studied in the Multi-Agent Reinforcement Learning literature. In fact, humans manually inspecting agent behavior often generate better credit evaluations than existing methods. We combine this observation with recent works which show Large Language Models demonstrate human-level performance at many pattern recognition tasks. Our key idea is to reformulate credit assignment to the two pattern recognition problems of sequence improvement and attribution, which motivates our novel LLM-MCA method. Our approach utilizes a centralized LLM reward-critic which numerically decomposes the environment reward based on the individualized contribution of each agent in the scenario. We then update the agents' policy networks based on this feedback. We also propose an extension LLM-TACA where our LLM critic performs explicit task assignment by passing an intermediary goal directly to each agent policy in the scenario. Both our methods far outperform the state-of-the-art on a variety of benchmarks, including Level-Based Foraging, Robotic Warehouse, and our new Spaceworld benchmark which incorporates collision-related safety constraints. As an artifact of our methods, we generate large trajectory datasets with each timestep annotated with per-agent reward information, as sampled from our LLM critics.
<div id='section'>Paperid: <span id='pid'>514, <a href='https://arxiv.org/pdf/2502.14200.pdf' target='_blank'>https://arxiv.org/pdf/2502.14200.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao Ma, Zhiqiang Pu, Yi Pan, Boyin Liu, Junlong Gao, Zhenyu Guo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.14200">Causal Mean Field Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Scalability remains a challenge in multi-agent reinforcement learning and is currently under active research. A framework named mean-field reinforcement learning (MFRL) could alleviate the scalability problem by employing the Mean Field Theory to turn a many-agent problem into a two-agent problem. However, this framework lacks the ability to identify essential interactions under nonstationary environments. Causality contains relatively invariant mechanisms behind interactions, though environments are nonstationary. Therefore, we propose an algorithm called causal mean-field Q-learning (CMFQ) to address the scalability problem. CMFQ is ever more robust toward the change of the number of agents though inheriting the compressed representation of MFRL's action-state space. Firstly, we model the causality behind the decision-making process of MFRL into a structural causal model (SCM). Then the essential degree of each interaction is quantified via intervening on the SCM. Furthermore, we design the causality-aware compact representation for behavioral information of agents as the weighted sum of all behavioral information according to their causal effects. We test CMFQ in a mixed cooperative-competitive game and a cooperative game. The result shows that our method has excellent scalability performance in both training in environments containing a large number of agents and testing in environments containing much more agents.
<div id='section'>Paperid: <span id='pid'>515, <a href='https://arxiv.org/pdf/2501.16138.pdf' target='_blank'>https://arxiv.org/pdf/2501.16138.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Richard Willis, Yali Du, Joel Z Leibo, Michael Luck
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.16138">Quantifying the Self-Interest Level of Markov Social Dilemmas</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces a novel method for estimating the self-interest level of Markov social dilemmas. We extend the concept of self-interest level from normal-form games to Markov games, providing a quantitative measure of the minimum reward exchange required to align individual and collective interests. We demonstrate our method on three environments from the Melting Pot suite, representing either common-pool resources or public goods. Our results illustrate how reward exchange can enable agents to transition from selfish to collective equilibria in a Markov social dilemma. This work contributes to multi-agent reinforcement learning by providing a practical tool for analysing complex, multistep social dilemmas. Our findings offer insights into how reward structures can promote or hinder cooperation, with potential applications in areas such as mechanism design.
<div id='section'>Paperid: <span id='pid'>516, <a href='https://arxiv.org/pdf/2501.12061.pdf' target='_blank'>https://arxiv.org/pdf/2501.12061.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Somnath Hazra, Pallab Dasgupta, Soumyajit Dey
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.12061">Tackling Uncertainties in Multi-Agent Reinforcement Learning through Integration of Agent Termination Dynamics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-Agent Reinforcement Learning (MARL) has gained significant traction for solving complex real-world tasks, but the inherent stochasticity and uncertainty in these environments pose substantial challenges to efficient and robust policy learning. While Distributional Reinforcement Learning has been successfully applied in single-agent settings to address risk and uncertainty, its application in MARL is substantially limited. In this work, we propose a novel approach that integrates distributional learning with a safety-focused loss function to improve convergence in cooperative MARL tasks. Specifically, we introduce a Barrier Function based loss that leverages safety metrics, identified from inherent faults in the system, into the policy learning process. This additional loss term helps mitigate risks and encourages safer exploration during the early stages of training. We evaluate our method in the StarCraft II micromanagement benchmark, where our approach demonstrates improved convergence and outperforms state-of-the-art baselines in terms of both safety and task completion. Our results suggest that incorporating safety considerations can significantly enhance learning performance in complex, multi-agent environments.
<div id='section'>Paperid: <span id='pid'>517, <a href='https://arxiv.org/pdf/2501.05329.pdf' target='_blank'>https://arxiv.org/pdf/2501.05329.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dmytro Kuzmenko, Nadiya Shvai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.05329">Knowledge Transfer in Model-Based Reinforcement Learning Agents for Efficient Multi-Task Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose an efficient knowledge transfer approach for model-based reinforcement learning, addressing the challenge of deploying large world models in resource-constrained environments. Our method distills a high-capacity multi-task agent (317M parameters) into a compact 1M parameter model, achieving state-of-the-art performance on the MT30 benchmark with a normalized score of 28.45, a substantial improvement over the original 1M parameter model's score of 18.93. This demonstrates the ability of our distillation technique to consolidate complex multi-task knowledge effectively. Additionally, we apply FP16 post-training quantization, reducing the model size by 50% while maintaining performance. Our work bridges the gap between the power of large models and practical deployment constraints, offering a scalable solution for efficient and accessible multi-task reinforcement learning in robotics and other resource-limited domains.
<div id='section'>Paperid: <span id='pid'>518, <a href='https://arxiv.org/pdf/2501.02221.pdf' target='_blank'>https://arxiv.org/pdf/2501.02221.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kanefumi Matsuyama, Kefan Su, Jiangxing Wang, Deheng Ye, Zongqing Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.02221">CORD: Generalizable Cooperation via Role Diversity</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cooperative multi-agent reinforcement learning (MARL) aims to develop agents that can collaborate effectively. However, most cooperative MARL methods overfit training agents, making learned policies not generalize well to unseen collaborators, which is a critical issue for real-world deployment. Some methods attempt to address the generalization problem but require prior knowledge or predefined policies of new teammates, limiting real-world applications. To this end, we propose a hierarchical MARL approach to enable generalizable cooperation via role diversity, namely CORD. CORD's high-level controller assigns roles to low-level agents by maximizing the role entropy with constraints. We show this constrained objective can be decomposed into causal influence in role that enables reasonable role assignment, and role heterogeneity that yields coherent, non-redundant role clusters. Evaluated on a variety of cooperative multi-agent tasks, CORD achieves better performance than baselines, especially in generalization tests. Ablation studies further demonstrate the efficacy of the constrained objective in generalizable cooperation.
<div id='section'>Paperid: <span id='pid'>519, <a href='https://arxiv.org/pdf/2501.01136.pdf' target='_blank'>https://arxiv.org/pdf/2501.01136.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nikolaos Bousias, Stefanos Pertigkiozoglou, Kostas Daniilidis, George Pappas
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.01136">Symmetries-enhanced Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent reinforcement learning has emerged as a powerful framework for enabling agents to learn complex, coordinated behaviors but faces persistent challenges regarding its generalization, scalability and sample efficiency. Recent advancements have sought to alleviate those issues by embedding intrinsic symmetries of the systems in the policy. Yet, most dynamical systems exhibit little to no symmetries to exploit. This paper presents a novel framework for embedding extrinsic symmetries in multi-agent system dynamics that enables the use of symmetry-enhanced methods to address systems with insufficient intrinsic symmetries, expanding the scope of equivariant learning to a wide variety of MARL problems. Central to our framework is the Group Equivariant Graphormer, a group-modular architecture specifically designed for distributed swarming tasks. Extensive experiments on a swarm of symmetry-breaking quadrotors validate the effectiveness of our approach, showcasing its potential for improved generalization and zero-shot scalability. Our method achieves significant reductions in collision rates and enhances task success rates across a diverse range of scenarios and varying swarm sizes.
<div id='section'>Paperid: <span id='pid'>520, <a href='https://arxiv.org/pdf/2412.07639.pdf' target='_blank'>https://arxiv.org/pdf/2412.07639.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zongkai Liu, Qian Lin, Chao Yu, Xiawei Wu, Yile Liang, Donghui Li, Xuetao Ding
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.07639">Offline Multi-Agent Reinforcement Learning via In-Sample Sequential Policy Optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Offline Multi-Agent Reinforcement Learning (MARL) is an emerging field that aims to learn optimal multi-agent policies from pre-collected datasets. Compared to single-agent case, multi-agent setting involves a large joint state-action space and coupled behaviors of multiple agents, which bring extra complexity to offline policy optimization. In this work, we revisit the existing offline MARL methods and show that in certain scenarios they can be problematic, leading to uncoordinated behaviors and out-of-distribution (OOD) joint actions. To address these issues, we propose a new offline MARL algorithm, named In-Sample Sequential Policy Optimization (InSPO). InSPO sequentially updates each agent's policy in an in-sample manner, which not only avoids selecting OOD joint actions but also carefully considers teammates' updated policies to enhance coordination. Additionally, by thoroughly exploring low-probability actions in the behavior policy, InSPO can well address the issue of premature convergence to sub-optimal solutions. Theoretically, we prove InSPO guarantees monotonic policy improvement and converges to quantal response equilibrium (QRE). Experimental results demonstrate the effectiveness of our method compared to current state-of-the-art offline MARL methods.
<div id='section'>Paperid: <span id='pid'>521, <a href='https://arxiv.org/pdf/2410.01364.pdf' target='_blank'>https://arxiv.org/pdf/2410.01364.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yutian Zhang, Guohong Zheng, Zhiyuan Liu, Quan Li, Haipeng Zeng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.01364">MARLens: Understanding Multi-agent Reinforcement Learning for Traffic Signal Control via Visual Analytics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The issue of traffic congestion poses a significant obstacle to the development of global cities. One promising solution to tackle this problem is intelligent traffic signal control (TSC). Recently, TSC strategies leveraging reinforcement learning (RL) have garnered attention among researchers. However, the evaluation of these models has primarily relied on fixed metrics like reward and queue length. This limited evaluation approach provides only a narrow view of the model's decision-making process, impeding its practical implementation. Moreover, effective TSC necessitates coordinated actions across multiple intersections. Existing visual analysis solutions fall short when applied in multi-agent settings. In this study, we delve into the challenge of interpretability in multi-agent reinforcement learning (MARL), particularly within the context of TSC. We propose MARLens a visual analytics system tailored to understand MARL-based TSC. Our system serves as a versatile platform for both RL and TSC researchers. It empowers them to explore the model's features from various perspectives, revealing its decision-making processes and shedding light on interactions among different agents. To facilitate quick identification of critical states, we have devised multiple visualization views, complemented by a traffic simulation module that allows users to replay specific training scenarios. To validate the utility of our proposed system, we present three comprehensive case studies, incorporate insights from domain experts through interviews, and conduct a user study. These collective efforts underscore the feasibility and effectiveness of MARLens in enhancing our understanding of MARL-based TSC systems and pave the way for more informed and efficient traffic management strategies.
<div id='section'>Paperid: <span id='pid'>522, <a href='https://arxiv.org/pdf/2601.19778.pdf' target='_blank'>https://arxiv.org/pdf/2601.19778.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ahmad Farooq, Kamran Iqbal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.19778">Reimagining Peer Review Process Through Multi-Agent Mechanism Design</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The software engineering research community faces a systemic crisis: peer review is failing under growing submissions, misaligned incentives, and reviewer fatigue. Community surveys reveal that researchers perceive the process as "broken." This position paper argues that these dysfunctions are mechanism design failures amenable to computational solutions. We propose modeling the research community as a stochastic multi-agent system and applying multi-agent reinforcement learning to design incentive-compatible protocols. We outline three interventions: a credit-based submission economy, MARL-optimized reviewer assignment, and hybrid verification of review consistency. We present threat models, equity considerations, and phased pilot metrics. This vision charts a research agenda toward sustainable peer review.
<div id='section'>Paperid: <span id='pid'>523, <a href='https://arxiv.org/pdf/2512.04918.pdf' target='_blank'>https://arxiv.org/pdf/2512.04918.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kailiang Liu, Ying Chen, Ralf Borndörfer, Thorsten Koch
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.04918">Multi-Agent Reinforcement Learning for Intraday Operating Rooms Scheduling under Uncertainty</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Intraday surgical scheduling is a multi-objective decision problem under uncertainty-balancing elective throughput, urgent and emergency demand, delays, sequence-dependent setups, and overtime. We formulate the problem as a cooperative Markov game and propose a multi-agent reinforcement learning (MARL) framework in which each operating room (OR) is an agent trained with centralized training and decentralized execution. All agents share a policy trained via Proximal Policy Optimization (PPO), which maps rich system states to actions, while a within-epoch sequential assignment protocol constructs conflict-free joint schedules across ORs. A mixed-integer pre-schedule provides reference starting times for electives; we impose type-specific quadratic delay penalties relative to these references and a terminal overtime penalty, yielding a single reward that captures throughput, timeliness, and staff workload. In simulations reflecting a realistic hospital mix (six ORs, eight surgery types, random urgent and emergency arrivals), the learned policy outperforms six rule-based heuristics across seven metrics and three evaluation subsets, and, relative to an ex post MIP oracle, quantifies optimality gaps. Policy analytics reveal interpretable behavior-prioritizing emergencies, batching similar cases to reduce setups, and deferring lower-value electives. We also derive a suboptimality bound for the sequential decomposition under simplifying assumptions. We discuss limitations-including OR homogeneity and the omission of explicit staffing constraints-and outline extensions. Overall, the approach offers a practical, interpretable, and tunable data-driven complement to optimization for real-time OR scheduling.
<div id='section'>Paperid: <span id='pid'>524, <a href='https://arxiv.org/pdf/2512.02406.pdf' target='_blank'>https://arxiv.org/pdf/2512.02406.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Oshada Jayasinghe, Farhana Choudhury, Egemen Tanin, Shanika Karunasekera
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.02406">Dynamic Configuration of On-Street Parking Spaces using Multi Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With increased travelling needs more than ever, traffic congestion has become a major concern in most urban areas. Allocating spaces for on-street parking, further hinders traffic flow, by limiting the effective road width available for driving. With the advancement of vehicle-to-infrastructure connectivity technologies, we explore how the impact of on-street parking on traffic congestion could be minimized, by dynamically configuring on-street parking spaces. Towards that end, we formulate dynamic on-street parking space configuration as an optimization problem, and we follow a data driven approach, considering the nature of our problem. Our proposed solution comprises a two-layer multi agent reinforcement learning based framework, which is inherently scalable to large road networks. The lane level agents are responsible for deciding the optimal parking space configuration for each lane, and we introduce a novel Deep Q-learning architecture which effectively utilizes long short term memory networks and graph attention networks to capture the spatio-temporal correlations evident in the given problem. The block level agents control the actions of the lane level agents and maintain a sufficient level of parking around the block. We conduct a set of comprehensive experiments using SUMO, on both synthetic data as well as real-world data from the city of Melbourne. Our experiments show that the proposed framework could reduce the average travel time loss of vehicles significantly, reaching upto 47%, with a negligible increase in the walking distance for parking.
<div id='section'>Paperid: <span id='pid'>525, <a href='https://arxiv.org/pdf/2511.23148.pdf' target='_blank'>https://arxiv.org/pdf/2511.23148.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mian Ibad Ali Shah, Marcos Eduardo Cruz Victorio, Maeve Duffy, Enda Barrett, Karl Mason
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.23148">Peer-to-Peer Energy Trading in Dairy Farms using Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The integration of renewable energy resources in rural areas, such as dairy farming communities, enables decentralized energy management through Peer-to-Peer (P2P) energy trading. This research highlights the role of P2P trading in efficient energy distribution and its synergy with advanced optimization techniques. While traditional rule-based methods perform well under stable conditions, they struggle in dynamic environments. To address this, Multi-Agent Reinforcement Learning (MARL), specifically Proximal Policy Optimization (PPO) and Deep Q-Networks (DQN), is combined with community/distributed P2P trading mechanisms. By incorporating auction-based market clearing, a price advisor agent, and load and battery management, the approach achieves significant improvements. Results show that, compared to baseline models, DQN reduces electricity costs by 14.2% in Ireland and 5.16% in Finland, while increasing electricity revenue by 7.24% and 12.73%, respectively. PPO achieves the lowest peak hour demand, reducing it by 55.5% in Ireland, while DQN reduces peak hour demand by 50.0% in Ireland and 27.02% in Finland. These improvements are attributed to both MARL algorithms and P2P energy trading, which together results in electricity cost and peak hour demand reduction, and increase electricity selling revenue. This study highlights the complementary strengths of DQN, PPO, and P2P trading in achieving efficient, adaptable, and sustainable energy management in rural communities.
<div id='section'>Paperid: <span id='pid'>526, <a href='https://arxiv.org/pdf/2511.15053.pdf' target='_blank'>https://arxiv.org/pdf/2511.15053.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pengcheng Dai, He Wang, Dongming Wang, Wenwu Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.15053">Distributed primal-dual algorithm for constrained multi-agent reinforcement learning under coupled policies</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this work, we investigate constrained multi-agent reinforcement learning (CMARL), where agents collaboratively maximize the sum of their local objectives while satisfying individual safety constraints. We propose a framework where agents adopt coupled policies that depend on both local states and parameters, as well as those of their $κ_p$-hop neighbors, with $κ_p>0$ denoting the coupling distance. A distributed primal-dual algorithm is further developed under this framework, wherein each agent has access only to state-action pairs within its $2κ_p$-hop neighborhood and to reward information within its $κ+ 2κ_p$-hop neighborhood, with $κ> 0$ representing the truncation distance. Moreover, agents are not permitted to directly share their true policy parameters or Lagrange multipliers. Instead, each agent constructs and maintains local estimates of these variables for other agents and employs such estimates to execute its policy. Additionally, these estimates are further updated and exchanged exclusively through an independent, time-varying networks, which enhances the overall system security. We establish that, with high probability, our algorithm can achieve an $ε$-first-order stationary convergence with an approximation error of $\mathcal{O}(γ^{\frac{κ+1}{κ_{p}}})$ for discount factor $γ\in(0,1)$. Finally, simulations in GridWorld environment are conducted to demonstrate the effectiveness of the proposed algorithm.
<div id='section'>Paperid: <span id='pid'>527, <a href='https://arxiv.org/pdf/2510.25929.pdf' target='_blank'>https://arxiv.org/pdf/2510.25929.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziyi Wang, Carmine Ventre, Maria Polukarov
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.25929">Multi-Agent Reinforcement Learning for Market Making: Competition without Collusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Algorithmic collusion has emerged as a central question in AI: Will the interaction between different AI agents deployed in markets lead to collusion? More generally, understanding how emergent behavior, be it a cartel or market dominance from more advanced bots, affects the market overall is an important research question. We propose a hierarchical multi-agent reinforcement learning framework to study algorithmic collusion in market making. The framework includes a self-interested market maker (Agent~A), which is trained in an uncertain environment shaped by an adversary, and three bottom-layer competitors: the self-interested Agent~B1 (whose objective is to maximize its own PnL), the competitive Agent~B2 (whose objective is to minimize the PnL of its opponent), and the hybrid Agent~B$^\star$, which can modulate between the behavior of the other two. To analyze how these agents shape the behavior of each other and affect market outcomes, we propose interaction-level metrics that quantify behavioral asymmetry and system-level dynamics, while providing signals potentially indicative of emergent interaction patterns. Experimental results show that Agent~B2 secures dominant performance in a zero-sum setting against B1, aggressively capturing order flow while tightening average spreads, thus improving market execution efficiency. In contrast, Agent~B$^\star$ exhibits a self-interested inclination when co-existing with other profit-seeking agents, securing dominant market share through adaptive quoting, yet exerting a milder adverse impact on the rewards of Agents~A and B1 compared to B2. These findings suggest that adaptive incentive control supports more sustainable strategic co-existence in heterogeneous agent environments and offers a structured lens for evaluating behavioral design in algorithmic trading systems.
<div id='section'>Paperid: <span id='pid'>528, <a href='https://arxiv.org/pdf/2510.13343.pdf' target='_blank'>https://arxiv.org/pdf/2510.13343.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shota Takayama, Katsuhide Fujita
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.13343">AOAD-MAT: Transformer-based multi-agent deep reinforcement learning model considering agents' order of action decisions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent reinforcement learning focuses on training the behaviors of multiple learning agents that coexist in a shared environment. Recently, MARL models, such as the Multi-Agent Transformer (MAT) and ACtion dEpendent deep Q-learning (ACE), have significantly improved performance by leveraging sequential decision-making processes. Although these models can enhance performance, they do not explicitly consider the importance of the order in which agents make decisions. In this paper, we propose an Agent Order of Action Decisions-MAT (AOAD-MAT), a novel MAT model that considers the order in which agents make decisions. The proposed model explicitly incorporates the sequence of action decisions into the learning process, allowing the model to learn and predict the optimal order of agent actions. The AOAD-MAT model leverages a Transformer-based actor-critic architecture that dynamically adjusts the sequence of agent actions. To achieve this, we introduce a novel MARL architecture that cooperates with a subtask focused on predicting the next agent to act, integrated into a Proximal Policy Optimization based loss function to synergistically maximize the advantage of the sequential decision-making. The proposed method was validated through extensive experiments on the StarCraft Multi-Agent Challenge and Multi-Agent MuJoCo benchmarks. The experimental results show that the proposed AOAD-MAT model outperforms existing MAT and other baseline models, demonstrating the effectiveness of adjusting the AOAD order in MARL.
<div id='section'>Paperid: <span id='pid'>529, <a href='https://arxiv.org/pdf/2510.12272.pdf' target='_blank'>https://arxiv.org/pdf/2510.12272.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Federico Gabriele, Aldo Glielmo, Marco Taboga
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.12272">Heterogeneous RBCs via deep multi-agent reinforcement learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current macroeconomic models with agent heterogeneity can be broadly divided into two main groups. Heterogeneous-agent general equilibrium (GE) models, such as those based on Heterogeneous Agents New Keynesian (HANK) or Krusell-Smith (KS) approaches, rely on GE and 'rational expectations', somewhat unrealistic assumptions that make the models very computationally cumbersome, which in turn limits the amount of heterogeneity that can be modelled. In contrast, agent-based models (ABMs) can flexibly encompass a large number of arbitrarily heterogeneous agents, but typically require the specification of explicit behavioural rules, which can lead to a lengthy trial-and-error model-development process. To address these limitations, we introduce MARL-BC, a framework that integrates deep multi-agent reinforcement learning (MARL) with Real Business Cycle (RBC) models. We demonstrate that MARL-BC can: (1) recover textbook RBC results when using a single agent; (2) recover the results of the mean-field KS model using a large number of identical agents; and (3) effectively simulate rich heterogeneity among agents, a hard task for traditional GE approaches. Our framework can be thought of as an ABM if used with a variety of heterogeneous interacting agents, and can reproduce GE results in limit cases. As such, it is a step towards a synthesis of these often opposed modelling paradigms.
<div id='section'>Paperid: <span id='pid'>530, <a href='https://arxiv.org/pdf/2510.03823.pdf' target='_blank'>https://arxiv.org/pdf/2510.03823.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Adam Haroon, Tristan Schuler
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.03823">Distributed Area Coverage with High Altitude Balloons Using Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>High Altitude Balloons (HABs) can leverage stratospheric wind layers for limited horizontal control, enabling applications in reconnaissance, environmental monitoring, and communications networks. Existing multi-agent HAB coordination approaches use deterministic methods like Voronoi partitioning and extremum seeking control for large global constellations, which perform poorly for smaller teams and localized missions. While single-agent HAB control using reinforcement learning has been demonstrated on HABs, coordinated multi-agent reinforcement learning (MARL) has not yet been investigated. This work presents the first systematic application of multi-agent reinforcement learning (MARL) to HAB coordination for distributed area coverage. We extend our previously developed reinforcement learning simulation environment (RLHAB) to support cooperative multi-agent learning, enabling multiple agents to operate simultaneously in realistic atmospheric conditions. We adapt QMIX for HAB area coverage coordination, leveraging Centralized Training with Decentralized Execution to address atmospheric vehicle coordination challenges. Our approach employs specialized observation spaces providing individual state, environmental context, and teammate data, with hierarchical rewards prioritizing coverage while encouraging spatial distribution. We demonstrate that QMIX achieves similar performance to the theoretically optimal geometric deterministic method for distributed area coverage, validating the MARL approach and providing a foundation for more complex autonomous multi-HAB missions where deterministic methods become intractable.
<div id='section'>Paperid: <span id='pid'>531, <a href='https://arxiv.org/pdf/2510.00274.pdf' target='_blank'>https://arxiv.org/pdf/2510.00274.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Maisha Maliha, Dean Hougen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.00274">MAGIC-MASK: Multi-Agent Guided Inter-Agent Collaboration with Mask-Based Explainability for Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding the decision-making process of Deep Reinforcement Learning agents remains a key challenge for deploying these systems in safety-critical and multi-agent environments. While prior explainability methods like StateMask, have advanced the identification of critical states, they remain limited by computational cost, exploration coverage, and lack of adaptation to multi-agent settings. To overcome these limitations, we propose a mathematically grounded framework, MAGIC-MASK (Multi-Agent Guided Inter-agent Collaboration with Mask-Based Explainability for Reinforcement Learning), that extends perturbation-based explanation to Multi-Agent Reinforcement Learning. Our method integrates Proximal Policy Optimization, adaptive epsilon-greedy exploration, and lightweight inter-agent collaboration to share masked state information and peer experience. This collaboration enables each agent to perform saliency-guided masking and share reward-based insights with peers, reducing the time required for critical state discovery, improving explanation fidelity, and leading to faster and more robust learning. The core novelty of our approach lies in generalizing explainability from single-agent to multi-agent systems through a unified mathematical formalism built on trajectory perturbation, reward fidelity analysis, and Kullback-Leibler divergence regularization. This framework yields localized, interpretable explanations grounded in probabilistic modeling and multi-agent Markov decision processes. We validate our framework on both single-agent and multi-agent benchmarks, including a multi-agent highway driving environment and Google Research Football, demonstrating that MAGIC-MASK consistently outperforms state-of-the-art baselines in fidelity, learning efficiency, and policy robustness while offering interpretable and transferable explanations.
<div id='section'>Paperid: <span id='pid'>532, <a href='https://arxiv.org/pdf/2509.19512.pdf' target='_blank'>https://arxiv.org/pdf/2509.19512.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Charles Dansereau, Junior-Samuel Lopez-Yepez, Karthik Soma, Antoine Fagette
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.19512">The Heterogeneous Multi-Agent Challenge</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-Agent Reinforcement Learning (MARL) is a growing research area which gained significant traction in recent years, extending Deep RL applications to a much wider range of problems. A particularly challenging class of problems in this domain is Heterogeneous Multi-Agent Reinforcement Learning (HeMARL), where agents with different sensors, resources, or capabilities must cooperate based on local information. The large number of real-world situations involving heterogeneous agents makes it an attractive research area, yet underexplored, as most MARL research focuses on homogeneous agents (e.g., a swarm of identical robots). In MARL and single-agent RL, standardized environments such as ALE and SMAC have allowed to establish recognized benchmarks to measure progress. However, there is a clear lack of such standardized testbed for cooperative HeMARL. As a result, new research in this field often uses simple environments, where most algorithms perform near optimally, or uses weakly heterogeneous MARL environments.
<div id='section'>Paperid: <span id='pid'>533, <a href='https://arxiv.org/pdf/2509.15519.pdf' target='_blank'>https://arxiv.org/pdf/2509.15519.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chao Li, Bingkun Bao, Yang Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.15519">Fully Decentralized Cooperative Multi-Agent Reinforcement Learning is A Context Modeling Problem</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper studies fully decentralized cooperative multi-agent reinforcement learning, where each agent solely observes the states, its local actions, and the shared rewards. The inability to access other agents' actions often leads to non-stationarity during value function updates and relative overgeneralization during value function estimation, hindering effective cooperative policy learning. However, existing works fail to address both issues simultaneously, due to their inability to model the joint policy of other agents in a fully decentralized setting. To overcome this limitation, we propose a novel method named Dynamics-Aware Context (DAC), which formalizes the task, as locally perceived by each agent, as an Contextual Markov Decision Process, and further addresses both non-stationarity and relative overgeneralization through dynamics-aware context modeling. Specifically, DAC attributes the non-stationary local task dynamics of each agent to switches between unobserved contexts, each corresponding to a distinct joint policy. Then, DAC models the step-wise dynamics distribution using latent variables and refers to them as contexts. For each agent, DAC introduces a context-based value function to address the non-stationarity issue during value function update. For value function estimation, an optimistic marginal value is derived to promote the selection of cooperative actions, thereby addressing the relative overgeneralization issue. Experimentally, we evaluate DAC on various cooperative tasks (including matrix game, predator and prey, and SMAC), and its superior performance against multiple baselines validates its effectiveness.
<div id='section'>Paperid: <span id='pid'>534, <a href='https://arxiv.org/pdf/2509.12927.pdf' target='_blank'>https://arxiv.org/pdf/2509.12927.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xingxing Hong, Yungong Wang, Dexin Jin, Ye Yuan, Ximing Huang, Zijian Wu, Wenxin Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.12927">HLSMAC: A New StarCraft Multi-Agent Challenge for High-Level Strategic Decision-Making</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Benchmarks are crucial for assessing multi-agent reinforcement learning (MARL) algorithms. While StarCraft II-related environments have driven significant advances in MARL, existing benchmarks like SMAC focus primarily on micromanagement, limiting comprehensive evaluation of high-level strategic intelligence. To address this, we introduce HLSMAC, a new cooperative MARL benchmark with 12 carefully designed StarCraft II scenarios based on classical stratagems from the Thirty-Six Stratagems. Each scenario corresponds to a specific stratagem and is designed to challenge agents with diverse strategic elements, including tactical maneuvering, timing coordination, and deception, thereby opening up avenues for evaluating high-level strategic decision-making capabilities. We also propose novel metrics across multiple dimensions beyond conventional win rate, such as ability utilization and advancement efficiency, to assess agents' overall performance within the HLSMAC environment. We integrate state-of-the-art MARL algorithms and LLM-based agents with our benchmark and conduct comprehensive experiments. The results demonstrate that HLSMAC serves as a robust testbed for advancing multi-agent strategic decision-making.
<div id='section'>Paperid: <span id='pid'>535, <a href='https://arxiv.org/pdf/2509.12048.pdf' target='_blank'>https://arxiv.org/pdf/2509.12048.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hoon Sagong, Heesu Kim, Hanbeen Hong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.12048">Hi-DARTS: Hierarchical Dynamically Adapting Reinforcement Trading System</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Conventional autonomous trading systems struggle to balance computational efficiency and market responsiveness due to their fixed operating frequency. We propose Hi-DARTS, a hierarchical multi-agent reinforcement learning framework that addresses this trade-off. Hi-DARTS utilizes a meta-agent to analyze market volatility and dynamically activate specialized Time Frame Agents for high-frequency or low-frequency trading as needed. During back-testing on AAPL stock from January 2024 to May 2025, Hi-DARTS yielded a cumulative return of 25.17% with a Sharpe Ratio of 0.75. This performance surpasses standard benchmarks, including a passive buy-and-hold strategy on AAPL (12.19% return) and the S&P 500 ETF (SPY) (20.01% return). Our work demonstrates that dynamic, hierarchical agents can achieve superior risk-adjusted returns while maintaining high computational efficiency.
<div id='section'>Paperid: <span id='pid'>536, <a href='https://arxiv.org/pdf/2509.10163.pdf' target='_blank'>https://arxiv.org/pdf/2509.10163.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Francisco Javier Esono Nkulu Andong, Qi Min
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.10163">Federated Multi-Agent Reinforcement Learning for Privacy-Preserving and Energy-Aware Resource Management in 6G Edge Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As sixth-generation (6G) networks move toward ultra-dense, intelligent edge environments, efficient resource management under stringent privacy, mobility, and energy constraints becomes critical. This paper introduces a novel Federated Multi-Agent Reinforcement Learning (Fed-MARL) framework that incorporates cross-layer orchestration of both the MAC layer and application layer for energy-efficient, privacy-preserving, and real-time resource management across heterogeneous edge devices. Each agent uses a Deep Recurrent Q-Network (DRQN) to learn decentralized policies for task offloading, spectrum access, and CPU energy adaptation based on local observations (e.g., queue length, energy, CPU usage, and mobility). To protect privacy, we introduce a secure aggregation protocol based on elliptic curve Diffie Hellman key exchange, which ensures accurate model updates without exposing raw data to semi-honest adversaries. We formulate the resource management problem as a partially observable multi-agent Markov decision process (POMMDP) with a multi-objective reward function that jointly optimizes latency, energy efficiency, spectral efficiency, fairness, and reliability under 6G-specific service requirements such as URLLC, eMBB, and mMTC. Simulation results demonstrate that Fed-MARL outperforms centralized MARL and heuristic baselines in task success rate, latency, energy efficiency, and fairness, while ensuring robust privacy protection and scalability in dynamic, resource-constrained 6G edge networks.
<div id='section'>Paperid: <span id='pid'>537, <a href='https://arxiv.org/pdf/2509.03682.pdf' target='_blank'>https://arxiv.org/pdf/2509.03682.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhengyang Li, Qijin Ji, Xinghong Ling, Quan Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.03682">A Comprehensive Review of Multi-Agent Reinforcement Learning in Video Games</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in multi-agent reinforcement learning (MARL) have demonstrated its application potential in modern games. Beginning with foundational work and progressing to landmark achievements such as AlphaStar in StarCraft II and OpenAI Five in Dota 2, MARL has proven capable of achieving superhuman performance across diverse game environments through techniques like self-play, supervised learning, and deep reinforcement learning. With its growing impact, a comprehensive review has become increasingly important in this field. This paper aims to provide a thorough examination of MARL's application from turn-based two-agent games to real-time multi-agent video games including popular genres such as Sports games, First-Person Shooter (FPS) games, Real-Time Strategy (RTS) games and Multiplayer Online Battle Arena (MOBA) games. We further analyze critical challenges posed by MARL in video games, including nonstationary, partial observability, sparse rewards, team coordination, and scalability, and highlight successful implementations in games like Rocket League, Minecraft, Quake III Arena, StarCraft II, Dota 2, Honor of Kings, etc. This paper offers insights into MARL in video game AI systems, proposes a novel method to estimate game complexity, and suggests future research directions to advance MARL and its applications in game development, inspiring further innovation in this rapidly evolving field.
<div id='section'>Paperid: <span id='pid'>538, <a href='https://arxiv.org/pdf/2508.08555.pdf' target='_blank'>https://arxiv.org/pdf/2508.08555.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tong Zhang, Yu Gou, Jun Liu, Jun-Hong Cui
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.08555">Traffic Load-Aware Resource Management Strategy for Underwater Wireless Sensor Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Underwater Wireless Sensor Networks (UWSNs) represent a promising technology that enables diverse underwater applications through acoustic communication. However, it encounters significant challenges including harsh communication environments, limited energy supply, and restricted signal transmission. This paper aims to provide efficient and reliable communication in underwater networks with limited energy and communication resources by optimizing the scheduling of communication links and adjusting transmission parameters (e.g., transmit power and transmission rate). The efficient and reliable communication multi-objective optimization problem (ERCMOP) is formulated as a decentralized partially observable Markov decision process (Dec-POMDP). A Traffic Load-Aware Resource Management (TARM) strategy based on deep multi-agent reinforcement learning (MARL) is presented to address this problem. Specifically, a traffic load-aware mechanism that leverages the overhear information from neighboring nodes is designed to mitigate the disparity between partial observations and global states. Moreover, by incorporating a solution space optimization algorithm, the number of candidate solutions for the deep MARL-based decision-making model can be effectively reduced, thereby optimizing the computational complexity. Simulation results demonstrate the adaptability of TARM in various scenarios with different transmission demands and collision probabilities, while also validating the effectiveness of the proposed approach in supporting efficient and reliable communication in underwater networks with limited resources.
<div id='section'>Paperid: <span id='pid'>539, <a href='https://arxiv.org/pdf/2508.08325.pdf' target='_blank'>https://arxiv.org/pdf/2508.08325.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hangcheng Zhao, Ron Berman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.08325">Algorithmic Collusion of Pricing and Advertising on E-commerce Platforms</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Online sellers have been adopting AI learning algorithms to automatically make product pricing and advertising decisions on e-commerce platforms. When sellers compete using such algorithms, one concern is that of tacit collusion - the algorithms learn to coordinate on higher than competitive. We empirically investigate whether these concerns are valid when sellers make pricing and advertising decisions together, i.e., two-dimensional decisions. Our empirical strategy is to analyze competition with multi-agent reinforcement learning, which we calibrate to a large-scale dataset collected from Amazon.com products. Our first contribution is to find conditions under which learning algorithms can facilitate win-win-win outcomes that are beneficial for consumers, sellers, and even the platform, when consumers have high search costs. In these cases the algorithms learn to coordinate on prices that are lower than competitive prices. The intuition is that the algorithms learn to coordinate on lower advertising bids, which lower advertising costs, leading to lower prices. Our second contribution is an analysis of a large-scale, high-frequency keyword-product dataset for more than 2 million products on Amazon.com. Our estimates of consumer search costs show a wide range of costs for different product keywords. We generate an algorithm usage and find a negative interaction between the estimated consumer search costs and the algorithm usage index, providing empirical evidence of beneficial collusion. Finally, we analyze the platform's strategic response. We find that reserve price adjustments will not increase profits for the platform, but commission adjustments will. Our analyses help alleviate some worries about the potentially harmful effects of competing learning algorithms, and can help sellers, platforms and policymakers to decide on whether to adopt or regulate such algorithms.
<div id='section'>Paperid: <span id='pid'>540, <a href='https://arxiv.org/pdf/2508.07679.pdf' target='_blank'>https://arxiv.org/pdf/2508.07679.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tong Zhang, Yu Gou, Jun Liu, Shanshan Song, Tingting Yang, Jun-Hong Cui
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.07679">Joint link scheduling and power allocation in imperfect and energy-constrained underwater wireless sensor networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Underwater wireless sensor networks (UWSNs) stand as promising technologies facilitating diverse underwater applications. However, the major design issues of the considered system are the severely limited energy supply and unexpected node malfunctions. This paper aims to provide fair, efficient, and reliable (FER) communication to the imperfect and energy-constrained UWSNs (IC-UWSNs). Therefore, we formulate a FER-communication optimization problem (FERCOP) and propose ICRL-JSA to solve the formulated problem. ICRL-JSA is a deep multi-agent reinforcement learning (MARL)-based optimizer for IC-UWSNs through joint link scheduling and power allocation, which automatically learns scheduling algorithms without human intervention. However, conventional RL methods are unable to address the challenges posed by underwater environments and IC-UWSNs. To construct ICRL-JSA, we integrate deep Q-network into IC-UWSNs and propose an advanced training mechanism to deal with complex acoustic channels, limited energy supplies, and unexpected node malfunctions. Simulation results demonstrate the superiority of the proposed ICRL-JSA scheme with an advanced training mechanism compared to various benchmark algorithms.
<div id='section'>Paperid: <span id='pid'>541, <a href='https://arxiv.org/pdf/2508.07578.pdf' target='_blank'>https://arxiv.org/pdf/2508.07578.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yu Gou, Tong Zhang, Jun Liu, Tingting Yang, Shanshan Song, Jun-Hong Cui
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.07578">Achieving Fair-Effective Communications and Robustness in Underwater Acoustic Sensor Networks: A Semi-Cooperative Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper investigates the fair-effective communication and robustness in imperfect and energy-constrained underwater acoustic sensor networks (IC-UASNs). Specifically, we investigate the impact of unexpected node malfunctions on the network performance under the time-varying acoustic channels. Each node is expected to satisfy Quality of Service (QoS) requirements. However, achieving individual QoS requirements may interfere with other concurrent communications. Underwater nodes rely excessively on the rationality of other underwater nodes when guided by fully cooperative approaches, making it difficult to seek a trade-off between individual QoS and global fair-effective communications under imperfect conditions. Therefore, this paper presents a SEmi-COoperative Power Allocation approach (SECOPA) that achieves fair-effective communication and robustness in IC-UASNs. The approach is distributed multi-agent reinforcement learning (MARL)-based, and the objectives are twofold. On the one hand, each intelligent node individually decides the transmission power to simultaneously optimize individual and global performance. On the other hand, advanced training algorithms are developed to provide imperfect environments for training robust models that can adapt to the time-varying acoustic channels and handle unexpected node failures in the network. Numerical results are presented to validate our proposed approach.
<div id='section'>Paperid: <span id='pid'>542, <a href='https://arxiv.org/pdf/2507.18333.pdf' target='_blank'>https://arxiv.org/pdf/2507.18333.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kale-ab Abebe Tessera, Leonard Hinckeldey, Riccardo Zamboni, David Abel, Amos Storkey
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.18333">Remembering the Markov Property in Cooperative MARL</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cooperative multi-agent reinforcement learning (MARL) is typically formalised as a Decentralised Partially Observable Markov Decision Process (Dec-POMDP), where agents must reason about the environment and other agents' behaviour. In practice, current model-free MARL algorithms use simple recurrent function approximators to address the challenge of reasoning about others using partial information. In this position paper, we argue that the empirical success of these methods is not due to effective Markov signal recovery, but rather to learning simple conventions that bypass environment observations and memory. Through a targeted case study, we show that co-adapting agents can learn brittle conventions, which then fail when partnered with non-adaptive agents. Crucially, the same models can learn grounded policies when the task design necessitates it, revealing that the issue is not a fundamental limitation of the learning models but a failure of the benchmark design. Our analysis also suggests that modern MARL environments may not adequately test the core assumptions of Dec-POMDPs. We therefore advocate for new cooperative environments built upon two core principles: (1) behaviours grounded in observations and (2) memory-based reasoning about other agents, ensuring success requires genuine skill rather than fragile, co-adapted agreements.
<div id='section'>Paperid: <span id='pid'>543, <a href='https://arxiv.org/pdf/2507.17188.pdf' target='_blank'>https://arxiv.org/pdf/2507.17188.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lijie Zheng, Ji He, Shih Yu Chang, Yulong Shen, Dusit Niyato
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.17188">LLM Meets the Sky: Heuristic Multi-Agent Reinforcement Learning for Secure Heterogeneous UAV Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work tackles the physical layer security (PLS) problem of maximizing the secrecy rate in heterogeneous UAV networks (HetUAVNs) under propulsion energy constraints. Unlike prior studies that assume uniform UAV capabilities or overlook energy-security trade-offs, we consider a realistic scenario where UAVs with diverse payloads and computation resources collaborate to serve ground terminals in the presence of eavesdroppers. To manage the complex coupling between UAV motion and communication, we propose a hierarchical optimization framework. The inner layer uses a semidefinite relaxation (SDR)-based S2DC algorithm combining penalty functions and difference-of-convex (d.c.) programming to solve the secrecy precoding problem with fixed UAV positions. The outer layer introduces a Large Language Model (LLM)-guided heuristic multi-agent reinforcement learning approach (LLM-HeMARL) for trajectory optimization. LLM-HeMARL efficiently incorporates expert heuristics policy generated by the LLM, enabling UAVs to learn energy-aware, security-driven trajectories without the inference overhead of real-time LLM calls. The simulation results show that our method outperforms existing baselines in secrecy rate and energy efficiency, with consistent robustness across varying UAV swarm sizes and random seeds.
<div id='section'>Paperid: <span id='pid'>544, <a href='https://arxiv.org/pdf/2507.16796.pdf' target='_blank'>https://arxiv.org/pdf/2507.16796.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mian Ibad Ali Shah, Enda Barrett, Karl Mason
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.16796">Uncertainty-Aware Knowledge Transformers for Peer-to-Peer Energy Trading with Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a novel framework for Peer-to-Peer (P2P) energy trading that integrates uncertainty-aware prediction with multi-agent reinforcement learning (MARL), addressing a critical gap in current literature. In contrast to previous works relying on deterministic forecasts, the proposed approach employs a heteroscedastic probabilistic transformer-based prediction model called Knowledge Transformer with Uncertainty (KTU) to explicitly quantify prediction uncertainty, which is essential for robust decision-making in the stochastic environment of P2P energy trading. The KTU model leverages domain-specific features and is trained with a custom loss function that ensures reliable probabilistic forecasts and confidence intervals for each prediction. Integrating these uncertainty-aware forecasts into the MARL framework enables agents to optimize trading strategies with a clear understanding of risk and variability. Experimental results show that the uncertainty-aware Deep Q-Network (DQN) reduces energy purchase costs by up to 5.7% without P2P trading and 3.2% with P2P trading, while increasing electricity sales revenue by 6.4% and 44.7%, respectively. Additionally, peak hour grid demand is reduced by 38.8% without P2P and 45.6% with P2P. These improvements are even more pronounced when P2P trading is enabled, highlighting the synergy between advanced forecasting and market mechanisms for resilient, economically efficient energy communities.
<div id='section'>Paperid: <span id='pid'>545, <a href='https://arxiv.org/pdf/2507.16479.pdf' target='_blank'>https://arxiv.org/pdf/2507.16479.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoyang Zhang, Mina Montazeri, Philipp Heer, Koen Kok, Nikolaos G. Paterakis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.16479">Arbitrage Tactics in the Local Markets via Hierarchical Multi-agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Strategic bidding tactics employed by prosumers in local markets, including the Local Electricity Market (LEM) and Local Flexibility Market (LFM), have attracted significant attention due to their potential to enhance economic benefits for market participants through optimized energy management and bidding. While existing research has explored strategic bidding in a single market with multi-agent reinforcement learning (MARL) algorithms, arbitrage opportunities across local markets remain unexplored. This paper introduces a hierarchical MARL (HMARL) algorithm designed to enable aggregator arbitrage across multiple local markets. The strategic behavior of these aggregators in local markets is modeled as a two-stage Markov game: the first stage involves the LEM, while the second stage encompasses both the LFM and the balancing market. To solve this two-stage Markov game, the HMARL framework assigns two sub-agents to each aggregator, a primary sub-agent and a secondary sub-agent. Without the arbitrage strategy, these sub-agents operate in silos, with the primary sub-agent focusing on first-stage profits and the secondary sub-agent on second-stage profits, each employing independent MARLs. On the contrary, when implementing the arbitrage strategy with the proposed HMARL, the sub-agents communicate and coordinate to perform arbitrage across multiple local markets, enhancing overall efficiency. The case study, conducted under a scenario where all aggregators employ the arbitrage strategy, shows that despite higher initial costs in the LEM, this strategy generates substantial savings in the LFM and the balancing market, resulting in a total profit increase of $40.6\%$ on average. This highlights the capability of the proposed HMARL to address the two-stage Markov game and facilitate arbitrage across local markets, thereby enhancing profitability for participants.
<div id='section'>Paperid: <span id='pid'>546, <a href='https://arxiv.org/pdf/2507.14995.pdf' target='_blank'>https://arxiv.org/pdf/2507.14995.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chengwei Lou, Zekai Jin, Wei Tang, Guangfei Geng, Jin Yang, Lu Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.14995">LLM-Enhanced Multi-Agent Reinforcement Learning with Expert Workflow for Real-Time P2P Energy Trading</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Real-time peer-to-peer (P2P) electricity markets dynamically adapt to fluctuations in renewable energy and variations in demand, maximizing economic benefits through instantaneous price responses while enhancing grid flexibility. However, scaling expert guidance for massive personalized prosumers poses critical challenges, including diverse decision-making demands and lack of customized modeling frameworks. This paper proposed an integrated large language model-multi-agent reinforcement learning (LLM-MARL) framework for real-time P2P energy trading to address challenges such as the limited technical capability of prosumers, the lack of expert experience, and security issues of distribution networks. LLMs are introduced as experts to generate personalized strategy, guiding MARL under the centralized training with decentralized execution (CTDE) paradigm through imitation learning. A differential attention-based critic network is designed to enhance convergence performance. Experimental results demonstrate that LLM generated strategies effectively substitute human experts. The proposed multi-agent imitation learning algorithms achieve significantly lower economic costs and voltage violation rates on test sets compared to baselines algorithms, while maintaining robust stability. This work provides an effective solution for real-time P2P electricity market decision-making by bridging expert knowledge with agent learning.
<div id='section'>Paperid: <span id='pid'>547, <a href='https://arxiv.org/pdf/2507.10913.pdf' target='_blank'>https://arxiv.org/pdf/2507.10913.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuangyao Huang, Haibo Zhang, Zhiyi Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.10913">A Learning Framework For Cooperative Collision Avoidance of UAV Swarms Leveraging Domain Knowledge</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a multi-agent reinforcement learning (MARL) framework for cooperative collision avoidance of UAV swarms leveraging domain knowledge-driven reward. The reward is derived from knowledge in the domain of image processing, approximating contours on a two-dimensional field. By modeling obstacles as maxima on the field, collisions are inherently avoided as contours never go through peaks or intersect. Additionally, counters are smooth and energy-efficient. Our framework enables training with large swarm sizes as the agent interaction is minimized and the need for complex credit assignment schemes or observation sharing mechanisms in state-of-the-art MARL approaches are eliminated. Moreover, UAVs obtain the ability to adapt to complex environments where contours may be non-viable or non-existent through intensive training. Extensive experiments are conducted to evaluate the performances of our framework against state-of-the-art MARL algorithms.
<div id='section'>Paperid: <span id='pid'>548, <a href='https://arxiv.org/pdf/2507.07320.pdf' target='_blank'>https://arxiv.org/pdf/2507.07320.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dongyu Wei, Xiaoren Xu, Shiwen Mao, Mingzhe Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.07320">Optimizing Communication and Device Clustering for Clustered Federated Learning with Differential Privacy</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, a secure and communication-efficient clustered federated learning (CFL) design is proposed. In our model, several base stations (BSs) with heterogeneous task-handling capabilities and multiple users with non-independent and identically distributed (non-IID) data jointly perform CFL training incorporating differential privacy (DP) techniques. Since each BS can process only a subset of the learning tasks and has limited wireless resource blocks (RBs) to allocate to users for federated learning (FL) model parameter transmission, it is necessary to jointly optimize RB allocation and user scheduling for CFL performance optimization. Meanwhile, our considered CFL method requires devices to use their limited data and FL model information to determine their task identities, which may introduce additional communication overhead. We formulate an optimization problem whose goal is to minimize the training loss of all learning tasks while considering device clustering, RB allocation, DP noise, and FL model transmission delay. To solve the problem, we propose a novel dynamic penalty function assisted value decomposed multi-agent reinforcement learning (DPVD-MARL) algorithm that enables distributed BSs to independently determine their connected users, RBs, and DP noise of the connected users but jointly minimize the training loss of all learning tasks across all BSs. Different from the existing MARL methods that assign a large penalty for invalid actions, we propose a novel penalty assignment scheme that assigns penalty depending on the number of devices that cannot meet communication constraints (e.g., delay), which can guide the MARL scheme to quickly find valid actions, thus improving the convergence speed. Simulation results show that the DPVD-MARL can improve the convergence rate by up to 20% and the ultimate accumulated rewards by 15% compared to independent Q-learning.
<div id='section'>Paperid: <span id='pid'>549, <a href='https://arxiv.org/pdf/2507.06997.pdf' target='_blank'>https://arxiv.org/pdf/2507.06997.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Deemah H. Tashman, Soumaya Cherkaoui, Walaa Hamouda
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.06997">Federated Learning-based MARL for Strengthening Physical-Layer Security in B5G Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper explores the application of a federated learning-based multi-agent reinforcement learning (MARL) strategy to enhance physical-layer security (PLS) in a multi-cellular network within the context of beyond 5G networks. At each cell, a base station (BS) operates as a deep reinforcement learning (DRL) agent that interacts with the surrounding environment to maximize the secrecy rate of legitimate users in the presence of an eavesdropper. This eavesdropper attempts to intercept the confidential information shared between the BS and its authorized users. The DRL agents are deemed to be federated since they only share their network parameters with a central server and not the private data of their legitimate users. Two DRL approaches, deep Q-network (DQN) and Reinforce deep policy gradient (RDPG), are explored and compared. The results demonstrate that RDPG converges more rapidly than DQN. In addition, we demonstrate that the proposed method outperforms the distributed DRL approach. Furthermore, the outcomes illustrate the trade-off between security and complexity.
<div id='section'>Paperid: <span id='pid'>550, <a href='https://arxiv.org/pdf/2506.05437.pdf' target='_blank'>https://arxiv.org/pdf/2506.05437.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Julien SoulÃ©, Jean-Paul Jamont, Michel Occello, Louis-Marie Traonouez, Paul ThÃ©ron
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.05437">A MARL-based Approach for Easing MAS Organization Engineering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-Agent Systems (MAS) have been successfully applied in industry for their ability to address complex, distributed problems, especially in IoT-based systems. Their efficiency in achieving given objectives and meeting design requirements is strongly dependent on the MAS organization during the engineering process of an application-specific MAS. To design a MAS that can achieve given goals, available methods rely on the designer's knowledge of the deployment environment. However, high complexity and low readability in some deployment environments make the application of these methods to be costly or raise safety concerns. In order to ease the MAS organization design regarding those concerns, we introduce an original Assisted MAS Organization Engineering Approach (AOMEA). AOMEA relies on combining a Multi-Agent Reinforcement Learning (MARL) process with an organizational model to suggest relevant organizational specifications to help in MAS engineering.
<div id='section'>Paperid: <span id='pid'>551, <a href='https://arxiv.org/pdf/2505.24113.pdf' target='_blank'>https://arxiv.org/pdf/2505.24113.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pengcheng Dai, Yuanqiu Mo, Wenwu Yu, Wei Ren
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.24113">Distributed Neural Policy Gradient Algorithm for Global Convergence of Networked Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper studies the networked multi-agent reinforcement learning (NMARL) problem, where the objective of agents is to collaboratively maximize the discounted average cumulative rewards. Different from the existing methods that suffer from poor expression due to linear function approximation, we propose a distributed neural policy gradient algorithm that features two innovatively designed neural networks, specifically for the approximate Q-functions and policy functions of agents. This distributed neural policy gradient algorithm consists of two key components: the distributed critic step and the decentralized actor step. In the distributed critic step, agents receive the approximate Q-function parameters from their neighboring agents via a time-varying communication networks to collaboratively evaluate the joint policy. In contrast, in the decentralized actor step, each agent updates its local policy parameter solely based on its own approximate Q-function. In the convergence analysis, we first establish the global convergence of agents for the joint policy evaluation in the distributed critic step. Subsequently, we rigorously demonstrate the global convergence of the overall distributed neural policy gradient algorithm with respect to the objective function. Finally, the effectiveness of the proposed algorithm is demonstrated by comparing it with a centralized algorithm through simulation in the robot path planning environment.
<div id='section'>Paperid: <span id='pid'>552, <a href='https://arxiv.org/pdf/2505.20579.pdf' target='_blank'>https://arxiv.org/pdf/2505.20579.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dane Malenfant, Blake A. Richards
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.20579">The challenge of hidden gifts in multi-agent reinforcement learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Sometimes we benefit from actions that others have taken even when we are unaware that they took those actions. For example, if your neighbor chooses not to take a parking spot in front of your house when you are not there, you can benefit, even without being aware that they took this action. These ``hidden gifts'' represent an interesting challenge for multi-agent reinforcement learning (MARL), since assigning credit when the beneficial actions of others are hidden is non-trivial. Here, we study the impact of hidden gifts with a very simple MARL task. In this task, agents in a grid-world environment have individual doors to unlock in order to obtain individual rewards. As well, if all the agents unlock their door the group receives a larger collective reward. However, there is only one key for all of the doors, such that the collective reward can only be obtained when the agents drop the key for others after they use it. Notably, there is nothing to indicate to an agent that the other agents have dropped the key, thus this act for others is a ``hidden gift''. We show that several different state-of-the-art MARL algorithms, including MARL specific architectures, fail to learn how to obtain the collective reward in this simple task. Interestingly, we find that decentralized actor-critic policy gradient agents can succeed when we provide them with information about their own action history, but MARL agents still cannot solve the task with action history. Finally, we derive a correction term for policy gradient agents, inspired by learning aware approaches, which reduces the variance in learning and helps them to converge to collective success more reliably. These results show that credit assignment in multi-agent settings can be particularly challenging in the presence of ``hidden gifts'', and demonstrate that self learning-awareness in decentralized agents can benefit these settings.
<div id='section'>Paperid: <span id='pid'>553, <a href='https://arxiv.org/pdf/2503.23615.pdf' target='_blank'>https://arxiv.org/pdf/2503.23615.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Julien SoulÃ©, Jean-Paul Jamont, Michel Occello, Louis-Marie Traonouez, Paul ThÃ©ron
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.23615">An Organizationally-Oriented Approach to Enhancing Explainability and Control in Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-Agent Reinforcement Learning can lead to the development of collaborative agent behaviors that show similarities with organizational concepts. Pushing forward this perspective, we introduce a novel framework that explicitly incorporates organizational roles and goals from the $\mathcal{M}OISE^+$ model into the MARL process, guiding agents to satisfy corresponding organizational constraints. By structuring training with roles and goals, we aim to enhance both the explainability and control of agent behaviors at the organizational level, whereas much of the literature primarily focuses on individual agents. Additionally, our framework includes a post-training analysis method to infer implicit roles and goals, offering insights into emergent agent behaviors. This framework has been applied across various MARL environments and algorithms, demonstrating coherence between predefined organizational specifications and those inferred from trained agents.
<div id='section'>Paperid: <span id='pid'>554, <a href='https://arxiv.org/pdf/2503.15615.pdf' target='_blank'>https://arxiv.org/pdf/2503.15615.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Joshua McClellan, Greyson Brothers, Furong Huang, Pratap Tokekar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.15615">PEnGUiN: Partially Equivariant Graph NeUral Networks for Sample Efficient MARL</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Equivariant Graph Neural Networks (EGNNs) have emerged as a promising approach in Multi-Agent Reinforcement Learning (MARL), leveraging symmetry guarantees to greatly improve sample efficiency and generalization. However, real-world environments often exhibit inherent asymmetries arising from factors such as external forces, measurement inaccuracies, or intrinsic system biases. This paper introduces \textit{Partially Equivariant Graph NeUral Networks (PEnGUiN)}, a novel architecture specifically designed to address these challenges. We formally identify and categorize various types of partial equivariance relevant to MARL, including subgroup equivariance, feature-wise equivariance, regional equivariance, and approximate equivariance. We theoretically demonstrate that PEnGUiN is capable of learning both fully equivariant (EGNN) and non-equivariant (GNN) representations within a unified framework. Through extensive experiments on a range of MARL problems incorporating various asymmetries, we empirically validate the efficacy of PEnGUiN. Our results consistently demonstrate that PEnGUiN outperforms both EGNNs and standard GNNs in asymmetric environments, highlighting their potential to improve the robustness and applicability of graph-based MARL algorithms in real-world scenarios.
<div id='section'>Paperid: <span id='pid'>555, <a href='https://arxiv.org/pdf/2503.13415.pdf' target='_blank'>https://arxiv.org/pdf/2503.13415.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Weiqiang Jin, Hongyang Du, Biao Zhao, Xingwu Tian, Bohang Shi, Guang Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.13415">A Comprehensive Survey on Multi-Agent Cooperative Decision-Making: Scenarios, Approaches, Challenges and Perspectives</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the rapid development of artificial intelligence, intelligent decision-making techniques have gradually surpassed human levels in various human-machine competitions, especially in complex multi-agent cooperative task scenarios. Multi-agent cooperative decision-making involves multiple agents working together to complete established tasks and achieve specific objectives. These techniques are widely applicable in real-world scenarios such as autonomous driving, drone navigation, disaster rescue, and simulated military confrontations. This paper begins with a comprehensive survey of the leading simulation environments and platforms used for multi-agent cooperative decision-making. Specifically, we provide an in-depth analysis for these simulation environments from various perspectives, including task formats, reward allocation, and the underlying technologies employed. Subsequently, we provide a comprehensive overview of the mainstream intelligent decision-making approaches, algorithms and models for multi-agent systems (MAS). Theseapproaches can be broadly categorized into five types: rule-based (primarily fuzzy logic), game theory-based, evolutionary algorithms-based, deep multi-agent reinforcement learning (MARL)-based, and large language models(LLMs)reasoning-based. Given the significant advantages of MARL andLLMs-baseddecision-making methods over the traditional rule, game theory, and evolutionary algorithms, this paper focuses on these multi-agent methods utilizing MARL and LLMs-based techniques. We provide an in-depth discussion of these approaches, highlighting their methodology taxonomies, advantages, and drawbacks. Further, several prominent research directions in the future and potential challenges of multi-agent cooperative decision-making are also detailed.
<div id='section'>Paperid: <span id='pid'>556, <a href='https://arxiv.org/pdf/2503.01458.pdf' target='_blank'>https://arxiv.org/pdf/2503.01458.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xu Wan, Chao Yang, Cheng Yang, Jie Song, Mingyang Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.01458">SrSv: Integrating Sequential Rollouts with Sequential Value Estimation for Multi-agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Although multi-agent reinforcement learning (MARL) has shown its success across diverse domains, extending its application to large-scale real-world systems still faces significant challenges. Primarily, the high complexity of real-world environments exacerbates the credit assignment problem, substantially reducing training efficiency. Moreover, the variability of agent populations in large-scale scenarios necessitates scalable decision-making mechanisms. To address these challenges, we propose a novel framework: Sequential rollout with Sequential value estimation (SrSv). This framework aims to capture agent interdependence and provide a scalable solution for cooperative MARL. Specifically, SrSv leverages the autoregressive property of the Transformer model to handle varying populations through sequential action rollout. Furthermore, to capture the interdependence of policy distributions and value functions among multiple agents, we introduce an innovative sequential value estimation methodology and integrates the value approximation into an attention-based sequential model. We evaluate SrSv on three benchmarks: Multi-Agent MuJoCo, StarCraft Multi-Agent Challenge, and DubinsCars. Experimental results demonstrate that SrSv significantly outperforms baseline methods in terms of training efficiency without compromising convergence performance. Moreover, when implemented in a large-scale DubinsCar system with 1,024 agents, our framework surpasses existing benchmarks, highlighting the excellent scalability of SrSv.
<div id='section'>Paperid: <span id='pid'>557, <a href='https://arxiv.org/pdf/2503.01069.pdf' target='_blank'>https://arxiv.org/pdf/2503.01069.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kareem Eissa, Rayal Prasad, Sarith Mohan, Ankur Kapoor, Dorin Comaniciu, Vivek Singh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.01069">Multi-Agent Reinforcement Learning with Long-Term Performance Objectives for Service Workforce Optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Workforce optimization plays a crucial role in efficient organizational operations where decision-making may span several different administrative and time scales. For instance, dispatching personnel to immediate service requests while managing talent acquisition with various expertise sets up a highly dynamic optimization problem. Existing work focuses on specific sub-problems such as resource allocation and facility location, which are solved with heuristics like local-search and, more recently, deep reinforcement learning. However, these may not accurately represent real-world scenarios where such sub-problems are not fully independent. Our aim is to fill this gap by creating a simulator that models a unified workforce optimization problem. Specifically, we designed a modular simulator to support the development of reinforcement learning methods for integrated workforce optimization problems. We focus on three interdependent aspects: personnel dispatch, workforce management, and personnel positioning. The simulator provides configurable parameterizations to help explore dynamic scenarios with varying levels of stochasticity and non-stationarity. To facilitate benchmarking and ablation studies, we also include heuristic and RL baselines for the above mentioned aspects.
<div id='section'>Paperid: <span id='pid'>558, <a href='https://arxiv.org/pdf/2502.20462.pdf' target='_blank'>https://arxiv.org/pdf/2502.20462.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Leopoldo Agorio, Sean Van Alen, Santiago Paternain, Miguel Calvo-Fullana, Juan Andres Bazerque
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.20462">Cooperative Multi-Agent Assignment over Stochastic Graphs via Constrained Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Constrained multi-agent reinforcement learning offers the framework to design scalable and almost surely feasible solutions for teams of agents operating in dynamic environments to carry out conflicting tasks. We address the challenges of multi-agent coordination through an unconventional formulation in which the dual variables are not driven to convergence but are free to cycle, enabling agents to adapt their policies dynamically based on real-time constraint satisfaction levels. The coordination relies on a light single-bit communication protocol over a network with stochastic connectivity. Using this gossiped information, agents update local estimates of the dual variables. Furthermore, we modify the local dual dynamics by introducing a contraction factor, which lets us use finite communication buffers and keep the estimation error bounded. Under this model, we provide theoretical guarantees of almost sure feasibility and corroborate them with numerical experiments in which a team of robots successfully patrols multiple regions, communicating under a time-varying ad-hoc network.
<div id='section'>Paperid: <span id='pid'>559, <a href='https://arxiv.org/pdf/2502.20068.pdf' target='_blank'>https://arxiv.org/pdf/2502.20068.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianyang Qi, Shibo Chen, Jun Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.20068">A Generative Model Enhanced Multi-Agent Reinforcement Learning Method for Electric Vehicle Charging Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the widespread adoption of electric vehicles (EVs), navigating for EV drivers to select a cost-effective charging station has become an important yet challenging issue due to dynamic traffic conditions, fluctuating electricity prices, and potential competition from other EVs. The state-of-the-art deep reinforcement learning (DRL) algorithms for solving this task still require global information about all EVs at the execution stage, which not only increases communication costs but also raises privacy issues among EV drivers. To overcome these drawbacks, we introduce a novel generative model-enhanced multi-agent DRL algorithm that utilizes only the EV's local information while achieving performance comparable to these state-of-the-art algorithms. Specifically, the policy network is implemented on the EV side, and a Conditional Variational Autoencoder-Long Short Term Memory (CVAE-LSTM)-based recommendation model is developed to provide recommendation information. Furthermore, a novel future charging competition encoder is designed to effectively compress global information, enhancing training performance. The multi-gradient descent algorithm (MGDA) is also utilized to adaptively balance the weight between the two parts of the training objective, resulting in a more stable training process. Simulations are conducted based on a practical area in XiÃ¡n, China. Experimental results show that our proposed algorithm, which relies on local information, outperforms existing local information-based methods and achieves less than 8\% performance loss compared to global information-based methods.
<div id='section'>Paperid: <span id='pid'>560, <a href='https://arxiv.org/pdf/2502.14606.pdf' target='_blank'>https://arxiv.org/pdf/2502.14606.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Raihana Ferdous, Fitsum Kifetew, Davide Prandi, Angelo Susi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.14606">Curiosity Driven Multi-agent Reinforcement Learning for 3D Game Testing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently testing of games via autonomous agents has shown great promise in tackling challenges faced by the game industry, which mainly relied on either manual testing or record/replay. In particular Reinforcement Learning (RL) solutions have shown potential by learning directly from playing the game without the need for human intervention. In this paper, we present cMarlTest, an approach for testing 3D games through curiosity driven Multi-Agent Reinforcement Learning (MARL). cMarlTest deploys multiple agents that work collaboratively to achieve the testing objective. The use of multiple agents helps resolve issues faced by a single agent approach. We carried out experiments on different levels of a 3D game comparing the performance of cMarlTest with a single agent RL variant. Results are promising where, considering three different types of coverage criteria, cMarlTest achieved higher coverage. cMarlTest was also more efficient in terms of the time taken, with respect to the single agent based variant.
<div id='section'>Paperid: <span id='pid'>561, <a href='https://arxiv.org/pdf/2502.06060.pdf' target='_blank'>https://arxiv.org/pdf/2502.06060.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bidipta Sarkar, Warren Xia, C. Karen Liu, Dorsa Sadigh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.06060">Training Language Models for Social Deduction with Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Communicating in natural language is a powerful tool in multi-agent settings, as it enables independent agents to share information in partially observable settings and allows zero-shot coordination with humans. However, most prior works are limited as they either rely on training with large amounts of human demonstrations or lack the ability to generate natural and useful communication strategies. In this work, we train language models to have productive discussions about their environment in natural language without any human demonstrations. We decompose the communication problem into listening and speaking. Our key idea is to leverage the agent's goal to predict useful information about the world as a dense reward signal that guides communication. Specifically, we improve a model's listening skills by training them to predict information about the environment based on discussions, and we simultaneously improve a model's speaking skills with multi-agent reinforcement learning by rewarding messages based on their influence on other agents. To investigate the role and necessity of communication in complex social settings, we study an embodied social deduction game based on Among Us, where the key question to answer is the identity of an adversarial imposter. We analyze emergent behaviors due to our technique, such as accusing suspects and providing evidence, and find that it enables strong discussions, doubling the win rates compared to standard RL. We release our code and models at https://socialdeductionllm.github.io/
<div id='section'>Paperid: <span id='pid'>562, <a href='https://arxiv.org/pdf/2501.06454.pdf' target='_blank'>https://arxiv.org/pdf/2501.06454.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Obed Morrison Atsu, Salmane Naoumi, Roberto Bomfin, Marwa Chafii
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.06454">Reinforcement Learning for Enhancing Sensing Estimation in Bistatic ISAC Systems with UAV Swarms</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces a novel Multi-Agent Reinforcement Learning (MARL) framework to enhance integrated sensing and communication (ISAC) networks using unmanned aerial vehicle (UAV) swarms as sensing radars. By framing the positioning and trajectory optimization of UAVs as a Partially Observable Markov Decision Process, we develop a MARL approach that leverages centralized training with decentralized execution to maximize the overall sensing performance. Specifically, we implement a decentralized cooperative MARL strategy to enable UAVs to develop effective communication protocols, therefore enhancing their environmental awareness and operational efficiency. Additionally, we augment the MARL solution with a transmission power adaptation technique to mitigate interference between the communicating drones and optimize the communication protocol efficiency. Moreover, a transmission power adaptation technique is incorporated to mitigate interference and optimize the learned communication protocol efficiency. Despite the increased complexity, our solution demonstrates robust performance and adaptability across various scenarios, providing a scalable and cost-effective enhancement for future ISAC networks.
<div id='section'>Paperid: <span id='pid'>563, <a href='https://arxiv.org/pdf/2501.00160.pdf' target='_blank'>https://arxiv.org/pdf/2501.00160.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>David Goll, Jobst Heitzig, Wolfram Barfuss
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.00160">Deterministic Model of Incremental Multi-Agent Boltzmann Q-Learning: Transient Cooperation, Metastability, and Oscillations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-Agent Reinforcement Learning involves agents that learn together in a shared environment, leading to emergent dynamics sensitive to initial conditions and parameter variations. A Dynamical Systems approach, which studies the evolution of multi-component systems over time, has uncovered some of the underlying dynamics by constructing deterministic approximation models of stochastic algorithms. In this work, we demonstrate that even in the simplest case of independent Q-learning with a Boltzmann exploration policy, significant discrepancies arise between the actual algorithm and previous approximations. We elaborate why these models actually approximate interesting variants rather than the original incremental algorithm. To explain the discrepancies, we introduce a new discrete-time approximation model that explicitly accounts for agents' update frequencies within the learning process and show that its dynamics fundamentally differ from the simplified dynamics of prior models. We illustrate the usefulness of our approach by applying it to the question of spontaneous cooperation in social dilemmas, specifically the Prisoner's Dilemma as the simplest case study. We identify conditions under which the learning behaviour appears as long-term stable cooperation from an external perspective. However, our model shows that this behaviour is merely a metastable transient phase and not a true equilibrium, making it exploitable. We further exemplify how specific parameter settings can significantly exacerbate the moving target problem in independent learning. Through a systematic analysis of our model, we show that increasing the discount factor induces oscillations, preventing convergence to a joint policy. These oscillations arise from a supercritical Neimark-Sacker bifurcation, which transforms the unique stable fixed point into an unstable focus surrounded by a stable limit cycle.
<div id='section'>Paperid: <span id='pid'>564, <a href='https://arxiv.org/pdf/2412.02091.pdf' target='_blank'>https://arxiv.org/pdf/2412.02091.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kee Siong Ng, Samuel Yang-Zhao, Timothy Cadogan-Cowper
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.02091">The Problem of Social Cost in Multi-Agent General Reinforcement Learning: Survey and Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The AI safety literature is full of examples of powerful AI agents that, in blindly pursuing a specific and usually narrow objective, ends up with unacceptable and even catastrophic collateral damage to others. In this paper, we consider the problem of social harms that can result from actions taken by learning and utility-maximising agents in a multi-agent environment. The problem of measuring social harms or impacts in such multi-agent settings, especially when the agents are artificial generally intelligent (AGI) agents, was listed as an open problem in Everitt et al, 2018. We attempt a partial answer to that open problem in the form of market-based mechanisms to quantify and control the cost of such social harms. The proposed setup captures many well-studied special cases and is more general than existing formulations of multi-agent reinforcement learning with mechanism design in two ways: (i) the underlying environment is a history-based general reinforcement learning environment like in AIXI; (ii) the reinforcement-learning agents participating in the environment can have different learning strategies and planning horizons. To demonstrate the practicality of the proposed setup, we survey some key classes of learning algorithms and present a few applications, including a discussion of the Paperclips problem and pollution control with a cap-and-trade system.
<div id='section'>Paperid: <span id='pid'>565, <a href='https://arxiv.org/pdf/2412.00661.pdf' target='_blank'>https://arxiv.org/pdf/2412.00661.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Emile Anand, Ishani Karmarkar, Guannan Qu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.00661">Mean-Field Sampling for Cooperative Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Designing efficient algorithms for multi-agent reinforcement learning (MARL) is fundamentally challenging because the size of the joint state and action spaces grows exponentially in the number of agents. These difficulties are exacerbated when balancing sequential global decision-making with local agent interactions. In this work, we propose a new algorithm $\texttt{SUBSAMPLE-MFQ}$ ($\textbf{Subsample}$-$\textbf{M}$ean-$\textbf{F}$ield-$\textbf{Q}$-learning) and a decentralized randomized policy for a system with $n$ agents. For any $k\leq n$, our algorithm learns a policy for the system in time polynomial in $k$. We prove that this learned policy converges to the optimal policy on the order of $\tilde{O}(1/\sqrt{k})$ as the number of subsampled agents $k$ increases. In particular, this bound is independent of the number of agents $n$.
<div id='section'>Paperid: <span id='pid'>566, <a href='https://arxiv.org/pdf/2411.15036.pdf' target='_blank'>https://arxiv.org/pdf/2411.15036.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zeyang Li, Navid Azizan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.15036">Safe Multi-Agent Reinforcement Learning with Convergence to Generalized Nash Equilibrium</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent reinforcement learning (MARL) has achieved notable success in cooperative tasks, demonstrating impressive performance and scalability. However, deploying MARL agents in real-world applications presents critical safety challenges. Current safe MARL algorithms are largely based on the constrained Markov decision process (CMDP) framework, which enforces constraints only on discounted cumulative costs and lacks an all-time safety assurance. Moreover, these methods often overlook the feasibility issue (the system will inevitably violate state constraints within certain regions of the constraint set), resulting in either suboptimal performance or increased constraint violations. To address these challenges, we propose a novel theoretical framework for safe MARL with $\textit{state-wise}$ constraints, where safety requirements are enforced at every state the agents visit. To resolve the feasibility issue, we leverage a control-theoretic notion of the feasible region, the controlled invariant set (CIS), characterized by the safety value function. We develop a multi-agent method for identifying CISs, ensuring convergence to a Nash equilibrium on the safety value function. By incorporating CIS identification into the learning process, we introduce a multi-agent dual policy iteration algorithm that guarantees convergence to a generalized Nash equilibrium in state-wise constrained cooperative Markov games, achieving an optimal balance between feasibility and performance. Furthermore, for practical deployment in complex high-dimensional systems, we propose $\textit{Multi-Agent Dual Actor-Critic}$ (MADAC), a safe MARL algorithm that approximates the proposed iteration scheme within the deep RL paradigm. Empirical evaluations on safe MARL benchmarks demonstrate that MADAC consistently outperforms existing methods, delivering much higher rewards while reducing constraint violations.
<div id='section'>Paperid: <span id='pid'>567, <a href='https://arxiv.org/pdf/2411.12130.pdf' target='_blank'>https://arxiv.org/pdf/2411.12130.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kejun Chen, Truc Nguyen, Malik Hassanaly
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.12130">Adversarial Multi-Agent Reinforcement Learning for Proactive False Data Injection Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Smart inverters are instrumental in the integration of renewable and distributed energy resources (DERs) into the electric grid. Such inverters rely on communication layers for continuous control and monitoring, potentially exposing them to cyber-physical attacks such as false data injection attacks (FDIAs). We propose to construct a defense strategy against a priori unknown FDIAs with a multi-agent reinforcement learning (MARL) framework. The first agent is an adversary that simulates and discovers various FDIA strategies, while the second agent is a defender in charge of detecting and localizing FDIAs. This approach enables the defender to be trained against new FDIAs continuously generated by the adversary. The numerical results demonstrate that the proposed MARL defender outperforms a supervised offline defender. Additionally, we show that the detection skills of an MARL defender can be combined with that of an offline defender through a transfer learning approach.
<div id='section'>Paperid: <span id='pid'>568, <a href='https://arxiv.org/pdf/2411.11057.pdf' target='_blank'>https://arxiv.org/pdf/2411.11057.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Medant Sharan, Chandranath Adak
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.11057">Reinforcing Competitive Multi-Agents for Playing 'So Long Sucker'</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper investigates the strategy game So Long Sucker (SLS) as a novel benchmark for multi-agent reinforcement learning (MARL). Unlike traditional board or video game testbeds, SLS is distinguished by its coalition formation, strategic deception, and dynamic elimination rules, making it a uniquely challenging environment for autonomous agents. We introduce the first publicly available computational framework for SLS, complete with a graphical user interface and benchmarking support for reinforcement learning algorithms. Using classical deep reinforcement learning methods (e.g., DQN, DDQN, and Dueling DQN), we train self-playing agents to learn the rules and basic strategies of SLS. Experimental results demonstrate that, although these agents achieve roughly half of the maximum attainable reward and consistently outperform random baselines, they require long training horizons (~2000 games) and still commit occasional illegal moves, highlighting both the promise and limitations of classical reinforcement learning. Our findings establish SLS as a negotiation-aware benchmark for MARL, opening avenues for future research that integrates game-theoretic reasoning, coalition-aware strategies, and advanced reinforcement learning architectures to better capture the social and adversarial dynamics of complex multi-agent games.
<div id='section'>Paperid: <span id='pid'>569, <a href='https://arxiv.org/pdf/2410.02581.pdf' target='_blank'>https://arxiv.org/pdf/2410.02581.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Joshua McClellan, Naveed Haghani, John Winder, Furong Huang, Pratap Tokekar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.02581">Boosting Sample Efficiency and Generalization in Multi-agent Reinforcement Learning via Equivariance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-Agent Reinforcement Learning (MARL) struggles with sample inefficiency and poor generalization [1]. These challenges are partially due to a lack of structure or inductive bias in the neural networks typically used in learning the policy. One such form of structure that is commonly observed in multi-agent scenarios is symmetry. The field of Geometric Deep Learning has developed Equivariant Graph Neural Networks (EGNN) that are equivariant (or symmetric) to rotations, translations, and reflections of nodes. Incorporating equivariance has been shown to improve learning efficiency and decrease error [ 2 ]. In this paper, we demonstrate that EGNNs improve the sample efficiency and generalization in MARL. However, we also show that a naive application of EGNNs to MARL results in poor early exploration due to a bias in the EGNN structure. To mitigate this bias, we present Exploration-enhanced Equivariant Graph Neural Networks or E2GN2. We compare E2GN2 to other common function approximators using common MARL benchmarks MPE and SMACv2. E2GN2 demonstrates a significant improvement in sample efficiency, greater final reward convergence, and a 2x-5x gain in over standard GNNs in our generalization tests. These results pave the way for more reliable and effective solutions in complex multi-agent systems.
<div id='section'>Paperid: <span id='pid'>570, <a href='https://arxiv.org/pdf/2409.02246.pdf' target='_blank'>https://arxiv.org/pdf/2409.02246.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Matthew Repasky, He Wang, Yao Xie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.02246">Multi-Agent Reinforcement Learning for Joint Police Patrol and Dispatch</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Police patrol units need to split their time between performing preventive patrol and being dispatched to serve emergency incidents. In the existing literature, patrol and dispatch decisions are often studied separately. We consider joint optimization of these two decisions to improve police operations efficiency and reduce response time to emergency calls. Methodology/results: We propose a novel method for jointly optimizing multi-agent patrol and dispatch to learn policies yielding rapid response times. Our method treats each patroller as an independent Q-learner (agent) with a shared deep Q-network that represents the state-action values. The dispatching decisions are chosen using mixed-integer programming and value function approximation from combinatorial action spaces. We demonstrate that this heterogeneous multi-agent reinforcement learning approach is capable of learning joint policies that outperform those optimized for patrol or dispatch alone. Managerial Implications: Policies jointly optimized for patrol and dispatch can lead to more effective service while targeting demonstrably flexible objectives, such as those encouraging efficiency and equity in response.
<div id='section'>Paperid: <span id='pid'>571, <a href='https://arxiv.org/pdf/2512.20201.pdf' target='_blank'>https://arxiv.org/pdf/2512.20201.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Heekang Song, Wan Choi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.20201">Joint Design of Embedded Index Coding and Beamforming for MIMO-based Distributed Computing via Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In distributed computing systems, reducing the communication load during the data shuffling phase is a critical challenge, as excessive inter-node transmissions are a major performance bottleneck. One promising approach to alleviate this burden is Embedded Index Coding (EIC), which exploits cached data at user nodes to encode transmissions more efficiently. However, most prior work on EIC has focused on minimizing code length in wired, error-free environments-an objective often suboptimal for wireless multiple-input multiple-output (MIMO) systems, where channel conditions and spatial multiplexing gains must be considered. This paper investigates the joint design of EIC and transmit beamforming in MIMO systems to minimize total transmission time, an NP-hard problem. We first present a conventional optimization method that determines the optimal EIC via exhaustive search. To address its prohibitive complexity and adapt to dynamic wireless environments, we propose a novel, low-complexity multi-agent reinforcement learning (MARL) framework. The proposed framework enables decentralized agents to act on local observations while effectively managing the hybrid action space of discrete EIC selection and continuous beamforming design. Simulation results demonstrate that the proposed MARL-based approach achieves near-optimal performance with significantly reduced complexity, underscoring its effectiveness and practicality for real-world wireless systems.
<div id='section'>Paperid: <span id='pid'>572, <a href='https://arxiv.org/pdf/2512.18604.pdf' target='_blank'>https://arxiv.org/pdf/2512.18604.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wencan Mao, Quanxi Zhou, Tomas Couso Coddou, Manabu Tsukada, Yunling Liu, Yusheng Ji
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.18604">Trajectory Planning for UAV-Based Smart Farming Using Imitation-Based Triple Deep Q-Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Unmanned aerial vehicles (UAVs) have emerged as a promising auxiliary platform for smart agriculture, capable of simultaneously performing weed detection, recognition, and data collection from wireless sensors. However, trajectory planning for UAV-based smart agriculture is challenging due to the high uncertainty of the environment, partial observations, and limited battery capacity of UAVs. To address these issues, we formulate the trajectory planning problem as a Markov decision process (MDP) and leverage multi-agent reinforcement learning (MARL) to solve it. Furthermore, we propose a novel imitation-based triple deep Q-network (ITDQN) algorithm, which employs an elite imitation mechanism to reduce exploration costs and utilizes a mediator Q-network over a double deep Q-network (DDQN) to accelerate and stabilize training and improve performance. Experimental results in both simulated and real-world environments demonstrate the effectiveness of our solution. Moreover, our proposed ITDQN outperforms DDQN by 4.43\% in weed recognition rate and 6.94\% in data collection rate.
<div id='section'>Paperid: <span id='pid'>573, <a href='https://arxiv.org/pdf/2512.00513.pdf' target='_blank'>https://arxiv.org/pdf/2512.00513.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xun Shao, Ryuuto Shimizu, Zhi Liu, Kaoru Ota, Mianxiong Dong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.00513">Truthful and Trustworthy IoT AI Agents via Immediate-Penalty Enforcement under Approximate VCG Mechanisms</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The deployment of autonomous AI agents in Internet of Things (IoT) energy systems requires decision-making mechanisms that remain robust, efficient, and trustworthy under real-time constraints and imperfect monitoring. While reinforcement learning enables adaptive prosumer behaviors, ensuring economic consistency and preventing strategic manipulation remain open challenges, particularly when sensing noise or partial observability reduces the operator's ability to verify actions. This paper introduces a trust-enforcement framework for IoT energy trading that combines an approximate Vickrey-Clarke-Groves (VCG) double auction with an immediate one-shot penalty. Unlike reputation- or history-based approaches, the proposed mechanism restores truthful reporting within a single round, even when allocation accuracy is approximate and monitoring is noisy. We theoretically characterize the incentive gap induced by approximation and derive a penalty threshold that guarantees truthful bidding under bounded sensing errors. To evaluate learning-enabled prosumers, we embed the mechanism into a multi-agent reinforcement learning environment reflecting stochastic generation, dynamic loads, and heterogeneous trading opportunities. Experiments show that improved allocation accuracy reduces deviation incentives, the required penalty matches analytical predictions, and learned bidding behaviors remain stable and interpretable despite imperfect monitoring. These results demonstrate that lightweight penalty designs can reliably align strategic IoT agents with socially efficient energy-trading outcomes.
<div id='section'>Paperid: <span id='pid'>574, <a href='https://arxiv.org/pdf/2511.17915.pdf' target='_blank'>https://arxiv.org/pdf/2511.17915.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yao Liu, Sampad Mohanty, Elizabeth Ondula, Bhaskar Krishnamachari
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.17915">DISPATCH -- Decentralized Informed Spatial Planning and Assignment of Tasks for Cooperative Heterogeneous Agents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Spatial task allocation in systems such as multi-robot delivery or ride-sharing requires balancing efficiency with fair service across tasks. Greedy assignment policies that match each agent to its highest-preference or lowest-cost task can maximize efficiency but often create inequities: some tasks receive disproportionately favorable service (e.g., shorter delays or better matches), while others face long waits or poor allocations. We study fairness in heterogeneous multi-agent systems where tasks vary in preference alignment and urgency. Most existing approaches either assume centralized coordination or largely ignore fairness under partial observability. Distinct from this prior work, we establish a connection between the Eisenberg-Gale (EG) equilibrium convex program and decentralized, partially observable multi-agent learning. Building on this connection, we develop two equilibrium-informed algorithms that integrate fairness and efficiency: (i) a multi-agent reinforcement learning (MARL) framework, EG-MARL, whose training is guided by centralized fair assignment algorithms (EG and a preference-aware Hungarian method); and (ii) a stochastic online optimization mechanism that performs guided exploration and subset-based fair assignment as tasks are discovered. We evaluate our frameworks across a range of team sizes and assignment formulations against centralized EG, Hungarian, and Min-Max Distance baselines. Both algorithms preserve the fairness-efficiency balance of the Eisenberg-Gale equilibrium under partial observability. EG-MARL achieves near-centralized coordination and reduced travel distances, while the stochastic online mechanism enables real-time allocation with competitive fairness. Together, these results demonstrate that spatially aware EG formulations can effectively guide decentralized coordination in agents with heterogeneous capabilities.
<div id='section'>Paperid: <span id='pid'>575, <a href='https://arxiv.org/pdf/2511.17165.pdf' target='_blank'>https://arxiv.org/pdf/2511.17165.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kesheng Chen, Wenjian Luo, Bang Zhang, Zeping Yin, Zipeng Ye
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.17165">MIR: Efficient Exploration in Episodic Multi-Agent Reinforcement Learning via Mutual Intrinsic Reward</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Episodic rewards present a significant challenge in reinforcement learning. While intrinsic reward methods have demonstrated effectiveness in single-agent rein-forcement learning scenarios, their application to multi-agent reinforcement learn-ing (MARL) remains problematic. The primary difficulties stem from two fac-tors: (1) the exponential sparsity of joint action trajectories that lead to rewards as the exploration space expands, and (2) existing methods often fail to account for joint actions that can influence team states. To address these challenges, this paper introduces Mutual Intrinsic Reward (MIR), a simple yet effective enhancement strategy for MARL with extremely sparse rewards like episodic rewards. MIR incentivizes individual agents to explore actions that affect their teammates, and when combined with original strategies, effectively stimulates team exploration and improves algorithm performance. For comprehensive experimental valida-tion, we extend the representative single-agent MiniGrid environment to create MiniGrid-MA, a series of MARL environments with sparse rewards. Our evalu-ation compares the proposed method against state-of-the-art approaches in the MiniGrid-MA setting, with experimental results demonstrating superior perfor-mance.
<div id='section'>Paperid: <span id='pid'>576, <a href='https://arxiv.org/pdf/2511.13133.pdf' target='_blank'>https://arxiv.org/pdf/2511.13133.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shudong Wang, Xinfei Wang, Chenhao Zhang, Shanchen Pang, Haiyuan Gui, Wenhao Ji, Xiaojian Liao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.13133">Soft Conflict-Resolution Decision Transformer for Offline Multi-Task Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-task reinforcement learning (MTRL) seeks to learn a unified policy for diverse tasks, but often suffers from gradient conflicts across tasks. Existing masking-based methods attempt to mitigate such conflicts by assigning task-specific parameter masks. However, our empirical study shows that coarse-grained binary masks have the problem of over-suppressing key conflicting parameters, hindering knowledge sharing across tasks. Moreover, different tasks exhibit varying conflict levels, yet existing methods use a one-size-fits-all fixed sparsity strategy to keep training stability and performance, which proves inadequate. These limitations hinder the model's generalization and learning efficiency. To address these issues, we propose SoCo-DT, a Soft Conflict-resolution method based by parameter importance. By leveraging Fisher information, mask values are dynamically adjusted to retain important parameters while suppressing conflicting ones. In addition, we introduce a dynamic sparsity adjustment strategy based on the Interquartile Range (IQR), which constructs task-specific thresholding schemes using the distribution of conflict and harmony scores during training. To enable adaptive sparsity evolution throughout training, we further incorporate an asymmetric cosine annealing schedule to continuously update the threshold. Experimental results on the Meta-World benchmark show that SoCo-DT outperforms the state-of-the-art method by 7.6% on MT50 and by 10.5% on the suboptimal dataset, demonstrating its effectiveness in mitigating gradient conflicts and improving overall multi-task performance.
<div id='section'>Paperid: <span id='pid'>577, <a href='https://arxiv.org/pdf/2511.11654.pdf' target='_blank'>https://arxiv.org/pdf/2511.11654.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sayambhu Sen, Shalabh Bhatnagar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.11654">Convergence of Multiagent Learning Systems for Traffic control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Rapid urbanization in cities like Bangalore has led to severe traffic congestion, making efficient Traffic Signal Control (TSC) essential. Multi-Agent Reinforcement Learning (MARL), often modeling each traffic signal as an independent agent using Q-learning, has emerged as a promising strategy to reduce average commuter delays. While prior work Prashant L A et. al has empirically demonstrated the effectiveness of this approach, a rigorous theoretical analysis of its stability and convergence properties in the context of traffic control has not been explored. This paper bridges that gap by focusing squarely on the theoretical basis of this multi-agent algorithm. We investigate the convergence problem inherent in using independent learners for the cooperative TSC task. Utilizing stochastic approximation methods, we formally analyze the learning dynamics. The primary contribution of this work is the proof that the specific multi-agent reinforcement learning algorithm for traffic control is proven to converge under the given conditions extending it from single agent convergence proofs for asynchronous value iteration.
<div id='section'>Paperid: <span id='pid'>578, <a href='https://arxiv.org/pdf/2511.10409.pdf' target='_blank'>https://arxiv.org/pdf/2511.10409.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kayla Boggess, Sarit Kraus, Lu Feng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.10409">Explaining Decentralized Multi-Agent Reinforcement Learning Policies</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-Agent Reinforcement Learning (MARL) has gained significant interest in recent years, enabling sequential decision-making across multiple agents in various domains. However, most existing explanation methods focus on centralized MARL, failing to address the uncertainty and nondeterminism inherent in decentralized settings. We propose methods to generate policy summarizations that capture task ordering and agent cooperation in decentralized MARL policies, along with query-based explanations for When, Why Not, and What types of user queries about specific agent behaviors. We evaluate our approach across four MARL domains and two decentralized MARL algorithms, demonstrating its generalizability and computational efficiency. User studies show that our summarizations and explanations significantly improve user question-answering performance and enhance subjective ratings on metrics such as understanding and satisfaction.
<div id='section'>Paperid: <span id='pid'>579, <a href='https://arxiv.org/pdf/2511.02192.pdf' target='_blank'>https://arxiv.org/pdf/2511.02192.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Linxin Hou, Qirui Wu, Zhihang Qin, Neil Banerjee, Yongxin Guo, Cecilia Laschi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.02192">A Quantitative Comparison of Centralised and Distributed Reinforcement Learning-Based Control for Soft Robotic Arms</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a quantitative comparison between centralised and distributed multi-agent reinforcement learning (MARL) architectures for controlling a soft robotic arm modelled as a Cosserat rod in simulation. Using PyElastica and the OpenAI Gym interface, we train both a global Proximal Policy Optimisation (PPO) controller and a Multi-Agent PPO (MAPPO) under identical budgets. Both approaches are based on the arm having $n$ number of controlled sections. The study systematically varies $n$ and evaluates the performance of the arm to reach a fixed target in three scenarios: default baseline condition, recovery from external disturbance, and adaptation to actuator failure. Quantitative metrics used for the evaluation are mean action magnitude, mean final distance, mean episode length, and success rate. The results show that there are no significant benefits of the distributed policy when the number of controlled sections $n\le4$. In very simple systems, when $n\le2$, the centralised policy outperforms the distributed one. When $n$ increases to $4< n\le 12$, the distributed policy shows a high sample efficiency. In these systems, distributed policy promotes a stronger success rate, resilience, and robustness under local observability and yields faster convergence given the same sample size. However, centralised policies achieve much higher time efficiency during training as it takes much less time to train the same size of samples. These findings highlight the trade-offs between centralised and distributed policy in reinforcement learning-based control for soft robotic systems and provide actionable design guidance for future sim-to-real transfer in soft rod-like manipulators.
<div id='section'>Paperid: <span id='pid'>580, <a href='https://arxiv.org/pdf/2510.25340.pdf' target='_blank'>https://arxiv.org/pdf/2510.25340.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Beiwen Zhang, Yongheng Liang, Hejun Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.25340">Multi-party Agent Relation Sampling for Multi-party Ad Hoc Teamwork</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent reinforcement learning (MARl) has achieved strong results in cooperative tasks but typically assumes fixed, fully controlled teams. Ad hoc teamwork (AHT) relaxes this by allowing collaboration with unknown partners, yet existing variants still presume shared conventions. We introduce Multil-party Ad Hoc Teamwork (MAHT), where controlled agents must coordinate with multiple mutually unfamiliar groups of uncontrolled teammates. To address this, we propose MARs, which builds a sparse skeleton graph and applies relational modeling to capture cross-group dvnamics. Experiments on MPE and starCralt ll show that MARs outperforms MARL and AHT baselines while converging faster.
<div id='section'>Paperid: <span id='pid'>581, <a href='https://arxiv.org/pdf/2510.17401.pdf' target='_blank'>https://arxiv.org/pdf/2510.17401.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>David Aguilera-Luzon, Dave de Jonge, Javier Larrosa
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.17401">MiCRO for Multilateral Negotiations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, a very simple new bilateral negotiation strategy called MiCRO was introduced that does not make use of any kind of opponent modeling or machine learning techniques and that does not require fine-tuning of any parameters. Despite its simplicity, it was shown that MiCRO performs similar to -- or even better than -- most state-of-the-art negotiation strategies. This lead its authors to argue that the benchmark domains on which negotiation algorithms are typically tested may be too simplistic. However, one question that was left open, was how MiCRO could be generalized to multilateral negotiations. In this paper we fill this gap by introducing a multilateral variant of MiCRO. We compare it with the winners of the Automated Negotiating Agents Competitions (ANAC) of 2015, 2017 and 2018 and show that it outperforms them. Furthermore, we perform an empirical game-theoretical analysis to show that our new version of MiCRO forms an empirical Nash equilibrium.
<div id='section'>Paperid: <span id='pid'>582, <a href='https://arxiv.org/pdf/2510.12555.pdf' target='_blank'>https://arxiv.org/pdf/2510.12555.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Andries Rosseau, Raphaël Avalos, Ann Nowé
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.12555">Inclusive Fitness as a Key Step Towards More Advanced Social Behaviors in Multi-Agent Reinforcement Learning Settings</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The competitive and cooperative forces of natural selection have driven the evolution of intelligence for millions of years, culminating in nature's vast biodiversity and the complexity of human minds. Inspired by this process, we propose a novel multi-agent reinforcement learning framework where each agent is assigned a genotype and where reward functions are modelled after the concept of inclusive fitness. An agent's genetic material may be shared with other agents, and our inclusive reward function naturally accounts for this. We study the resulting social dynamics in two types of network games with prisoner's dilemmas and find that our results align with well-established principles from biology, such as Hamilton's rule. Furthermore, we outline how this framework can extend to more open-ended environments with spatial and temporal structure, finite resources, and evolving populations. We hypothesize the emergence of an arms race of strategies, where each new strategy is a gradual improvement over earlier adaptations of other agents, effectively producing a multi-agent autocurriculum analogous to biological evolution. In contrast to the binary team-based structures prevalent in earlier research, our gene-based reward structure introduces a spectrum of cooperation ranging from full adversity to full cooperativeness based on genetic similarity, enabling unique non team-based social dynamics. For example, one agent having a mutual cooperative relationship with two other agents, while the two other agents behave adversarially towards each other. We argue that incorporating inclusive fitness in agents provides a foundation for the emergence of more strategically advanced and socially intelligent agents.
<div id='section'>Paperid: <span id='pid'>583, <a href='https://arxiv.org/pdf/2508.20315.pdf' target='_blank'>https://arxiv.org/pdf/2508.20315.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>RexCharles Donatus, Kumater Ter, Ore-Ofe Ajayi, Daniel Udekwe
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.20315">Multi-Agent Reinforcement Learning in Intelligent Transportation Systems: A Comprehensive Survey</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The growing complexity of urban mobility and the demand for efficient, sustainable, and adaptive solutions have positioned Intelligent Transportation Systems (ITS) at the forefront of modern infrastructure innovation. At the core of ITS lies the challenge of autonomous decision-making across dynamic, large scale, and uncertain environments where multiple agents traffic signals, autonomous vehicles, or fleet units must coordinate effectively. Multi Agent Reinforcement Learning (MARL) offers a promising paradigm for addressing these challenges by enabling distributed agents to jointly learn optimal strategies that balance individual objectives with system wide efficiency. This paper presents a comprehensive survey of MARL applications in ITS. We introduce a structured taxonomy that categorizes MARL approaches according to coordination models and learning algorithms, spanning value based, policy based, actor critic, and communication enhanced frameworks. Applications are reviewed across key ITS domains, including traffic signal control, connected and autonomous vehicle coordination, logistics optimization, and mobility on demand systems. Furthermore, we highlight widely used simulation platforms such as SUMO, CARLA, and CityFlow that support MARL experimentation, along with emerging benchmarks. The survey also identifies core challenges, including scalability, non stationarity, credit assignment, communication constraints, and the sim to real transfer gap, which continue to hinder real world deployment.
<div id='section'>Paperid: <span id='pid'>584, <a href='https://arxiv.org/pdf/2508.12633.pdf' target='_blank'>https://arxiv.org/pdf/2508.12633.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yaqi Xu, Yan Shi, Jin Tian, Fanzeng Xia, Tongxin Li, Shanzhi Chen, Yuming Ge
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.12633">DCT-MARL: A Dynamic Communication Topology-Based MARL Algorithm for Connected Vehicle Platoon Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the rapid advancement of vehicular communication facilities and autonomous driving technologies, connected vehicle platooning has emerged as a promising approach to improve traffic efficiency and driving safety. Reliable Vehicle-to-Vehicle (V2V) communication is critical to achieving efficient cooperative control. However, in the real-world traffic environment, V2V communication may suffer from time-varying delay and packet loss, leading to degraded control performance and even safety risks. To mitigate the adverse effects of non-ideal communication, this paper proposes a Dynamic Communication Topology based Multi-Agent Reinforcement Learning (DCT-MARL) algorithm for robust cooperative platoon control. Specifically, the state space is augmented with historical control action and delay to enhance robustness against communication delay. To mitigate the impact of packet loss, a multi-key gated communication mechanism is introduced, which dynamically adjusts the communication topology based on the correlation between vehicles and their current communication status. Simulation results demonstrate that the proposed DCT-MARL significantly outperforms state-of-the-art methods in terms of string stability and driving comfort, validating its superior robustness and effectiveness.
<div id='section'>Paperid: <span id='pid'>585, <a href='https://arxiv.org/pdf/2508.12296.pdf' target='_blank'>https://arxiv.org/pdf/2508.12296.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bin Wang, Jiwen Zhang, Song Wang, Dan Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.12296">A robust and compliant robotic assembly control strategy for batch precision assembly task with uncertain fit types and fit amounts</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In some high-precision industrial applications, robots are deployed to perform precision assembly tasks on mass batches of manufactured pegs and holes. If the peg and hole are designed with transition fit, machining errors may lead to either a clearance or an interference fit for a specific pair of components, with uncertain fit amounts. This paper focuses on the robotic batch precision assembly task involving components with uncertain fit types and fit amounts, and proposes an efficient methodology to construct the robust and compliant assembly control strategy. Specifically, the batch precision assembly task is decomposed into multiple deterministic subtasks, and a force-vision fusion controller-driven reinforcement learning method and a multi-task reinforcement learning training method (FVFC-MTRL) are proposed to jointly learn multiple compliance control strategies for these subtasks. Subsequently, the multi-teacher policy distillation approach is designed to integrate multiple trained strategies into a unified student network, thereby establishing a robust control strategy. Real-world experiments demonstrate that the proposed method successfully constructs the robust control strategy for high-precision assembly task with different fit types and fit amounts. Moreover, the MTRL framework significantly improves training efficiency, and the final developed control strategy achieves superior force compliance and higher success rate compared with many existing methods.
<div id='section'>Paperid: <span id='pid'>586, <a href='https://arxiv.org/pdf/2508.06871.pdf' target='_blank'>https://arxiv.org/pdf/2508.06871.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aleksandar Todorov, Juan Cardenas-Cartagena, Rafael F. Cunha, Marco Zullich, Matthia Sabatelli
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.06871">Sparsity-Driven Plasticity in Multi-Task Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Plasticity loss, a diminishing capacity to adapt as training progresses, is a critical challenge in deep reinforcement learning. We examine this issue in multi-task reinforcement learning (MTRL), where higher representational flexibility is crucial for managing diverse and potentially conflicting task demands. We systematically explore how sparsification methods, particularly Gradual Magnitude Pruning (GMP) and Sparse Evolutionary Training (SET), enhance plasticity and consequently improve performance in MTRL agents. We evaluate these approaches across distinct MTRL architectures (shared backbone, Mixture of Experts, Mixture of Orthogonal Experts) on standardized MTRL benchmarks, comparing against dense baselines, and a comprehensive range of alternative plasticity-inducing or regularization methods. Our results demonstrate that both GMP and SET effectively mitigate key indicators of plasticity degradation, such as neuron dormancy and representational collapse. These plasticity improvements often correlate with enhanced multi-task performance, with sparse agents frequently outperforming dense counterparts and achieving competitive results against explicit plasticity interventions. Our findings offer insights into the interplay between plasticity, network sparsity, and MTRL designs, highlighting dynamic sparsification as a robust but context-sensitive tool for developing more adaptable MTRL systems.
<div id='section'>Paperid: <span id='pid'>587, <a href='https://arxiv.org/pdf/2508.06061.pdf' target='_blank'>https://arxiv.org/pdf/2508.06061.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ainur Zhaikhan, Malek Khammassi, Ali H. Sayed
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.06061">Policy Optimization in Multi-Agent Settings under Partially Observable Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work leverages adaptive social learning to estimate partially observable global states in multi-agent reinforcement learning (MARL) problems. Unlike existing methods, the proposed approach enables the concurrent operation of social learning and reinforcement learning. Specifically, it alternates between a single step of social learning and a single step of MARL, eliminating the need for the time- and computation-intensive two-timescale learning frameworks. Theoretical guarantees are provided to support the effectiveness of the proposed method. Simulation results verify that the performance of the proposed methodology can approach that of reinforcement learning when the true state is known.
<div id='section'>Paperid: <span id='pid'>588, <a href='https://arxiv.org/pdf/2508.01060.pdf' target='_blank'>https://arxiv.org/pdf/2508.01060.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ibrahim Althamary, Chen-Fu Chou, Chih-Wei Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.01060">Connectivity Management in Satellite-Aided Vehicular Networks with Multi-Head Attention-Based State Estimation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Managing connectivity in integrated satellite-terrestrial vehicular networks is critical for 6G, yet is challenged by dynamic conditions and partial observability. This letter introduces the Multi-Agent Actor-Critic with Satellite-Aided Multi-head self-attention (MAAC-SAM), a novel multi-agent reinforcement learning framework that enables vehicles to autonomously manage connectivity across Vehicle-to-Satellite (V2S), Vehicle-to-Infrastructure (V2I), and Vehicle-to-Vehicle (V2V) links. Our key innovation is the integration of a multi-head attention mechanism, which allows for robust state estimation even with fluctuating and limited information sharing among vehicles. The framework further leverages self-imitation learning (SIL) and fingerprinting to improve learning efficiency and real-time decisions. Simulation results, based on realistic SUMO traffic models and 3GPP-compliant configurations, demonstrate that MAAC-SAM outperforms state-of-the-art terrestrial and satellite-assisted baselines by up to 14% in transmission utility and maintains high estimation accuracy across varying vehicle densities and sharing levels.
<div id='section'>Paperid: <span id='pid'>589, <a href='https://arxiv.org/pdf/2507.11566.pdf' target='_blank'>https://arxiv.org/pdf/2507.11566.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fuda van Diggelen, Tugay Alperen KaragÃ¼zel, Andres Garcia Rincon, A. E. Eiben, Dario Floreano, Eliseo Ferrante
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.11566">Emergent Heterogeneous Swarm Control Through Hebbian Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we introduce Hebbian learning as a novel method for swarm robotics, enabling the automatic emergence of heterogeneity. Hebbian learning presents a biologically inspired form of neural adaptation that solely relies on local information. By doing so, we resolve several major challenges for learning heterogeneous control: 1) Hebbian learning removes the complexity of attributing emergent phenomena to single agents through local learning rules, thus circumventing the micro-macro problem; 2) uniform Hebbian learning rules across all swarm members limit the number of parameters needed, mitigating the curse of dimensionality with scaling swarm sizes; and 3) evolving Hebbian learning rules based on swarm-level behaviour minimises the need for extensive prior knowledge typically required for optimising heterogeneous swarms. This work demonstrates that with Hebbian learning heterogeneity naturally emerges, resulting in swarm-level behavioural switching and in significantly improved swarm capabilities. It also demonstrates how the evolution of Hebbian learning rules can be a valid alternative to Multi Agent Reinforcement Learning in standard benchmarking tasks.
<div id='section'>Paperid: <span id='pid'>590, <a href='https://arxiv.org/pdf/2507.10251.pdf' target='_blank'>https://arxiv.org/pdf/2507.10251.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenjing Zhang, Wei Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.10251">ToMacVF : Temporal Macro-action Value Factorization for Asynchronous Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing asynchronous MARL methods based on MacDec-POMDP typically construct training trajectory buffers by simply sampling limited and biased data at the endpoints of macro-actions, and directly apply conventional MARL methods on the buffers. As a result, these methods lead to an incomplete and inaccurate representation of the macro-action execution process, along with unsuitable credit assignments. To solve these problems, the Temporal Macro-action Value Factorization (ToMacVF) is proposed to achieve fine-grained temporal credit assignment for macro-action contributions. A centralized training buffer, called Macro-action Segmented Joint Experience Replay Trajectory (Mac-SJERT), is designed to incorporate with ToMacVF to collect accurate and complete macro-action execution information, supporting a more comprehensive and precise representation of the macro-action process. To ensure principled and fine-grained asynchronous value factorization, the consistency requirement between joint and individual macro-action selection called Temporal Macro-action based IGM (To-Mac-IGM) is formalized, proving that it generalizes the synchronous cases. Based on To-Mac-IGM, a modularized ToMacVF architecture, which satisfies CTDE principle, is designed to conveniently integrate previous value factorization methods. Next, the ToMacVF algorithm is devised as an implementation of the ToMacVF architecture. Experimental results demonstrate that, compared to asynchronous baselines, our ToMacVF algorithm not only achieves optimal performance but also exhibits strong adaptability and robustness across various asynchronous multi-agent experimental scenarios.
<div id='section'>Paperid: <span id='pid'>591, <a href='https://arxiv.org/pdf/2507.08610.pdf' target='_blank'>https://arxiv.org/pdf/2507.08610.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Parag Dutta, Ambedkar Dukkipati
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.08610">Emergent Natural Language with Communication Games for Improving Image Captioning Capabilities without Additional Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Image captioning is an important problem in developing various AI systems, and these tasks require large volumes of annotated images to train the models. Since all existing labelled datasets are already used for training the large Vision Language Models (VLMs), it becomes challenging to improve the performance of the same. Considering this, it is essential to consider the unsupervised image captioning performance, which remains relatively under-explored. To that end, we propose LoGIC (Lewis Communication Game for Image Captioning), a Multi-agent Reinforcement Learning game. The proposed method consists of two agents, a 'speaker' and a 'listener', with the objective of learning a strategy for communicating in natural language. We train agents in the cooperative common-reward setting using the GRPO algorithm and show that improvement in image captioning performance emerges as a consequence of the agents learning to play the game. We show that using pre-trained VLMs as the 'speaker' and Large Language Model (LLM) for language understanding in the 'listener', we achieved a $46$ BLEU score after fine-tuning using LoGIC without additional labels, a $2$ units advantage in absolute metrics compared to the $44$ BLEU score of the vanilla VLM. Additionally, we replace the VLM from the 'speaker' with lightweight components: (i) a ViT for image perception and (ii) a GPT2 language generation, and train them from scratch using LoGIC, obtaining a $31$ BLEU score in the unsupervised setting, a $10$ points advantage over existing unsupervised image-captioning methods.
<div id='section'>Paperid: <span id='pid'>592, <a href='https://arxiv.org/pdf/2507.02698.pdf' target='_blank'>https://arxiv.org/pdf/2507.02698.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Thomas Hazenberg, Yao Ma, Seyed Sahand Mohammadi Ziabari, Marijn van Rijswijk
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.02698">Multi-Agent Reinforcement Learning for Dynamic Pricing in Supply Chains: Benchmarking Strategic Agent Behaviours under Realistically Simulated Market Conditions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This study investigates how Multi-Agent Reinforcement Learning (MARL) can improve dynamic pricing strategies in supply chains, particularly in contexts where traditional ERP systems rely on static, rule-based approaches that overlook strategic interactions among market actors. While recent research has applied reinforcement learning to pricing, most implementations remain single-agent and fail to model the interdependent nature of real-world supply chains. This study addresses that gap by evaluating the performance of three MARL algorithms: MADDPG, MADQN, and QMIX against static rule-based baselines, within a simulated environment informed by real e-commerce transaction data and a LightGBM demand prediction model. Results show that rule-based agents achieve near-perfect fairness (Jain's Index: 0.9896) and the highest price stability (volatility: 0.024), but they fully lack competitive dynamics. Among MARL agents, MADQN exhibits the most aggressive pricing behaviour, with the highest volatility and the lowest fairness (0.5844). MADDPG provides a more balanced approach, supporting market competition (share volatility: 9.5 pp) while maintaining relatively high fairness (0.8819) and stable pricing. These findings suggest that MARL introduces emergent strategic behaviour not captured by static pricing rules and may inform future developments in dynamic pricing.
<div id='section'>Paperid: <span id='pid'>593, <a href='https://arxiv.org/pdf/2506.19846.pdf' target='_blank'>https://arxiv.org/pdf/2506.19846.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ai Han, Junxing Hu, Pu Wei, Zhiqian Zhang, Yuhang Guo, Jiawei Lu, Zicheng Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.19846">JoyAgents-R1: Joint Evolution Dynamics for Versatile Multi-LLM Agents with Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent reinforcement learning (MARL) has emerged as a prominent paradigm for increasingly complex tasks. However, joint evolution across heterogeneous agents remains challenging due to cooperative inefficiency and training instability. In this paper, we propose the joint evolution dynamics for MARL called JoyAgents-R1, which first applies Group Relative Policy Optimization (GRPO) to the joint training of heterogeneous multi-agents. By iteratively refining agents' large language models (LLMs) and memories, the method achieves holistic equilibrium with optimal decision-making and memory capabilities. Specifically, JoyAgents-R1 first implements node-wise Monte Carlo sampling on the behavior of each agent across entire reasoning trajectories to enhance GRPO sampling efficiency while maintaining policy diversity. Then, our marginal benefit-driven selection strategy identifies top-$K$ sampling groups with maximal reward fluctuations, enabling targeted agent model updates that improve training stability and maximize joint benefits through cost-effective parameter adjustments. Meanwhile, JoyAgents-R1 introduces an adaptive memory evolution mechanism that repurposes GRPO rewards as cost-free supervisory signals to eliminate repetitive reasoning and accelerate convergence. Experiments across general and domain-specific scenarios demonstrate that JoyAgents-R1 achieves performance comparable to that of larger LLMs while built on smaller open-source models.
<div id='section'>Paperid: <span id='pid'>594, <a href='https://arxiv.org/pdf/2506.05236.pdf' target='_blank'>https://arxiv.org/pdf/2506.05236.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Maxime Toquebiau, Jae-Yun Jun, FaÃ¯z Benamar, Nicolas Bredeche
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.05236">Towards Language-Augmented Multi-Agent Deep Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Most prior works on communication in multi-agent reinforcement learning have focused on emergent communication, which often results in inefficient and non-interpretable systems. Inspired by the role of language in natural intelligence, we investigate how grounding agents in a human-defined language can improve the learning and coordination of embodied agents. We propose a framework in which agents are trained not only to act but also to produce and interpret natural language descriptions of their observations. This language-augmented learning serves a dual role: enabling efficient and interpretable communication between agents, and guiding representation learning. We demonstrate that language-augmented agents outperform emergent communication baselines across various tasks. Our analysis reveals that language grounding leads to more informative internal representations, better generalization to new partners, and improved capability for human-agent interaction. These findings demonstrate the effectiveness of integrating structured language into multi-agent learning and open avenues for more interpretable and capable multi-agent systems.
<div id='section'>Paperid: <span id='pid'>595, <a href='https://arxiv.org/pdf/2505.00787.pdf' target='_blank'>https://arxiv.org/pdf/2505.00787.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lucas N. Alegre, Ana L. C. Bazzan, AndrÃ© Barreto, Bruno C. da Silva
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.00787">Constructing an Optimal Behavior Basis for the Option Keyboard</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-task reinforcement learning aims to quickly identify solutions for new tasks with minimal or no additional interaction with the environment. Generalized Policy Improvement (GPI) addresses this by combining a set of base policies to produce a new one that is at least as good -- though not necessarily optimal -- as any individual base policy. Optimality can be ensured, particularly in the linear-reward case, via techniques that compute a Convex Coverage Set (CCS). However, these are computationally expensive and do not scale to complex domains. The Option Keyboard (OK) improves upon GPI by producing policies that are at least as good -- and often better. It achieves this through a learned meta-policy that dynamically combines base policies. However, its performance critically depends on the choice of base policies. This raises a key question: is there an optimal set of base policies -- an optimal behavior basis -- that enables zero-shot identification of optimal solutions for any linear tasks? We solve this open problem by introducing a novel method that efficiently constructs such an optimal behavior basis. We show that it significantly reduces the number of base policies needed to ensure optimality in new tasks. We also prove that it is strictly more expressive than a CCS, enabling particular classes of non-linear tasks to be solved optimally. We empirically evaluate our technique in challenging domains and show that it outperforms state-of-the-art approaches, increasingly so as task complexity increases.
<div id='section'>Paperid: <span id='pid'>596, <a href='https://arxiv.org/pdf/2504.13424.pdf' target='_blank'>https://arxiv.org/pdf/2504.13424.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yang Shen, Shuqi Chai, Bing Li, Xiaodong Luo, Qingjiang Shi, Rongqing Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.13424">Decentralized Handover Parameter Optimization with MARL for Load Balancing in 5G Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In cellular networks, cell handover refers to the process where a device switches from one base station to another, and this mechanism is crucial for balancing the load among different cells. Traditionally, engineers would manually adjust parameters based on experience. However, the explosive growth in the number of cells has rendered manual tuning impractical. Existing research tends to overlook critical engineering details in order to simplify handover problems. In this paper, we classify cell handover into three types, and jointly model their mutual influence. To achieve load balancing, we propose a multi-agent-reinforcement-learning (MARL)-based scheme to automatically optimize the parameters. To reduce the agent interaction costs, a distributed training is implemented based on consensus approximation of global average load, and it is shown that the approximation error is bounded. Experimental results show that our proposed scheme outperforms existing benchmarks in balancing load and improving network performance.
<div id='section'>Paperid: <span id='pid'>597, <a href='https://arxiv.org/pdf/2504.04438.pdf' target='_blank'>https://arxiv.org/pdf/2504.04438.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wang Zhang, Chenguang Liu, Yue Pi, Yong Zhang, Hairong Huang, Baoquan Rao, Yulong Ding, Shuanghua Yang, Jie Jiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.04438">DRAMA: A Dynamic Packet Routing Algorithm using Multi-Agent Reinforcement Learning with Emergent Communication</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The continuous expansion of network data presents a pressing challenge for conventional routing algorithms. As the demand escalates, these algorithms are struggling to cope. In this context, reinforcement learning (RL) and multi-agent reinforcement learning (MARL) algorithms emerge as promising solutions. However, the urgency and importance of the problem are clear, as existing RL/MARL-based routing approaches lack effective communication in run time among routers, making it challenging for individual routers to adapt to complex and dynamic changing networks. More importantly, they lack the ability to deal with dynamically changing network topology, especially the addition of the router, due to the non-scalability of their neural networks. This paper proposes a novel dynamic routing algorithm, DRAMA, incorporating emergent communication in multi-agent reinforcement learning. Through emergent communication, routers could learn how to communicate effectively to maximize the optimization objectives. Meanwhile, a new Q-network and graph-based emergent communication are introduced to dynamically adapt to the changing network topology without retraining while ensuring robust performance. Experimental results showcase DRAMA's superior performance over the traditional routing algorithm and other RL/MARL-based algorithms, achieving a higher delivery rate and lower latency in diverse network scenarios, including dynamic network load and topology. Moreover, an ablation experiment validates the prospect of emergent communication in facilitating packet routing.
<div id='section'>Paperid: <span id='pid'>598, <a href='https://arxiv.org/pdf/2503.22779.pdf' target='_blank'>https://arxiv.org/pdf/2503.22779.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junkai Hu, Li Xia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.22779">Policy Optimization and Multi-agent Reinforcement Learning for Mean-variance Team Stochastic Games</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We study a long-run mean-variance team stochastic game (MV-TSG), where each agent shares a common mean-variance objective for the system and takes actions independently to maximize it. MV-TSG has two main challenges. First, the variance metric is neither additive nor Markovian in a dynamic setting. Second, simultaneous policy updates of all agents lead to a non-stationary environment for each individual agent. Both challenges make dynamic programming inapplicable. In this paper, we study MV-TSGs from the perspective of sensitivity-based optimization. The performance difference and performance derivative formulas for joint policies are derived, which provide optimization information for MV-TSGs. We prove the existence of a deterministic Nash policy for this problem. Subsequently, we propose a Mean-Variance Multi-Agent Policy Iteration (MV-MAPI) algorithm with a sequential update scheme, where individual agent policies are updated one by one in a given order. We prove that the MV-MAPI algorithm converges to a first-order stationary point of the objective function. By analyzing the local geometry of stationary points, we derive specific conditions for stationary points to be (local) Nash equilibria, and further, strict local optima. To solve large-scale MV-TSGs in scenarios with unknown environmental parameters, we extend the idea of trust region methods to MV-MAPI and develop a multi-agent reinforcement learning algorithm named Mean-Variance Multi-Agent Trust Region Policy Optimization (MV-MATRPO). We derive a performance lower bound for each update of joint policies. Finally, numerical experiments on energy management in multiple microgrid systems are conducted.
<div id='section'>Paperid: <span id='pid'>599, <a href='https://arxiv.org/pdf/2503.10907.pdf' target='_blank'>https://arxiv.org/pdf/2503.10907.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xueting Luo, Hao Deng, Jihong Yang, Yao Shen, Huanhuan Guo, Zhiyuan Sun, Mingqing Liu, Jiming Wei, Shengjie Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.10907">H2-MARL: Multi-Agent Reinforcement Learning for Pareto Optimality in Hospital Capacity Strain and Human Mobility during Epidemic</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The necessity of achieving an effective balance between minimizing the losses associated with restricting human mobility and ensuring hospital capacity has gained significant attention in the aftermath of COVID-19. Reinforcement learning (RL)-based strategies for human mobility management have recently advanced in addressing the dynamic evolution of cities and epidemics; however, they still face challenges in achieving coordinated control at the township level and adapting to cities of varying scales. To address the above issues, we propose a multi-agent RL approach that achieves Pareto optimality in managing hospital capacity and human mobility (H2-MARL), applicable across cities of different scales. We first develop a township-level infection model with online-updatable parameters to simulate disease transmission and construct a city-wide dynamic spatiotemporal epidemic simulator. On this basis, H2-MARL is designed to treat each division as an agent, with a trade-off dual-objective reward function formulated and an experience replay buffer enriched with expert knowledge built. To evaluate the effectiveness of the model, we construct a township-level human mobility dataset containing over one billion records from four representative cities of varying scales. Extensive experiments demonstrate that H2-MARL has the optimal dual-objective trade-off capability, which can minimize hospital capacity strain while minimizing human mobility restriction loss. Meanwhile, the applicability of the proposed model to epidemic control in cities of varying scales is verified, which showcases its feasibility and versatility in practical applications.
<div id='section'>Paperid: <span id='pid'>600, <a href='https://arxiv.org/pdf/2503.05092.pdf' target='_blank'>https://arxiv.org/pdf/2503.05092.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Adam Labiosa, Josiah P. Hanna
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.05092">Multi-Robot Collaboration through Reinforcement Learning and Abstract Simulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Teams of people coordinate to perform complex tasks by forming abstract mental models of world and agent dynamics. The use of abstract models contrasts with much recent work in robot learning that uses a high-fidelity simulator and reinforcement learning (RL) to obtain policies for physical robots. Motivated by this difference, we investigate the extent to which so-called abstract simulators can be used for multi-agent reinforcement learning (MARL) and the resulting policies successfully deployed on teams of physical robots. An abstract simulator models the robot's target task at a high-level of abstraction and discards many details of the world that could impact optimal decision-making. Policies are trained in an abstract simulator then transferred to the physical robot by making use of separately-obtained low-level perception and motion control modules. We identify three key categories of modifications to the abstract simulator that enable policy transfer to physical robots: simulation fidelity enhancements, training optimizations and simulation stochasticity. We then run an empirical study with extensive ablations to determine the value of each modification category for enabling policy transfer in cooperative robot soccer tasks. We also compare the performance of policies produced by our method with a well-tuned non-learning-based behavior architecture from the annual RoboCup competition and find that our approach leads to a similar level of performance. Broadly we show that MARL can be use to train cooperative physical robot behaviors using highly abstract models of the world.
<div id='section'>Paperid: <span id='pid'>601, <a href='https://arxiv.org/pdf/2503.04094.pdf' target='_blank'>https://arxiv.org/pdf/2503.04094.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Seth Karten, Andy Luu Nguyen, Chi Jin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.04094">PokÃ©Champ: an Expert-level Minimax Language Agent</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce PokÃ©Champ, a minimax agent powered by Large Language Models (LLMs) for PokÃ©mon battles. Built on a general framework for two-player competitive games, PokÃ©Champ leverages the generalist capabilities of LLMs to enhance minimax tree search. Specifically, LLMs replace three key modules: (1) player action sampling, (2) opponent modeling, and (3) value function estimation, enabling the agent to effectively utilize gameplay history and human knowledge to reduce the search space and address partial observability. Notably, our framework requires no additional LLM training. We evaluate PokÃ©Champ in the popular Gen 9 OU format. When powered by GPT-4o, it achieves a win rate of 76% against the best existing LLM-based bot and 84% against the strongest rule-based bot, demonstrating its superior performance. Even with an open-source 8-billion-parameter Llama 3.1 model, PokÃ©Champ consistently outperforms the previous best LLM-based bot, PokÃ©llmon powered by GPT-4o, with a 64% win rate. PokÃ©Champ attains a projected Elo of 1300-1500 on the PokÃ©mon Showdown online ladder, placing it among the top 30%-10% of human players. In addition, this work compiles the largest real-player PokÃ©mon battle dataset, featuring over 3 million games, including more than 500k high-Elo matches. Based on this dataset, we establish a series of battle benchmarks and puzzles to evaluate specific battling skills. We further provide key updates to the local game engine. We hope this work fosters further research that leverage PokÃ©mon battle as benchmark to integrate LLM technologies with game-theoretic algorithms addressing general multiagent problems. Videos, code, and dataset available at https://sites.google.com/view/pokechamp-llm.
<div id='section'>Paperid: <span id='pid'>602, <a href='https://arxiv.org/pdf/2503.02189.pdf' target='_blank'>https://arxiv.org/pdf/2503.02189.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dickness Kakitahi Kwesiga, Angshuman Guin, Michael Hunter
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.02189">Adaptive Traffic Signal Control based on Multi-Agent Reinforcement Learning. Case Study on a simulated real-world corridor</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Previous studies that have formulated multi-agent reinforcement learning (RL) algorithms for adaptive traffic signal control have primarily used value-based RL methods. However, recent literature has shown that policy-based methods may perform better in partially observable environments. Additionally, RL methods remain largely untested for real-world normally signal timing plans because of the simplifying assumptions common in the literature. The current study attempts to address these gaps and formulates a multi-agent proximal policy optimization (MA-PPO) algorithm to implement adaptive and coordinated traffic control along an arterial corridor. The formulated MA-PPO has a centralized-critic architecture under a centralized training and decentralized execution framework. Agents are designed to allow selection and implementation of up to eight signal phases, as commonly implemented in field controllers. The formulated algorithm is tested on a simulated real-world seven intersection corridor. The speed of convergence for each agent was found to depend on the size of the action space, which depends on the number and sequence of signal phases. The performance of the formulated MA-PPO adaptive control algorithm is compared with the field implemented actuated-coordinated signal control (ASC), modeled using PTV-Vissim-MaxTime software in the loop simulation (SILs). The trained MA-PPO performed significantly better than the ASC for all movements. Compared to ASC the MA-PPO showed 2% and 24% improvements in travel time in the primary and secondary coordination directions, respectively. For cross streets movements MA-PPO also showed significant crossing time reductions. Volume sensitivity experiments revealed that the formulated MA-PPO demonstrated good stability, robustness, and adaptability to changes in traffic demand.
<div id='section'>Paperid: <span id='pid'>603, <a href='https://arxiv.org/pdf/2503.00372.pdf' target='_blank'>https://arxiv.org/pdf/2503.00372.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yugu Li, Zehong Cao, Jianglin Qiao, Siyi Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.00372">Nucleolus Credit Assignment for Effective Coalitions in Multi-agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In cooperative multi-agent reinforcement learning (MARL), agents typically form a single grand coalition based on credit assignment to tackle a composite task, often resulting in suboptimal performance. This paper proposed a nucleolus-based credit assignment grounded in cooperative game theory, enabling the autonomous partitioning of agents into multiple small coalitions that can effectively identify and complete subtasks within a larger composite task. Specifically, our designed nucleolus Q-learning could assign fair credits to each agent, and the nucleolus Q-operator provides theoretical guarantees with interpretability for both learning convergence and the stability of the formed small coalitions. Through experiments on Predator-Prey and StarCraft scenarios across varying difficulty levels, our approach demonstrated the emergence of multiple effective coalitions during MARL training, leading to faster learning and superior performance in terms of win rate and cumulative rewards especially in hard and super-hard environments, compared to four baseline methods. Our nucleolus-based credit assignment showed the promise for complex composite tasks requiring effective subteams of agents.
<div id='section'>Paperid: <span id='pid'>604, <a href='https://arxiv.org/pdf/2502.12605.pdf' target='_blank'>https://arxiv.org/pdf/2502.12605.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kyeonghyeon Park, David Molina Concha, Hyun-Rok Lee, Chi-Guhn Lee, Taesik Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.12605">Hypernetwork-based approach for optimal composition design in partially controlled multi-agent systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Partially Controlled Multi-Agent Systems (PCMAS) are comprised of controllable agents, managed by a system designer, and uncontrollable agents, operating autonomously. This study addresses an optimal composition design problem in PCMAS, which involves the system designer's problem, determining the optimal number and policies of controllable agents, and the uncontrollable agents' problem, identifying their best-response policies. Solving this bi-level optimization problem is computationally intensive, as it requires repeatedly solving multi-agent reinforcement learning problems under various compositions for both types of agents. To address these challenges, we propose a novel hypernetwork-based framework that jointly optimizes the system's composition and agent policies. Unlike traditional methods that train separate policy networks for each composition, the proposed framework generates policies for both controllable and uncontrollable agents through a unified hypernetwork. This approach enables efficient information sharing across similar configurations, thereby reducing computational overhead. Additional improvements are achieved by incorporating reward parameter optimization and mean action networks. Using real-world New York City taxi data, we demonstrate that our framework outperforms existing methods in approximating equilibrium policies. Our experimental results show significant improvements in key performance metrics, such as order response rate and served demand, highlighting the practical utility of controlling agents and their potential to enhance decision-making in PCMAS.
<div id='section'>Paperid: <span id='pid'>605, <a href='https://arxiv.org/pdf/2502.08681.pdf' target='_blank'>https://arxiv.org/pdf/2502.08681.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Barbera de Mol, Davide Barbieri, Jan Viebahn, Davide Grossi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.08681">Centrally Coordinated Multi-Agent Reinforcement Learning for Power Grid Topology Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Power grid operation is becoming more complex due to the increase in generation of renewable energy. The recent series of Learning To Run a Power Network (L2RPN) competitions have encouraged the use of artificial agents to assist human dispatchers in operating power grids. However, the combinatorial nature of the action space poses a challenge to both conventional optimizers and learned controllers. Action space factorization, which breaks down decision-making into smaller sub-tasks, is one approach to tackle the curse of dimensionality. In this study, we propose a centrally coordinated multi-agent (CCMA) architecture for action space factorization. In this approach, regional agents propose actions and subsequently a coordinating agent selects the final action. We investigate several implementations of the CCMA architecture, and benchmark in different experimental settings against various L2RPN baseline approaches. The CCMA architecture exhibits higher sample efficiency and superior final performance than the baseline approaches. The results suggest high potential of the CCMA approach for further application in higher-dimensional L2RPN as well as real-world power grid settings.
<div id='section'>Paperid: <span id='pid'>606, <a href='https://arxiv.org/pdf/2502.04773.pdf' target='_blank'>https://arxiv.org/pdf/2502.04773.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>George Papadopoulos, Andreas Kontogiannis, Foteini Papadopoulou, Chaido Poulianou, Ioannis Koumentis, George Vouros
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.04773">An Extended Benchmarking of Multi-Agent Reinforcement Learning Algorithms in Complex Fully Cooperative Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-Agent Reinforcement Learning (MARL) has recently emerged as a significant area of research. However, MARL evaluation often lacks systematic diversity, hindering a comprehensive understanding of algorithms' capabilities. In particular, cooperative MARL algorithms are predominantly evaluated on benchmarks such as SMAC and GRF, which primarily feature team game scenarios without assessing adequately various aspects of agents' capabilities required in fully cooperative real-world tasks such as multi-robot cooperation and warehouse, resource management, search and rescue, and human-AI cooperation. Moreover, MARL algorithms are mainly evaluated on low dimensional state spaces, and thus their performance on high-dimensional (e.g., image) observations is not well-studied. To fill this gap, this paper highlights the crucial need for expanding systematic evaluation across a wider array of existing benchmarks. To this end, we conduct extensive evaluation and comparisons of well-known MARL algorithms on complex fully cooperative benchmarks, including tasks with images as agents' observations. Interestingly, our analysis shows that many algorithms, hailed as state-of-the-art on SMAC and GRF, may underperform standard MARL baselines on fully cooperative benchmarks. Finally, towards more systematic and better evaluation of cooperative MARL algorithms, we have open-sourced PyMARLzoo+, an extension of the widely used (E)PyMARL libraries, which addresses an open challenge from [TBG++21], facilitating seamless integration and support with all benchmarks of PettingZoo, as well as Overcooked, PressurePlate, Capture Target and Box Pushing.
<div id='section'>Paperid: <span id='pid'>607, <a href='https://arxiv.org/pdf/2502.03506.pdf' target='_blank'>https://arxiv.org/pdf/2502.03506.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruoning Zhang, Siying Wang, Wenyu Chen, Yang Zhou, Zhitong Zhao, Zixuan Zhang, Ruijie Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.03506">Optimistic Îµ-Greedy Exploration for Cooperative Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The Centralized Training with Decentralized Execution (CTDE) paradigm is widely used in cooperative multi-agent reinforcement learning. However, due to the representational limitations of traditional monotonic value decomposition methods, algorithms can underestimate optimal actions, leading policies to suboptimal solutions. To address this challenge, we propose Optimistic $Îµ$-Greedy Exploration, focusing on enhancing exploration to correct value estimations. The underestimation arises from insufficient sampling of optimal actions during exploration, as our analysis indicated. We introduce an optimistic updating network to identify optimal actions and sample actions from its distribution with a probability of $Îµ$ during exploration, increasing the selection frequency of optimal actions. Experimental results in various environments reveal that the Optimistic $Îµ$-Greedy Exploration effectively prevents the algorithm from suboptimal solutions and significantly improves its performance compared to other algorithms.
<div id='section'>Paperid: <span id='pid'>608, <a href='https://arxiv.org/pdf/2502.03125.pdf' target='_blank'>https://arxiv.org/pdf/2502.03125.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yang Zhou, Siying Wang, Wenyu Chen, Ruoning Zhang, Zhitong Zhao, Zixuan Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.03125">Double Distillation Network for Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent reinforcement learning typically employs a centralized training-decentralized execution (CTDE) framework to alleviate the non-stationarity in environment. However, the partial observability during execution may lead to cumulative gap errors gathered by agents, impairing the training of effective collaborative policies. To overcome this challenge, we introduce the Double Distillation Network (DDN), which incorporates two distillation modules aimed at enhancing robust coordination and facilitating the collaboration process under constrained information. The external distillation module uses a global guiding network and a local policy network, employing distillation to reconcile the gap between global training and local execution. In addition, the internal distillation module introduces intrinsic rewards, drawn from state information, to enhance the exploration capabilities of agents. Extensive experiments demonstrate that DDN significantly improves performance across multiple scenarios.
<div id='section'>Paperid: <span id='pid'>609, <a href='https://arxiv.org/pdf/2502.02875.pdf' target='_blank'>https://arxiv.org/pdf/2502.02875.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Siying Wang, Yang Zhou, Zhitong Zhao, Ruoning Zhang, Jinliang Shao, Wenyu Chen, Yuhua Cheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.02875">Heterogeneous Value Decomposition Policy Fusion for Multi-Agent Cooperation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Value decomposition (VD) has become one of the most prominent solutions in cooperative multi-agent reinforcement learning. Most existing methods generally explore how to factorize the joint value and minimize the discrepancies between agent observations and characteristics of environmental states. However, direct decomposition may result in limited representation or difficulty in optimization. Orthogonal to designing a new factorization scheme, in this paper, we propose Heterogeneous Policy Fusion (HPF) to integrate the strengths of various VD methods. We construct a composite policy set to select policies for interaction adaptively. Specifically, this adaptive mechanism allows agents' trajectories to benefit from diverse policy transitions while incorporating the advantages of each factorization method. Additionally, HPF introduces a constraint between these heterogeneous policies to rectify the misleading update caused by the unexpected exploratory or suboptimal non-cooperation. Experimental results on cooperative tasks show HPF's superior performance over multiple baselines, proving its effectiveness and ease of implementation.
<div id='section'>Paperid: <span id='pid'>610, <a href='https://arxiv.org/pdf/2502.01971.pdf' target='_blank'>https://arxiv.org/pdf/2502.01971.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianyu Ren, Xuan Yao, Yang Li, Xiao-Jun Zeng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.01971">Bottom-Up Reputation Promotes Cooperation with Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reputation serves as a powerful mechanism for promoting cooperation in multi-agent systems, as agents are more inclined to cooperate with those of good social standing. While existing multi-agent reinforcement learning methods typically rely on predefined social norms to assign reputations, the question of how a population reaches a consensus on judgement when agents hold private, independent views remains unresolved. In this paper, we propose a novel bottom-up reputation learning method, Learning with Reputation Reward (LR2), designed to promote cooperative behaviour through rewards shaping based on assigned reputation. Our agent architecture includes a dilemma policy that determines cooperation by considering the impact on neighbours, and an evaluation policy that assigns reputations to affect the actions of neighbours while optimizing self-objectives. It operates using local observations and interaction-based rewards, without relying on centralized modules or predefined norms. Our findings demonstrate the effectiveness and adaptability of LR2 across various spatial social dilemma scenarios. Interestingly, we find that LR2 stabilizes and enhances cooperation not only with reward reshaping from bottom-up reputation but also by fostering strategy clustering in structured populations, thereby creating environments conducive to sustained cooperation.
<div id='section'>Paperid: <span id='pid'>611, <a href='https://arxiv.org/pdf/2502.01568.pdf' target='_blank'>https://arxiv.org/pdf/2502.01568.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Benjamin A. Spiegel, Lucas Gelfond, George Konidaris
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.01568">Visual Theory of Mind Enables the Invention of Proto-Writing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Symbolic writing systems are graphical semiotic codes that are ubiquitous in modern society but are otherwise absent in the animal kingdom. Anthropological evidence suggests that the earliest forms of some writing systems originally consisted of iconic pictographs, which signify their referent via visual resemblance. While previous studies have examined the emergence and, separately, the evolution of pictographic systems through a computational lens, most employ non-naturalistic methodologies that make it difficult to draw clear analogies to human and animal cognition. We develop a multi-agent reinforcement learning testbed for emergent communication called a Signification Game, and formulate a model of inferential communication that enables agents to leverage visual theory of mind to communicate actions using pictographs. Our model, which is situated within a broader formalism for animal communication, sheds light on the cognitive and cultural processes underlying the emergence of proto-writing.
<div id='section'>Paperid: <span id='pid'>612, <a href='https://arxiv.org/pdf/2501.10367.pdf' target='_blank'>https://arxiv.org/pdf/2501.10367.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mengxian Li, Qi Wang, Yongjun Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.10367">GTDE: Grouped Training with Decentralized Execution for Multi-agent Actor-Critic</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid advancement of multi-agent reinforcement learning (MARL) has given rise to diverse training paradigms to learn the policies of each agent in the multi-agent system. The paradigms of decentralized training and execution (DTDE) and centralized training with decentralized execution (CTDE) have been proposed and widely applied. However, as the number of agents increases, the inherent limitations of these frameworks significantly degrade the performance metrics, such as win rate, total reward, etc. To reduce the influence of the increasing number of agents on the performance metrics, we propose a novel training paradigm of grouped training decentralized execution (GTDE). This framework eliminates the need for a centralized module and relies solely on local information, effectively meeting the training requirements of large-scale multi-agent systems. Specifically, we first introduce an adaptive grouping module, which divides each agent into different groups based on their observation history. To implement end-to-end training, GTDE uses Gumbel-Sigmoid for efficient point-to-point sampling on the grouping distribution while ensuring gradient backpropagation. To adapt to the uncertainty in the number of members in a group, two methods are used to implement a group information aggregation module that merges member information within the group. Empirical results show that in a cooperative environment with 495 agents, GTDE increased the total reward by an average of 382\% compared to the baseline. In a competitive environment with 64 agents, GTDE achieved a 100\% win rate against the baseline.
<div id='section'>Paperid: <span id='pid'>613, <a href='https://arxiv.org/pdf/2412.20475.pdf' target='_blank'>https://arxiv.org/pdf/2412.20475.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sheng Cen, Qiying Pan, Yifei Zhu, Bo Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.20475">SatFlow: Scalable Network Planning for LEO Mega-Constellations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Low-earth-orbit (LEO) satellite communication networks have evolved into mega-constellations with hundreds to thousands of satellites inter-connecting with inter-satellite links (ISLs). Network planning, which plans for network resources and architecture to improve the network performance and save operational costs, is crucial for satellite network management. However, due to the large scale of mega-constellations, high dynamics of satellites, and complex distribution of real-world traffic, it is extremely challenging to conduct scalable network planning on mega-constellations with high performance. In this paper, we propose SatFlow, a distributed and hierarchical network planning framework to plan for the network topology, traffic allocation, and fine-grained ISL terminal power allocation for mega-constellations. To tackle the hardness of the original problem, we decompose the grand problem into two hierarchical sub-problems, tackled by two-tier modules. A multi-agent reinforcement learning approach is proposed for the upper-level module so that the overall laser energy consumption and ISL operational costs can be minimized; A distributed alternating step algorithm is proposed for the lower-level module so that the laser energy consumption could be minimized with low time complexity for a given topology. Extensive simulations on various mega-constellations validate SatFlow's scalability on the constellation size, reducing the flow violation ratio by up to 21.0% and reducing the total costs by up to 89.4%, compared with various state-of-the-art benchmarks.
<div id='section'>Paperid: <span id='pid'>614, <a href='https://arxiv.org/pdf/2412.15700.pdf' target='_blank'>https://arxiv.org/pdf/2412.15700.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Guangchong Zhou, Zeren Zhang, Guoliang Fan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.15700">AIR: Unifying Individual and Collective Exploration in Cooperative Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Exploration in cooperative multi-agent reinforcement learning (MARL) remains challenging for value-based agents due to the absence of an explicit policy. Existing approaches include individual exploration based on uncertainty towards the system and collective exploration through behavioral diversity among agents. However, the introduction of additional structures often leads to reduced training efficiency and infeasible integration of these methods. In this paper, we propose Adaptive exploration via Identity Recognition~(AIR), which consists of two adversarial components: a classifier that recognizes agent identities from their trajectories, and an action selector that adaptively adjusts the mode and degree of exploration. We theoretically prove that AIR can facilitate both individual and collective exploration during training, and experiments also demonstrate the efficiency and effectiveness of AIR across various tasks.
<div id='section'>Paperid: <span id='pid'>615, <a href='https://arxiv.org/pdf/2412.15388.pdf' target='_blank'>https://arxiv.org/pdf/2412.15388.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sharlin Utke, Jeremie Houssineau, Giovanni Montana
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.15388">Investigating Relational State Abstraction in Collaborative MARL</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper explores the impact of relational state abstraction on sample efficiency and performance in collaborative Multi-Agent Reinforcement Learning. The proposed abstraction is based on spatial relationships in environments where direct communication between agents is not allowed, leveraging the ubiquity of spatial reasoning in real-world multi-agent scenarios. We introduce MARC (Multi-Agent Relational Critic), a simple yet effective critic architecture incorporating spatial relational inductive biases by transforming the state into a spatial graph and processing it through a relational graph neural network. The performance of MARC is evaluated across six collaborative tasks, including a novel environment with heterogeneous agents. We conduct a comprehensive empirical analysis, comparing MARC against state-of-the-art MARL baselines, demonstrating improvements in both sample efficiency and asymptotic performance, as well as its potential for generalization. Our findings suggest that a minimal integration of spatial relational inductive biases as abstraction can yield substantial benefits without requiring complex designs or task-specific engineering. This work provides insights into the potential of relational state abstraction to address sample efficiency, a key challenge in MARL, offering a promising direction for developing more efficient algorithms in spatially complex environments.
<div id='section'>Paperid: <span id='pid'>616, <a href='https://arxiv.org/pdf/2411.19359.pdf' target='_blank'>https://arxiv.org/pdf/2411.19359.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dickness Kakitahi Kwesiga, Suyash Chandra Vishnoi, Angshuman Guin, Michael Hunter
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.19359">Integrating Transit Signal Priority into Multi-Agent Reinforcement Learning based Traffic Signal Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This study integrates Transit Signal Priority (TSP) into multi-agent reinforcement learning (MARL) based traffic signal control. The first part of the study develops adaptive signal control based on MARL for a pair of coordinated intersections in a microscopic simulation environment. The two agents, one for each intersection, are centrally trained using a value decomposition network (VDN) architecture. The trained agents show slightly better performance compared to coordinated actuated signal control based on overall intersection delay at v/c of 0.95. In the second part of the study the trained signal control agents are used as background signal controllers while developing event-based TSP agents. In one variation, independent TSP agents are formulated and trained under a decentralized training and decentralized execution (DTDE) framework to implement TSP at each intersection. In the second variation, the two TSP agents are centrally trained under a centralized training and decentralized execution (CTDE) framework and VDN architecture to select and implement coordinated TSP strategies across the two intersections. In both cases the agents converge to the same bus delay value, but independent agents show high instability throughout the training process. For the test runs, the two independent agents reduce bus delay across the two intersections by 22% compared to the no TSP case while the coordinated TSP agents achieve 27% delay reduction. In both cases, there is only a slight increase in delay for a majority of the side street movements.
<div id='section'>Paperid: <span id='pid'>617, <a href='https://arxiv.org/pdf/2411.06601.pdf' target='_blank'>https://arxiv.org/pdf/2411.06601.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rohit Bokade, Xiaoning Jin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.06601">OffLight: An Offline Multi-Agent Reinforcement Learning Framework for Traffic Signal Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Efficient traffic control (TSC) is essential for urban mobility, but traditional systems struggle to handle the complexity of real-world traffic. Multi-agent Reinforcement Learning (MARL) offers adaptive solutions, but online MARL requires extensive interactions with the environment, making it costly and impractical. Offline MARL mitigates these challenges by using historical traffic data for training but faces significant difficulties with heterogeneous behavior policies in real-world datasets, where mixed-quality data complicates learning. We introduce OffLight, a novel offline MARL framework designed to handle heterogeneous behavior policies in TSC datasets. To improve learning efficiency, OffLight incorporates Importance Sampling (IS) to correct for distributional shifts and Return-Based Prioritized Sampling (RBPS) to focus on high-quality experiences. OffLight utilizes a Gaussian Mixture Variational Graph Autoencoder (GMM-VGAE) to capture the diverse distribution of behavior policies from local observations. Extensive experiments across real-world urban traffic scenarios show that OffLight outperforms existing offline RL methods, achieving up to a 7.8% reduction in average travel time and 11.2% decrease in queue length. Ablation studies confirm the effectiveness of OffLight's components in handling heterogeneous data and improving policy performance. These results highlight OffLight's scalability and potential to improve urban traffic management without the risks of online learning.
<div id='section'>Paperid: <span id='pid'>618, <a href='https://arxiv.org/pdf/2410.18202.pdf' target='_blank'>https://arxiv.org/pdf/2410.18202.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rohit Bokade, Xiaoning Jin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.18202">PyTSC: A Unified Platform for Multi-Agent Reinforcement Learning in Traffic Signal Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-Agent Reinforcement Learning (MARL) presents a promising approach for addressing the complexity of Traffic Signal Control (TSC) in urban environments. However, existing platforms for MARL-based TSC research face challenges such as slow simulation speeds and convoluted, difficult-to-maintain codebases. To address these limitations, we introduce PyTSC, a robust and flexible simulation environment that facilitates the training and evaluation of MARL algorithms for TSC. PyTSC integrates multiple simulators, such as SUMO and CityFlow, and offers a streamlined API, empowering researchers to explore a broad spectrum of MARL approaches efficiently. PyTSC accelerates experimentation and provides new opportunities for advancing intelligent traffic management systems in real-world applications.
<div id='section'>Paperid: <span id='pid'>619, <a href='https://arxiv.org/pdf/2410.17373.pdf' target='_blank'>https://arxiv.org/pdf/2410.17373.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dongsu Lee, Minhae Kwon
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.17373">Episodic Future Thinking Mechanism for Multi-agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding cognitive processes in multi-agent interactions is a primary goal in cognitive science. It can guide the direction of artificial intelligence (AI) research toward social decision-making in multi-agent systems, which includes uncertainty from character heterogeneity. In this paper, we introduce an episodic future thinking (EFT) mechanism for a reinforcement learning (RL) agent, inspired by cognitive processes observed in animals. To enable future thinking functionality, we first develop a multi-character policy that captures diverse characters with an ensemble of heterogeneous policies. Here, the character of an agent is defined as a different weight combination on reward components, representing distinct behavioral preferences. The future thinking agent collects observation-action trajectories of the target agents and uses the pre-trained multi-character policy to infer their characters. Once the character is inferred, the agent predicts the upcoming actions of target agents and simulates the potential future scenario. This capability allows the agent to adaptively select the optimal action, considering the predicted future scenario in multi-agent interactions. To evaluate the proposed mechanism, we consider the multi-agent autonomous driving scenario with diverse driving traits and multiple particle environments. Simulation results demonstrate that the EFT mechanism with accurate character inference leads to a higher reward than existing multi-agent solutions. We also confirm that the effect of reward improvement remains valid across societies with different levels of character diversity.
<div id='section'>Paperid: <span id='pid'>620, <a href='https://arxiv.org/pdf/2410.01763.pdf' target='_blank'>https://arxiv.org/pdf/2410.01763.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rebekah A. GelpÃ­, Yikai Tang, Ethan C. Jackson, William A. Cunningham
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.01763">Social coordination perpetuates stereotypic expectations and behaviors across generations in deep multi-agent reinforcement learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite often being perceived as morally objectionable, stereotypes are a common feature of social groups, a phenomenon that has often been attributed to biased motivations or limits on the ability to process information. We argue that one reason for this continued prevalence is that pre-existing expectations about how others will behave, in the context of social coordination, can change the behaviors of one's social partners, creating the very stereotype one expected to see, even in the absence of other potential sources of stereotyping. We use a computational model of dynamic social coordination to illustrate how this "feedback loop" can emerge, engendering and entrenching stereotypic behavior, and then show that human behavior on the task generates a comparable feedback loop. Notably, people's choices on the task are not related to social dominance or system justification, suggesting biased motivations are not necessary to maintain these stereotypes.
<div id='section'>Paperid: <span id='pid'>621, <a href='https://arxiv.org/pdf/2409.13571.pdf' target='_blank'>https://arxiv.org/pdf/2409.13571.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jaeyeon Jang, Diego Klabjan, Han Liu, Nital S. Patel, Xiuqi Li, Balakrishnan Ananthanarayanan, Husam Dauod, Tzung-Han Juang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.13571">Scalable Multi-agent Reinforcement Learning for Factory-wide Dynamic Scheduling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Real-time dynamic scheduling is a crucial but notoriously challenging task in modern manufacturing processes due to its high decision complexity. Recently, reinforcement learning (RL) has been gaining attention as an impactful technique to handle this challenge. However, classical RL methods typically rely on human-made dispatching rules, which are not suitable for large-scale factory-wide scheduling. To bridge this gap, this paper applies a leader-follower multi-agent RL (MARL) concept to obtain desired coordination after decomposing the scheduling problem into a set of sub-problems that are handled by each individual agent for scalability. We further strengthen the procedure by proposing a rule-based conversion algorithm to prevent catastrophic loss of production capacity due to an agent's error. Our experimental results demonstrate that the proposed model outperforms the state-of-the-art deep RL-based scheduling models in various aspects. Additionally, the proposed model provides the most robust scheduling performance to demand changes. Overall, the proposed MARL-based scheduling model presents a promising solution to the real-time scheduling problem, with potential applications in various manufacturing industries.
<div id='section'>Paperid: <span id='pid'>622, <a href='https://arxiv.org/pdf/2601.18284.pdf' target='_blank'>https://arxiv.org/pdf/2601.18284.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hsiao-Chuan Chang, Sheng-You Huang, Yen-Chi Chen, I-Chen Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.18284">VissimRL: A Multi-Agent Reinforcement Learning Framework for Traffic Signal Control Based on Vissim</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Traffic congestion remains a major challenge for urban transportation, leading to significant economic and environmental impacts. Traffic Signal Control (TSC) is one of the key measures to mitigate congestion, and recent studies have increasingly applied Reinforcement Learning (RL) for its adaptive capabilities. With respect to SUMO and CityFlow, the simulator Vissim offers high-fidelity driver behavior modeling and wide industrial adoption but remains underutilized in RL research due to its complex interface and lack of standardized frameworks. To address this gap, this paper proposes VissimRL, a modular RL framework for TSC that encapsulates Vissim's COM interface through a high-level Python API, offering standardized environments for both single- and multi-agent training. Experiments show that VissimRL significantly reduces development effort while maintaining runtime efficiency, and supports consistent improvements in traffic performance during training, as well as emergent coordination in multi-agent control. Overall, VissimRL demonstrates the feasibility of applying RL in high-fidelity simulations and serves as a bridge between academic research and practical applications in intelligent traffic signal control.
<div id='section'>Paperid: <span id='pid'>623, <a href='https://arxiv.org/pdf/2601.17069.pdf' target='_blank'>https://arxiv.org/pdf/2601.17069.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shahil Shaik, Jonathon M. Smereka, Yue Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.17069">Multi-Agent Deep Reinforcement Learning Under Constrained Communications</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Centralized training with decentralized execution (CTDE) has been the dominant paradigm in multi-agent reinforcement learning (MARL), but its reliance on global state information during training introduces scalability, robustness, and generalization bottlenecks. Moreover, in practical scenarios such as adding/dropping teammates or facing environment dynamics that differ from the training, CTDE methods can be brittle and costly to retrain, whereas distributed approaches allow agents to adapt using only local information and peer-to-peer communication. We present a distributed MARL framework that removes the need for centralized critics or global information. Firstly, we develop a novel Distributed Graph Attention Network (D-GAT) that performs global state inference through multi-hop communication, where agents integrate neighbor features via input-dependent attention weights in a fully distributed manner. Leveraging D-GAT, we develop the distributed graph-attention MAPPO (DG-MAPPO) -- a distributed MARL framework where agents optimize local policies and value functions using local observations, multi-hop communication, and shared/averaged rewards. Empirical evaluation on the StarCraftII Multi-Agent Challenge, Google Research Football, and Multi-Agent Mujoco demonstrates that our method consistently outperforms strong CTDE baselines, achieving superior coordination across a wide range of cooperative tasks with both homogeneous and heterogeneous teams. Our distributed MARL framework provides a principled and scalable solution for robust collaboration, eliminating the need for centralized training or global observability. To the best of our knowledge, DG-MAPPO appears to be the first to fully eliminate reliance on privileged centralized information, enabling agents to learn and act solely through peer-to-peer communication.
<div id='section'>Paperid: <span id='pid'>624, <a href='https://arxiv.org/pdf/2601.12028.pdf' target='_blank'>https://arxiv.org/pdf/2601.12028.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kun-Yan Jiang, Wei-Yu Chiu, Yuan-Po Tsai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.12028">Profit Maximization for Electric Vehicle Charging Stations Using Multiagent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Electric vehicles (EVs) are increasingly integrated into power grids, offering economic and environmental benefits but introducing challenges due to uncoordinated charging. This study addresses the profit maximization problem for multiple EV charging stations (EVCSs) equipped with energy storage systems (ESS) and renewable energy sources (RES), with the capability for energy trading. We propose a Double Hypernetwork QMIX-based multi-agent reinforcement learning (MARL) framework to optimize cooperative energy management under uncertainty in EV demand, renewable generation, and real-time electricity prices. The framework mitigates overestimation bias in value estimation, enables distributed decision-making, and incorporates an internal energy trading mechanism. Numerical experiments using real-world data demonstrate that, compared to standard QMIX, the proposed method achieves approximately 5.3% and 12.7% higher total profit for the two regions, respectively, highlighting its economic and operational efficiency. Additionally, the approach maintains robust performance under varying levels of EV demand uncertainty and renewable energy fluctuations.
<div id='section'>Paperid: <span id='pid'>625, <a href='https://arxiv.org/pdf/2601.07152.pdf' target='_blank'>https://arxiv.org/pdf/2601.07152.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aja Khanal, Kaushik T. Ranade, Rishabh Agrawal, Kalyan S. Basu, Apurva Narayan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.07152">Agents of Diffusion: Enhancing Diffusion Language Models with Multi-Agent Reinforcement Learning for Structured Data Generation (Extended Version)</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generating high-quality structured data such as JSON records, remains a fundamental challenge for large language models (LLMs), particularly when semantic richness must coexist with strict schema adherence. While autoregressive LLMs offer strong structural consistency, they often struggle with semantic variation and output diversity. In contrast, diffusion language models (DLMs) introduce powerful mechanisms for semantic richness and bidirectional decoding, yet lack the inductive biases needed for reliable structure preservation. We present Agents of Diffusion (AoD), a novel framework that unifies the generative flexibility of DLMs with the reasoning capabilities of autoregressive models through language-mediated reinforcement learning. AoD frames structured text generation as a multi-agent alignment process, where a prompt optimization agent collaborates with a judge agent to iteratively guide a DLM using natural language feedback. This approach enables controllable, schema-consistent generation without modifying model parameters or relying on handcrafted constraints. AoD advances the state of controllable generation by demonstrating that diffusion models, when supervised by cooperative agents, can achieve both high semantic novelty and structural fidelity. Across multiple structured data benchmarks, AoD consistently outperforms diffusion and autoregressive baselines, establishing a new path forward for structure-aware, diversity-enhanced text synthesis.
<div id='section'>Paperid: <span id='pid'>626, <a href='https://arxiv.org/pdf/2512.22876.pdf' target='_blank'>https://arxiv.org/pdf/2512.22876.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Maksim Kryzhanovskiy, Svetlana Glazyrina, Roman Ischenko, Konstantin Vorontsov
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.22876">Reinforcement Networks: novel framework for collaborative Multi-Agent Reinforcement Learning tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Modern AI systems often comprise multiple learnable components that can be naturally organized as graphs. A central challenge is the end-to-end training of such systems without restrictive architectural or training assumptions. Such tasks fit the theory and approaches of the collaborative Multi-Agent Reinforcement Learning (MARL) field. We introduce Reinforcement Networks, a general framework for MARL that organizes agents as vertices in a directed acyclic graph (DAG). This structure extends hierarchical RL to arbitrary DAGs, enabling flexible credit assignment and scalable coordination while avoiding strict topologies, fully centralized training, and other limitations of current approaches. We formalize training and inference methods for the Reinforcement Networks framework and connect it to the LevelEnv concept to support reproducible construction, training, and evaluation. We demonstrate the effectiveness of our approach on several collaborative MARL setups by developing several Reinforcement Networks models that achieve improved performance over standard MARL baselines. Beyond empirical gains, Reinforcement Networks unify hierarchical, modular, and graph-structured views of MARL, opening a principled path toward designing and training complex multi-agent systems. We conclude with theoretical and practical directions - richer graph morphologies, compositional curricula, and graph-aware exploration. That positions Reinforcement Networks as a foundation for a new line of research in scalable, structured MARL.
<div id='section'>Paperid: <span id='pid'>627, <a href='https://arxiv.org/pdf/2512.22381.pdf' target='_blank'>https://arxiv.org/pdf/2512.22381.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mohammad Zakaria Haider, Amit Kumar Podder, Prabin Mali, Aranya Chakrabortty, Sumit Paudyal, Mohammad Ashiqur Rahman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.22381">PHANTOM: Physics-Aware Adversarial Attacks against Federated Learning-Coordinated EV Charging Management System</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid deployment of electric vehicle charging stations (EVCS) within distribution networks necessitates intelligent and adaptive control to maintain the grid's resilience and reliability. In this work, we propose PHANTOM, a physics-aware adversarial network that is trained and optimized through a multi-agent reinforcement learning model. PHANTOM integrates a physics-informed neural network (PINN) enabled by federated learning (FL) that functions as a digital twin of EVCS-integrated systems, ensuring physically consistent modeling of operational dynamics and constraints. Building on this digital twin, we construct a multi-agent RL environment that utilizes deep Q-networks (DQN) and soft actor-critic (SAC) methods to derive adversarial false data injection (FDI) strategies capable of bypassing conventional detection mechanisms. To examine the broader grid-level consequences, a transmission and distribution (T and D) dual simulation platform is developed, allowing us to capture cascading interactions between EVCS disturbances at the distribution level and the operations of the bulk transmission system. Results demonstrate how learned attack policies disrupt load balancing and induce voltage instabilities that propagate across T and D boundaries. These findings highlight the critical need for physics-aware cybersecurity to ensure the resilience of large-scale vehicle-grid integration.
<div id='section'>Paperid: <span id='pid'>628, <a href='https://arxiv.org/pdf/2512.18558.pdf' target='_blank'>https://arxiv.org/pdf/2512.18558.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuwei Pei, Joran Borger, Arda Kosay, Muhammed O. Sayin, Saeed Ahmed
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.18558">Distributionally Robust Multi-Agent Reinforcement Learning for Intelligent Traffic Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Learning-based traffic signal control is typically optimized for average performance under a few nominal demand patterns, which can result in poor behavior under atypical traffic conditions. To address this, we develop a distributionally robust multi-agent reinforcement learning framework for signal control on a 3x3 urban grid calibrated from a contiguous 3x3 subarea of central Athens covered by the pNEUMA trajectory dataset (Barmpounakis and Geroliminis, 2020). Our approach proceeds in three stages. First, we train a baseline multi-agent RL controller in which each intersection is governed by a proximal policy optimization agent with discrete signal phases, using a centralized training, decentralized execution paradigm. Second, to capture demand uncertainty, we construct eight heterogeneous origin-destination-based traffic scenarios-one directly derived from pNEUMA and seven synthetically generated-to span a wide range of spatial and temporal demand patterns. Over this scenario set, we train a contextual-bandit worst-case estimator that assigns mixture weights to estimate adversarial demand distributions conditioned on context. Finally, without modifying the controller architecture, we fine-tune the baseline multi-agent reinforcement learning agents under these estimated worst-case mixtures to obtain a distributionally robust multi-agent reinforcement learning controller. Across all eight scenarios, as well as on an unseen validation network based on the Sioux Falls configuration, the distributionally robust multi-agent reinforcement learning controller consistently reduces horizon-averaged queues and increases average speeds relative to the baseline, achieving up to 51% shorter queues and 38% higher speeds on the worst-performing scenarios.
<div id='section'>Paperid: <span id='pid'>629, <a href='https://arxiv.org/pdf/2512.18309.pdf' target='_blank'>https://arxiv.org/pdf/2512.18309.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Harsh Rathva, Ojas Srivastava, Pruthwik Mishra
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.18309">Embedded Safety-Aligned Intelligence via Differentiable Internal Alignment Embeddings</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce Embedded Safety-Aligned Intelligence (ESAI), a theoretical framework for multi-agent reinforcement learning that embeds alignment constraints directly into agents internal representations using differentiable internal alignment embeddings. Unlike external reward shaping or post-hoc safety constraints, internal alignment embeddings are learned latent variables that predict externalized harm through counterfactual reasoning and modulate policy updates toward harm reduction through attention and graph-based propagation. The ESAI framework integrates four mechanisms: differentiable counterfactual alignment penalties computed from soft reference distributions, alignment-weighted perceptual attention, Hebbian associative memory supporting temporal credit assignment, and similarity-weighted graph diffusion with bias mitigation controls. We analyze stability conditions for bounded internal embeddings under Lipschitz continuity and spectral constraints, discuss computational complexity, and examine theoretical properties including contraction behavior and fairness-performance tradeoffs. This work positions ESAI as a conceptual contribution to differentiable alignment mechanisms in multi-agent systems. We identify open theoretical questions regarding convergence guarantees, embedding dimensionality, and extension to high-dimensional environments. Empirical evaluation is left to future work.
<div id='section'>Paperid: <span id='pid'>630, <a href='https://arxiv.org/pdf/2512.10439.pdf' target='_blank'>https://arxiv.org/pdf/2512.10439.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Niccolò Grillo, James Rowbottom, Pietro Liò, Carola Bibiane Schönlieb, Stefania Fresca
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.10439">HypeR Adaptivity: Joint $hr$-Adaptive Meshing via Hypergraph Multi-Agent Deep Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Adaptive mesh refinement is central to the efficient solution of partial differential equations (PDEs) via the finite element method (FEM). Classical $r$-adaptivity optimizes vertex positions but requires solving expensive auxiliary PDEs such as the Monge-Ampère equation, while classical $h$-adaptivity modifies topology through element subdivision but suffers from expensive error indicator computation and is constrained by isotropic refinement patterns that impose accuracy ceilings. Combined $hr$-adaptive techniques naturally outperform single-modality approaches, yet inherit both computational bottlenecks and the restricted cost-accuracy trade-off. Emerging machine learning methods for adaptive mesh refinement seek to overcome these limitations, but existing approaches address $h$-adaptivity or $r$-adaptivity in isolation. We present HypeR, a deep reinforcement learning framework that jointly optimizes mesh relocation and refinement. HypeR casts the joint adaptation problem using tools from hypergraph neural networks and multi-agent reinforcement learning. Refinement is formulated as a heterogeneous multi-agent Markov decision process (MDP) where element agents decide discrete refinement actions, while relocation follows an anisotropic diffusion-based policy on vertex agents with provable prevention of mesh tangling. The reward function combines local and global error reduction to promote general accuracy. Across benchmark PDEs, HypeR reduces approximation error by up to 6--10$\times$ versus state-of-art $h$-adaptive baselines at comparable element counts, breaking through the uniform refinement accuracy ceiling that constrains subdivision-only methods. The framework produces meshes with improved shape metrics and alignment to solution anisotropy, demonstrating that jointly learned $hr$-adaptivity strategies can substantially enhance the capabilities of automated mesh generation.
<div id='section'>Paperid: <span id='pid'>631, <a href='https://arxiv.org/pdf/2512.07588.pdf' target='_blank'>https://arxiv.org/pdf/2512.07588.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>James Rudd-Jones, María Pérez-Ortiz, Mirco Musolesi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.07588">Understanding Individual Decision-Making in Multi-Agent Reinforcement Learning: A Dynamical Systems Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Analysing learning behaviour in Multi-Agent Reinforcement Learning (MARL) environments is challenging, in particular with respect to \textit{individual} decision-making. Practitioners frequently tend to study or compare MARL algorithms from a qualitative perspective largely due to the inherent stochasticity in practical algorithms arising from random dithering exploration strategies, environment transition noise, and stochastic gradient updates to name a few. Traditional analytical approaches, such as replicator dynamics, often rely on mean-field approximations to remove stochastic effects, but this simplification, whilst able to provide general overall trends, might lead to dissonance between analytical predictions and actual realisations of individual trajectories. In this paper, we propose a novel perspective on MARL systems by modelling them as \textit{coupled stochastic dynamical systems}, capturing both agent interactions and environmental characteristics. Leveraging tools from dynamical systems theory, we analyse the stability and sensitivity of agent behaviour at individual level, which are key dimensions for their practical deployments, for example, in presence of strict safety requirements. This framework allows us, for the first time, to rigorously study MARL dynamics taking into consideration their inherent stochasticity, providing a deeper understanding of system behaviour and practical insights for the design and control of multi-agent learning processes.
<div id='section'>Paperid: <span id='pid'>632, <a href='https://arxiv.org/pdf/2511.07707.pdf' target='_blank'>https://arxiv.org/pdf/2511.07707.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Manonmani Sekar, Nasim Nezamoddini
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.07707">A Negotiation-Based Multi-Agent Reinforcement Learning Approach for Dynamic Scheduling of Reconfigurable Manufacturing Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reconfigurable manufacturing systems (RMS) are critical for future market adjustment given their rapid adaptation to fluctuations in consumer demands, the introduction of new technological advances, and disruptions in linked supply chain sections. The adjustable hard settings of such systems require a flexible soft planning mechanism that enables realtime production planning and scheduling amid the existing complexity and variability in their configuration settings. This study explores the application of multi agent reinforcement learning (MARL) for dynamic scheduling in soft planning of the RMS settings. In the proposed framework, deep Qnetwork (DQN) agents trained in centralized training learn optimal job machine assignments in real time while adapting to stochastic events such as machine breakdowns and reconfiguration delays. The model also incorporates a negotiation with an attention mechanism to enhance state representation and improve decision focus on critical system features. Key DQN enhancements including prioritized experience replay, nstep returns, double DQN and soft target update are used to stabilize and accelerate learning. Experiments conducted in a simulated RMS environment demonstrate that the proposed approach outperforms baseline heuristics in reducing makespan and tardiness while improving machine utilization. The reconfigurable manufacturing environment was extended to simulate realistic challenges, including machine failures and reconfiguration times. Experimental results show that while the enhanced DQN agent is effective in adapting to dynamic conditions, machine breakdowns increase variability in key performance metrics such as makespan, throughput, and total tardiness. The results confirm the advantages of applying the MARL mechanism for intelligent and adaptive scheduling in dynamic reconfigurable manufacturing environments.
<div id='section'>Paperid: <span id='pid'>633, <a href='https://arxiv.org/pdf/2511.04904.pdf' target='_blank'>https://arxiv.org/pdf/2511.04904.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bassel Al Omari, Michael Matthews, Alexander Rutherford, Jakob Nicolaus Foerster
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.04904">Multi-Agent Craftax: Benchmarking Open-Ended Multi-Agent Reinforcement Learning at the Hyperscale</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Progress in multi-agent reinforcement learning (MARL) requires challenging benchmarks that assess the limits of current methods. However, existing benchmarks often target narrow short-horizon challenges that do not adequately stress the long-term dependencies and generalization capabilities inherent in many multi-agent systems. To address this, we first present \textit{Craftax-MA}: an extension of the popular open-ended RL environment, Craftax, that supports multiple agents and evaluates a wide range of general abilities within a single environment. Written in JAX, \textit{Craftax-MA} is exceptionally fast with a training run using 250 million environment interactions completing in under an hour. To provide a more compelling challenge for MARL, we also present \textit{Craftax-Coop}, an extension introducing heterogeneous agents, trading and more mechanics that require complex cooperation among agents for success. We provide analysis demonstrating that existing algorithms struggle with key challenges in this benchmark, including long-horizon credit assignment, exploration and cooperation, and argue for its potential to drive long-term research in MARL.
<div id='section'>Paperid: <span id='pid'>634, <a href='https://arxiv.org/pdf/2511.03676.pdf' target='_blank'>https://arxiv.org/pdf/2511.03676.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Taito Tashiro, Tomoko Yonezawa, Hirotake Yamazoe
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.03676">Unconscious and Intentional Human Motion Cues for Expressive Robot-Arm Motion Design</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This study investigates how human motion cues can be used to design expressive robot-arm movements. Using the imperfect-information game Geister, we analyzed two types of human piece-moving motions: natural gameplay (unconscious tendencies) and instructed expressions (intentional cues). Based on these findings, we created phase-specific robot motions by varying movement speed and stop duration, and evaluated observer impressions under two presentation modalities: a physical robot and a recorded video. Results indicate that late-phase motion timing, particularly during withdrawal, plays an important role in impression formation and that physical embodiment enhances the interpretability of motion cues. These findings provide insights for designing expressive robot motions based on human timing behavior.
<div id='section'>Paperid: <span id='pid'>635, <a href='https://arxiv.org/pdf/2510.27659.pdf' target='_blank'>https://arxiv.org/pdf/2510.27659.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alireza Saleh Abadi, Leen-Kiat Soh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.27659">Challenges in Credit Assignment for Multi-Agent Reinforcement Learning in Open Agent Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the rapidly evolving field of multi-agent reinforcement learning (MARL), understanding the dynamics of open systems is crucial. Openness in MARL refers to the dynam-ic nature of agent populations, tasks, and agent types with-in a system. Specifically, there are three types of openness as reported in (Eck et al. 2023) [2]: agent openness, where agents can enter or leave the system at any time; task openness, where new tasks emerge, and existing ones evolve or disappear; and type openness, where the capabil-ities and behaviors of agents change over time. This report provides a conceptual and empirical review, focusing on the interplay between openness and the credit assignment problem (CAP). CAP involves determining the contribution of individual agents to the overall system performance, a task that becomes increasingly complex in open environ-ments. Traditional credit assignment (CA) methods often assume static agent populations, fixed and pre-defined tasks, and stationary types, making them inadequate for open systems. We first conduct a conceptual analysis, in-troducing new sub-categories of openness to detail how events like agent turnover or task cancellation break the assumptions of environmental stationarity and fixed team composition that underpin existing CAP methods. We then present an empirical study using representative temporal and structural algorithms in an open environment. The results demonstrate that openness directly causes credit misattribution, evidenced by unstable loss functions and significant performance degradation.
<div id='section'>Paperid: <span id='pid'>636, <a href='https://arxiv.org/pdf/2510.26389.pdf' target='_blank'>https://arxiv.org/pdf/2510.26389.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenchang Duan, Yaoliang Yu, Jiwan He, Yi Shi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.26389">Adaptive Context Length Optimization with Low-Frequency Truncation for Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, deep multi-agent reinforcement learning (MARL) has demonstrated promising performance for solving challenging tasks, such as long-term dependencies and non-Markovian environments. Its success is partly attributed to conditioning policies on large fixed context length. However, such large fixed context lengths may lead to limited exploration efficiency and redundant information. In this paper, we propose a novel MARL framework to obtain adaptive and effective contextual information. Specifically, we design a central agent that dynamically optimizes context length via temporal gradient analysis, enhancing exploration to facilitate convergence to global optima in MARL. Furthermore, to enhance the adaptive optimization capability of the context length, we present an efficient input representation for the central agent, which effectively filters redundant information. By leveraging a Fourier-based low-frequency truncation method, we extract global temporal trends across decentralized agents, providing an effective and efficient representation of the MARL environment. Extensive experiments demonstrate that the proposed method achieves state-of-the-art (SOTA) performance on long-term dependency tasks, including PettingZoo, MiniGrid, Google Research Football (GRF), and StarCraft Multi-Agent Challenge v2 (SMACv2).
<div id='section'>Paperid: <span id='pid'>637, <a href='https://arxiv.org/pdf/2508.15764.pdf' target='_blank'>https://arxiv.org/pdf/2508.15764.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kiarash Kazari, Ezzeldin Shereen, GyÃ¶rgy DÃ¡n
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.15764">Distributed Detection of Adversarial Attacks in Multi-Agent Reinforcement Learning with Continuous Action Space</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We address the problem of detecting adversarial attacks against cooperative multi-agent reinforcement learning with continuous action space. We propose a decentralized detector that relies solely on the local observations of the agents and makes use of a statistical characterization of the normal behavior of observable agents. The proposed detector utilizes deep neural networks to approximate the normal behavior of agents as parametric multivariate Gaussian distributions. Based on the predicted density functions, we define a normality score and provide a characterization of its mean and variance. This characterization allows us to employ a two-sided CUSUM procedure for detecting deviations of the normality score from its mean, serving as a detector of anomalous behavior in real-time. We evaluate our scheme on various multi-agent PettingZoo benchmarks against different state-of-the-art attack methods, and our results demonstrate the effectiveness of our method in detecting impactful adversarial attacks. Particularly, it outperforms the discrete counterpart by achieving AUC-ROC scores of over 0.95 against the most impactful attacks in all evaluated environments.
<div id='section'>Paperid: <span id='pid'>638, <a href='https://arxiv.org/pdf/2508.11618.pdf' target='_blank'>https://arxiv.org/pdf/2508.11618.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jungang Chen, Seyyed A. Hosseini
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.11618">Optimal CO2 storage management considering safety constraints in multi-stakeholder multi-site CCS projects: a game theoretic perspective</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Carbon capture and storage (CCS) projects typically involve a diverse array of stakeholders or players from public, private, and regulatory sectors, each with different objectives and responsibilities. Given the complexity, scale, and long-term nature of CCS operations, determining whether individual stakeholders can independently maximize their interests or whether collaborative coalition agreements are needed remains a central question for effective CCS project planning and management. CCS projects are often implemented in geologically connected sites, where shared geological features such as pressure space and reservoir pore capacity can lead to competitive behavior among stakeholders. Furthermore, CO2 storage sites are often located in geologically mature basins that previously served as sites for hydrocarbon extraction or wastewater disposal in order to leverage existing infrastructures, which makes unilateral optimization even more complicated and unrealistic.
  In this work, we propose a paradigm based on Markov games to quantitatively investigate how different coalition structures affect the goals of stakeholders. We frame this multi-stakeholder multi-site problem as a multi-agent reinforcement learning problem with safety constraints. Our approach enables agents to learn optimal strategies while compliant with safety regulations. We present an example where multiple operators are injecting CO2 into their respective project areas in a geologically connected basin. To address the high computational cost of repeated simulations of high-fidelity models, a previously developed surrogate model based on the Embed-to-Control (E2C) framework is employed. Our results demonstrate the effectiveness of the proposed framework in addressing optimal management of CO2 storage when multiple stakeholders with various objectives and goals are involved.
<div id='section'>Paperid: <span id='pid'>639, <a href='https://arxiv.org/pdf/2508.09541.pdf' target='_blank'>https://arxiv.org/pdf/2508.09541.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gang Chen, Guoxin Wang, Anton van Beek, Zhenjun Ming, Yan Yan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.09541">Emergence of Hierarchies in Multi-Agent Self-Organizing Systems Pursuing a Joint Objective</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent self-organizing systems (MASOS) exhibit key characteristics including scalability, adaptability, flexibility, and robustness, which have contributed to their extensive application across various fields. However, the self-organizing nature of MASOS also introduces elements of unpredictability in their emergent behaviors. This paper focuses on the emergence of dependency hierarchies during task execution, aiming to understand how such hierarchies arise from agents' collective pursuit of the joint objective, how they evolve dynamically, and what factors govern their development. To investigate this phenomenon, multi-agent reinforcement learning (MARL) is employed to train MASOS for a collaborative box-pushing task. By calculating the gradients of each agent's actions in relation to the states of other agents, the inter-agent dependencies are quantified, and the emergence of hierarchies is analyzed through the aggregation of these dependencies. Our results demonstrate that hierarchies emerge dynamically as agents work towards a joint objective, with these hierarchies evolving in response to changing task requirements. Notably, these dependency hierarchies emerge organically in response to the shared objective, rather than being a consequence of pre-configured rules or parameters that can be fine-tuned to achieve specific results. Furthermore, the emergence of hierarchies is influenced by the task environment and network initialization conditions. Additionally, hierarchies in MASOS emerge from the dynamic interplay between agents' "Talent" and "Effort" within the "Environment." "Talent" determines an agent's initial influence on collective decision-making, while continuous "Effort" within the "Environment" enables agents to shift their roles and positions within the system.
<div id='section'>Paperid: <span id='pid'>640, <a href='https://arxiv.org/pdf/2508.09275.pdf' target='_blank'>https://arxiv.org/pdf/2508.09275.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Amine Andam, Jamal Bentahar, Mustapha Hedabou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.09275">Constrained Black-Box Attacks Against Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Collaborative multi-agent reinforcement learning (c-MARL) has rapidly evolved, offering state-of-the-art algorithms for real-world applications, including sensitive domains. However, a key challenge to its widespread adoption is the lack of a thorough investigation into its vulnerabilities to adversarial attacks. Existing work predominantly focuses on training-time attacks or unrealistic scenarios, such as access to policy weights or the ability to train surrogate policies. In this paper, we investigate new vulnerabilities under more realistic and constrained conditions, assuming an adversary can only collect and perturb the observations of deployed agents. We also consider scenarios where the adversary has no access at all. We propose simple yet highly effective algorithms for generating adversarial perturbations designed to misalign how victim agents perceive their environment. Our approach is empirically validated on three benchmarks and 22 environments, demonstrating its effectiveness across diverse algorithms and environments. Furthermore, we show that our algorithm is sample-efficient, requiring only 1,000 samples compared to the millions needed by previous methods.
<div id='section'>Paperid: <span id='pid'>641, <a href='https://arxiv.org/pdf/2508.08669.pdf' target='_blank'>https://arxiv.org/pdf/2508.08669.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yizhou Zhang, Eric Mazumdar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.08669">Convergent Q-Learning for Infinite-Horizon General-Sum Markov Games through Behavioral Economics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Risk-aversion and bounded rationality are two key characteristics of human decision-making. Risk-averse quantal-response equilibrium (RQE) is a solution concept that incorporates these features, providing a more realistic depiction of human decision making in various strategic environments compared to a Nash equilibrium. Furthermore a class of RQE has recently been shown in arXiv:2406.14156 to be universally computationally tractable in all finite-horizon Markov games, allowing for the development of multi-agent reinforcement learning algorithms with convergence guarantees. In this paper, we expand upon the study of RQE and analyze their computation in both two-player normal form games and discounted infinite-horizon Markov games. For normal form games we adopt a monotonicity-based approach allowing us to generalize previous results. We first show uniqueness and Lipschitz continuity of RQE with respect to player's payoff matrices under monotonicity assumptions, and then provide conditions on the players' degrees of risk aversion and bounded rationality that ensure monotonicity. We then focus on discounted infinite-horizon Markov games. We define the risk-averse quantal-response Bellman operator and prove its contraction under further conditions on the players' risk-aversion, bounded rationality, and temporal discounting. This yields a Q-learning based algorithm with convergence guarantees for all infinite-horizon general-sum Markov games.
<div id='section'>Paperid: <span id='pid'>642, <a href='https://arxiv.org/pdf/2507.14658.pdf' target='_blank'>https://arxiv.org/pdf/2507.14658.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Faizan Contractor, Li Li, Ranwa Al Mallah
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.14658">Learning to Communicate in Multi-Agent Reinforcement Learning for Autonomous Cyber Defence</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Popular methods in cooperative Multi-Agent Reinforcement Learning with partially observable environments typically allow agents to act independently during execution, which may limit the coordinated effect of the trained policies. However, by sharing information such as known or suspected ongoing threats, effective communication can lead to improved decision-making in the cyber battle space. We propose a game design where defender agents learn to communicate and defend against imminent cyber threats by playing training games in the Cyber Operations Research Gym, using the Differentiable Inter Agent Learning algorithm adapted to the cyber operational environment. The tactical policies learned by these autonomous agents are akin to those of human experts during incident responses to avert cyber threats. In addition, the agents simultaneously learn minimal cost communication messages while learning their defence tactical policies.
<div id='section'>Paperid: <span id='pid'>643, <a href='https://arxiv.org/pdf/2507.13846.pdf' target='_blank'>https://arxiv.org/pdf/2507.13846.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kathrin Korte, Christian Medeiros Adriano, Sona Ghahremani, Holger Giese
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.13846">Causal Knowledge Transfer for Multi-Agent Reinforcement Learning in Dynamic Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>[Context] Multi-agent reinforcement learning (MARL) has achieved notable success in environments where agents must learn coordinated behaviors. However, transferring knowledge across agents remains challenging in non-stationary environments with changing goals. [Problem] Traditional knowledge transfer methods in MARL struggle to generalize, and agents often require costly retraining to adapt. [Approach] This paper introduces a causal knowledge transfer framework that enables RL agents to learn and share compact causal representations of paths within a non-stationary environment. As the environment changes (new obstacles), agents' collisions require adaptive recovery strategies. We model each collision as a causal intervention instantiated as a sequence of recovery actions (a macro) whose effect corresponds to a causal knowledge of how to circumvent the obstacle while increasing the chances of achieving the agent's goal (maximizing cumulative reward). This recovery action macro is transferred online from a second agent and is applied in a zero-shot fashion, i.e., without retraining, just by querying a lookup model with local context information (collisions). [Results] Our findings reveal two key insights: (1) agents with heterogeneous goals were able to bridge about half of the gap between random exploration and a fully retrained policy when adapting to new environments, and (2) the impact of causal knowledge transfer depends on the interplay between environment complexity and agents' heterogeneous goals.
<div id='section'>Paperid: <span id='pid'>644, <a href='https://arxiv.org/pdf/2507.00550.pdf' target='_blank'>https://arxiv.org/pdf/2507.00550.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bruce Fang, Danyi Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.00550">Collaborative Multi-Agent Reinforcement Learning Approach for Elastic Cloud Resource Scaling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper addresses the challenges of rapid resource variation and highly uncertain task loads in cloud computing environments. It proposes an optimization method for elastic cloud resource scaling based on a multi-agent system. The method deploys multiple autonomous agents to perceive resource states in parallel and make local decisions. While maintaining the distributed nature of the system, it introduces a collaborative value function to achieve global coordination. This improves the responsiveness of resource scheduling and enhances overall system performance. To strengthen system foresight, a lightweight state prediction model is designed. It assists agents in identifying future workload trends and optimizes the selection of scaling actions. For policy training, the method adopts a centralized training and decentralized execution reinforcement learning framework. This enables agents to learn effectively and coordinate strategies under conditions of incomplete information. The paper also constructs typical cloud scenarios, including multi-tenancy and burst traffic, to evaluate the proposed method. The evaluation focuses on resource isolation, service quality assurance, and robustness. Experimental results show that the proposed multi-agent scaling strategy outperforms existing methods in resource utilization, SLA violation control, and scheduling latency. The results demonstrate strong adaptability and intelligent regulation. This provides an efficient and reliable new approach to solving the problem of elastic resource scaling in complex cloud platforms.
<div id='section'>Paperid: <span id='pid'>645, <a href='https://arxiv.org/pdf/2506.22876.pdf' target='_blank'>https://arxiv.org/pdf/2506.22876.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shayak Nandi, Fernanda M. Eliott
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.22876">Cooperation as a Black Box: Conceptual Fluctuation and Diagnostic Tools for Misalignment in MAS</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Misalignment in Multi-Agent Systems (MAS) is frequently treated as a technical failure. Yet, issues may arise from the conceptual design phase, where semantic ambiguity and normative projection occur. The Rabbit-Duck illusion illustrates how perspective-dependent readings of agent behavior, such as the conflation of cooperation-coordination, can create epistemic instability; e.g., coordinated agents in cooperative Multi-Agent Reinforcement Learning (MARL) benchmarks being interpreted as morally aligned, despite being optimized for shared utility maximization only. Motivated by three drivers of meaning-level misalignment in MAS (coordination-cooperation ambiguity, conceptual fluctuation, and semantic instability), we introduce the Misalignment Mosaic: a framework for diagnosing how misalignment emerges through language, framing, and design assumptions. The Mosaic comprises four components: 1. Terminological Inconsistency, 2. Interpretive Ambiguity, 3. Concept-to-Code Decay, and 4. Morality as Cooperation. Building on insights from the Morality-as-Cooperation Theory, we call for consistent meaning-level grounding in MAS to ensure systems function as intended: technically and ethically. This need is particularly urgent as MAS principles influence broader Artificial Intelligence (AI) workflows, amplifying risks in trust, interpretability, and governance. While this work focuses on the coordination/cooperation ambiguity, the Mosaic generalizes to other overloaded terms, such as alignment, autonomy, and trust.
<div id='section'>Paperid: <span id='pid'>646, <a href='https://arxiv.org/pdf/2506.12894.pdf' target='_blank'>https://arxiv.org/pdf/2506.12894.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Naoto Yoshida, Kingson Man
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.12894">Homeostatic Coupling for Prosocial Behavior</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>When regarding the suffering of others, we often experience personal distress and feel compelled to help\footnote{Preprint. Under review.}. Inspired by living systems, we investigate the emergence of prosocial behavior among autonomous agents that are motivated by homeostatic self-regulation. We perform multi-agent reinforcement learning, treating each agent as a vulnerable homeostat charged with maintaining its own well-being. We introduce an empathy-like mechanism to share homeostatic states between agents: an agent can either \emph{observe} their partner's internal state ({\bf cognitive empathy}) or the agent's internal state can be \emph{directly coupled} to that of their partner ({\bf affective empathy}). In three simple multi-agent environments, we show that prosocial behavior arises only under homeostatic coupling - when the distress of a partner can affect one's own well-being. Additionally, we show that empathy can be learned: agents can ``decode" their partner's external emotive states to infer the partner's internal homeostatic states. Assuming some level of physiological similarity, agents reference their own emotion-generation functions to invert the mapping from outward display to internal state. Overall, we demonstrate the emergence of prosocial behavior when homeostatic agents learn to ``read" the emotions of others and then to empathize, or feel as they feel.
<div id='section'>Paperid: <span id='pid'>647, <a href='https://arxiv.org/pdf/2506.06565.pdf' target='_blank'>https://arxiv.org/pdf/2506.06565.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Emilia Rivas, Sabrina Saika, Ahtesham Bakht, Aritran Piplai, Nathaniel D. Bastian, Ankit Shah
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.06565">Adapting Under Fire: Multi-Agent Reinforcement Learning for Adversarial Drift in Network Security</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Evolving attacks are a critical challenge for the long-term success of Network Intrusion Detection Systems (NIDS). The rise of these changing patterns has exposed the limitations of traditional network security methods. While signature-based methods are used to detect different types of attacks, they often fail to detect unknown attacks. Moreover, the system requires frequent updates with new signatures as the attackers are constantly changing their tactics. In this paper, we design an environment where two agents improve their policies over time. The adversarial agent, referred to as the red agent, perturbs packets to evade the intrusion detection mechanism, whereas the blue agent learns new defensive policies using drift adaptation techniques to counter the attacks. Both agents adapt iteratively: the red agent responds to the evolving NIDS, while the blue agent adjusts to emerging attack patterns. By studying the model's learned policy, we offer concrete insights into drift adaptation techniques with high utility. Experiments show that the blue agent boosts model accuracy by 30% with just 2 to 3 adaptation steps using only 25 to 30 samples each.
<div id='section'>Paperid: <span id='pid'>648, <a href='https://arxiv.org/pdf/2506.04265.pdf' target='_blank'>https://arxiv.org/pdf/2506.04265.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mengda Ji, Genjiu Xu, Liying Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.04265">CORA: Coalitional Rational Advantage Decomposition for Multi-Agent Policy Gradients</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work focuses on the credit assignment problem in cooperative multi-agent reinforcement learning (MARL). Sharing the global advantage among agents often leads to suboptimal policy updates as it fails to account for the distinct contributions of agents. Although numerous methods consider global or individual contributions for credit assignment, a detailed analysis at the coalition level remains lacking in many approaches. This work analyzes the over-updating problem during multi-agent policy updates from a coalition-level perspective. To address this issue, we propose a credit assignment method called Coalitional Rational Advantage Decomposition (CORA). CORA evaluates coalitional advantages via marginal contributions from all possible coalitions and decomposes advantages using the core solution from cooperative game theory, ensuring coalitional rationality. To reduce computational overhead, CORA employs random coalition sampling. Experiments on matrix games, differential games, and multi-agent collaboration benchmarks demonstrate that CORA outperforms strong baselines, particularly in tasks with multiple local optima. These findings highlight the importance of coalition-aware credit assignment for improving MARL performance.
<div id='section'>Paperid: <span id='pid'>649, <a href='https://arxiv.org/pdf/2504.12777.pdf' target='_blank'>https://arxiv.org/pdf/2504.12777.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>James Rudd-Jones, Mirco Musolesi, MarÃ­a PÃ©rez-Ortiz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.12777">Multi-Agent Reinforcement Learning Simulation for Environmental Policy Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Climate policy development faces significant challenges due to deep uncertainty, complex system dynamics, and competing stakeholder interests. Climate simulation methods, such as Earth System Models, have become valuable tools for policy exploration. However, their typical use is for evaluating potential polices, rather than directly synthesizing them. The problem can be inverted to optimize for policy pathways, but the traditional optimization approaches often struggle with non-linear dynamics, heterogeneous agents, and comprehensive uncertainty quantification. We propose a framework for augmenting climate simulations with Multi-Agent Reinforcement Learning (MARL) to address these limitations. We identify key challenges at the interface between climate simulations and the application of MARL in the context of policy synthesis, including reward definition, scalability with increasing agents and state spaces, uncertainty propagation across linked systems, and solution validation. Additionally, we discuss challenges in making MARL-derived solutions interpretable and useful for policy-makers. Our framework provides a foundation for more sophisticated climate policy exploration while acknowledging important limitations and areas for future research.
<div id='section'>Paperid: <span id='pid'>650, <a href='https://arxiv.org/pdf/2503.24296.pdf' target='_blank'>https://arxiv.org/pdf/2503.24296.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yubo Zhang, Pedro Botelho, Trevor Gordon, Gil Zussman, Igor Kadota
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.24296">Fair Dynamic Spectrum Access via Fully Decentralized Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We consider a decentralized wireless network with several source-destination pairs sharing a limited number of orthogonal frequency bands. Sources learn to adapt their transmissions (specifically, their band selection strategy) over time, in a decentralized manner, without sharing information with each other. Sources can only observe the outcome of their own transmissions (i.e., success or collision), having no prior knowledge of the network size or of the transmission strategy of other sources. The goal of each source is to maximize their own throughput while striving for network-wide fairness. We propose a novel fully decentralized Reinforcement Learning (RL)-based solution that achieves fairness without coordination. The proposed Fair Share RL (FSRL) solution combines: (i) state augmentation with a semi-adaptive time reference; (ii) an architecture that leverages risk control and time difference likelihood; and (iii) a fairness-driven reward structure. We evaluate FSRL in more than 50 network settings with different number of agents, different amounts of available spectrum, in the presence of jammers, and in an ad-hoc setting. Simulation results suggest that, when we compare FSRL with a common baseline RL algorithm from the literature, FSRL can be up to 89.0% fairer (as measured by Jain's fairness index) in stringent settings with several sources and a single frequency band, and 48.1% fairer on average.
<div id='section'>Paperid: <span id='pid'>651, <a href='https://arxiv.org/pdf/2503.08728.pdf' target='_blank'>https://arxiv.org/pdf/2503.08728.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yihong Li, Chengwei Zhang, Furui Zhan, Wanting Liu, Kailing Zhou, Longji Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.08728">Enhancing Traffic Signal Control through Model-based Reinforcement Learning and Policy Reuse</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent reinforcement learning (MARL) has shown significant potential in traffic signal control (TSC). However, current MARL-based methods often suffer from insufficient generalization due to the fixed traffic patterns and road network conditions used during training. This limitation results in poor adaptability to new traffic scenarios, leading to high retraining costs and complex deployment. To address this challenge, we propose two algorithms: PLight and PRLight. PLight employs a model-based reinforcement learning approach, pretraining control policies and environment models using predefined source-domain traffic scenarios. The environment model predicts the state transitions, which facilitates the comparison of environmental features. PRLight further enhances adaptability by adaptively selecting pre-trained PLight agents based on the similarity between the source and target domains to accelerate the learning process in the target domain. We evaluated the algorithms through two transfer settings: (1) adaptability to different traffic scenarios within the same road network, and (2) generalization across different road networks. The results show that PRLight significantly reduces the adaptation time compared to learning from scratch in new TSC scenarios, achieving optimal performance using similarities between available and target scenarios.
<div id='section'>Paperid: <span id='pid'>652, <a href='https://arxiv.org/pdf/2503.07678.pdf' target='_blank'>https://arxiv.org/pdf/2503.07678.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kailing Zhou, Chengwei Zhang, Furui Zhan, Wanting Liu, Yihong Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.07678">Using a single actor to output personalized policy for different intersections</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, with the development of Multi-agent reinforcement learning (MARL), adaptive traffic signal control (ATSC) has achieved satisfactory results. In traffic scenarios with multiple intersections, MARL treats each intersection as an agent and optimizes traffic signal control strategies through learning and real-time decision-making. Considering that observation distributions of intersections might be different in real-world scenarios, shared parameter methods might lack diversity and thus lead to high generalization requirements in the shared-policy network. A typical solution is to increase the size of network parameters. However, simply increasing the scale of the network does not necessarily improve policy generalization, which is validated in our experiments. Accordingly, an approach that considers both the personalization of intersections and the efficiency of parameter sharing is required. To this end, we propose Hyper-Action Multi-Head Proximal Policy Optimization (HAMH-PPO), a Centralized Training with Decentralized Execution (CTDE) MARL method that utilizes a shared PPO policy network to deliver personalized policies for intersections with non-iid observation distributions. The centralized critic in HAMH-PPO uses graph attention units to calculate the graph representations of all intersections and outputs a set of value estimates with multiple output heads for each intersection. The decentralized execution actor takes the local observation history as input and output distributions of action as well as a so-called hyper-action to balance the multiple values estimated from the centralized critic to further guide the updating of TSC policies. The combination of hyper-action and multi-head values enables multiple agents to share a single actor-critic while achieving personalized policies.
<div id='section'>Paperid: <span id='pid'>653, <a href='https://arxiv.org/pdf/2503.02913.pdf' target='_blank'>https://arxiv.org/pdf/2503.02913.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zilin Zhao, Chishui Chen, Haotian Shi, Jiale Chen, Xuanlin Yue, Zhejian Yang, Yang Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.02913">Towards Robust Multi-UAV Collaboration: MARL with Noise-Resilient Communication and Attention Mechanisms</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Efficient path planning for unmanned aerial vehicles (UAVs) is crucial in remote sensing and information collection. As task scales expand, the cooperative deployment of multiple UAVs significantly improves information collection efficiency. However, collaborative communication and decision-making for multiple UAVs remain major challenges in path planning, especially in noisy environments. To efficiently accomplish complex information collection tasks in 3D space and address robust communication issues, we propose a multi-agent reinforcement learning (MARL) framework for UAV path planning based on the Counterfactual Multi-Agent Policy Gradients (COMA) algorithm. The framework incorporates attention mechanism-based UAV communication protocol and training-deployment system, significantly improving communication robustness and individual decision-making capabilities in noisy conditions. Experiments conducted on both synthetic and real-world datasets demonstrate that our method outperforms existing algorithms in terms of path planning efficiency and robustness, especially in noisy environments, achieving a 78\% improvement in entropy reduction.
<div id='section'>Paperid: <span id='pid'>654, <a href='https://arxiv.org/pdf/2503.01440.pdf' target='_blank'>https://arxiv.org/pdf/2503.01440.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hyungho Na, Kwanghyeon Lee, Sumin Lee, Il-Chul Moon
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.01440">Trajectory-Class-Aware Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the context of multi-agent reinforcement learning, generalization is a challenge to solve various tasks that may require different joint policies or coordination without relying on policies specialized for each task. We refer to this type of problem as a multi-task, and we train agents to be versatile in this multi-task setting through a single training process. To address this challenge, we introduce TRajectory-class-Aware Multi-Agent reinforcement learning (TRAMA). In TRAMA, agents recognize a task type by identifying the class of trajectories they are experiencing through partial observations, and the agents use this trajectory awareness or prediction as additional information for action policy. To this end, we introduce three primary objectives in TRAMA: (a) constructing a quantized latent space to generate trajectory embeddings that reflect key similarities among them; (b) conducting trajectory clustering using these trajectory embeddings; and (c) building a trajectory-class-aware policy. Specifically for (c), we introduce a trajectory-class predictor that performs agent-wise predictions on the trajectory class; and we design a trajectory-class representation model for each trajectory class. Each agent takes actions based on this trajectory-class representation along with its partial observation for task-aware execution. The proposed method is evaluated on various tasks, including multi-task problems built upon StarCraft II. Empirical results show further performance improvements over state-of-the-art baselines.
<div id='section'>Paperid: <span id='pid'>655, <a href='https://arxiv.org/pdf/2502.02901.pdf' target='_blank'>https://arxiv.org/pdf/2502.02901.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Christine Konicki, Mithun Chakraborty, Michael P. Wellman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.02901">Policy Abstraction and Nash Refinement in Tree-Exploiting PSRO</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Policy Space Response Oracles (PSRO) interleaves empirical game-theoretic analysis with deep reinforcement learning (DRL) to solve games too complex for traditional analytic methods. Tree-exploiting PSRO (TE-PSRO) is a variant of this approach that iteratively builds a coarsened empirical game model in extensive form using data obtained from querying a simulator that represents a detailed description of the game. We make two main methodological advances to TE-PSRO that enhance its applicability to complex games of imperfect information. First, we introduce a scalable representation for the empirical game tree where edges correspond to implicit policies learned through DRL. These policies cover conditions in the underlying game abstracted in the game model, supporting sustainable growth of the tree over epochs. Second, we leverage extensive form in the empirical model by employing refined Nash equilibria to direct strategy exploration. To enable this, we give a modular and scalable algorithm based on generalized backward induction for computing a subgame perfect equilibrium (SPE) in an imperfect-information game. We experimentally evaluate our approach on a suite of games including an alternating-offer bargaining game with outside offers; our results demonstrate that TE-PSRO converges toward equilibrium faster when new strategies are generated based on SPE rather than Nash equilibrium, and with reasonable time/memory requirements for the growing empirical model.
<div id='section'>Paperid: <span id='pid'>656, <a href='https://arxiv.org/pdf/2502.02071.pdf' target='_blank'>https://arxiv.org/pdf/2502.02071.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yan Chen, Cheng Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.02071">Sequential Multi-objective Multi-agent Reinforcement Learning Approach for Predictive Maintenance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing predictive maintenance (PdM) methods typically focus solely on whether to replace system components without considering the costs incurred by inspection. However, a well-considered approach should be able to minimize Remaining Useful Life (RUL) at engine replacement while maximizing inspection interval. To achieve this, multi-agent reinforcement learning (MARL) can be introduced. However, due to the sequential and mutually constraining nature of these 2 objectives, conventional MARL is not applicable. Therefore, this paper introduces a novel framework and develops a Sequential Multi-objective Multi-agent Proximal Policy Optimization (SMOMA-PPO) algorithm. Furthermore, to provide comprehensive and effective degradation information to RL agents, we also employed Gated Recurrent Unit, quantile regression, and probability distribution fitting to develop a GRU-based RUL Prediction (GRP) model. Experiments demonstrate that the GRP method significantly improves the accuracy of RUL predictions in the later stages of system operation compared to existing methods. When incorporating its output into SMOMA-PPO, we achieve at least a 15% reduction in average RUL without unscheduled replacements (UR), nearly a 10% increase in inspection interval, and an overall decrease in maintenance costs. Importantly, our approach offers a new perspective for addressing multi-objective maintenance planning with sequential constraints, effectively enhancing system reliability and reducing maintenance expenses.
<div id='section'>Paperid: <span id='pid'>657, <a href='https://arxiv.org/pdf/2501.13592.pdf' target='_blank'>https://arxiv.org/pdf/2501.13592.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Claire Bizon Monroc, Ana BuÅ¡iÄ, Donatien Dubuc, Jiamin Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.13592">WFCRL: A Multi-Agent Reinforcement Learning Benchmark for Wind Farm Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The wind farm control problem is challenging, since conventional model-based control strategies require tractable models of complex aerodynamical interactions between the turbines and suffer from the curse of dimension when the number of turbines increases. Recently, model-free and multi-agent reinforcement learning approaches have been used to address this challenge. In this article, we introduce WFCRL (Wind Farm Control with Reinforcement Learning), the first open suite of multi-agent reinforcement learning environments for the wind farm control problem. WFCRL frames a cooperative Multi-Agent Reinforcement Learning (MARL) problem: each turbine is an agent and can learn to adjust its yaw, pitch or torque to maximize the common objective (e.g. the total power production of the farm). WFCRL also offers turbine load observations that will allow to optimize the farm performance while limiting turbine structural damages. Interfaces with two state-of-the-art farm simulators are implemented in WFCRL: a static simulator (FLORIS) and a dynamic simulator (FAST.Farm). For each simulator, $10$ wind layouts are provided, including $5$ real wind farms. Two state-of-the-art online MARL algorithms are implemented to illustrate the scaling challenges. As learning online on FAST.Farm is highly time-consuming, WFCRL offers the possibility of designing transfer learning strategies from FLORIS to FAST.Farm.
<div id='section'>Paperid: <span id='pid'>658, <a href='https://arxiv.org/pdf/2501.13448.pdf' target='_blank'>https://arxiv.org/pdf/2501.13448.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yulong Hu, Siyuan Feng, Sen Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.13448">BMG-Q: Localized Bipartite Match Graph Attention Q-Learning for Ride-Pooling Order Dispatch</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces Localized Bipartite Match Graph Attention Q-Learning (BMG-Q), a novel Multi-Agent Reinforcement Learning (MARL) algorithm framework tailored for ride-pooling order dispatch. BMG-Q advances ride-pooling decision-making process with the localized bipartite match graph underlying the Markov Decision Process, enabling the development of novel Graph Attention Double Deep Q Network (GATDDQN) as the MARL backbone to capture the dynamic interactions among ride-pooling vehicles in fleet. Our approach enriches the state information for each agent with GATDDQN by leveraging a localized bipartite interdependence graph and enables a centralized global coordinator to optimize order matching and agent behavior using Integer Linear Programming (ILP). Enhanced by gradient clipping and localized graph sampling, our GATDDQN improves scalability and robustness. Furthermore, the inclusion of a posterior score function in the ILP captures the online exploration-exploitation trade-off and reduces the potential overestimation bias of agents, thereby elevating the quality of the derived solutions. Through extensive experiments and validation, BMG-Q has demonstrated superior performance in both training and operations for thousands of vehicle agents, outperforming benchmark reinforcement learning frameworks by around 10% in accumulative rewards and showing a significant reduction in overestimation bias by over 50%. Additionally, it maintains robustness amidst task variations and fleet size changes, establishing BMG-Q as an effective, scalable, and robust framework for advancing ride-pooling order dispatch operations.
<div id='section'>Paperid: <span id='pid'>659, <a href='https://arxiv.org/pdf/2501.08020.pdf' target='_blank'>https://arxiv.org/pdf/2501.08020.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Juan Palma-Borda, Eduardo GuzmÃ¡n, MarÃ­a-Victoria Belmonte
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.08020">Cooperative Patrol Routing: Optimizing Urban Crime Surveillance through Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The effective design of patrol strategies is a difficult and complex problem, especially in medium and large areas. The objective is to plan, in a coordinated manner, the optimal routes for a set of patrols in a given area, in order to achieve maximum coverage of the area, while also trying to minimize the number of patrols. In this paper, we propose a multi-agent reinforcement learning (MARL) model, based on a decentralized partially observable Markov decision process, to plan unpredictable patrol routes within an urban environment represented as an undirected graph. The model attempts to maximize a target function that characterizes the environment within a given time frame. Our model has been tested to optimize police patrol routes in three medium-sized districts of the city of Malaga. The aim was to maximize surveillance coverage of the most crime-prone areas, based on actual crime data in the city. To address this problem, several MARL algorithms have been studied, and among these the Value Decomposition Proximal Policy Optimization (VDPPO) algorithm exhibited the best performance. We also introduce a novel metric, the coverage index, for the evaluation of the coverage performance of the routes generated by our model. This metric is inspired by the predictive accuracy index (PAI), which is commonly used in criminology to detect hotspots. Using this metric, we have evaluated the model under various scenarios in which the number of agents (or patrols), their starting positions, and the level of information they can observe in the environment have been modified. Results show that the coordinated routes generated by our model achieve a coverage of more than $90\%$ of the $3\%$ of graph nodes with the highest crime incidence, and $65\%$ for $20\%$ of these nodes; $3\%$ and $20\%$ represent the coverage standards for police resource allocation.
<div id='section'>Paperid: <span id='pid'>660, <a href='https://arxiv.org/pdf/2412.15639.pdf' target='_blank'>https://arxiv.org/pdf/2412.15639.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lunjun Liu, Weilai Jiang, Yaonan Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.15639">Tacit Learning with Adaptive Information Selection for Cooperative Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In multi-agent reinforcement learning (MARL), the centralized training with decentralized execution (CTDE) framework has gained widespread adoption due to its strong performance. However, the further development of CTDE faces two key challenges. First, agents struggle to autonomously assess the relevance of input information for cooperative tasks, impairing their decision-making abilities. Second, in communication-limited scenarios with partial observability, agents are unable to access global information, restricting their ability to collaborate effectively from a global perspective. To address these challenges, we introduce a novel cooperative MARL framework based on information selection and tacit learning. In this framework, agents gradually develop implicit coordination during training, enabling them to infer the cooperative behavior of others in a discrete space without communication, relying solely on local information. Moreover, we integrate gating and selection mechanisms, allowing agents to adaptively filter information based on environmental changes, thereby enhancing their decision-making capabilities. Experiments on popular MARL benchmarks show that our framework can be seamlessly integrated with state-of-the-art algorithms, leading to significant performance improvements.
<div id='section'>Paperid: <span id='pid'>661, <a href='https://arxiv.org/pdf/2412.12547.pdf' target='_blank'>https://arxiv.org/pdf/2412.12547.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziang Wang, Lei Wang, Qi Yi, Yimin Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.12547">A MARL Based Multi-Target Tracking Algorithm Under Jamming Against Radar</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Unmanned aerial vehicles (UAVs) have played an increasingly important role in military operations and social life. Among all application scenarios, multi-target tracking tasks accomplished by UAV swarms have received extensive attention. However, when UAVs use radar to track targets, the tracking performance can be severely compromised by jammers. To track targets in the presence of jammers, UAVs can use passive radar to position the jammer. This paper proposes a system where a UAV swarm selects the radar's active or passive work mode to track multiple differently located and potentially jammer-carrying targets. After presenting the optimization problem and proving its solving difficulty, we use a multi-agent reinforcement learning algorithm to solve this control problem. We also propose a mechanism based on simulated annealing algorithm to avoid cases where UAV actions violate constraints. Simulation experiments demonstrate the effectiveness of the proposed algorithm.
<div id='section'>Paperid: <span id='pid'>662, <a href='https://arxiv.org/pdf/2412.12103.pdf' target='_blank'>https://arxiv.org/pdf/2412.12103.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Naoto Yoshida, Kingson Man
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.12103">Empathic Coupling of Homeostatic States for Intrinsic Prosociality</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>When regarding the suffering of others, we often experience personal distress and feel compelled to help. Inspired by living systems, we investigate the emergence of prosocial behavior among autonomous agents that are motivated by homeostatic self-regulation. We perform multi-agent reinforcement learning, treating each agent as a vulnerable homeostat charged with maintaining its own well-being. We introduce an empathy-like mechanism to share homeostatic states between agents: an agent can either \emph{observe} their partner's internal state (cognitive empathy) or the agent's internal state can be \emph{directly coupled} to that of their partner's (affective empathy). In three simple multi-agent environments, we show that prosocial behavior arises only under homeostatic coupling - when the distress of a partner can affect one's own well-being. Our findings specify the type and role of empathy in artificial agents capable of prosocial behavior.
<div id='section'>Paperid: <span id='pid'>663, <a href='https://arxiv.org/pdf/2411.09856.pdf' target='_blank'>https://arxiv.org/pdf/2411.09856.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaoxuan Hou, Jiayi Yuan, Joel Z. Leibo, Natasha Jaques
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.09856">InvestESG: A multi-agent reinforcement learning benchmark for studying climate investment as a social dilemma</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>InvestESG is a novel multi-agent reinforcement learning (MARL) benchmark designed to study the impact of Environmental, Social, and Governance (ESG) disclosure mandates on corporate climate investments. The benchmark models an intertemporal social dilemma where companies balance short-term profit losses from climate mitigation efforts and long-term benefits from reducing climate risk, while ESG-conscious investors attempt to influence corporate behavior through their investment decisions. Companies allocate capital across mitigation, greenwashing, and resilience, with varying strategies influencing climate outcomes and investor preferences. We are releasing open-source versions of InvestESG in both PyTorch and JAX, which enable scalable and hardware-accelerated simulations for investigating competing incentives in mitigate climate change. Our experiments show that without ESG-conscious investors with sufficient capital, corporate mitigation efforts remain limited under the disclosure mandate. However, when a critical mass of investors prioritizes ESG, corporate cooperation increases, which in turn reduces climate risks and enhances long-term financial stability. Additionally, providing more information about global climate risks encourages companies to invest more in mitigation, even without investor involvement. Our findings align with empirical research using real-world data, highlighting MARL's potential to inform policy by providing insights into large-scale socio-economic challenges through efficient testing of alternative policy and market designs.
<div id='section'>Paperid: <span id='pid'>664, <a href='https://arxiv.org/pdf/2411.07634.pdf' target='_blank'>https://arxiv.org/pdf/2411.07634.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Maria Zampella, Urtzi Otamendi, Xabier Belaunzaran, Arkaitz Artetxe, Igor G. Olaizola, Giuseppe Longo, Basilio Sierra
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.07634">Exploring Multi-Agent Reinforcement Learning for Unrelated Parallel Machine Scheduling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Scheduling problems pose significant challenges in resource, industry, and operational management. This paper addresses the Unrelated Parallel Machine Scheduling Problem (UPMS) with setup times and resources using a Multi-Agent Reinforcement Learning (MARL) approach. The study introduces the Reinforcement Learning environment and conducts empirical analyses, comparing MARL with Single-Agent algorithms. The experiments employ various deep neural network policies for single- and Multi-Agent approaches. Results demonstrate the efficacy of the Maskable extension of the Proximal Policy Optimization (PPO) algorithm in Single-Agent scenarios and the Multi-Agent PPO algorithm in Multi-Agent setups. While Single-Agent algorithms perform adequately in reduced scenarios, Multi-Agent approaches reveal challenges in cooperative learning but a scalable capacity. This research contributes insights into applying MARL techniques to scheduling optimization, emphasizing the need for algorithmic sophistication balanced with scalability for intelligent scheduling solutions.
<div id='section'>Paperid: <span id='pid'>665, <a href='https://arxiv.org/pdf/2411.05683.pdf' target='_blank'>https://arxiv.org/pdf/2411.05683.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Indranil Sur, Aswin Raghavan, Abrar Rahman, James Z Hare, Daniel Cassenti, Carl Busart
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.05683">Data-Driven Distributed Common Operational Picture from Heterogeneous Platforms using Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The integration of unmanned platforms equipped with advanced sensors promises to enhance situational awareness and mitigate the "fog of war" in military operations. However, managing the vast influx of data from these platforms poses a significant challenge for Command and Control (C2) systems. This study presents a novel multi-agent learning framework to address this challenge. Our method enables autonomous and secure communication between agents and humans, which in turn enables real-time formation of an interpretable Common Operational Picture (COP). Each agent encodes its perceptions and actions into compact vectors, which are then transmitted, received and decoded to form a COP encompassing the current state of all agents (friendly and enemy) on the battlefield. Using Deep Reinforcement Learning (DRL), we jointly train COP models and agent's action selection policies. We demonstrate resilience to degraded conditions such as denied GPS and disrupted communications. Experimental validation is performed in the Starcraft-2 simulation environment to evaluate the precision of the COPs and robustness of policies. We report less than 5% error in COPs and policies resilient to various adversarial conditions. In summary, our contributions include a method for autonomous COP formation, increased resilience through distributed prediction, and joint training of COP models and multi-agent RL policies. This research advances adaptive and resilient C2, facilitating effective control of heterogeneous unmanned platforms.
<div id='section'>Paperid: <span id='pid'>666, <a href='https://arxiv.org/pdf/2411.04681.pdf' target='_blank'>https://arxiv.org/pdf/2411.04681.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sven Banisch, Dennis Jacob, Tom Willaert, Eckehard Olbrich
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.04681">A dynamical model of platform choice and online segregation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In order to truly understand how social media might shape online discourses or contribute to societal polarization, we need refined models of platform choice, that is: models that help us understand why users prefer one social media platform over another. This study develops a dynamic model of platform selection, extending Social Feedback Theory by incorporating multi-agent reinforcement learning to capture how user decisions are shaped by past rewards across different platforms. A key parameter ($Î¼$) in the model governs users' tendencies to either seek approval from like-minded peers or engage with opposing views. Our findings reveal that online environments can evolve into suboptimal states characterized by polarized, strongly opinionated echo chambers, even when users prefer diverse perspectives. Interestingly, this polarizing state coexists with another equilibrium, where users gravitate toward a single dominant platform, marginalizing other platforms into extremity. Using agent-based simulations and dynamical systems analysis, our model underscores the complex interplay of user preferences and platform dynamics, offering insights into how digital spaces might be better managed to foster diverse discourse.
<div id='section'>Paperid: <span id='pid'>667, <a href='https://arxiv.org/pdf/2410.21521.pdf' target='_blank'>https://arxiv.org/pdf/2410.21521.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sriniketh Vangaru, Daniel Rosen, Dylan Green, Raphael Rodriguez, Maxwell Wiecek, Amos Johnson, Alyse M. Jones, William C. Headley
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.21521">A Multi-Agent Reinforcement Learning Testbed for Cognitive Radio Applications</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Technological trends show that Radio Frequency Reinforcement Learning (RFRL) will play a prominent role in the wireless communication systems of the future. Applications of RFRL range from military communications jamming to enhancing WiFi networks. Before deploying algorithms for these purposes, they must be trained in a simulation environment to ensure adequate performance. For this reason, we previously created the RFRL Gym: a standardized, accessible tool for the development and testing of reinforcement learning (RL) algorithms in the wireless communications space. This environment leveraged the OpenAI Gym framework and featured customizable simulation scenarios within the RF spectrum. However, the RFRL Gym was limited to training a single RL agent per simulation; this is not ideal, as most real-world RF scenarios will contain multiple intelligent agents in cooperative, competitive, or mixed settings, which is a natural consequence of spectrum congestion. Therefore, through integration with Ray RLlib, multi-agent reinforcement learning (MARL) functionality for training and assessment has been added to the RFRL Gym, making it even more of a robust tool for RF spectrum simulation. This paper provides an overview of the updated RFRL Gym environment. In this work, the general framework of the tool is described relative to comparable existing resources, highlighting the significant additions and refactoring we have applied to the Gym. Afterward, results from testing various RF scenarios in the MARL environment and future additions are discussed.
<div id='section'>Paperid: <span id='pid'>668, <a href='https://arxiv.org/pdf/2410.18112.pdf' target='_blank'>https://arxiv.org/pdf/2410.18112.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rui Du, Kai Zhao, Jinlong Hou, Qiang Zhang, Peter Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.18112">OPTIMA: Optimized Policy for Intelligent Multi-Agent Systems Enables Coordination-Aware Autonomous Vehicles</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Coordination among connected and autonomous vehicles (CAVs) is advancing due to developments in control and communication technologies. However, much of the current work is based on oversimplified and unrealistic task-specific assumptions, which may introduce vulnerabilities. This is critical because CAVs not only interact with their environment but are also integral parts of it. Insufficient exploration can result in policies that carry latent risks, highlighting the need for methods that explore the environment both extensively and efficiently. This work introduces OPTIMA, a novel distributed reinforcement learning framework for cooperative autonomous vehicle tasks. OPTIMA alternates between thorough data sampling from environmental interactions and multi-agent reinforcement learning algorithms to optimize CAV cooperation, emphasizing both safety and efficiency. Our goal is to improve the generality and performance of CAVs in highly complex and crowded scenarios. Furthermore, the industrial-scale distributed training system easily adapts to different algorithms, reward functions, and strategies.
<div id='section'>Paperid: <span id='pid'>669, <a href='https://arxiv.org/pdf/2410.07976.pdf' target='_blank'>https://arxiv.org/pdf/2410.07976.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Baraah A. M. Sidahmed, Tatjana Chavdarova
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.07976">Addressing Rotational Learning Dynamics in Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent reinforcement learning (MARL) has emerged as a powerful paradigm for solving complex problems through agents' cooperation and competition, finding widespread applications across domains. Despite its success, MARL faces a reproducibility crisis. We show that, in part, this issue is related to the rotational optimization dynamics arising from competing agents' objectives, and require methods beyond standard optimization algorithms. We reframe MARL approaches using Variational Inequalities (VIs), offering a unified framework to address such issues. Leveraging optimization techniques designed for VIs, we propose a general approach for integrating gradient-based VI methods capable of handling rotational dynamics into existing MARL algorithms. Empirical results demonstrate significant performance improvements across benchmarks. In zero-sum games, Rock--paper--scissors and Matching pennies, VI methods achieve better convergence to equilibrium strategies, and in the Multi-Agent Particle Environment: Predator-prey, they also enhance team coordination. These results underscore the transformative potential of advanced optimization techniques in MARL.
<div id='section'>Paperid: <span id='pid'>670, <a href='https://arxiv.org/pdf/2410.07426.pdf' target='_blank'>https://arxiv.org/pdf/2410.07426.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kamil Khan, Sudeep Pasricha
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.07426">CAFEEN: A Cooperative Approach for Energy Efficient NoCs with Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In emerging high-performance Network-on-Chip (NoC) architectures, efficient power management is crucial to minimize energy consumption. We propose a novel framework called CAFEEN that employs both heuristic-based fine-grained and machine learning-based coarse-grained power-gating for energy-efficient NoCs. CAFEEN uses a fine-grained method to activate only essential NoC buffers during lower network loads. It switches to a coarse-grained method at peak loads to minimize compounding wake-up overhead using multi-agent reinforcement learning. Results show that CAFEEN adaptively balances power-efficiency with performance, reducing total energy by 2.60x for single application workloads and 4.37x for multi-application workloads, compared to state-of-the-art NoC power-gating frameworks.
<div id='section'>Paperid: <span id='pid'>671, <a href='https://arxiv.org/pdf/2410.04631.pdf' target='_blank'>https://arxiv.org/pdf/2410.04631.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mathias Jackermeier, Alessandro Abate
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.04631">DeepLTL: Learning to Efficiently Satisfy Complex LTL Specifications for Multi-Task RL</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Linear temporal logic (LTL) has recently been adopted as a powerful formalism for specifying complex, temporally extended tasks in multi-task reinforcement learning (RL). However, learning policies that efficiently satisfy arbitrary specifications not observed during training remains a challenging problem. Existing approaches suffer from several shortcomings: they are often only applicable to finite-horizon fragments of LTL, are restricted to suboptimal solutions, and do not adequately handle safety constraints. In this work, we propose a novel learning approach to address these concerns. Our method leverages the structure of BÃ¼chi automata, which explicitly represent the semantics of LTL specifications, to learn policies conditioned on sequences of truth assignments that lead to satisfying the desired formulae. Experiments in a variety of discrete and continuous domains demonstrate that our approach is able to zero-shot satisfy a wide range of finite- and infinite-horizon specifications, and outperforms existing methods in terms of both satisfaction probability and efficiency. Code available at: https://deep-ltl.github.io/
<div id='section'>Paperid: <span id='pid'>672, <a href='https://arxiv.org/pdf/2409.17443.pdf' target='_blank'>https://arxiv.org/pdf/2409.17443.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Cameron Mehlman, Joseph Abramov, Gregory Falco
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.17443">Cat-and-Mouse Satellite Dynamics: Divergent Adversarial Reinforcement Learning for Contested Multi-Agent Space Operations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As space becomes increasingly crowded and contested, robust autonomous capabilities for multi-agent environments are gaining critical importance. Current autonomous systems in space primarily rely on optimization-based path planning or long-range orbital maneuvers, which have not yet proven effective in adversarial scenarios where one satellite is actively pursuing another. We introduce Divergent Adversarial Reinforcement Learning (DARL), a two-stage Multi-Agent Reinforcement Learning (MARL) approach designed to train autonomous evasion strategies for satellites engaged with multiple adversarial spacecraft. Our method enhances exploration during training by promoting diverse adversarial strategies, leading to more robust and adaptable evader models. We validate DARL through a cat-and-mouse satellite scenario, modeled as a partially observable multi-agent capture the flag game where two adversarial `cat' spacecraft pursue a single `mouse' evader. DARL's performance is compared against several benchmarks, including an optimization-based satellite path planner, demonstrating its ability to produce highly robust models for adversarial multi-agent space environments.
<div id='section'>Paperid: <span id='pid'>673, <a href='https://arxiv.org/pdf/2409.05291.pdf' target='_blank'>https://arxiv.org/pdf/2409.05291.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Feng Zhu, Robert W. Heath, Aritra Mitra
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.05291">Towards Fast Rates for Federated and Multi-Task Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We consider a setting involving $N$ agents, where each agent interacts with an environment modeled as a Markov Decision Process (MDP). The agents' MDPs differ in their reward functions, capturing heterogeneous objectives/tasks. The collective goal of the agents is to communicate intermittently via a central server to find a policy that maximizes the average of long-term cumulative rewards across environments. The limited existing work on this topic either only provide asymptotic rates, or generate biased policies, or fail to establish any benefits of collaboration. In response, we propose Fast-FedPG - a novel federated policy gradient algorithm with a carefully designed bias-correction mechanism. Under a gradient-domination condition, we prove that our algorithm guarantees (i) fast linear convergence with exact gradients, and (ii) sub-linear rates that enjoy a linear speedup w.r.t. the number of agents with noisy, truncated policy gradients. Notably, in each case, the convergence is to a globally optimal policy with no heterogeneity-induced bias. In the absence of gradient-domination, we establish convergence to a first-order stationary point at a rate that continues to benefit from collaboration.
<div id='section'>Paperid: <span id='pid'>674, <a href='https://arxiv.org/pdf/2601.08237.pdf' target='_blank'>https://arxiv.org/pdf/2601.08237.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoran Su, Yandong Sun, Congjia Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.08237">The End of Reward Engineering: How LLMs Are Redefining Multi-Agent Coordination</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reward engineering, the manual specification of reward functions to induce desired agent behavior, remains a fundamental challenge in multi-agent reinforcement learning. This difficulty is amplified by credit assignment ambiguity, environmental non-stationarity, and the combinatorial growth of interaction complexity. We argue that recent advances in large language models (LLMs) point toward a shift from hand-crafted numerical rewards to language-based objective specifications. Prior work has shown that LLMs can synthesize reward functions directly from natural language descriptions (e.g., EUREKA) and adapt reward formulations online with minimal human intervention (e.g., CARD). In parallel, the emerging paradigm of Reinforcement Learning from Verifiable Rewards (RLVR) provides empirical evidence that language-mediated supervision can serve as a viable alternative to traditional reward engineering. We conceptualize this transition along three dimensions: semantic reward specification, dynamic reward adaptation, and improved alignment with human intent, while noting open challenges related to computational overhead, robustness to hallucination, and scalability to large multi-agent systems. We conclude by outlining a research direction in which coordination arises from shared semantic representations rather than explicitly engineered numerical signals.
<div id='section'>Paperid: <span id='pid'>675, <a href='https://arxiv.org/pdf/2601.04401.pdf' target='_blank'>https://arxiv.org/pdf/2601.04401.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Arsyi Aziz, Peng Wei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.04401">Transformer-based Multi-agent Reinforcement Learning for Separation Assurance in Structured and Unstructured Airspaces</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Conventional optimization-based metering depends on strict adherence to precomputed schedules, which limits the flexibility required for the stochastic operations of Advanced Air Mobility (AAM). In contrast, multi-agent reinforcement learning (MARL) offers a decentralized, adaptive framework that can better handle uncertainty, required for safe aircraft separation assurance. Despite this advantage, current MARL approaches often overfit to specific airspace structures, limiting their adaptability to new configurations. To improve generalization, we recast the MARL problem in a relative polar state space and train a transformer encoder model across diverse traffic patterns and intersection angles. The learned model provides speed advisories to resolve conflicts while maintaining aircraft near their desired cruising speeds. In our experiments, we evaluated encoder depths of 1, 2, and 3 layers in both structured and unstructured airspaces, and found that a single encoder configuration outperformed deeper variants, yielding near-zero near mid-air collision rates and shorter loss-of-separation infringements than the deeper configurations. Additionally, we showed that the same configuration outperforms a baseline model designed purely with attention. Together, our results suggest that the newly formulated state representation, novel design of neural network architecture, and proposed training strategy provide an adaptable and scalable decentralized solution for aircraft separation assurance in both structured and unstructured airspaces.
<div id='section'>Paperid: <span id='pid'>676, <a href='https://arxiv.org/pdf/2601.03301.pdf' target='_blank'>https://arxiv.org/pdf/2601.03301.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Guotao Li, Shaoyun Xu, Yuexing Hao, Yang Wang, Yuhui Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.03301">PC2P: Multi-Agent Path Finding via Personalized-Enhanced Communication and Crowd Perception</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Distributed Multi-Agent Path Finding (MAPF) integrated with Multi-Agent Reinforcement Learning (MARL) has emerged as a prominent research focus, enabling real-time cooperative decision-making in partially observable environments through inter-agent communication. However, due to insufficient collaborative and perceptual capabilities, existing methods are inadequate for scaling across diverse environmental conditions. To address these challenges, we propose PC2P, a novel distributed MAPF method derived from a Q-learning-based MARL framework. Initially, we introduce a personalized-enhanced communication mechanism based on dynamic graph topology, which ascertains the core aspects of ``who" and ``what" in interactive process through three-stage operations: selection, generation, and aggregation. Concurrently, we incorporate local crowd perception to enrich agents' heuristic observation, thereby strengthening the model's guidance for effective actions via the integration of static spatial constraints and dynamic occupancy changes. To resolve extreme deadlock issues, we propose a region-based deadlock-breaking strategy that leverages expert guidance to implement efficient coordination within confined areas. Experimental results demonstrate that PC2P achieves superior performance compared to state-of-the-art distributed MAPF methods in varied environments. Ablation studies further confirm the effectiveness of each module for overall performance.
<div id='section'>Paperid: <span id='pid'>677, <a href='https://arxiv.org/pdf/2512.20624.pdf' target='_blank'>https://arxiv.org/pdf/2512.20624.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mazyar Taghavi, Javad Vahidi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.20624">Quantum-Inspired Multi Agent Reinforcement Learning for Exploration Exploitation Optimization in UAV-Assisted 6G Network Deployment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This study introduces a quantum inspired framework for optimizing the exploration exploitation tradeoff in multiagent reinforcement learning, applied to UAVassisted 6G network deployment. We consider a cooperative scenario where ten intelligent UAVs autonomously coordinate to maximize signal coverage and support efficient network expansion under partial observability and dynamic conditions. The proposed approach integrates classical MARL algorithms with quantum-inspired optimization techniques, leveraging variational quantum circuits VQCs as the core structure and employing the Quantum Approximate Optimization Algorithm QAOA as a representative VQC based method for combinatorial optimization. Complementary probabilistic modeling is incorporated through Bayesian inference, Gaussian processes, and variational inference to capture latent environmental dynamics. A centralized training with decentralized execution CTDE paradigm is adopted, where shared memory and local view grids enhance local observability among agents. Comprehensive experiments including scalability tests, sensitivity analysis, and comparisons with PPO and DDPG baselines demonstrate that the proposed framework improves sample efficiency, accelerates convergence, and enhances coverage performance while maintaining robustness. Radar chart and convergence analyses further show that QI MARL achieves a superior balance between exploration and exploitation compared to classical methods. All implementation code and supplementary materials are publicly available on GitHub to ensure reproducibility.
<div id='section'>Paperid: <span id='pid'>678, <a href='https://arxiv.org/pdf/2512.15790.pdf' target='_blank'>https://arxiv.org/pdf/2512.15790.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Akhil Sharma, Shaikh Yaser Arafat, Jai Kumar Sharma, Ken Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.15790">Bilevel Optimization for Covert Memory Tampering in Heterogeneous Multi-Agent Architectures (XAMT)</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The increasing operational reliance on complex Multi-Agent Systems (MAS) across safety-critical domains necessitates rigorous adversarial robustness assessment. Modern MAS are inherently heterogeneous, integrating conventional Multi-Agent Reinforcement Learning (MARL) with emerging Large Language Model (LLM) agent architectures utilizing Retrieval-Augmented Generation (RAG). A critical shared vulnerability is reliance on centralized memory components: the shared Experience Replay (ER) buffer in MARL and the external Knowledge Base (K) in RAG agents. This paper proposes XAMT (Bilevel Optimization for Covert Memory Tampering in Heterogeneous Multi-Agent Architectures), a novel framework that formalizes attack generation as a bilevel optimization problem. The Upper Level minimizes perturbation magnitude (delta) to enforce covertness while maximizing system behavior divergence toward an adversary-defined target (Lower Level). We provide rigorous mathematical instantiations for CTDE MARL algorithms and RAG-based LLM agents, demonstrating that bilevel optimization uniquely crafts stealthy, minimal-perturbation poisons evading detection heuristics. Comprehensive experimental protocols utilize SMAC and SafeRAG benchmarks to quantify effectiveness at sub-percent poison rates (less than or equal to 1 percent in MARL, less than or equal to 0.1 percent in RAG). XAMT defines a new unified class of training-time threats essential for developing intrinsically secure MAS, with implications for trust, formal verification, and defensive strategies prioritizing intrinsic safety over perimeter-based detection.
<div id='section'>Paperid: <span id='pid'>679, <a href='https://arxiv.org/pdf/2512.09682.pdf' target='_blank'>https://arxiv.org/pdf/2512.09682.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mika Persson, Jonas Lidman, Jacob Ljungberg, Samuel Sandelius, Adam Andersson
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.09682">Dynamic one-time delivery of critical data by small and sparse UAV swarms: a model problem for MARL scaling studies</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work presents a conceptual study on the application of Multi-Agent Reinforcement Learning (MARL) for decentralized control of unmanned aerial vehicles to relay a critical data package to a known position. For this purpose, a family of deterministic games is introduced, designed for scaling studies for MARL. A robust baseline policy is proposed, which is based on restricting agent motion envelopes and applying Dijkstra's algorithm. Experimental results show that two off-the-shelf MARL algorithms perform competitively with the baseline for a small number of agents, but scalability issues arise as the number of agents increase.
<div id='section'>Paperid: <span id='pid'>680, <a href='https://arxiv.org/pdf/2512.09410.pdf' target='_blank'>https://arxiv.org/pdf/2512.09410.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jialin Ying, Zhihao Li, Zicheng Dong, Guohua Wu, Yihuan Liao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.09410">Generalizable Collaborative Search-and-Capture in Cluttered Environments via Path-Guided MAPPO and Directional Frontier Allocation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Collaborative pursuit-evasion in cluttered environments presents significant challenges due to sparse rewards and constrained Fields of View (FOV). Standard Multi-Agent Reinforcement Learning (MARL) often suffers from inefficient exploration and fails to scale to large scenarios. We propose PGF-MAPPO (Path-Guided Frontier MAPPO), a hierarchical framework bridging topological planning with reactive control. To resolve local minima and sparse rewards, we integrate an A*-based potential field for dense reward shaping. Furthermore, we introduce Directional Frontier Allocation, combining Farthest Point Sampling (FPS) with geometric angle suppression to enforce spatial dispersion and accelerate coverage. The architecture employs a parameter-shared decentralized critic, maintaining O(1) model complexity suitable for robotic swarms. Experiments demonstrate that PGF-MAPPO achieves superior capture efficiency against faster evaders. Policies trained on 10x10 maps exhibit robust zero-shot generalization to unseen 20x20 environments, significantly outperforming rule-based and learning-based baselines.
<div id='section'>Paperid: <span id='pid'>681, <a href='https://arxiv.org/pdf/2512.08877.pdf' target='_blank'>https://arxiv.org/pdf/2512.08877.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ryan LeRoy, Jack Kolb
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.08877">IPPO Learns the Game, Not the Team: A Study on Generalization in Heterogeneous Agent Teams</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-Agent Reinforcement Learning (MARL) is commonly deployed in settings where agents are trained via self-play with homogeneous teammates, often using parameter sharing and a single policy architecture. This opens the question: to what extent do self-play PPO agents learn general coordination strategies grounded in the underlying game, compared to overfitting to their training partners' behaviors? This paper investigates the question using the Heterogeneous Multi-Agent Challenge (HeMAC) environment, which features distinct Observer and Drone agents with complementary capabilities. We introduce Rotating Policy Training (RPT), an approach that rotates heterogeneous teammate policies of different learning algorithms during training, to expose the agent to a broader range of partner strategies. When playing alongside a withheld teammate policy (DDQN), we find that RPT achieves similar performance to a standard self-play baseline, IPPO, where all agents were trained sharing a single PPO policy. This result indicates that in this heterogeneous multi-agent setting, the IPPO baseline generalizes to novel teammate algorithms despite not experiencing teammate diversity during training. This shows that a simple IPPO baseline may possess the level of generalization to novel teammates that a diverse training regimen was designed to achieve.
<div id='section'>Paperid: <span id='pid'>682, <a href='https://arxiv.org/pdf/2512.06990.pdf' target='_blank'>https://arxiv.org/pdf/2512.06990.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Krishna Arun, Moinak Bhattachrya, Paras Goel
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.06990">Utilizing Multi-Agent Reinforcement Learning with Encoder-Decoder Architecture Agents to Identify Optimal Resection Location in Glioblastoma Multiforme Patients</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Currently, there is a noticeable lack of AI in the medical field to support doctors in treating heterogenous brain tumors such as Glioblastoma Multiforme (GBM), the deadliest human cancer in the world with a five-year survival rate of just 5.1%. This project develops an AI system offering the only end-to-end solution by aiding doctors with both diagnosis and treatment planning. In the diagnosis phase, a sequential decision-making framework consisting of 4 classification models (Convolutional Neural Networks and Support Vector Machine) are used. Each model progressively classifies the patient's brain into increasingly specific categories, with the final step being named diagnosis. For treatment planning, an RL system consisting of 3 generative models is used. First, the resection model (diffusion model) analyzes the diagnosed GBM MRI and predicts a possible resection outcome. Second, the radiotherapy model (Spatio-Temporal Vision Transformer) generates an MRI of the brain's progression after a user-defined number of weeks. Third, the chemotherapy model (Diffusion Model) produces the post-treatment MRI. A survival rate calculator (Convolutional Neural Network) then checks if the generated post treatment MRI has a survival rate within 15% of the user defined target. If not, a feedback loop using proximal policy optimization iterates over this system until an optimal resection location is identified. When compared to existing solutions, this project found 3 key findings: (1) Using a sequential decision-making framework consisting of 4 small diagnostic models reduced computing costs by 22.28x, (2) Transformers regression capabilities decreased tumor progression inference time by 113 hours, and (3) Applying Augmentations resembling Real-life situations improved overall DICE scores by 2.9%. These results project to increase survival rates by 0.9%, potentially saving approximately 2,250 lives.
<div id='section'>Paperid: <span id='pid'>683, <a href='https://arxiv.org/pdf/2512.04653.pdf' target='_blank'>https://arxiv.org/pdf/2512.04653.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pouria Yazdani, Arash Rezaali, Monireh Abdoos
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.04653">Semi Centralized Training Decentralized Execution Architecture for Multi Agent Deep Reinforcement Learning in Traffic Signal Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent reinforcement learning (MARL) has emerged as a promising paradigm for adaptive traffic signal control (ATSC) of multiple intersections. Existing approaches typically follow either a fully centralized or a fully decentralized design. Fully centralized approaches suffer from the curse of dimensionality, and reliance on a single learning server, whereas purely decentralized approaches operate under severe partial observability and lack explicit coordination resulting in suboptimal performance. These limitations motivate region-based MARL, where the network is partitioned into smaller, tightly coupled intersections that form regions, and training is organized around these regions. This paper introduces a Semi-Centralized Training, Decentralized Execution (SEMI-CTDE) architecture for multi intersection ATSC. Within each region, SEMI-CTDE performs centralized training with regional parameter sharing and employs composite state and reward formulations that jointly encode local and regional information. The architecture is highly transferable across different policy backbones and state-reward instantiations. Building on this architecture, we implement two models with distinct design objectives. A multi-perspective experimental analysis of the two implemented SEMI-CTDE-based models covering ablations of the architecture's core elements including rule based and fully decentralized baselines shows that they achieve consistently superior performance and remain effective across a wide range of traffic densities and distributions.
<div id='section'>Paperid: <span id='pid'>684, <a href='https://arxiv.org/pdf/2512.03166.pdf' target='_blank'>https://arxiv.org/pdf/2512.03166.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aya Taourirte, Md Sohag Mia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.03166">Multi-Agent Reinforcement Learning and Real-Time Decision-Making in Robotic Soccer for Virtual Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The deployment of multi-agent systems in dynamic, adversarial environments like robotic soccer necessitates real-time decision-making, sophisticated cooperation, and scalable algorithms to avoid the curse of dimensionality. While Reinforcement Learning (RL) offers a promising framework, existing methods often struggle with the multi-granularity of tasks (long-term strategy vs. instant actions) and the complexity of large-scale agent interactions. This paper presents a unified Multi-Agent Reinforcement Learning (MARL) framework that addresses these challenges. First, we establish a baseline using Proximal Policy Optimization (PPO) within a client-server architecture for real-time action scheduling, with PPO demonstrating superior performance (4.32 avg. goals, 82.9% ball control). Second, we introduce a Hierarchical RL (HRL) structure based on the options framework to decompose the problem into a high-level trajectory planning layer (modeled as a Semi-Markov Decision Process) and a low-level action execution layer, improving global strategy (avg. goals increased to 5.26). Finally, to ensure scalability, we integrate mean-field theory into the HRL framework, simplifying many-agent interactions into a single agent vs. the population average. Our mean-field actor-critic method achieves a significant performance boost (5.93 avg. goals, 89.1% ball control, 92.3% passing accuracy) and enhanced training stability. Extensive simulations of 4v4 matches in the Webots environment validate our approach, demonstrating its potential for robust, scalable, and cooperative behavior in complex multi-agent domains.
<div id='section'>Paperid: <span id='pid'>685, <a href='https://arxiv.org/pdf/2512.02442.pdf' target='_blank'>https://arxiv.org/pdf/2512.02442.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Changhee Lee, Jeongmin Rhee, DongHwa Shin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.02442">A Visual Analytics System to Understand Behaviors of Multi Agents in Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-Agent Reinforcement Learning (MARL) is a branch of machine learning in which agents interact and learn optimal policies through trial and error, addressing complex scenarios where multiple agents interact and learn in the same environment at the same time. Analyzing and understanding these complex interactions is challenging, and existing analysis methods are limited in their ability to fully reflect and interpret this complexity. To address these challenges, we provide MARLViz, a visual analytics system for visualizing and analyzing the policies and interactions of agents in MARL environments. The system is designed to visually show the difference in behavior of agents under different environment settings and help users understand complex interaction patterns. In this study, we analyzed agents with similar behaviors and selected scenarios to understand the interactions of the agents, which made it easier to understand the strategies of agents in MARL.
<div id='section'>Paperid: <span id='pid'>686, <a href='https://arxiv.org/pdf/2511.18449.pdf' target='_blank'>https://arxiv.org/pdf/2511.18449.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Paniz Parastar, Giuseppe Caso, Jesus Alberto Omana Iglesias, Andra Lutu, Ozgu Alay
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.18449">Energy-Efficient Task Computation at the Edge for Vehicular Services</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-access edge computing (MEC) is a promising solution for providing the computational resources and low latency required by vehicular services such as autonomous driving. It enables cars to offload computationally intensive tasks to nearby servers. Effective offloading involves determining when to offload tasks, selecting the appropriate MEC site, and efficiently allocating resources to ensure good performance. Car mobility poses significant challenges to guaranteeing reliable task completion, and today we still lack energy efficient solutions to this problem, especially when considering real-world car mobility traces. In this paper, we begin by examining the mobility patterns of cars using data obtained from a leading mobile network operator in Europe. Based on the insights from this analysis, we design an optimization problem for task computation and offloading, considering both static and mobility scenarios. Our objective is to minimize the total energy consumption at the cars and at the MEC nodes while satisfying the latency requirements of various tasks. We evaluate our solution, based on multi-agent reinforcement learning, both in simulations and in a realistic setup that relies on datasets from the operator. Our solution shows a significant reduction of user dissatisfaction and task interruptions in both static and mobile scenarios, while achieving energy savings of 47 percent in the static case and 14 percent in the mobile case compared to state-of-the-art schemes.
<div id='section'>Paperid: <span id='pid'>687, <a href='https://arxiv.org/pdf/2511.15284.pdf' target='_blank'>https://arxiv.org/pdf/2511.15284.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jonas De Maeyer, Hossein Yarahmadi, Moharram Challenger
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.15284">Path Planning through Multi-Agent Reinforcement Learning in Dynamic Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Path planning in dynamic environments is a fundamental challenge in intelligent transportation and robotics, where obstacles and conditions change over time, introducing uncertainty and requiring continuous adaptation. While existing approaches often assume complete environmental unpredictability or rely on global planners, these assumptions limit scalability and practical deployment in real-world settings. In this paper, we propose a scalable, region-aware reinforcement learning (RL) framework for path planning in dynamic environments. Our method builds on the observation that environmental changes, although dynamic, are often localized within bounded regions. To exploit this, we introduce a hierarchical decomposition of the environment and deploy distributed RL agents that adapt to changes locally. We further propose a retraining mechanism based on sub-environment success rates to determine when policy updates are necessary. Two training paradigms are explored: single-agent Q-learning and multi-agent federated Q-learning, where local Q-tables are aggregated periodically to accelerate the learning process. Unlike prior work, we evaluate our methods in more realistic settings, where multiple simultaneous obstacle changes and increasing difficulty levels are present. Results show that the federated variants consistently outperform their single-agent counterparts and closely approach the performance of A* Oracle while maintaining shorter adaptation times and robust scalability. Although initial training remains time-consuming in large environments, our decentralized framework eliminates the need for a global planner and lays the groundwork for future improvements using deep RL and flexible environment decomposition.
<div id='section'>Paperid: <span id='pid'>688, <a href='https://arxiv.org/pdf/2511.14910.pdf' target='_blank'>https://arxiv.org/pdf/2511.14910.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yassine Ibork, Myounggyu Won, Lokesh Das
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.14910">Z-Merge: Multi-Agent Reinforcement Learning for On-Ramp Merging with Zone-Specific V2X Traffic Information</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Ramp merging is a critical and challenging task for autonomous vehicles (AVs), particularly in mixed traffic environments with human-driven vehicles (HVs). Existing approaches typically rely on either lane-changing or inter-vehicle gap creation strategies based solely on local or neighboring information, often leading to suboptimal performance in terms of safety and traffic efficiency. In this paper, we present a V2X (vehicle-to-everything communication)-assisted Multiagent Reinforcement Learning (MARL) framework for on-ramp merging that effectively coordinates the complex interplay between lane-changing and inter-vehicle gap adaptation strategies by utilizing zone-specific global information available from a roadside unit (RSU). The merging control problem is formulated as a Multiagent Partially Observable Markov Decision Process (MA-POMDP), where agents leverage both local and global observations through V2X communication. To support both discrete and continuous control decisions, we design a hybrid action space and adopt a parameterized deep Q-learning approach. Extensive simulations, integrating the SUMO traffic simulator and the MOSAIC V2X simulator, demonstrate that our framework significantly improves merging success rate, traffic efficiency, and road safety across diverse traffic scenarios.
<div id='section'>Paperid: <span id='pid'>689, <a href='https://arxiv.org/pdf/2511.09792.pdf' target='_blank'>https://arxiv.org/pdf/2511.09792.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianmeng Hu, Yongzheng Cui, Rui Tang, Biao Luo, Ke Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.09792">Beyond Monotonicity: Revisiting Factorization Principles in Multi-Agent Q-Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Value decomposition is a central approach in multi-agent reinforcement learning (MARL), enabling centralized training with decentralized execution by factorizing the global value function into local values. To ensure individual-global-max (IGM) consistency, existing methods either enforce monotonicity constraints, which limit expressive power, or adopt softer surrogates at the cost of algorithmic complexity. In this work, we present a dynamical systems analysis of non-monotonic value decomposition, modeling learning dynamics as continuous-time gradient flow. We prove that, under approximately greedy exploration, all zero-loss equilibria violating IGM consistency are unstable saddle points, while only IGM-consistent solutions are stable attractors of the learning dynamics. Extensive experiments on both synthetic matrix games and challenging MARL benchmarks demonstrate that unconstrained, non-monotonic factorization reliably recovers IGM-optimal solutions and consistently outperforms monotonic baselines. Additionally, we investigate the influence of temporal-difference targets and exploration strategies, providing actionable insights for the design of future value-based MARL algorithms.
<div id='section'>Paperid: <span id='pid'>690, <a href='https://arxiv.org/pdf/2511.07778.pdf' target='_blank'>https://arxiv.org/pdf/2511.07778.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ao Ding, Licheng Sun, Yongjie Hou, Huaqing Zhang, Hongbin Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.07778">A Historical Interaction-Enhanced Shapley Policy Gradient Algorithm for Multi-Agent Credit Assignment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent reinforcement learning (MARL) has demonstrated remarkable performance in multi-agent collaboration problems and has become a prominent topic in artificial intelligence research in recent years. However, traditional credit assignment schemes in MARL cannot reliably capture individual contributions in strongly coupled tasks while maintaining training stability, which leads to limited generalization capabilities and hinders algorithm performance. To address these challenges, we propose a Historical Interaction-Enhanced Shapley Policy Gradient Algorithm (HIS) for Multi-Agent Credit Assignment, which employs a hybrid credit assignment mechanism to balance base rewards with individual contribution incentives. By utilizing historical interaction data to calculate the Shapley value in a sample-efficient manner, HIS enhances the agent's ability to perceive its own contribution, while retaining the global reward to maintain training stability. Additionally, we provide theoretical guarantees for the hybrid credit assignment mechanism, ensuring that the assignment results it generates are both efficient and stable. We evaluate the proposed algorithm in three widely used continuous-action benchmark environments: Multi-Agent Particle Environment, Multi-Agent MuJoCo, and Bi-DexHands. Experimental results demonstrate that HIS outperforms state-of-the-art methods, particularly excelling in strongly coupled, complex collaborative tasks.
<div id='section'>Paperid: <span id='pid'>691, <a href='https://arxiv.org/pdf/2511.02314.pdf' target='_blank'>https://arxiv.org/pdf/2511.02314.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jueye Zhang, Chao Yang, Youfang Lai, Kai-Wen Li, Wenting Yan, Yunzhou Xia, Haimei Zhang, Jingjing Zhou, Gen Yang, Chen Lin, Tian Li, Yibao Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.02314">Large-scale automatic carbon ion treatment planning for head and neck cancers via parallel multi-agent reinforcement learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Head-and-neck cancer (HNC) planning is difficult because multiple critical organs-at-risk (OARs) are close to complex targets. Intensity-modulated carbon-ion therapy (IMCT) offers superior dose conformity and OAR sparing but remains slow due to relative biological effectiveness (RBE) modeling, leading to laborious, experience-based, and often suboptimal tuning of many treatment-planning parameters (TPPs). Recent deep learning (DL) methods are limited by data bias and plan feasibility, while reinforcement learning (RL) struggles to efficiently explore the exponentially large TPP search space. We propose a scalable multi-agent RL (MARL) framework for parallel tuning of 45 TPPs in IMCT. It uses a centralized-training decentralized-execution (CTDE) QMIX backbone with Double DQN, Dueling DQN, and recurrent encoding (DRQN) for stable learning in a high-dimensional, non-stationary environment. To enhance efficiency, we (1) use compact historical DVH vectors as state inputs, (2) apply a linear action-to-value transform mapping small discrete actions to uniform parameter adjustments, and (3) design an absolute, clinically informed piecewise reward aligned with plan scores. A synchronous multi-process worker system interfaces with the PHOENIX TPS for parallel optimization and accelerated data collection. On a head-and-neck dataset (10 training, 10 testing), the method tuned 45 parameters simultaneously and produced plans comparable to or better than expert manual ones (relative plan score: RL $85.93\pm7.85%$ vs Manual $85.02\pm6.92%$), with significant (p-value $<$ 0.05) improvements for five OARs. The framework efficiently explores high-dimensional TPP spaces and generates clinically competitive IMCT plans through direct TPS interaction, notably improving OAR sparing.
<div id='section'>Paperid: <span id='pid'>692, <a href='https://arxiv.org/pdf/2511.01310.pdf' target='_blank'>https://arxiv.org/pdf/2511.01310.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sureyya Akin, Kavita Srivastava, Prateek B. Kapoor, Pradeep G. Sethi, Sunita Q. Patel, Rahu Srivastava
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.01310">From Pixels to Cooperation Multi Agent Reinforcement Learning based on Multimodal World Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Learning cooperative multi-agent policies directly from high-dimensional, multimodal sensory inputs like pixels and audio (from pixels) is notoriously sample-inefficient. Model-free Multi-Agent Reinforcement Learning (MARL) algorithms struggle with the joint challenge of representation learning, partial observability, and credit assignment. To address this, we propose a novel framework based on a shared, generative Multimodal World Model (MWM). Our MWM is trained to learn a compressed latent representation of the environment's dynamics by fusing distributed, multimodal observations from all agents using a scalable attention-based mechanism. Subsequently, we leverage this learned MWM as a fast, "imagined" simulator to train cooperative MARL policies (e.g., MAPPO) entirely within its latent space, decoupling representation learning from policy learning. We introduce a new set of challenging multimodal, multi-agent benchmarks built on a 3D physics simulator. Our experiments demonstrate that our MWM-MARL framework achieves orders-of-magnitude greater sample efficiency compared to state-of-the-art model-free MARL baselines. We further show that our proposed multimodal fusion is essential for task success in environments with sensory asymmetry and that our architecture provides superior robustness to sensor-dropout, a critical feature for real-world deployment.
<div id='section'>Paperid: <span id='pid'>693, <a href='https://arxiv.org/pdf/2508.14131.pdf' target='_blank'>https://arxiv.org/pdf/2508.14131.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junjie Qi, Siqi Mao, Tianyi Tan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.14131">An Improved Multi-Agent Algorithm for Cooperative and Competitive Environments by Identifying and Encouraging Cooperation among Agents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose an improved algorithm by identifying and encouraging cooperative behavior in multi-agent environments. First, we analyze the shortcomings of existing algorithms in addressing multi-agent reinforcement learning problems. Then, based on the existing algorithm MADDPG, we introduce a new parameter to increase the reward that an agent can obtain when cooperative behavior among agents is identified. Finally, we compare our improved algorithm with MADDPG in environments from PettingZoo. The results show that the new algorithm helps agents achieve both higher team rewards and individual rewards.
<div id='section'>Paperid: <span id='pid'>694, <a href='https://arxiv.org/pdf/2508.06836.pdf' target='_blank'>https://arxiv.org/pdf/2508.06836.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xutong Zhao, Yaqi Xie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.06836">Multi-level Advantage Credit Assignment for Cooperative Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cooperative multi-agent reinforcement learning (MARL) aims to coordinate multiple agents to achieve a common goal. A key challenge in MARL is credit assignment, which involves assessing each agent's contribution to the shared reward. Given the diversity of tasks, agents may perform different types of coordination, with rewards attributed to diverse and often overlapping agent subsets. In this work, we formalize the credit assignment level as the number of agents cooperating to obtain a reward, and address scenarios with multiple coexisting levels. We introduce a multi-level advantage formulation that performs explicit counterfactual reasoning to infer credits across distinct levels. Our method, Multi-level Advantage Credit Assignment (MACA), captures agent contributions at multiple levels by integrating advantage functions that reason about individual, joint, and correlated actions. Utilizing an attention-based framework, MACA identifies correlated agent relationships and constructs multi-level advantages to guide policy learning. Comprehensive experiments on challenging Starcraft v1\&v2 tasks demonstrate MACA's superior performance, underscoring its efficacy in complex credit assignment scenarios.
<div id='section'>Paperid: <span id='pid'>695, <a href='https://arxiv.org/pdf/2508.06767.pdf' target='_blank'>https://arxiv.org/pdf/2508.06767.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Arman Dogru, R. Irem Bor-Yaliniz, Nimal Gamini Senarath
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.06767">PANAMA: A Network-Aware MARL Framework for Multi-Agent Path Finding in Digital Twin Ecosystems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Digital Twins (DTs) are transforming industries through advanced data processing and analysis, positioning the world of DTs, Digital World, as a cornerstone of nextgeneration technologies including embodied AI. As robotics and automated systems scale, efficient data-sharing frameworks and robust algorithms become critical. We explore the pivotal role of data handling in next-gen networks, focusing on dynamics between application and network providers (AP/NP) in DT ecosystems. We introduce PANAMA, a novel algorithm with Priority Asymmetry for Network Aware Multi-agent Reinforcement Learning (MARL) based multi-agent path finding (MAPF). By adopting a Centralized Training with Decentralized Execution (CTDE) framework and asynchronous actor-learner architectures, PANAMA accelerates training while enabling autonomous task execution by embodied AI. Our approach demonstrates superior pathfinding performance in accuracy, speed, and scalability compared to existing benchmarks. Through simulations, we highlight optimized data-sharing strategies for scalable, automated systems, ensuring resilience in complex, real-world environments. PANAMA bridges the gap between network-aware decision-making and robust multi-agent coordination, advancing the synergy between DTs, wireless networks, and AI-driven automation.
<div id='section'>Paperid: <span id='pid'>696, <a href='https://arxiv.org/pdf/2508.02421.pdf' target='_blank'>https://arxiv.org/pdf/2508.02421.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Akshay Dodwadmath, Setareh Maghsudi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.02421">Emergence of Fair Leaders via Mediators in Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Stackelberg games and their resulting equilibria have received increasing attention in the multi-agent reinforcement learning literature. Each stage of a traditional Stackelberg game involves a leader(s) acting first, followed by the followers. In situations where the roles of leader(s) and followers can be interchanged, the designated role can have considerable advantages, for example, in first-mover advantage settings. Then the question arises: Who should be the leader and when? A bias in the leader selection process can lead to unfair outcomes. This problem is aggravated if the agents are self-interested and care only about their goals and rewards. We formally define this leader selection problem and show its relation to fairness in agents' returns. Furthermore, we propose a multi-agent reinforcement learning framework that maximizes fairness by integrating mediators. Mediators have previously been used in the simultaneous action setting with varying levels of control, such as directly performing agents' actions or just recommending them. Our framework integrates mediators in the Stackelberg setting with minimal control (leader selection). We show that the presence of mediators leads to self-interested agents taking fair actions, resulting in higher overall fairness in agents' returns.
<div id='section'>Paperid: <span id='pid'>697, <a href='https://arxiv.org/pdf/2508.00159.pdf' target='_blank'>https://arxiv.org/pdf/2508.00159.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jobst Heitzig, Ram Potham
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.00159">Model-Based Soft Maximization of Suitable Metrics of Long-Term Human Power</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Power is a key concept in AI safety: power-seeking as an instrumental goal, sudden or gradual disempowerment of humans, power balance in human-AI interaction and international AI governance. At the same time, power as the ability to pursue diverse goals is essential for wellbeing.
  This paper explores the idea of promoting both safety and wellbeing by forcing AI agents explicitly to empower humans and to manage the power balance between humans and AI agents in a desirable way. Using a principled, partially axiomatic approach, we design a parametrizable and decomposable objective function that represents an inequality- and risk-averse long-term aggregate of human power. It takes into account humans' bounded rationality and social norms, and, crucially, considers a wide variety of possible human goals.
  We derive algorithms for computing that metric by backward induction or approximating it via a form of multi-agent reinforcement learning from a given world model. We exemplify the consequences of (softly) maximizing this metric in a variety of paradigmatic situations and describe what instrumental sub-goals it will likely imply. Our cautious assessment is that softly maximizing suitable aggregate metrics of human power might constitute a beneficial objective for agentic AI systems that is safer than direct utility-based objectives.
<div id='section'>Paperid: <span id='pid'>698, <a href='https://arxiv.org/pdf/2507.20377.pdf' target='_blank'>https://arxiv.org/pdf/2507.20377.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Farshid Nooshi, Suining He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.20377">Multi-Agent Reinforcement Learning for Dynamic Mobility Resource Allocation with Hierarchical Adaptive Grouping</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Allocating mobility resources (e.g., shared bikes/e-scooters, ride-sharing vehicles) is crucial for rebalancing the mobility demand and supply in the urban environments. We propose in this work a novel multi-agent reinforcement learning named Hierarchical Adaptive Grouping-based Parameter Sharing (HAG-PS) for dynamic mobility resource allocation. HAG-PS aims to address two important research challenges regarding multi-agent reinforcement learning for mobility resource allocation: (1) how to dynamically and adaptively share the mobility resource allocation policy (i.e., how to distribute mobility resources) across agents (i.e., representing the regional coordinators of mobility resources); and (2) how to achieve memory-efficient parameter sharing in an urban-scale setting. To address the above challenges, we have provided following novel designs within HAG-PS. To enable dynamic and adaptive parameter sharing, we have designed a hierarchical approach that consists of global and local information of the mobility resource states (e.g., distribution of mobility resources). We have developed an adaptive agent grouping approach in order to split or merge the groups of agents based on their relative closeness of encoded trajectories (i.e., states, actions, and rewards). We have designed a learnable identity (ID) embeddings to enable agent specialization beyond simple parameter copy. We have performed extensive experimental studies based on real-world NYC bike sharing data (a total of more than 1.2 million trips), and demonstrated the superior performance (e.g., improved bike availability) of HAG-PS compared with other baseline approaches.
<div id='section'>Paperid: <span id='pid'>699, <a href='https://arxiv.org/pdf/2507.16382.pdf' target='_blank'>https://arxiv.org/pdf/2507.16382.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chenhao Yao, Zike Yuan, Xiaoxu Liu, Chi Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.16382">Application of LLM Guided Reinforcement Learning in Formation Control with Collision Avoidance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-Agent Systems (MAS) excel at accomplishing complex objectives through the collaborative efforts of individual agents. Among the methodologies employed in MAS, Multi-Agent Reinforcement Learning (MARL) stands out as one of the most efficacious algorithms. However, when confronted with the complex objective of Formation Control with Collision Avoidance (FCCA): designing an effective reward function that facilitates swift convergence of the policy network to an optimal solution. In this paper, we introduce a novel framework that aims to overcome this challenge. By giving large language models (LLMs) on the prioritization of tasks and the observable information available to each agent, our framework generates reward functions that can be dynamically adjusted online based on evaluation outcomes by employing more advanced evaluation metrics rather than the rewards themselves. This mechanism enables the MAS to simultaneously achieve formation control and obstacle avoidance in dynamic environments with enhanced efficiency, requiring fewer iterations to reach superior performance levels. Our empirical studies, conducted in both simulation and real-world settings, validate the practicality and effectiveness of our proposed approach.
<div id='section'>Paperid: <span id='pid'>700, <a href='https://arxiv.org/pdf/2507.15455.pdf' target='_blank'>https://arxiv.org/pdf/2507.15455.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hee Jun Yang, Minjung Gim, Yeoneung Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.15455">Solving nonconvex Hamilton--Jacobi--Isaacs equations with PINN-based policy iteration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a mesh-free policy iteration framework that combines classical dynamic programming with physics-informed neural networks (PINNs) to solve high-dimensional, nonconvex Hamilton--Jacobi--Isaacs (HJI) equations arising in stochastic differential games and robust control. The method alternates between solving linear second-order PDEs under fixed feedback policies and updating the controls via pointwise minimax optimization using automatic differentiation. Under standard Lipschitz and uniform ellipticity assumptions, we prove that the value function iterates converge locally uniformly to the unique viscosity solution of the HJI equation. The analysis establishes equi-Lipschitz regularity of the iterates, enabling provable stability and convergence without requiring convexity of the Hamiltonian. Numerical experiments demonstrate the accuracy and scalability of the method. In a two-dimensional stochastic path-planning game with a moving obstacle, our method matches finite-difference benchmarks with relative $L^2$-errors below %10^{-2}%. In five- and ten-dimensional publisher-subscriber differential games with anisotropic noise, the proposed approach consistently outperforms direct PINN solvers, yielding smoother value functions and lower residuals. Our results suggest that integrating PINNs with policy iteration is a practical and theoretically grounded method for solving high-dimensional, nonconvex HJI equations, with potential applications in robotics, finance, and multi-agent reinforcement learning.
<div id='section'>Paperid: <span id='pid'>701, <a href='https://arxiv.org/pdf/2507.07074.pdf' target='_blank'>https://arxiv.org/pdf/2507.07074.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Farhaan Ebadulla, Dharini Hindlatti, Srinivaasan NS, Apoorva VH, Ayman Aftab
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.07074">Graph-Based Complexity Metrics for Multi-Agent Curriculum Learning: A Validated Approach to Task Ordering in Cooperative Coordination Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent reinforcement learning (MARL) faces significant challenges in task sequencing and curriculum design, particularly for cooperative coordination scenarios. While curriculum learning has demonstrated success in single-agent domains, principled approaches for multi-agent coordination remain limited due to the absence of validated task complexity metrics. This approach presents a graph-based coordination complexity metric that integrates agent dependency entropy, spatial interference patterns, and goal overlap analysis to predict task difficulty in multi-agent environments. The complexity metric achieves strong empirical validation with rho = 0.952 correlation (p < 0.001) between predicted complexity and empirical difficulty determined by random agent performance evaluation. This approach evaluates the curriculum learning framework using MADDPG across two distinct coordination environments: achieving 56x performance improvement in tight coordination tasks (MultiWalker) and demonstrating systematic task progression in cooperative navigation (Simple Spread). Through systematic analysis, coordination tightness emerges as a predictor of curriculum learning effectiveness, where environments requiring strict agent interdependence benefit substantially from structured progression. This approach provides a validated complexity metric for multi-agent curriculum design and establishes empirical guidelines for multi-robot coordination applications.
<div id='section'>Paperid: <span id='pid'>702, <a href='https://arxiv.org/pdf/2507.06278.pdf' target='_blank'>https://arxiv.org/pdf/2507.06278.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kemboi Cheruiyot, Nickson Kiprotich, Vyacheslav Kungurtsev, Kennedy Mugo, Vivian Mwirigi, Marvin Ngesa
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.06278">A Survey of Multi Agent Reinforcement Learning: Federated Learning and Cooperative and Noncooperative Decentralized Regimes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The increasing interest in research and innovation towards the development of autonomous agents presents a number of complex yet important scenarios of multiple AI Agents interacting with each other in an environment. The particular setting can be understood as exhibiting three possibly topologies of interaction - centrally coordinated cooperation, ad-hoc interaction and cooperation, and settings with noncooperative incentive structures. This article presents a comprehensive survey of all three domains, defined under the formalism of Federal Reinforcement Learning (RL), Decentralized RL, and Noncooperative RL, respectively. Highlighting the structural similarities and distinctions, we review the state of the art in these subjects, primarily explored and developed only recently in the literature. We include the formulations as well as known theoretical guarantees and highlights and limitations of numerical performance.
<div id='section'>Paperid: <span id='pid'>703, <a href='https://arxiv.org/pdf/2506.20039.pdf' target='_blank'>https://arxiv.org/pdf/2506.20039.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Koorosh Moslemi, Chi-Guhn Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.20039">Learning Bilateral Team Formation in Cooperative Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Team formation and the dynamics of team-based learning have drawn significant interest in the context of Multi-Agent Reinforcement Learning (MARL). However, existing studies primarily focus on unilateral groupings, predefined teams, or fixed-population settings, leaving the effects of algorithmic bilateral grouping choices in dynamic populations underexplored. To address this gap, we introduce a framework for learning two-sided team formation in dynamic multi-agent systems. Through this study, we gain insight into what algorithmic properties in bilateral team formation influence policy performance and generalization. We validate our approach using widely adopted multi-agent scenarios, demonstrating competitive performance and improved generalization in most scenarios.
<div id='section'>Paperid: <span id='pid'>704, <a href='https://arxiv.org/pdf/2506.18537.pdf' target='_blank'>https://arxiv.org/pdf/2506.18537.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Azad Deihim, Eduardo Alonso, Dimitra Apostolopoulou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.18537">Transformer World Model for Sample Efficient Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present the Multi-Agent Transformer World Model (MATWM), a novel transformer-based world model designed for multi-agent reinforcement learning in both vector- and image-based environments. MATWM combines a decentralized imagination framework with a semi-centralized critic and a teammate prediction module, enabling agents to model and anticipate the behavior of others under partial observability. To address non-stationarity, we incorporate a prioritized replay mechanism that trains the world model on recent experiences, allowing it to adapt to agents' evolving policies. We evaluated MATWM on a broad suite of benchmarks, including the StarCraft Multi-Agent Challenge, PettingZoo, and MeltingPot. MATWM achieves state-of-the-art performance, outperforming both model-free and prior world model approaches, while demonstrating strong sample efficiency, achieving near-optimal performance in as few as 50K environment interactions. Ablation studies confirm the impact of each component, with substantial gains in coordination-heavy tasks.
<div id='section'>Paperid: <span id='pid'>705, <a href='https://arxiv.org/pdf/2506.18126.pdf' target='_blank'>https://arxiv.org/pdf/2506.18126.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiang Yuming, Li Sizhao, Li Rongpeng, Zhao Zhifeng, Zhang Honggang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.18126">Decentralized Consensus Inference-based Hierarchical Reinforcement Learning for Multi-Constrained UAV Pursuit-Evasion Game</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multiple quadrotor unmanned aerial vehicle (UAV) systems have garnered widespread research interest and fostered tremendous interesting applications, especially in multi-constrained pursuit-evasion games (MC-PEG). The Cooperative Evasion and Formation Coverage (CEFC) task, where the UAV swarm aims to maximize formation coverage across multiple target zones while collaboratively evading predators, belongs to one of the most challenging issues in MC-PEG, especially under communication-limited constraints. This multifaceted problem, which intertwines responses to obstacles, adversaries, target zones, and formation dynamics, brings up significant high-dimensional complications in locating a solution. In this paper, we propose a novel two-level framework (i.e., Consensus Inference-based Hierarchical Reinforcement Learning (CI-HRL)), which delegates target localization to a high-level policy, while adopting a low-level policy to manage obstacle avoidance, navigation, and formation. Specifically, in the high-level policy, we develop a novel multi-agent reinforcement learning module, Consensus-oriented Multi-Agent Communication (ConsMAC), to enable agents to perceive global information and establish consensus from local states by effectively aggregating neighbor messages. Meanwhile, we leverage an Alternative Training-based Multi-agent proximal policy optimization (AT-M) and policy distillation to accomplish the low-level control. The experimental results, including the high-fidelity software-in-the-loop (SITL) simulations, validate that CI-HRL provides a superior solution with enhanced swarm's collaborative evasion and task completion capabilities.
<div id='section'>Paperid: <span id='pid'>706, <a href='https://arxiv.org/pdf/2506.15587.pdf' target='_blank'>https://arxiv.org/pdf/2506.15587.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Martino Brambati, Antonio Celani, Marco Gherardi, Francesco Ginelli
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.15587">Learning to flock in open space by avoiding collisions and staying together</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We investigate the emergence of cohesive flocking in open, boundless space using a multi-agent reinforcement learning framework. Agents integrate positional and orientational information from their closest topological neighbours and learn to balance alignment and attractive interactions by optimizing a local cost function that penalizes both excessive separation and close-range crowding. The resulting Vicsek-like dynamics is robust to algorithmic implementation details and yields cohesive collective motion with high polar order. The optimal policy is dominated by strong aligning interactions when agents are sufficiently close to their neighbours, and a flexible combination of alignment and attraction at larger separations. We further characterize the internal structure and dynamics of the resulting groups using liquid-state metrics and neighbour exchange rates, finding qualitative agreement with empirical observations in starling flocks. These results suggest that flocking may emerge in groups of moving agents as an adaptive response to the biological imperatives of staying together while avoiding collisions.
<div id='section'>Paperid: <span id='pid'>707, <a href='https://arxiv.org/pdf/2506.05894.pdf' target='_blank'>https://arxiv.org/pdf/2506.05894.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Philipp Plank, Yufei Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.05894">Policy Optimization for Continuous-time Linear-Quadratic Graphon Mean Field Games</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent reinforcement learning, despite its popularity and empirical success, faces significant scalability challenges in large-population dynamic games. Graphon mean field games (GMFGs) offer a principled framework for approximating such games while capturing heterogeneity among players. In this paper, we propose and analyze a policy optimization framework for continuous-time, finite-horizon linear-quadratic GMFGs. Exploiting the structural properties of GMFGs, we design an efficient policy parameterization in which each player's policy is represented as an affine function of their private state, with a shared slope function and player-specific intercepts. We develop a bilevel optimization algorithm that alternates between policy gradient updates for best-response computation under a fixed population distribution, and distribution updates using the resulting policies. We prove linear convergence of the policy gradient steps to best-response policies and establish global convergence of the overall algorithm to the Nash equilibrium. The analysis relies on novel landscape characterizations over infinite-dimensional policy spaces. Numerical experiments demonstrate the convergence and robustness of the proposed algorithm under varying graphon structures, noise levels, and action frequencies.
<div id='section'>Paperid: <span id='pid'>708, <a href='https://arxiv.org/pdf/2506.02841.pdf' target='_blank'>https://arxiv.org/pdf/2506.02841.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tom Danino, Nahum Shimkin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.02841">Ensemble-MIX: Enhancing Sample Efficiency in Multi-Agent RL Using Ensemble Methods</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent reinforcement learning (MARL) methods have achieved state-of-the-art results on a range of multi-agent tasks. Yet, MARL algorithms typically require significantly more environment interactions than their single-agent counterparts to converge, a problem exacerbated by the difficulty in exploring over a large joint action space and the high variance intrinsic to MARL environments. To tackle these issues, we propose a novel algorithm that combines a decomposed centralized critic with decentralized ensemble learning, incorporating several key contributions. The main component in our scheme is a selective exploration method that leverages ensemble kurtosis. We extend the global decomposed critic with a diversity-regularized ensemble of individual critics and utilize its excess kurtosis to guide exploration toward high-uncertainty states and actions. To improve sample efficiency, we train the centralized critic with a novel truncated variation of the TD($Î»$) algorithm, enabling efficient off-policy learning with reduced variance. On the actor side, our suggested algorithm adapts the mixed samples approach to MARL, mixing on-policy and off-policy loss functions for training the actors. This approach balances between stability and efficiency and outperforms purely off-policy learning. The evaluation shows our method outperforms state-of-the-art baselines on standard MARL benchmarks, including a variety of SMAC II maps.
<div id='section'>Paperid: <span id='pid'>709, <a href='https://arxiv.org/pdf/2506.00797.pdf' target='_blank'>https://arxiv.org/pdf/2506.00797.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianglin Ding, Jingcheng Tang, Gangshan Jing
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.00797">Action Dependency Graphs for Globally Optimal Coordinated Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Action-dependent individual policies, which incorporate both environmental states and the actions of other agents in decision-making, have emerged as a promising paradigm for achieving global optimality in multi-agent reinforcement learning (MARL). However, the existing literature often adopts auto-regressive action-dependent policies, where each agent's policy depends on the actions of all preceding agents. This formulation incurs substantial computational complexity as the number of agents increases, thereby limiting scalability. In this work, we consider a more generalized class of action-dependent policies, which do not necessarily follow the auto-regressive form. We propose to use the `action dependency graph (ADG)' to model the inter-agent action dependencies. Within the context of MARL problems structured by coordination graphs, we prove that an action-dependent policy with a sparse ADG can achieve global optimality, provided the ADG satisfies specific conditions specified by the coordination graph. Building on this theoretical foundation, we develop a tabular policy iteration algorithm with guaranteed global optimality. Furthermore, we integrate our framework into several SOTA algorithms and conduct experiments in complex environments. The empirical results affirm the robustness and applicability of our approach in more general scenarios, underscoring its potential for broader MARL challenges.
<div id='section'>Paperid: <span id='pid'>710, <a href='https://arxiv.org/pdf/2505.21985.pdf' target='_blank'>https://arxiv.org/pdf/2505.21985.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Naoto Yoshida, Tadahiro Taniguchi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.21985">Reward-Independent Messaging for Decentralized Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In multi-agent reinforcement learning (MARL), effective communication improves agent performance, particularly under partial observability. We propose MARL-CPC, a framework that enables communication among fully decentralized, independent agents without parameter sharing. MARL-CPC incorporates a message learning model based on collective predictive coding (CPC) from emergent communication research. Unlike conventional methods that treat messages as part of the action space and assume cooperation, MARL-CPC links messages to state inference, supporting communication in non-cooperative, reward-independent settings. We introduce two algorithms -Bandit-CPC and IPPO-CPC- and evaluate them in non-cooperative MARL tasks. Benchmarks show that both outperform standard message-as-action approaches, establishing effective communication even when messages offer no direct benefit to the sender. These results highlight MARL-CPC's potential for enabling coordination in complex, decentralized environments.
<div id='section'>Paperid: <span id='pid'>711, <a href='https://arxiv.org/pdf/2505.19637.pdf' target='_blank'>https://arxiv.org/pdf/2505.19637.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Byunghyun Yoo, Younghwan Shin, Hyunwoo Kim, Euisok Chung, Jeongmin Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.19637">Adaptive Episode Length Adjustment for Multi-agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In standard reinforcement learning, an episode is defined as a sequence of interactions between agents and the environment, which terminates upon reaching a terminal state or a pre-defined episode length. Setting a shorter episode length enables the generation of multiple episodes with the same number of data samples, thereby facilitating an exploration of diverse states. While shorter episodes may limit the collection of long-term interactions, they may offer significant advantages when properly managed. For example, trajectory truncation in single-agent reinforcement learning has shown how the benefits of shorter episodes can be leveraged despite the trade-off of reduced long-term interaction experiences. However, this approach remains underexplored in MARL. This paper proposes a novel MARL approach, Adaptive Episode Length Adjustment (AELA), where the episode length is initially limited and gradually increased based on an entropy-based assessment of learning progress. By starting with shorter episodes, agents can focus on learning effective strategies for initial states and minimize time spent in dead-end states. The use of entropy as an assessment metric prevents premature convergence to suboptimal policies and ensures balanced training over varying episode lengths. We validate our approach using the StarCraft Multi-agent Challenge (SMAC) and a modified predator-prey environment, demonstrating significant improvements in both convergence speed and overall performance compared to existing methods. To the best of our knowledge, this is the first study to adaptively adjust episode length in MARL based on learning progress.
<div id='section'>Paperid: <span id='pid'>712, <a href='https://arxiv.org/pdf/2505.11461.pdf' target='_blank'>https://arxiv.org/pdf/2505.11461.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wesley A Suttle, Vipul K Sharma, Brian M Sadler
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.11461">Signal attenuation enables scalable decentralized multi-agent reinforcement learning over networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent reinforcement learning (MARL) methods typically require that agents enjoy global state observability, preventing development of decentralized algorithms and limiting scalability. Recent work has shown that, under assumptions on decaying inter-agent influence, global observability can be replaced by local neighborhood observability at each agent, enabling decentralization and scalability. Real-world applications enjoying such decay properties remain underexplored, however, despite the fact that signal power decay, or signal attenuation, due to path loss is an intrinsic feature of many problems in wireless communications and radar networks. In this paper, we show that signal attenuation enables decentralization in MARL by considering the illustrative special case of performing power allocation for target detection in a radar network. To achieve this, we propose two new constrained multi-agent Markov decision process formulations of this power allocation problem, derive local neighborhood approximations for global value function and policy gradient estimates and establish corresponding error bounds, and develop decentralized saddle point policy gradient algorithms for solving the proposed problems. Our approach, though oriented towards the specific radar network problem we consider, provides a useful model for extensions to additional problems in wireless communications and radar networks.
<div id='section'>Paperid: <span id='pid'>713, <a href='https://arxiv.org/pdf/2505.07207.pdf' target='_blank'>https://arxiv.org/pdf/2505.07207.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chiqiang Liu, Dazi Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.07207">HYGMA: Hypergraph Coordination Networks with Dynamic Grouping for Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cooperative multi-agent reinforcement learning faces significant challenges in effectively organizing agent relationships and facilitating information exchange, particularly when agents need to adapt their coordination patterns dynamically. This paper presents a novel framework that integrates dynamic spectral clustering with hypergraph neural networks to enable adaptive group formation and efficient information processing in multi-agent systems. The proposed framework dynamically constructs and updates hypergraph structures through spectral clustering on agents' state histories, enabling higher-order relationships to emerge naturally from agent interactions. The hypergraph structure is enhanced with attention mechanisms for selective information processing, providing an expressive and efficient way to model complex agent relationships. This architecture can be implemented in both value-based and policy-based paradigms through a unified objective combining task performance with structural regularization. Extensive experiments on challenging cooperative tasks demonstrate that our method significantly outperforms state-of-the-art approaches in both sample efficiency and final performance.
<div id='section'>Paperid: <span id='pid'>714, <a href='https://arxiv.org/pdf/2505.03949.pdf' target='_blank'>https://arxiv.org/pdf/2505.03949.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>John Christopher Tidwell, John Storm Tidwell
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.03949">Deep Q-Network (DQN) multi-agent reinforcement learning (MARL) for Stock Trading</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This project addresses the challenge of automated stock trading, where traditional methods and direct reinforcement learning (RL) struggle with market noise, complexity, and generalization. Our proposed solution is an integrated deep learning framework combining a Convolutional Neural Network (CNN) to identify patterns in technical indicators formatted as images, a Long Short-Term Memory (LSTM) network to capture temporal dependencies across both price history and technical indicators, and a Deep Q-Network (DQN) agent which learns the optimal trading policy (buy, sell, hold) based on the features extracted by the CNN and LSTM.
<div id='section'>Paperid: <span id='pid'>715, <a href='https://arxiv.org/pdf/2505.01453.pdf' target='_blank'>https://arxiv.org/pdf/2505.01453.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bharathkumar Hegde, Melanie Bouroche
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.01453">Safe and Efficient CAV Lane Changing using Decentralised Safety Shields</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Lane changing is a complex decision-making problem for Connected and Autonomous Vehicles (CAVs) as it requires balancing traffic efficiency with safety. Although traffic efficiency can be improved by using vehicular communication for training lane change controllers using Multi-Agent Reinforcement Learning (MARL), ensuring safety is difficult. To address this issue, we propose a decentralised Hybrid Safety Shield (HSS) that combines optimisation and a rule-based approach to guarantee safety. Our method applies control barrier functions to constrain longitudinal and lateral control inputs of a CAV to ensure safe manoeuvres. Additionally, we present an architecture to integrate HSS with MARL, called MARL-HSS, to improve traffic efficiency while ensuring safety. We evaluate MARL-HSS using a gym-like environment that simulates an on-ramp merging scenario with two levels of traffic densities, such as light and moderate densities. The results show that HSS provides a safety guarantee by strictly enforcing a dynamic safety constraint defined on a time headway, even in moderate traffic density that offers challenging lane change scenarios. Moreover, the proposed method learns stable policies compared to the baseline, a state-of-the-art MARL lane change controller without a safety shield. Further policy evaluation shows that our method achieves a balance between safety and traffic efficiency with zero crashes and comparable average speeds in light and moderate traffic densities.
<div id='section'>Paperid: <span id='pid'>716, <a href='https://arxiv.org/pdf/2505.00540.pdf' target='_blank'>https://arxiv.org/pdf/2505.00540.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ian O'Flynn, Harun Å iljak
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.00540">Emergence of Roles in Robotic Teams with Model Sharing and Limited Communication</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a reinforcement learning strategy for use in multi-agent foraging systems in which the learning is centralised to a single agent and its model is periodically disseminated among the population of non-learning agents. In a domain where multi-agent reinforcement learning (MARL) is the common approach, this approach aims to significantly reduce the computational and energy demands compared to approaches such as MARL and centralised learning models. By developing high performing foraging agents, these approaches can be translated into real-world applications such as logistics, environmental monitoring, and autonomous exploration. A reward function was incorporated into this approach that promotes role development among agents, without explicit directives. This led to the differentiation of behaviours among the agents. The implicit encouragement of role differentiation allows for dynamic actions in which agents can alter roles dependent on their interactions with the environment without the need for explicit communication between agents.
<div id='section'>Paperid: <span id='pid'>717, <a href='https://arxiv.org/pdf/2504.20529.pdf' target='_blank'>https://arxiv.org/pdf/2504.20529.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Costas Mylonas, Emmanouel Varvarigos, Georgios Tsaousoglou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.20529">Safe Bottom-Up Flexibility Provision from Distributed Energy Resources</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Modern renewables-based power systems need to tap on the flexibility of Distributed Energy Resources (DERs) connected to distribution networks. It is important, however, that DER owners/users remain in control of their assets, decisions, and objectives. At the same time, the dynamic landscape of DER-penetrated distribution networks calls for agile, data-driven flexibility management frameworks. In the face of these developments, the Multi-Agent Reinforcement Learning (MARL) paradigm is gaining significant attention, as a distributed and data-driven decision-making policy. This paper addresses the need for bottom-up DER management decisions to account for the distribution network's safety-related constraints. While the related literature on safe MARL typically assumes that network characteristics are available and incorporated into the policy's safety layer, which implies active DSO engagement, this paper ensures that self-organized DER communities are enabled to provide distribution-network-safe flexibility services without relying on the aspirational and problematic requirement of bringing the DSO in the decision-making loop.
<div id='section'>Paperid: <span id='pid'>718, <a href='https://arxiv.org/pdf/2504.08417.pdf' target='_blank'>https://arxiv.org/pdf/2504.08417.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Paul J. Pritz, Kin K. Leung
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.08417">Belief States for Cooperative Multi-Agent Reinforcement Learning under Partial Observability</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reinforcement learning in partially observable environments is typically challenging, as it requires agents to learn an estimate of the underlying system state. These challenges are exacerbated in multi-agent settings, where agents learn simultaneously and influence the underlying state as well as each others' observations. We propose the use of learned beliefs on the underlying state of the system to overcome these challenges and enable reinforcement learning with fully decentralized training and execution. Our approach leverages state information to pre-train a probabilistic belief model in a self-supervised fashion. The resulting belief states, which capture both inferred state information as well as uncertainty over this information, are then used in a state-based reinforcement learning algorithm to create an end-to-end model for cooperative multi-agent reinforcement learning under partial observability. By separating the belief and reinforcement learning tasks, we are able to significantly simplify the policy and value function learning tasks and improve both the convergence speed and the final performance. We evaluate our proposed method on diverse partially observable multi-agent tasks designed to exhibit different variants of partial observability.
<div id='section'>Paperid: <span id='pid'>719, <a href='https://arxiv.org/pdf/2504.06684.pdf' target='_blank'>https://arxiv.org/pdf/2504.06684.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Delin Zhao, Yanbo Shan, Chang Liu, Shenghang Lin, Yingxin Shou, Bin Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.06684">SDHN: Skewness-Driven Hypergraph Networks for Enhanced Localized Multi-Robot Coordination</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-Agent Reinforcement Learning is widely used for multi-robot coordination, where simple graphs typically model pairwise interactions. However, such representations fail to capture higher-order collaborations, limiting effectiveness in complex tasks. While hypergraph-based approaches enhance cooperation, existing methods often generate arbitrary hypergraph structures and lack adaptability to environmental uncertainties. To address these challenges, we propose the Skewness-Driven Hypergraph Network (SDHN), which employs stochastic Bernoulli hyperedges to explicitly model higher-order multi-robot interactions. By introducing a skewness loss, SDHN promotes an efficient structure with Small-Hyperedge Dominant Hypergraph, allowing robots to prioritize localized synchronization while still adhering to the overall information, similar to human coordination. Extensive experiments on Moving Agents in Formation and Robotic Warehouse tasks validate SDHN's effectiveness, demonstrating superior performance over state-of-the-art baselines.
<div id='section'>Paperid: <span id='pid'>720, <a href='https://arxiv.org/pdf/2503.18201.pdf' target='_blank'>https://arxiv.org/pdf/2503.18201.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Georg Ziegner, Michael Choi, Hung Mac Chan Le, Sahil Sakhuja, Arash Sarmadi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.18201">Iterative Multi-Agent Reinforcement Learning: A Novel Approach Toward Real-World Multi-Echelon Inventory Optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-echelon inventory optimization (MEIO) is critical for effective supply chain management, but its inherent complexity can pose significant challenges. Heuristics are commonly used to address this complexity, yet they often face limitations in scope and scalability. Recent research has found deep reinforcement learning (DRL) to be a promising alternative to traditional heuristics, offering greater versatility by utilizing dynamic decision-making capabilities. However, since DRL is known to struggle with the curse of dimensionality, its relevance to complex real-life supply chain scenarios is still to be determined. This thesis investigates DRL's applicability to MEIO problems of increasing complexity. A state-of-the-art DRL model was replicated, enhanced, and tested across 13 supply chain scenarios, combining diverse network structures and parameters. To address DRL's challenges with dimensionality, additional models leveraging graph neural networks (GNNs) and multi-agent reinforcement learning (MARL) were developed, culminating in the novel iterative multi-agent reinforcement learning (IMARL) approach. IMARL demonstrated superior scalability, effectiveness, and reliability in optimizing inventory policies, consistently outperforming benchmarks. These findings confirm the potential of DRL, particularly IMARL, to address real-world supply chain challenges and call for additional research to further expand its applicability.
<div id='section'>Paperid: <span id='pid'>721, <a href='https://arxiv.org/pdf/2503.13553.pdf' target='_blank'>https://arxiv.org/pdf/2503.13553.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Philipp D. Siedler, Ian Gemp
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.13553">LLM-Mediated Guidance of MARL Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In complex multi-agent environments, achieving efficient learning and desirable behaviours is a significant challenge for Multi-Agent Reinforcement Learning (MARL) systems. This work explores the potential of combining MARL with Large Language Model (LLM)-mediated interventions to guide agents toward more desirable behaviours. Specifically, we investigate how LLMs can be used to interpret and facilitate interventions that shape the learning trajectories of multiple agents. We experimented with two types of interventions, referred to as controllers: a Natural Language (NL) Controller and a Rule-Based (RB) Controller. The NL Controller, which uses an LLM to simulate human-like interventions, showed a stronger impact than the RB Controller. Our findings indicate that agents particularly benefit from early interventions, leading to more efficient training and higher performance. Both intervention types outperform the baseline without interventions, highlighting the potential of LLM-mediated guidance to accelerate training and enhance MARL performance in challenging environments.
<div id='section'>Paperid: <span id='pid'>722, <a href='https://arxiv.org/pdf/2503.09755.pdf' target='_blank'>https://arxiv.org/pdf/2503.09755.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Guangyi Liu, Suzan Iloglu, Michael Caldara, Joseph W. Durham, Michael M. Zavlanos
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.09755">Distributionally Robust Multi-Agent Reinforcement Learning for Dynamic Chute Mapping</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In Amazon robotic warehouses, the destination-to-chute mapping problem is crucial for efficient package sorting. Often, however, this problem is complicated by uncertain and dynamic package induction rates, which can lead to increased package recirculation. To tackle this challenge, we introduce a Distributionally Robust Multi-Agent Reinforcement Learning (DRMARL) framework that learns a destination-to-chute mapping policy that is resilient to adversarial variations in induction rates. Specifically, DRMARL relies on group distributionally robust optimization (DRO) to learn a policy that performs well not only on average but also on each individual subpopulation of induction rates within the group that capture, for example, different seasonality or operation modes of the system. This approach is then combined with a novel contextual bandit-based predictor of the worst-case induction distribution for each state-action pair, significantly reducing the cost of exploration and thereby increasing the learning efficiency and scalability of our framework. Extensive simulations demonstrate that DRMARL achieves robust chute mapping in the presence of varying induction distributions, reducing package recirculation by an average of 80\% in the simulation scenario.
<div id='section'>Paperid: <span id='pid'>723, <a href='https://arxiv.org/pdf/2503.08493.pdf' target='_blank'>https://arxiv.org/pdf/2503.08493.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>F. GiarrÃ¨, I. A. Meer, M. Masoudi, M. Ozger, C. Cavdar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.08493">Hierarchical Multi Agent DRL for Soft Handovers Between Edge Clouds in Open RAN</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-connectivity (MC) for aerial users via a set of ground access points offers the potential for highly reliable communication. Within an open radio access network (O-RAN) architecture, edge clouds (ECs) enable MC with low latency for users within their coverage area. However, ensuring seamless service continuity for transitional users-those moving between the coverage areas of neighboring ECs-poses challenges due to centralized processing demands. To address this, we formulate a problem facilitating soft handovers between ECs, ensuring seamless transitions while maintaining service continuity for all users. We propose a hierarchical multi-agent reinforcement learning (HMARL) algorithm to dynamically determine the optimal functional split configuration for transitional and non-transitional users. Simulation results show that the proposed approach outperforms the conventional functional split in terms of the percentage of users maintaining service continuity, with at most 4% optimality gap. Additionally, HMARL achieves better scalability compared to the static baselines.
<div id='section'>Paperid: <span id='pid'>724, <a href='https://arxiv.org/pdf/2503.07397.pdf' target='_blank'>https://arxiv.org/pdf/2503.07397.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kha Vo, Chin-Teng Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.07397">Q-MARL: A quantum-inspired algorithm using neural message passing for large-scale multi-agent reinforcement learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Inspired by a graph-based technique for predicting molecular properties in quantum chemistry -- atoms' position within molecules in three-dimensional space -- we present Q-MARL, a completely decentralised learning architecture that supports very large-scale multi-agent reinforcement learning scenarios without the need for strong assumptions like common rewards or agent order. The key is to treat each agent as relative to its surrounding agents in an environment that is presumed to change dynamically. Hence, in each time step, an agent is the centre of its own neighbourhood and also a neighbour to many other agents. Each role is formulated as a sub-graph, and each sub-graph is used as a training sample. A message-passing neural network supports full-scale vertex and edge interaction within a local neighbourhood, while a parameter governing the depth of the sub-graphs eases the training burden. During testing, an agent's actions are locally ensembled across all the sub-graphs that contain it, resulting in robust decisions. Where other approaches struggle to manage 50 agents, Q-MARL can easily marshal thousands. A detailed theoretical analysis proves improvement and convergence, and simulations with the typical collaborative and competitive scenarios show dramatically faster training speeds and reduced training losses.
<div id='section'>Paperid: <span id='pid'>725, <a href='https://arxiv.org/pdf/2503.06747.pdf' target='_blank'>https://arxiv.org/pdf/2503.06747.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Diego Bolliger, Lorenz Zauter, Robert Ziegler
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.06747">Fully-Decentralized MADDPG with Networked Agents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we devise three actor-critic algorithms with decentralized training for multi-agent reinforcement learning in cooperative, adversarial, and mixed settings with continuous action spaces. To this goal, we adapt the MADDPG algorithm by applying a networked communication approach between agents. We introduce surrogate policies in order to decentralize the training while allowing for local communication during training. The decentralized algorithms achieve comparable results to the original MADDPG in empirical tests, while reducing computational cost. This is more pronounced with larger numbers of agents.
<div id='section'>Paperid: <span id='pid'>726, <a href='https://arxiv.org/pdf/2503.03889.pdf' target='_blank'>https://arxiv.org/pdf/2503.03889.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Muhammad Waseem, Kshitij Bhatta, Chen Li, Qing Chang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.03889">Pretrained LLMs as Real-Time Controllers for Robot Operated Serial Production Line</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The manufacturing industry is undergoing a transformative shift, driven by cutting-edge technologies like 5G, AI, and cloud computing. Despite these advancements, effective system control, which is crucial for optimizing production efficiency, remains a complex challenge due to the intricate, knowledge-dependent nature of manufacturing processes and the reliance on domain-specific expertise. Conventional control methods often demand heavy customization, considerable computational resources, and lack transparency in decision-making. In this work, we investigate the feasibility of using Large Language Models (LLMs), particularly GPT-4, as a straightforward, adaptable solution for controlling manufacturing systems, specifically, mobile robot scheduling. We introduce an LLM-based control framework to assign mobile robots to different machines in robot assisted serial production lines, evaluating its performance in terms of system throughput. Our proposed framework outperforms traditional scheduling approaches such as First-Come-First-Served (FCFS), Shortest Processing Time (SPT), and Longest Processing Time (LPT). While it achieves performance that is on par with state-of-the-art methods like Multi-Agent Reinforcement Learning (MARL), it offers a distinct advantage by delivering comparable throughput without the need for extensive retraining. These results suggest that the proposed LLM-based solution is well-suited for scenarios where technical expertise, computational resources, and financial investment are limited, while decision transparency and system scalability are critical concerns.
<div id='section'>Paperid: <span id='pid'>727, <a href='https://arxiv.org/pdf/2503.01941.pdf' target='_blank'>https://arxiv.org/pdf/2503.01941.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Marc Speckmann, Theresa Eimer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.01941">Task Scheduling & Forgetting in Multi-Task Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reinforcement learning (RL) agents can forget tasks they have previously been trained on. There is a rich body of work on such forgetting effects in humans. Therefore we look for commonalities in the forgetting behavior of humans and RL agents across tasks and test the viability of forgetting prevention measures from learning theory in RL. We find that in many cases, RL agents exhibit forgetting curves similar to those of humans. Methods like Leitner or SuperMemo have been shown to be effective at counteracting human forgetting, but we demonstrate they do not transfer as well to RL. We identify a likely cause: asymmetrical learning and retention patterns between tasks that cannot be captured by retention-based or performance-based curriculum strategies.
<div id='section'>Paperid: <span id='pid'>728, <a href='https://arxiv.org/pdf/2503.00684.pdf' target='_blank'>https://arxiv.org/pdf/2503.00684.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Maria Ana Cardei, Afsaneh Doryab
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.00684">Factorized Deep Q-Network for Cooperative Multi-Agent Reinforcement Learning in Victim Tagging</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Mass casualty incidents (MCIs) are a growing concern, characterized by complexity and uncertainty that demand adaptive decision-making strategies. The victim tagging step in the emergency medical response must be completed quickly and is crucial for providing information to guide subsequent time-constrained response actions. In this paper, we present a mathematical formulation of multi-agent victim tagging to minimize the time it takes for responders to tag all victims. Five distributed heuristics are formulated and evaluated with simulation experiments. The heuristics considered are on-the go, practical solutions that represent varying levels of situational uncertainty in the form of global or local communication capabilities, showcasing practical constraints. We further investigate the performance of a multi-agent reinforcement learning (MARL) strategy, factorized deep Q-network (FDQN), to minimize victim tagging time as compared to baseline heuristics. Extensive simulations demonstrate that between the heuristics, methods with local communication are more efficient for adaptive victim tagging, specifically choosing the nearest victim with the option to replan. Analyzing all experiments, we find that our FDQN approach outperforms heuristics in smaller-scale scenarios, while heuristics excel in more complex scenarios. Our experiments contain diverse complexities that explore the upper limits of MARL capabilities for real-world applications and reveal key insights.
<div id='section'>Paperid: <span id='pid'>729, <a href='https://arxiv.org/pdf/2501.15735.pdf' target='_blank'>https://arxiv.org/pdf/2501.15735.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Madan Dahal, Mojtaba Vaezi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.15735">Selective Experience Sharing in Reinforcement Learning Enhances Interference Management</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a novel multi-agent reinforcement learning (RL) approach for inter-cell interference mitigation, in which agents selectively share their experiences with other agents. Each base station is equipped with an agent, which receives signal-to-interference-plus-noise ratio from its own associated users. This information is used to evaluate and selectively share experiences with neighboring agents. The idea is that even a few pertinent experiences from other agents can lead to effective learning. This approach enables fully decentralized training and execution, minimizes information sharing between agents and significantly reduces communication overhead, which is typically the burden of interference management. The proposed method outperforms state-of-the-art multi-agent RL techniques where training is done in a decentralized manner. Furthermore, with a 75% reduction in experience sharing, the proposed algorithm achieves 98% of the spectral efficiency obtained by algorithms sharing all experiences.
<div id='section'>Paperid: <span id='pid'>730, <a href='https://arxiv.org/pdf/2501.13727.pdf' target='_blank'>https://arxiv.org/pdf/2501.13727.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haikuo Du, Fandi Gou, Yunze Cai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.13727">Scalable Safe Multi-Agent Reinforcement Learning for Multi-Agent System</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Safety and scalability are two critical challenges faced by practical Multi-Agent Systems (MAS). However, existing Multi-Agent Reinforcement Learning (MARL) algorithms that rely solely on reward shaping are ineffective in ensuring safety, and their scalability is rather limited due to the fixed-size network output. To address these issues, we propose a novel framework, Scalable Safe MARL (SS-MARL), to enhance the safety and scalability of MARL methods. Leveraging the inherent graph structure of MAS, we design a multi-layer message passing network to aggregate local observations and communications of varying sizes. Furthermore, we develop a constrained joint policy optimization method in the setting of local observation to improve safety. Simulation experiments demonstrate that SS-MARL achieves a better trade-off between optimality and safety compared to baselines, and its scalability significantly outperforms the latest methods in scenarios with a large number of agents.
<div id='section'>Paperid: <span id='pid'>731, <a href='https://arxiv.org/pdf/2501.12199.pdf' target='_blank'>https://arxiv.org/pdf/2501.12199.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tuo Zhang, Leonardo Stella, Julian Barreiro-Gomez
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.12199">Experience-replay Innovative Dynamics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite its groundbreaking success, multi-agent reinforcement learning (MARL) still suffers from instability and nonstationarity. Replicator dynamics, the most well-known model from evolutionary game theory (EGT), provide a theoretical framework for the convergence of the trajectories to Nash equilibria and, as a result, have been used to ensure formal guarantees for MARL algorithms in stable game settings. However, they exhibit the opposite behavior in other settings, which poses the problem of finding alternatives to ensure convergence. In contrast, innovative dynamics, such as the Brown-von Neumann-Nash (BNN) or Smith, result in periodic trajectories with the potential to approximate Nash equilibria. Yet, no MARL algorithms based on these dynamics have been proposed. In response to this challenge, we develop a novel experience replay-based MARL algorithm that incorporates revision protocols as tunable hyperparameters. We demonstrate, by appropriately adjusting the revision protocols, that the behavior of our algorithm mirrors the trajectories resulting from these dynamics. Importantly, our contribution provides a framework capable of extending the theoretical guarantees of MARL algorithms beyond replicator dynamics. Finally, we corroborate our theoretical findings with empirical results.
<div id='section'>Paperid: <span id='pid'>732, <a href='https://arxiv.org/pdf/2412.20523.pdf' target='_blank'>https://arxiv.org/pdf/2412.20523.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Neil De La Fuente, Miquel Noguer i Alonso, Guim CasadellÃ
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.20523">Game Theory and Multi-Agent Reinforcement Learning : From Nash Equilibria to Evolutionary Dynamics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper explores advanced topics in complex multi-agent systems building upon our previous work. We examine four fundamental challenges in Multi-Agent Reinforcement Learning (MARL): non-stationarity, partial observability, scalability with large agent populations, and decentralized learning. The paper provides mathematical formulations and analysis of recent algorithmic advancements designed to address these challenges, with a particular focus on their integration with game-theoretic concepts. We investigate how Nash equilibria, evolutionary game theory, correlated equilibrium, and adversarial dynamics can be effectively incorporated into MARL algorithms to improve learning outcomes. Through this comprehensive analysis, we demonstrate how the synthesis of game theory and MARL can enhance the robustness and effectiveness of multi-agent systems in complex, dynamic environments.
<div id='section'>Paperid: <span id='pid'>733, <a href='https://arxiv.org/pdf/2412.19064.pdf' target='_blank'>https://arxiv.org/pdf/2412.19064.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shenghong He, Chao Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.19064">Hierarchical Multi-agent Meta-Reinforcement Learning for Cross-channel Bidding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Real-time bidding (RTB) plays a pivotal role in online advertising ecosystems. Advertisers employ strategic bidding to optimize their advertising impact while adhering to various financial constraints, such as the return-on-investment (ROI) and cost-per-click (CPC). Primarily focusing on bidding with fixed budget constraints, traditional approaches cannot effectively manage the dynamic budget allocation problem where the goal is to achieve global optimization of bidding performance across multiple channels with a shared budget. In this paper, we propose a hierarchical multi-agent reinforcement learning framework for multi-channel bidding optimization. In this framework, the top-level strategy applies a CPC constrained diffusion model to dynamically allocate budgets among the channels according to their distinct features and complex interdependencies, while the bottom-level strategy adopts a state-action decoupled actor-critic method to address the problem of extrapolation errors in offline learning caused by out-of-distribution actions and a context-based meta-channel knowledge learning method to improve the state representation capability of the policy based on the shared knowledge among different channels. Comprehensive experiments conducted on a large scale real-world industrial dataset from the Meituan ad bidding platform demonstrate that our method achieves a state-of-the-art performance.
<div id='section'>Paperid: <span id='pid'>734, <a href='https://arxiv.org/pdf/2412.15573.pdf' target='_blank'>https://arxiv.org/pdf/2412.15573.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Joshua Holder, Natasha Jaques, Mehran Mesbahi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.15573">Multi Agent Reinforcement Learning for Sequential Satellite Assignment Problems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Assignment problems are a classic combinatorial optimization problem in which a group of agents must be assigned to a group of tasks such that maximum utility is achieved while satisfying assignment constraints. Given the utility of each agent completing each task, polynomial-time algorithms exist to solve a single assignment problem in its simplest form. However, in many modern-day applications such as satellite constellations, power grids, and mobile robot scheduling, assignment problems unfold over time, with the utility for a given assignment depending heavily on the state of the system. We apply multi-agent reinforcement learning to this problem, learning the value of assignments by bootstrapping from a known polynomial-time greedy solver and then learning from further experience. We then choose assignments using a distributed optimal assignment mechanism rather than by selecting them directly. We demonstrate that this algorithm is theoretically justified and avoids pitfalls experienced by other RL algorithms in this setting. Finally, we show that our algorithm significantly outperforms other methods in the literature, even while scaling to realistic scenarios with hundreds of agents and tasks.
<div id='section'>Paperid: <span id='pid'>735, <a href='https://arxiv.org/pdf/2412.06333.pdf' target='_blank'>https://arxiv.org/pdf/2412.06333.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>F. Bredell, H. A. Engelbrecht, J. C. Schoeman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.06333">Augmenting the action space with conventions to improve multi-agent cooperation in Hanabi</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The card game Hanabi is considered a strong medium for the testing and development of multi-agent reinforcement learning (MARL) algorithms, due to its cooperative nature, partial observability, limited communication and remarkable complexity. Previous research efforts have explored the capabilities of MARL algorithms within Hanabi, focusing largely on advanced architecture design and algorithmic manipulations to achieve state-of-the-art performance for various number of cooperators. However, this often leads to complex solution strategies with high computational cost and requiring large amounts of training data. For humans to solve the Hanabi game effectively, they require the use of conventions, which often allows for a means to implicitly convey ideas or knowledge based on a predefined, and mutually agreed upon, set of "rules" or principles. Multi-agent problems containing partial observability, especially when limited communication is present, can benefit greatly from the use of implicit knowledge sharing. In this paper, we propose a novel approach to augmenting an agent's action space using conventions, which act as a sequence of special cooperative actions that span over and include multiple time steps and multiple agents, requiring agents to actively opt in for it to reach fruition. These conventions are based on existing human conventions, and result in a significant improvement on the performance of existing techniques for self-play and cross-play for various number of cooperators within Hanabi.
<div id='section'>Paperid: <span id='pid'>736, <a href='https://arxiv.org/pdf/2412.04369.pdf' target='_blank'>https://arxiv.org/pdf/2412.04369.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoran Su, Joseph Y. J. Chow
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.04369">Intersection-Aware Assessment of EMS Accessibility in NYC: A Data-Driven Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Emergency response times are critical in densely populated urban environments like New York City (NYC), where traffic congestion significantly impedes emergency vehicle (EMV) mobility. This study introduces an intersection-aware emergency medical service (EMS) accessibility model to evaluate and improve EMV travel times across NYC. Integrating intersection density metrics, road network characteristics, and demographic data, the model identifies vulnerable regions with inadequate EMS coverage. The analysis reveals that densely interconnected areas, such as parts of Staten Island, Queens, and Manhattan, experience significant accessibility deficits due to intersection delays and sparse medical infrastructure. To address these challenges, this study explores the adoption of EMVLight, a multi-agent reinforcement learning framework, which demonstrates the potential to reduce intersection delays by 50\%, increasing EMS accessibility to 95\% of NYC residents within the critical benchmark of 4 minutes. Results indicate that advanced traffic signal control (TSC) systems can alleviate congestion-induced delays while improving equity in emergency response. The findings provide actionable insights for urban planning and policy interventions to enhance EMS accessibility and ensure timely care for underserved populations.
<div id='section'>Paperid: <span id='pid'>737, <a href='https://arxiv.org/pdf/2412.04341.pdf' target='_blank'>https://arxiv.org/pdf/2412.04341.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ke Sun, Huan Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.04341">Reinforcement Learning for Freeway Lane-Change Regulation via Connected Vehicles</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Lane change decision-making is a complex task due to intricate vehicle-vehicle and vehicle-infrastructure interactions. Existing algorithms for lane-change control often depend on vehicles with a certain level of autonomy (e.g., autonomous or connected autonomous vehicles). To address the challenges posed by low penetration rates of autonomous vehicles and the high costs of precise data collection, this study proposes a dynamic lane change regulation design based on multi-agent reinforcement learning (MARL) to enhance freeway traffic efficiency. The proposed framework leverages multi-lane macroscopic traffic models that describe spatial-temporal dynamics of the density and speed for each lane. Lateral traffic flow between adjacent lanes, resulting from aggregated lane-changing behaviors, is modeled as source terms exchanged between the partial differential equations (PDEs). We propose a lane change regulation strategy using MARL, where one agent is placed at each discretized lane grid. The state of each agent is represented by aggregated vehicle attributes within its grid, generated from the SUMO microscopic simulation environment. The agent's actions are lane-change regulations for vehicles in its grid. Specifically, lane-change regulation signals are computed at a centralized traffic management center and then broadcast to connected vehicles in the corresponding lane grids. Compared to vehicle-level maneuver control, this approach achieves a higher regulation rate by leveraging vehicle connectivity while introducing no critical safety concerns, and accommodating varying levels of connectivity and autonomy within the traffic system. The proposed model is simulated and evaluated in varied traffic scenarios and demand conditions. Experimental results demonstrate that the method improves overall traffic efficiency with minimal additional energy consumption while maintaining driving safety.
<div id='section'>Paperid: <span id='pid'>738, <a href='https://arxiv.org/pdf/2412.02057.pdf' target='_blank'>https://arxiv.org/pdf/2412.02057.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Anubha Mahajan, Shreya Hegde, Ethan Shay, Daniel Wu, Aviva Prins
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.02057">Comparative Analysis of Multi-Agent Reinforcement Learning Policies for Crop Planning Decision Support</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In India, the majority of farmers are classified as small or marginal, making their livelihoods particularly vulnerable to economic losses due to market saturation and climate risks. Effective crop planning can significantly impact their expected income, yet existing decision support systems (DSS) often provide generic recommendations that fail to account for real-time market dynamics and the interactions among multiple farmers. In this paper, we evaluate the viability of three multi-agent reinforcement learning (MARL) approaches for optimizing total farmer income and promoting fairness in crop planning: Independent Q-Learning (IQL), where each farmer acts independently without coordination, Agent-by-Agent (ABA), which sequentially optimizes each farmer's policy in relation to the others, and the Multi-agent Rollout Policy, which jointly optimizes all farmers' actions for global reward maximization. Our results demonstrate that while IQL offers computational efficiency with linear runtime, it struggles with coordination among agents, leading to lower total rewards and an unequal distribution of income. Conversely, the Multi-agent Rollout policy achieves the highest total rewards and promotes equitable income distribution among farmers but requires significantly more computational resources, making it less practical for large numbers of agents. ABA strikes a balance between runtime efficiency and reward optimization, offering reasonable total rewards with acceptable fairness and scalability. These findings highlight the importance of selecting appropriate MARL approaches in DSS to provide personalized and equitable crop planning recommendations, advancing the development of more adaptive and farmer-centric agricultural decision-making systems.
<div id='section'>Paperid: <span id='pid'>739, <a href='https://arxiv.org/pdf/2411.12246.pdf' target='_blank'>https://arxiv.org/pdf/2411.12246.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>David Ge, Hao Ji
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.12246">Efficient Training in Multi-Agent Reinforcement Learning: A Communication-Free Framework for the Box-Pushing Problem</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Self-organizing systems consist of autonomous agents that can perform complex tasks and adapt to dynamic environments without a central controller. Prior research often relies on reinforcement learning to enable agents to gain the skills needed for task completion, such as in the box-pushing environment. However, when agents push from opposing directions during exploration, they tend to exert equal and opposite forces on the box, resulting in minimal displacement and inefficient training. This paper proposes a model called Shared Pool of Information (SPI), which enables information to be accessible to all agents and facilitates coordination, reducing force conflicts among agents and enhancing exploration efficiency. Through computer simulations, we demonstrate that SPI not only expedites the training process but also requires fewer steps per episode, significantly improving the agents' collaborative effectiveness.
<div id='section'>Paperid: <span id='pid'>740, <a href='https://arxiv.org/pdf/2411.10459.pdf' target='_blank'>https://arxiv.org/pdf/2411.10459.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Brian Mintz, Feng Fu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.10459">Evolutionary Multi-agent Reinforcement Learning in Group Social Dilemmas</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reinforcement learning (RL) is a powerful machine learning technique that has been successfully applied to a wide variety of problems. However, it can be unpredictable and produce suboptimal results in complicated learning environments. This is especially true when multiple agents learn simultaneously, which creates a complex system that is often analytically intractable. Our work considers the fundamental framework of Q-learning in Public Goods Games, where RL individuals must work together to achieve a common goal. This setting allows us to study the tragedy of the commons and free rider effects in AI cooperation, an emerging field with potential to resolve challenging obstacles to the wider application of artificial intelligence. While this social dilemma has been mainly investigated through traditional and evolutionary game theory, our approach bridges the gap between these two by studying agents with an intermediate level of intelligence. Specifically, we consider the influence of learning parameters on cooperation levels in simulations and a limiting system of differential equations, as well as the effect of evolutionary pressures on exploration rate in both of these models. We find selection for higher and lower levels of exploration, as well as attracting values, and a condition that separates these in a restricted class of games. Our work enhances the theoretical understanding of evolutionary Q-learning, and extends our knowledge of the evolution of machine behavior in social dilemmas.
<div id='section'>Paperid: <span id='pid'>741, <a href='https://arxiv.org/pdf/2411.08896.pdf' target='_blank'>https://arxiv.org/pdf/2411.08896.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruili Zhao, Jun Cai, Jiangtao Luo, Junpeng Gao, Yongyi Ran
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.08896">Demand-Aware Beam Hopping and Power Allocation for Load Balancing in Digital Twin empowered LEO Satellite Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Low-Earth orbit (LEO) satellites utilizing beam hopping (BH) technology offer extensive coverage, low latency, high bandwidth, and significant flexibility. However, the uneven geographical distribution and temporal variability of ground traffic demands, combined with the high mobility of LEO satellites, present significant challenges for efficient beam resource utilization. Traditional BH methods based on GEO satellites fail to address issues such as satellite interference, overlapping coverage, and mobility. This paper explores a Digital Twin (DT)-based collaborative resource allocation network for multiple LEO satellites with overlapping coverage areas. A two-tier optimization problem, focusing on load balancing and cell service fairness, is proposed to maximize throughput and minimize inter-cell service delay. The DT layer optimizes the allocation of overlapping coverage cells by designing BH patterns for each satellite, while the LEO layer optimizes power allocation for each selected service cell. At the DT layer, an Actor-Critic network is deployed on each agent, with a global critic network in the cloud center. The A3C algorithm is employed to optimize the DT layer. Concurrently, the LEO layer optimization is performed using a Multi-Agent Reinforcement Learning algorithm, where each beam functions as an independent agent. The simulation results show that this method reduces satellite load disparity by about 72.5% and decreases the average delay to 12ms. Additionally, our approach outperforms other benchmarks in terms of throughput, ensuring a better alignment between offered and requested data.
<div id='section'>Paperid: <span id='pid'>742, <a href='https://arxiv.org/pdf/2411.04867.pdf' target='_blank'>https://arxiv.org/pdf/2411.04867.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Satchit Chatterji, Erman Acar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.04867">Think Smart, Act SMARL! Analyzing Probabilistic Logic Shields for Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Safe reinforcement learning (RL) is crucial for real-world applications, and multi-agent interactions introduce additional safety challenges. While Probabilistic Logic Shields (PLS) has been a powerful proposal to enforce safety in single-agent RL, their generalizability to multi-agent settings remains unexplored. In this paper, we address this gap by conducting extensive analyses of PLS within decentralized, multi-agent environments, and in doing so, propose $\textbf{Shielded Multi-Agent Reinforcement Learning (SMARL)}$ as a general framework for steering MARL towards norm-compliant outcomes. Our key contributions are: (1) a novel Probabilistic Logic Temporal Difference (PLTD) update for shielded, independent Q-learning, which incorporates probabilistic constraints directly into the value update process; (2) a probabilistic logic policy gradient method for shielded PPO with formal safety guarantees for MARL; and (3) comprehensive evaluation across symmetric and asymmetrically shielded $n$-player game-theoretic benchmarks, demonstrating fewer constraint violations and significantly better cooperation under normative constraints. These results position SMARL as an effective mechanism for equilibrium selection, paving the way toward safer, socially aligned multi-agent systems.
<div id='section'>Paperid: <span id='pid'>743, <a href='https://arxiv.org/pdf/2410.22578.pdf' target='_blank'>https://arxiv.org/pdf/2410.22578.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ying Li, Changling Li, Jiyao Chen, Christine Roinou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.22578">Energy-Aware Multi-Agent Reinforcement Learning for Collaborative Execution in Mission-Oriented Drone Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Mission-oriented drone networks have been widely used for structural inspection, disaster monitoring, border surveillance, etc. Due to the limited battery capacity of drones, mission execution strategy impacts network performance and mission completion. However, collaborative execution is a challenging problem for drones in such a dynamic environment as it also involves efficient trajectory design. We leverage multi-agent reinforcement learning (MARL) to manage the challenge in this study, letting each drone learn to collaboratively execute tasks and plan trajectories based on its current status and environment. Simulation results show that the proposed collaborative execution model can successfully complete the mission at least 80% of the time, regardless of task locations and lengths, and can even achieve a 100% success rate when the task density is not way too sparse. To the best of our knowledge, our work is one of the pioneer studies on leveraging MARL on collaborative execution for mission-oriented drone networks; the unique value of this work lies in drone battery level driving our model design.
<div id='section'>Paperid: <span id='pid'>744, <a href='https://arxiv.org/pdf/2410.21290.pdf' target='_blank'>https://arxiv.org/pdf/2410.21290.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Y. Wang, Y. Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.21290">Multiple Ships Cooperative Navigation and Collision Avoidance using Multi-agent Reinforcement Learning with Communication</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the real world, unmanned surface vehicles (USV) often need to coordinate with each other to accomplish specific tasks. However, achieving cooperative control in multi-agent systems is challenging due to issues such as non-stationarity and partial observability. Recent advancements in Multi-Agent Reinforcement Learning (MARL) provide new perspectives to address these challenges. Therefore, we propose using the multi-agent deep deterministic policy gradient (MADDPG) algorithm with communication to address multiple ships' cooperation problems under partial observability. We developed two tasks based on OpenAI's gym environment: cooperative navigation and cooperative collision avoidance. In these tasks, ships must not only learn effective control strategies but also establish communication protocols with other agents. We analyze the impact of external noise on communication, the effect of inter-agent communication on performance, and the communication patterns learned by the agents. The results demonstrate that our proposed framework effectively addresses cooperative navigation and collision avoidance among multiple vessels, significantly outperforming traditional single-agent algorithms. Agents establish a consistent communication protocol, enabling them to compensate for missing information through shared observations and achieve better coordination.
<div id='section'>Paperid: <span id='pid'>745, <a href='https://arxiv.org/pdf/2410.18631.pdf' target='_blank'>https://arxiv.org/pdf/2410.18631.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Niki Kotecha, Antonio del Rio Chanona
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.18631">Leveraging Graph Neural Networks and Multi-Agent Reinforcement Learning for Inventory Control in Supply Chains</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Inventory control in modern supply chains has attracted significant attention due to the increasing number of disruptive shocks and the challenges posed by complex dynamics, uncertainties, and limited collaboration. Traditional methods, which often rely on static parameters, struggle to adapt to changing environments. This paper proposes a Multi-Agent Reinforcement Learning (MARL) framework with Graph Neural Networks (GNNs) for state representation to address these limitations.
  Our approach redefines the action space by parameterizing heuristic inventory control policies, making it adaptive as the parameters dynamically adjust based on system conditions. By leveraging the inherent graph structure of supply chains, our framework enables agents to learn the system's topology, and we employ a centralized learning, decentralized execution scheme that allows agents to learn collaboratively while overcoming information-sharing constraints. Additionally, we incorporate global mean pooling and regularization techniques to enhance performance.
  We test the capabilities of our proposed approach on four different supply chain configurations and conduct a sensitivity analysis. This work paves the way for utilizing MARL-GNN frameworks to improve inventory management in complex, decentralized supply chain environments.
<div id='section'>Paperid: <span id='pid'>746, <a href='https://arxiv.org/pdf/2410.18621.pdf' target='_blank'>https://arxiv.org/pdf/2410.18621.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wonhyung Choi, Inkyung Ahn
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.18621">Evolutionary Dispersal of Ecological Species via Multi-Agent Deep Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding species dynamics in heterogeneous environments is essential for ecosystem studies. Traditional models assumed homogeneous habitats, but recent approaches include spatial and temporal variability, highlighting species migration. We adopt starvation-driven diffusion (SDD) models as nonlinear diffusion to describe species dispersal based on local resource conditions, showing advantages for species survival. However, accurate prediction remains challenging due to model simplifications. This study uses multi-agent reinforcement learning (MARL) with deep Q-networks (DQN) to simulate single species and predator-prey interactions, incorporating SDD-type rewards. Our simulations reveal evolutionary dispersal strategies, providing insights into species dispersal mechanisms and validating traditional mathematical models.
<div id='section'>Paperid: <span id='pid'>747, <a href='https://arxiv.org/pdf/2410.17068.pdf' target='_blank'>https://arxiv.org/pdf/2410.17068.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianan Bai, Zheng Chen, Erik. G. Larsson
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.17068">Delay-Constrained Grant-Free Random Access in MIMO Systems: Distributed Pilot Allocation and Power Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We study a delay-constrained grant-free random access system with a multi-antenna base station. The users randomly generate data packets with expiration deadlines, which are then transmitted from data queues on a first-in first-out basis. To deliver a packet, a user needs to succeed in both random access phase (sending a pilot without collision) and data transmission phase (achieving a required data rate with imperfect channel information) before the packet expires. We develop a distributed, cross-layer policy that allows the users to dynamically and independently choose their pilots and transmit powers to achieve a high effective sum throughput with fairness consideration. Our policy design involves three key components: 1) a proxy of the instantaneous data rate that depends only on macroscopic environment variables and transmission decisions, considering pilot collisions and imperfect channel estimation; 2) a quantitative, instantaneous measure of fairness within each communication round; and 3) a deep learning-based, multi-agent control framework with centralized training and distributed execution. The proposed framework benefits from an accurate, differentiable objective function for training, thereby achieving a higher sample efficiency compared with a conventional application of model-free, multi-agent reinforcement learning algorithms. The performance of the proposed approach is verified by simulations under highly dynamic and heterogeneous scenarios.
<div id='section'>Paperid: <span id='pid'>748, <a href='https://arxiv.org/pdf/2410.15335.pdf' target='_blank'>https://arxiv.org/pdf/2410.15335.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ali Kahe, Hamed Kebriaei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.15335">A Distributed Primal-Dual Method for Constrained Multi-agent Reinforcement Learning with General Parameterization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper proposes a novel distributed approach for solving a cooperative Constrained Multi-agent Reinforcement Learning (CMARL) problem, where agents seek to minimize a global objective function subject to shared constraints. Unlike existing methods that rely on centralized training or coordination, our approach enables fully decentralized online learning, with each agent maintaining local estimates of both primal and dual variables. Specifically, we develop a distributed primal-dual algorithm based on actor-critic methods, leveraging local information to estimate Lagrangian multipliers. We establish consensus among the Lagrangian multipliers across agents and prove the convergence of our algorithm to an equilibrium point, analyzing the sub-optimality of this equilibrium compared to the exact solution of the unparameterized problem. Furthermore, we introduce a constrained cooperative Cournot game with stochastic dynamics as a test environment to evaluate the algorithm's performance in complex, real-world scenarios.
<div id='section'>Paperid: <span id='pid'>749, <a href='https://arxiv.org/pdf/2409.09509.pdf' target='_blank'>https://arxiv.org/pdf/2409.09509.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shatayu Kulkarni, Sabine Brunswicker
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.09509">Learning Nudges for Conditional Cooperation: A Multi-Agent Reinforcement Learning Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The public goods game describes a social dilemma in which a large proportion of agents act as conditional cooperators (CC): they only act cooperatively if they see others acting cooperatively because they satisfice with the social norm to be in line with what others are doing instead of optimizing cooperation. CCs are guided by aspiration-based reinforcement learning guided by past experiences of interactions with others and satisficing aspirations. In many real-world settings, reinforcing social norms do not emerge. In this paper, we propose that an optimizing reinforcement agent can facilitate cooperation through nudges, i.e. indirect mechanisms for cooperation to happen. The agent's goal is to motivate CCs into cooperation through its own actions to create social norms that signal that others are cooperating. We introduce a multi-agent reinforcement learning model for public goods games, with 3 CC learning agents using aspirational reinforcement learning and 1 nudging agent using deep reinforcement learning to learn nudges that optimize cooperation. For our nudging agent, we model two distinct reward functions, one maximizing the total game return (sum DRL) and one maximizing the number of cooperative contributions contributions higher than a proportional threshold (prop DRL). Our results show that our aspiration-based RL model for CC agents is consistent with empirically observed CC behavior. Games combining 3 CC RL agents and one nudging RL agent outperform the baseline consisting of 4 CC RL agents only. The sum DRL nudging agent increases the total sum of contributions by 8.22% and the total proportion of cooperative contributions by 12.42%, while the prop nudging DRL increases the total sum of contributions by 8.85% and the total proportion of cooperative contributions by 14.87%. Our findings advance the literature on public goods games and reinforcement learning.
<div id='section'>Paperid: <span id='pid'>750, <a href='https://arxiv.org/pdf/2409.03875.pdf' target='_blank'>https://arxiv.org/pdf/2409.03875.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Benjamin Heymann, Marc Lanctot
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.03875">Learning in Games with Progressive Hiding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>When learning to play an imperfect information game, it is often easier to first start with the basic mechanics of the game rules. For example, one can play several example rounds with private cards revealed to all players to better understand the basic actions and their effects. Building on this intuition, this paper introduces {\it progressive hiding}, an algorithm that balances learning the basic mechanics of an imperfect information game and satisfying the information constraints. Progressive hiding is inspired by methods from stochastic multistage optimization, such as scenario decomposition and progressive hedging. We prove that it enables the adaptation of counterfactual regret minimization to games where perfect recall is not satisfied. Numerical experiments illustrate that progressive hiding produces notable improvements in several settings.
<div id='section'>Paperid: <span id='pid'>751, <a href='https://arxiv.org/pdf/2409.00754.pdf' target='_blank'>https://arxiv.org/pdf/2409.00754.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiaming Yin, Weixiong Rao, Yu Xiao, Keshuang Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.00754">Cooperative Path Planning with Asynchronous Multiagent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we study the shortest path problem (SPP) with multiple source-destination pairs (MSD), namely MSD-SPP, to minimize average travel time of all shortest paths. The inherent traffic capacity limits within a road network contributes to the competition among vehicles. Multi-agent reinforcement learning (MARL) model cannot offer effective and efficient path planning cooperation due to the asynchronous decision making setting in MSD-SPP, where vehicles (a.k.a agents) cannot simultaneously complete routing actions in the previous time step. To tackle the efficiency issue, we propose to divide an entire road network into multiple sub-graphs and subsequently execute a two-stage process of inter-region and intra-region route planning. To address the asynchronous issue, in the proposed asyn-MARL framework, we first design a global state, which exploits a low-dimensional vector to implicitly represent the joint observations and actions of multi-agents. Then we develop a novel trajectory collection mechanism to decrease the redundancy in training trajectories. Additionally, we design a novel actor network to facilitate the cooperation among vehicles towards the same or close destinations and a reachability graph aimed at preventing infinite loops in routing paths. On both synthetic and real road networks, our evaluation result demonstrates that our approach outperforms state-of-the-art planning approaches.
<div id='section'>Paperid: <span id='pid'>752, <a href='https://arxiv.org/pdf/2601.12886.pdf' target='_blank'>https://arxiv.org/pdf/2601.12886.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Christoph Wittner
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.12886">Communication Methods in Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent reinforcement learning is a promising research area that extends established reinforcement learning approaches to problems formulated as multi-agent systems. Recently, a multitude of communication methods have been introduced to this field to address problems such as partially observable environments, non-stationarity, and exponentially growing action spaces. Communication further enables efficient cooperation among all agents interacting in an environment. This work aims at providing an overview of communication techniques in multi-agent reinforcement learning. By an in-depth analysis of 29 publications on this topic, the strengths and weaknesses of explicit, implicit, attention-based, graph-based, and hierarchical/role-based communication are evaluated. The results of this comparison show that there is no general, optimal communication framework for every problem. On the contrary, the choice of communication depends heavily on the problem at hand. The comparison also highlights the importance of communication methods with low computational overhead to enable scalability to environments where many agents interact. Finally, the paper discusses current research gaps, emphasizing the need for standardized benchmarking of system-level metrics and improved robustness under realistic communication conditions to enhance the real-world applicability of these approaches.
<div id='section'>Paperid: <span id='pid'>753, <a href='https://arxiv.org/pdf/2601.04177.pdf' target='_blank'>https://arxiv.org/pdf/2601.04177.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoran Su
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.04177">Hierarchical GNN-Based Multi-Agent Learning for Dynamic Queue-Jump Lane and Emergency Vehicle Corridor Formation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Emergency vehicles require rapid passage through congested traffic, yet existing strategies fail to adapt to dynamic conditions. We propose a novel hierarchical graph neural network (GNN)-based multi-agent reinforcement learning framework to coordinate connected vehicles for emergency corridor formation. Our approach uses a high-level planner for global strategy and low-level controllers for trajectory execution, utilizing graph attention networks to scale with variable agent counts. Trained via Multi-Agent Proximal Policy Optimization (MAPPO), the system reduces emergency vehicle travel time by 28.3% compared to baselines and 44.6% compared to uncoordinated traffic in simulations. The design achieves near-zero collision rates (0.3%) while maintaining 81% of background traffic efficiency. Ablation and generalization studies confirm the framework's robustness across diverse scenarios. These results demonstrate the effectiveness of combining GNNs with hierarchical learning for intelligent transportation systems.
<div id='section'>Paperid: <span id='pid'>754, <a href='https://arxiv.org/pdf/2512.06645.pdf' target='_blank'>https://arxiv.org/pdf/2512.06645.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Muyang Fan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.06645">Analyzing Collision Rates in Large-Scale Mixed Traffic Control via Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vehicle collisions remain a major challenge in large-scale mixed traffic systems, especially when human-driven vehicles (HVs) and robotic vehicles (RVs) interact under dynamic and uncertain conditions. Although Multi-Agent Reinforcement Learning (MARL) offers promising capabilities for traffic signal control, ensuring safety in such environments remains difficult. As a direct indicator of traffic risk, the collision rate must be well understood and incorporated into traffic control design. This study investigates the primary factors influencing collision rates in a MARL-governed Mixed Traffic Control (MTC) network. We examine three dimensions: total vehicle count, signalized versus unsignalized intersection configurations, and turning-movement strategies. Through controlled simulation experiments, we evaluate how each factor affects collision likelihood. The results show that collision rates are sensitive to traffic density, the level of signal coordination, and turning-control design. These findings provide practical insights for improving the safety and robustness of MARL-based mixed traffic control systems, supporting the development of intelligent transportation systems in which both efficiency and safety are jointly optimized.
<div id='section'>Paperid: <span id='pid'>755, <a href='https://arxiv.org/pdf/2512.06595.pdf' target='_blank'>https://arxiv.org/pdf/2512.06595.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Joe Shymanski
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.06595">ChargingBoul: A Competitive Negotiating Agent with Novel Opponent Modeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Automated negotiation has emerged as a critical area of research in multiagent systems, with applications spanning e-commerce, resource allocation, and autonomous decision-making. This paper presents ChargingBoul, a negotiating agent that competed in the 2022 Automated Negotiating Agents Competition (ANAC) and placed second in individual utility by an exceptionally narrow margin. ChargingBoul employs a lightweight yet effective strategy that balances concession and opponent modeling to achieve high negotiation outcomes. The agent classifies opponents based on bid patterns, dynamically adjusts its bidding strategy, and applies a concession policy in later negotiation stages to maximize utility while fostering agreements. We evaluate ChargingBoul's performance using competition results and subsequent studies that have utilized the agent in negotiation research. Our analysis highlights ChargingBoul's effectiveness across diverse opponent strategies and its contributions to advancing automated negotiation techniques. We also discuss potential enhancements, including more sophisticated opponent modeling and adaptive bidding heuristics, to improve its performance further.
<div id='section'>Paperid: <span id='pid'>756, <a href='https://arxiv.org/pdf/2511.23315.pdf' target='_blank'>https://arxiv.org/pdf/2511.23315.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Azusa Yamaguchi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.23315">Emergent Coordination and Phase Structure in Independent Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A clearer understanding of when coordination emerges, fluctuates, or collapses in decentralized multi-agent reinforcement learning (MARL) is increasingly sought in order to characterize the dynamics of multi-agent learning systems. We revisit fully independent Q-learning (IQL) as a minimal decentralized testbed and run large-scale experiments across environment size L and agent density rho. We construct a phase map using two axes - the cooperative success rate (CSR) and a stability index derived from TD-error variance - revealing three distinct regimes: a coordinated and stable phase, a fragile transition region, and a jammed or disordered phase. A sharp double Instability Ridge separates these regimes and corresponds to persistent kernel drift, the time-varying shift of each agent's effective transition kernel induced by others' policy updates. Synchronization analysis further shows that temporal alignment is required for sustained cooperation, and that competition between drift and synchronization generates the fragile regime. Removing agent identifiers eliminates drift entirely and collapses the three-phase structure, demonstrating that small inter-agent asymmetries are a necessary driver of drift. Overall, the results show that decentralized MARL exhibits a coherent phase structure governed by the interaction between scale, density, and kernel drift, suggesting that emergent coordination behaves as a distribution-interaction-driven phase phenomenon.
<div id='section'>Paperid: <span id='pid'>757, <a href='https://arxiv.org/pdf/2511.19562.pdf' target='_blank'>https://arxiv.org/pdf/2511.19562.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Abraham Itzhak Weinberg
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.19562">Trust-Based Social Learning for Communication (TSLEC) Protocol Evolution in Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Emergent communication in multi-agent systems typically occurs through independent learning, resulting in slow convergence and potentially suboptimal protocols. We introduce TSLEC (Trust-Based Social Learning with Emergent Communication), a framework where agents explicitly teach successful strategies to peers, with knowledge transfer modulated by learned trust relationships. Through experiments with 100 episodes across 30 random seeds, we demonstrate that trust-based social learning reduces episodes-to-convergence by 23.9% (p < 0.001, Cohen's d = 1.98) compared to independent emergence, while producing compositional protocols (C = 0.38) that remain robust under dynamic objectives (Phi > 0.867 decoding accuracy). Trust scores strongly correlate with teaching quality (r = 0.743, p < 0.001), enabling effective knowledge filtering. Our results establish that explicit social learning fundamentally accelerates emergent communication in multi-agent coordination.
<div id='section'>Paperid: <span id='pid'>758, <a href='https://arxiv.org/pdf/2511.19253.pdf' target='_blank'>https://arxiv.org/pdf/2511.19253.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Boyuan Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.19253">MAESTRO: Multi-Agent Environment Shaping through Task and Reward Optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cooperative Multi-Agent Reinforcement Learning (MARL) faces two major design bottlenecks: crafting dense reward functions and constructing curricula that avoid local optima in high-dimensional, non-stationary environments. Existing approaches rely on fixed heuristics or use Large Language Models (LLMs) directly in the control loop, which is costly and unsuitable for real-time systems. We propose MAESTRO (Multi-Agent Environment Shaping through Task and Reward Optimization), a framework that moves the LLM outside the execution loop and uses it as an offline training architect. MAESTRO introduces two generative components: (i) a semantic curriculum generator that creates diverse, performance-driven traffic scenarios, and (ii) an automated reward synthesizer that produces executable Python reward functions adapted to evolving curriculum difficulty. These components guide a standard MARL backbone (MADDPG) without increasing inference cost at deployment. We evaluate MAESTRO on large-scale traffic signal control (Hangzhou, 16 intersections) and conduct controlled ablations. Results show that combining LLM-generated curricula with LLM-generated reward shaping yields improved performance and stability. Across four seeds, the full system achieves +4.0% higher mean return (163.26 vs. 156.93) and 2.2% better risk-adjusted performance (Sharpe 1.53 vs. 0.70) over a strong curriculum baseline. These findings highlight LLMs as effective high-level designers for cooperative MARL training.
<div id='section'>Paperid: <span id='pid'>759, <a href='https://arxiv.org/pdf/2511.17654.pdf' target='_blank'>https://arxiv.org/pdf/2511.17654.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Deepak Bolleddu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.17654">Dialogue Diplomats: An End-to-End Multi-Agent Reinforcement Learning System for Automated Conflict Resolution and Consensus Building</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Conflict resolution and consensus building represent critical challenges in multi-agent systems, negotiations, and collaborative decision-making processes. This paper introduces Dialogue Diplomats, a novel end-to-end multi-agent reinforcement learning (MARL) framework designed for automated conflict resolution and consensus building in complex, dynamic environments. The proposed system integrates advanced deep reinforcement learning architectures with dialogue-based negotiation protocols, enabling autonomous agents to engage in sophisticated conflict resolution through iterative communication and strategic adaptation. We present three primary contributions: first, a novel Hierarchical Consensus Network (HCN) architecture that combines attention mechanisms with graph neural networks to model inter-agent dependencies and conflict dynamics. second, a Progressive Negotiation Protocol (PNP) that structures multi-round dialogue interactions with adaptive concession strategies; and third, a Context-Aware Reward Shaping mechanism that balances individual agent objectives with collective consensus goals.
<div id='section'>Paperid: <span id='pid'>760, <a href='https://arxiv.org/pdf/2511.15716.pdf' target='_blank'>https://arxiv.org/pdf/2511.15716.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Abraham Itzhak Weinberg
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.15716">MACIE: Multi-Agent Causal Intelligence Explainer for Collective Behavior Understanding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As Multi Agent Reinforcement Learning systems are used in safety critical applications. Understanding why agents make decisions and how they achieve collective behavior is crucial. Existing explainable AI methods struggle in multi agent settings. They fail to attribute collective outcomes to individuals, quantify emergent behaviors, or capture complex interactions. We present MACIE Multi Agent Causal Intelligence Explainer, a framework combining structural causal models, interventional counterfactuals, and Shapley values to provide comprehensive explanations. MACIE addresses three questions. First, each agent's causal contribution using interventional attribution scores. Second, system level emergent intelligence through synergy metrics separating collective effects from individual contributions. Third, actionable explanations using natural language narratives synthesizing causal insights. We evaluate MACIE across four MARL scenarios: cooperative, competitive, and mixed motive. Results show accurate outcome attribution, mean phi_i equals 5.07, standard deviation less than 0.05, detection of positive emergence in cooperative tasks, synergy index up to 0.461, and efficient computation, 0.79 seconds per dataset on CPU. MACIE uniquely combines causal rigor, emergence quantification, and multi agent support while remaining practical for real time use. This represents a step toward interpretable, trustworthy, and accountable multi agent AI.
<div id='section'>Paperid: <span id='pid'>761, <a href='https://arxiv.org/pdf/2511.00039.pdf' target='_blank'>https://arxiv.org/pdf/2511.00039.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Krishna Kumar Neelakanta Pillai Santha Kumari Amma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.00039">Graph-Attentive MAPPO for Dynamic Retail Pricing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Dynamic pricing in retail requires policies that adapt to shifting demand while coordinating decisions across related products. We present a systematic empirical study of multi-agent reinforcement learning for retail price optimization, comparing a strong MAPPO baseline with a graph-attention-augmented variant (MAPPO+GAT) that leverages learned interactions among products. Using a simulated pricing environment derived from real transaction data, we evaluate profit, stability across random seeds, fairness across products, and training efficiency under a standardized evaluation protocol. The results indicate that MAPPO provides a robust and reproducible foundation for portfolio-level price control, and that MAPPO+GAT further enhances performance by sharing information over the product graph without inducing excessive price volatility. These results indicate that graph-integrated MARL provides a more scalable and stable solution than independent learners for dynamic retail pricing, offering practical advantages in multi-product decision-making.
<div id='section'>Paperid: <span id='pid'>762, <a href='https://arxiv.org/pdf/2511.00034.pdf' target='_blank'>https://arxiv.org/pdf/2511.00034.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aditya Akella
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.00034">On the Fundamental Limitations of Decentralized Learnable Reward Shaping in Cooperative Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in learnable reward shaping have shown promise in single-agent reinforcement learning by automatically discovering effective feedback signals. However, the effectiveness of decentralized learnable reward shaping in cooperative multi-agent settings remains poorly understood. We propose DMARL-RSA, a fully decentralized system where each agent learns individual reward shaping, and evaluate it on cooperative navigation tasks in the simple_spread_v3 environment. Despite sophisticated reward learning, DMARL-RSA achieves only -24.20 +/- 0.09 average reward, compared to MAPPO with centralized training at 1.92 +/- 0.87 -- a 26.12-point gap. DMARL-RSA performs similarly to simple independent learning (IPPO: -23.19 +/- 0.96), indicating that advanced reward shaping cannot overcome fundamental decentralized coordination limitations. Interestingly, decentralized methods achieve higher landmark coverage (0.888 +/- 0.029 for DMARL-RSA, 0.960 +/- 0.045 for IPPO out of 3 total) but worse overall performance than centralized MAPPO (0.273 +/- 0.008 landmark coverage) -- revealing a coordination paradox between local optimization and global performance. Analysis identifies three critical barriers: (1) non-stationarity from concurrent policy updates, (2) exponential credit assignment complexity, and (3) misalignment between individual reward optimization and global objectives. These results establish empirical limits for decentralized reward learning and underscore the necessity of centralized coordination for effective multi-agent cooperation.
<div id='section'>Paperid: <span id='pid'>763, <a href='https://arxiv.org/pdf/2510.06138.pdf' target='_blank'>https://arxiv.org/pdf/2510.06138.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rushiv Arora
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.06138">Multi-Task Reinforcement Learning with Language-Encoded Gated Policy Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-task reinforcement learning often relies on task metadata -- such as brief natural-language descriptions -- to guide behavior across diverse objectives. We present Lexical Policy Networks (LEXPOL), a language-conditioned mixture-of-policies architecture for multi-task RL. LEXPOL encodes task metadata with a text encoder and uses a learned gating module to select or blend among multiple sub-policies, enabling end-to-end training across tasks. On MetaWorld benchmarks, LEXPOL matches or exceeds strong multi-task baselines in success rate and sample efficiency, without task-specific retraining. To analyze the mechanism, we further study settings with fixed expert policies obtained independently of the gate and show that the learned language gate composes these experts to produce behaviors appropriate to novel task descriptions and unseen task combinations. These results indicate that natural-language metadata can effectively index and recombine reusable skills within a single policy.
<div id='section'>Paperid: <span id='pid'>764, <a href='https://arxiv.org/pdf/2510.00022.pdf' target='_blank'>https://arxiv.org/pdf/2510.00022.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ansh Kamthan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.00022">Learning to Lead Themselves: Agentic AI in MAS using MARL</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As autonomous systems move from prototypes to real deployments, the ability of multiple agents to make decentralized, cooperative decisions becomes a core requirement. This paper examines how agentic artificial intelligence, agents that act independently, adaptively and proactively can improve task allocation and coordination in multi-agent systems, with primary emphasis on drone delivery and secondary relevance to warehouse automation. We formulate the problem in a cooperative multi-agent reinforcement learning setting and implement a lightweight multi-agent Proximal Policy Optimization, called IPPO, approach in PyTorch under a centralized-training, decentralized-execution paradigm. Experiments are conducted in PettingZoo environment, where multiple homogeneous drones or agents must self-organize to cover distinct targets without explicit communication.
<div id='section'>Paperid: <span id='pid'>765, <a href='https://arxiv.org/pdf/2509.23026.pdf' target='_blank'>https://arxiv.org/pdf/2509.23026.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yue Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.23026">Game-Theoretic Understandings of Multi-Agent Systems with Multiple Objectives</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In practical multi-agent systems, agents often have diverse objectives, which makes the system more complex, as each agent's performance across multiple criteria depends on the joint actions of all agents, creating intricate strategic trade-offs. To address this, we introduce the Multi-Objective Markov Game (MOMG), a framework for multi-agent reinforcement learning with multiple objectives. We propose the Pareto-Nash Equilibrium (PNE) as the primary solution concept, where no agent can unilaterally improve one objective without sacrificing performance on another. We prove existence of PNE, and establish an equivalence between the PNE and the set of Nash Equilibria of MOMG's corresponding linearly scalarized games, enabling solutions of MOMG by transferring to a standard single-objective Markov game. However, we note that computing a PNE is theoretically and computationally challenging, thus we propose and study weaker but more tractable solution concepts. Building on these foundations, we develop online learning algorithm that identify a single solution to MOMGs. Furthermore, we propose a two-phase, preference-free algorithm that decouples exploration from planning. Our algorithm enables computation of a PNE for any given preference profile without collecting new samples, providing an efficient methodological characterization of the entire Pareto-Nash front.
<div id='section'>Paperid: <span id='pid'>766, <a href='https://arxiv.org/pdf/2509.04731.pdf' target='_blank'>https://arxiv.org/pdf/2509.04731.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Brennen Hill
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.04731">Hierarchical Task Environments as the Next Frontier for Embodied World Models in Robot Soccer</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in agent development have focused on scaling model size and raw interaction data, mirroring the successes seen in large language models. However, for complex, long-horizon multi-agent tasks such as robotic soccer, this end-to-end approach often fails due to intractable exploration spaces and sparse rewards. This position paper argues that the next frontier in developing embodied world models is not merely increasing the fidelity or size of environments, but scaling their structural complexity through explicit hierarchical scaffolding. We posit that an effective world model for decision-making must model not only the world's physics but also its task semantics. Drawing from a systematic review of 2024 research in low-resource multi-agent soccer, we identify a clear trend towards integrating symbolic and hierarchical methods, such as Hierarchical Task Networks (HTNs) and Bayesian Strategy Networks (BSNs), with multi-agent reinforcement learning (MARL). These methods decompose complex goals into manageable subgoals, creating an intrinsic curriculum that shapes agent learning. We propose that such structured environments are essential for bridging the gap between simple, reactive behaviors and sophisticated, strategic team play. We further extend this principle, proposing that this scaffolding can be generalized to other complex domains and dynamically generated by Large Language Models (LLMs), which act as generative world models of tasks. By building environments with explicit, composable task layers, we can guide agent exploration more efficiently, generate meaningful learning signals, and ultimately train more capable and general-purpose agents with fewer resources than purely end-to-end approaches.
<div id='section'>Paperid: <span id='pid'>767, <a href='https://arxiv.org/pdf/2508.20784.pdf' target='_blank'>https://arxiv.org/pdf/2508.20784.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yifan Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.20784">Single Agent Robust Deep Reinforcement Learning for Bus Fleet Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Bus bunching remains a challenge for urban transit due to stochastic traffic and passenger demand. Traditional solutions rely on multi-agent reinforcement learning (MARL) in loop-line settings, which overlook realistic operations characterized by heterogeneous routes, timetables, fluctuating demand, and varying fleet sizes. We propose a novel single-agent reinforcement learning (RL) framework for bus holding control that avoids the data imbalance and convergence issues of MARL under near-realistic simulation. A bidirectional timetabled network with dynamic passenger demand is constructed. The key innovation is reformulating the multi-agent problem into a single-agent one by augmenting the state space with categorical identifiers (vehicle ID, station ID, time period) in addition to numerical features (headway, occupancy, velocity). This high-dimensional encoding enables single-agent policies to capture inter-agent dependencies, analogous to projecting non-separable inputs into a higher-dimensional space. We further design a structured reward function aligned with operational goals: instead of exponential penalties on headway deviations, a ridge-shaped reward balances uniform headways and schedule adherence. Experiments show that our modified soft actor-critic (SAC) achieves more stable and superior performance than benchmarks, including MADDPG (e.g., -430k vs. -530k under stochastic conditions). These results demonstrate that single-agent deep RL, when enhanced with categorical structuring and schedule-aware rewards, can effectively manage bus holding in non-loop, real-world contexts. This paradigm offers a robust, scalable alternative to MARL frameworks, particularly where agent-specific experiences are imbalanced.
<div id='section'>Paperid: <span id='pid'>768, <a href='https://arxiv.org/pdf/2508.17671.pdf' target='_blank'>https://arxiv.org/pdf/2508.17671.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sam Ganzfried
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.17671">Consistent Opponent Modeling of Static Opponents in Imperfect-Information Games</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The goal of agents in multi-agent environments is to maximize total reward against the opposing agents that are encountered. Following a game-theoretic solution concept, such as Nash equilibrium, may obtain a strong performance in some settings; however, such approaches fail to capitalize on historical and observed data from repeated interactions against our opponents. Opponent modeling algorithms integrate machine learning techniques to exploit suboptimal opponents utilizing available data; however, the effectiveness of such approaches in imperfect-information games to date is quite limited. We show that existing opponent modeling approaches fail to satisfy a simple desirable property even against static opponents drawn from a known prior distribution; namely, they do not guarantee that the model approaches the opponent's true strategy even in the limit as the number of game iterations approaches infinity. We develop a new algorithm that is able to achieve this property and runs efficiently by solving a convex minimization problem based on the sequence-form game representation using projected gradient descent. The algorithm is guaranteed to efficiently converge to the opponent's true strategy given observations from gameplay and possibly additional historical data if it is available.
<div id='section'>Paperid: <span id='pid'>769, <a href='https://arxiv.org/pdf/2507.10566.pdf' target='_blank'>https://arxiv.org/pdf/2507.10566.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hung Ming Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.10566">AI Mother Tongue: Self-Emergent Communication in MARL via Endogenous Symbol Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In Decentralized Multi-Agent Reinforcement Learning (MARL), the development of Emergent Communication has long been constrained by the ``Joint Exploration Dilemma'', leading agents to fall into a ``Communication Vacuum Equilibrium'' . Traditional methods address this by introducing inductive biases to facilitate communication emergence . This study fundamentally questions whether such artificial inductive biases are, in fact, over-engineering. Through experiments with the ``AI Mother Tongue'' (AIM) framework, based on a Vector Quantized Variational Autoencoder (VQ-VAE), we demonstrate that when agents possess an endogenous symbol system, their neural representations naturally exhibit spontaneous semantic compression and Nash equilibrium-driven semantic convergence, achieving effective symbolic communication without external inductive biases. This aligns with recent neuroscience findings suggesting that the human brain does not directly use human language for internal thought , and resonates with research on ``soft thinking'' capabilities in Large Language Models (LLMs) . Compared to traditional explicit communication methods, AIM demonstrates stronger generality and efficiency. The interpretable analysis toolkit developed in this study confirms that symbol usage exhibits a significant power-law distribution, leading to three major theoretical insights: the ``Neural Communication Hypothesis'', the ``Tool-First Principle'', and the ``Semantic Interpretability Paradigm''. Future research will explore the integration of Hierarchical Quantized Variational Autoencoders (HQ-VAE) to enhance AIM's complex expressive capabilities and investigate the potential for ``Reinforcement Learning (RL) Low-Level Pre-training''. This discovery offers new avenues for bridging symbolism and connectionism.
<div id='section'>Paperid: <span id='pid'>770, <a href='https://arxiv.org/pdf/2507.07302.pdf' target='_blank'>https://arxiv.org/pdf/2507.07302.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ashish Kumar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.07302">Application of LLMs to Multi-Robot Path Planning and Task Allocation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Efficient exploration is a well known problem in deep reinforcement learning and this problem is exacerbated in multi-agent reinforcement learning due the intrinsic complexities of such algorithms. There are several approaches to efficiently explore an environment to learn to solve tasks by multi-agent operating in that environment, of which, the idea of expert exploration is investigated in this work. More specifically, this work investigates the application of large-language models as expert planners for efficient exploration in planning based tasks for multiple agents.
<div id='section'>Paperid: <span id='pid'>771, <a href='https://arxiv.org/pdf/2506.22445.pdf' target='_blank'>https://arxiv.org/pdf/2506.22445.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Saad Alqithami
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.22445">Hierarchical Adversarially-Resilient Multi-Agent Reinforcement Learning for Cyber-Physical Systems Security</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cyber-Physical Systems play a critical role in the infrastructure of various sectors, including manufacturing, energy distribution, and autonomous transportation systems. However, their increasing connectivity renders them highly vulnerable to sophisticated cyber threats, such as adaptive and zero-day attacks, against which traditional security methods like rule-based intrusion detection and single-agent reinforcement learning prove insufficient. To overcome these challenges, this paper introduces a novel Hierarchical Adversarially-Resilient Multi-Agent Reinforcement Learning (HAMARL) framework. HAMARL employs a hierarchical structure consisting of local agents dedicated to subsystem security and a global coordinator that oversees and optimizes comprehensive, system-wide defense strategies. Furthermore, the framework incorporates an adversarial training loop designed to simulate and anticipate evolving cyber threats, enabling proactive defense adaptation. Extensive experimental evaluations conducted on a simulated industrial IoT testbed indicate that HAMARL substantially outperforms traditional multi-agent reinforcement learning approaches, significantly improving attack detection accuracy, reducing response times, and ensuring operational continuity. The results underscore the effectiveness of combining hierarchical multi-agent coordination with adversarially-aware training to enhance the resilience and security of next-generation CPS.
<div id='section'>Paperid: <span id='pid'>772, <a href='https://arxiv.org/pdf/2506.14164.pdf' target='_blank'>https://arxiv.org/pdf/2506.14164.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hanzhong Cao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.14164">Light Aircraft Game : Basic Implementation and training results analysis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper investigates multi-agent reinforcement learning (MARL) in a partially observable, cooperative-competitive combat environment known as LAG. We describe the environment's setup, including agent actions, hierarchical controls, and reward design across different combat modes such as No Weapon and ShootMissile. Two representative algorithms are evaluated: HAPPO, an on-policy hierarchical variant of PPO, and HASAC, an off-policy method based on soft actor-critic. We analyze their training stability, reward progression, and inter-agent coordination capabilities. Experimental results show that HASAC performs well in simpler coordination tasks without weapons, while HAPPO demonstrates stronger adaptability in more dynamic and expressive scenarios involving missile combat. These findings provide insights into the trade-offs between on-policy and off-policy methods in multi-agent settings.
<div id='section'>Paperid: <span id='pid'>773, <a href='https://arxiv.org/pdf/2506.12497.pdf' target='_blank'>https://arxiv.org/pdf/2506.12497.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ali Baheri
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.12497">Wasserstein-Barycenter Consensus for Cooperative Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cooperative multi-agent reinforcement learning (MARL) demands principled mechanisms to align heterogeneous policies while preserving the capacity for specialized behavior. We introduce a novel consensus framework that defines the team strategy as the entropic-regularized $p$-Wasserstein barycenter of agents' joint state--action visitation measures. By augmenting each agent's policy objective with a soft penalty proportional to its Sinkhorn divergence from this barycenter, the proposed approach encourages coherent group behavior without enforcing rigid parameter sharing. We derive an algorithm that alternates between Sinkhorn-barycenter computation and policy-gradient updates, and we prove that, under standard Lipschitz and compactness assumptions, the maximal pairwise policy discrepancy contracts at a geometric rate. Empirical evaluation on a cooperative navigation case study demonstrates that our OT-barycenter consensus outperforms an independent learners baseline in convergence speed and final coordination success.
<div id='section'>Paperid: <span id='pid'>774, <a href='https://arxiv.org/pdf/2506.09331.pdf' target='_blank'>https://arxiv.org/pdf/2506.09331.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Arjun Vaithilingam Sudhakar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.09331">Multi-Agent Language Models: Advancing Cooperation, Coordination, and Adaptation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Modern Large Language Models (LLMs) exhibit impressive zero-shot and few-shot generalization capabilities across complex natural language tasks, enabling their widespread use as virtual assistants for diverse applications such as translation and summarization. Despite being trained solely on large corpora of text without explicit supervision on author intent, LLMs appear to infer the underlying meaning of textual interactions. This raises a fundamental question: can LLMs model and reason about the intentions of others, i.e., do they possess a form of theory of mind? Understanding other's intentions is crucial for effective collaboration, which underpins human societal success and is essential for cooperative interactions among multiple agents, including humans and autonomous systems. In this work, we investigate the theory of mind in LLMs through the lens of cooperative multi-agent reinforcement learning (MARL), where agents learn to collaborate via repeated interactions, mirroring human social reasoning. Our approach aims to enhance artificial agent's ability to adapt and cooperate with both artificial and human partners. By leveraging LLM-based agents capable of natural language interaction, we move towards creating hybrid human-AI systems that can foster seamless collaboration, with broad implications for the future of human-artificial interaction.
<div id='section'>Paperid: <span id='pid'>775, <a href='https://arxiv.org/pdf/2506.09215.pdf' target='_blank'>https://arxiv.org/pdf/2506.09215.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Greyson Brothers
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.09215">Robust Noise Attenuation via Adaptive Pooling of Transformer Outputs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We investigate the design of pooling methods used to summarize the outputs of transformer embedding models, primarily motivated by reinforcement learning and vision applications. This work considers problems where a subset of the input vectors contains requisite information for a downstream task (signal) while the rest are distractors (noise). By framing pooling as vector quantization with the goal of minimizing signal loss, we demonstrate that the standard methods used to aggregate transformer outputs, AvgPool, MaxPool, and ClsToken, are vulnerable to performance collapse as the signal-to-noise ratio (SNR) of inputs fluctuates. We then show that an attention-based adaptive pooling method can approximate the signal-optimal vector quantizer within derived error bounds for any SNR. Our theoretical results are first validated by supervised experiments on a synthetic dataset designed to isolate the SNR problem, then generalized to standard relational reasoning, multi-agent reinforcement learning, and vision benchmarks with noisy observations, where transformers with adaptive pooling display superior robustness across tasks.
<div id='section'>Paperid: <span id='pid'>776, <a href='https://arxiv.org/pdf/2506.04251.pdf' target='_blank'>https://arxiv.org/pdf/2506.04251.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhengyang Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.04251">Language-Guided Multi-Agent Learning in Simulations: A Unified Framework and Evaluation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces LLM-MARL, a unified framework that incorporates large language models (LLMs) into multi-agent reinforcement learning (MARL) to enhance coordination, communication, and generalization in simulated game environments. The framework features three modular components of Coordinator, Communicator, and Memory, which dynamically generate subgoals, facilitate symbolic inter-agent messaging, and support episodic recall. Training combines PPO with a language-conditioned loss and LLM query gating. LLM-MARL is evaluated in Google Research Football, MAgent Battle, and StarCraft II. Results show consistent improvements over MAPPO and QMIX in win rate, coordination score, and zero-shot generalization. Ablation studies demonstrate that subgoal generation and language-based messaging each contribute significantly to performance gains. Qualitative analysis reveals emergent behaviors such as role specialization and communication-driven tactics. By bridging language modeling and policy learning, this work contributes to the design of intelligent, cooperative agents in interactive simulations. It offers a path forward for leveraging LLMs in multi-agent systems used for training, games, and human-AI collaboration.
<div id='section'>Paperid: <span id='pid'>777, <a href='https://arxiv.org/pdf/2505.23960.pdf' target='_blank'>https://arxiv.org/pdf/2505.23960.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Henry Conklin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.23960">Information Structure in Mappings: An Approach to Learning, Representation, and Generalisation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite the remarkable success of large large-scale neural networks, we still lack unified notation for thinking about and describing their representational spaces. We lack methods to reliably describe how their representations are structured, how that structure emerges over training, and what kinds of structures are desirable. This thesis introduces quantitative methods for identifying systematic structure in a mapping between spaces, and leverages them to understand how deep-learning models learn to represent information, what representational structures drive generalisation, and how design decisions condition the structures that emerge. To do this I identify structural primitives present in a mapping, along with information theoretic quantifications of each. These allow us to analyse learning, structure, and generalisation across multi-agent reinforcement learning models, sequence-to-sequence models trained on a single task, and Large Language Models. I also introduce a novel, performant, approach to estimating the entropy of vector space, that allows this analysis to be applied to models ranging in size from 1 million to 12 billion parameters.
  The experiments here work to shed light on how large-scale distributed models of cognition learn, while allowing us to draw parallels between those systems and their human analogs. They show how the structures of language and the constraints that give rise to them in many ways parallel the kinds of structures that drive performance of contemporary neural networks.
<div id='section'>Paperid: <span id='pid'>778, <a href='https://arxiv.org/pdf/2505.18457.pdf' target='_blank'>https://arxiv.org/pdf/2505.18457.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Abir Ray
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.18457">EdgeAgentX: A Novel Framework for Agentic AI at the Edge in Military Communication Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces EdgeAgentX, a novel framework integrating federated learning (FL), multi-agent reinforcement learning (MARL), and adversarial defense mechanisms, tailored for military communication networks. EdgeAgentX significantly improves autonomous decision-making, reduces latency, enhances throughput, and robustly withstands adversarial disruptions, as evidenced by comprehensive simulations.
<div id='section'>Paperid: <span id='pid'>779, <a href='https://arxiv.org/pdf/2505.14544.pdf' target='_blank'>https://arxiv.org/pdf/2505.14544.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Saahil Mahato
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.14544">Smart Traffic Signals: Comparing MARL and Fixed-Time Strategies</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Urban traffic congestion, particularly at intersections, significantly impacts travel time, fuel consumption, and emissions. Traditional fixed-time signal control systems often lack the adaptability to manage dynamic traffic patterns effectively. This study explores the application of multi-agent reinforcement learning (MARL) to optimize traffic signal coordination across multiple intersections within a simulated environment. Utilizing Pygame, a simulation was developed to model a network of interconnected intersections with randomly generated vehicle flows to reflect realistic traffic variability. A decentralized MARL controller was implemented, in which each traffic signal operates as an autonomous agent, making decisions based on local observations and information from neighboring agents. Performance was evaluated against a baseline fixed-time controller using metrics such as average vehicle wait time and overall throughput. The MARL approach demonstrated statistically significant improvements, including reduced average waiting times and improved throughput. These findings suggest that MARL-based dynamic control strategies hold substantial promise for improving urban traffic management efficiency. More research is recommended to address scalability and real-world implementation challenges.
<div id='section'>Paperid: <span id='pid'>780, <a href='https://arxiv.org/pdf/2505.09756.pdf' target='_blank'>https://arxiv.org/pdf/2505.09756.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhaoyang Shi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.09756">Community-based Multi-Agent Reinforcement Learning with Transfer and Active Exploration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a new framework for multi-agent reinforcement learning (MARL), where the agents cooperate in a time-evolving network with latent community structures and mixed memberships. Unlike traditional neighbor-based or fixed interaction graphs, our community-based framework captures flexible and abstract coordination patterns by allowing each agent to belong to multiple overlapping communities. Each community maintains shared policy and value functions, which are aggregated by individual agents according to personalized membership weights. We also design actor-critic algorithms that exploit this structure: agents inherit community-level estimates for policy updates and value learning, enabling structured information sharing without requiring access to other agents' policies. Importantly, our approach supports both transfer learning by adapting to new agents or tasks via membership estimation, and active learning by prioritizing uncertain communities during exploration. Theoretically, we establish convergence guarantees under linear function approximation for both actor and critic updates. To our knowledge, this is the first MARL framework that integrates community structure, transferability, and active learning with provable guarantees.
<div id='section'>Paperid: <span id='pid'>781, <a href='https://arxiv.org/pdf/2505.02215.pdf' target='_blank'>https://arxiv.org/pdf/2505.02215.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mannan Bhardwaj
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.02215">Interpretable Emergent Language Using Inter-Agent Transformers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper explores the emergence of language in multi-agent reinforcement learning (MARL) using transformers. Existing methods such as RIAL, DIAL, and CommNet enable agent communication but lack interpretability. We propose Differentiable Inter-Agent Transformers (DIAT), which leverage self-attention to learn symbolic, human-understandable communication protocols. Through experiments, DIAT demonstrates the ability to encode observations into interpretable vocabularies and meaningful embeddings, effectively solving cooperative tasks. These results highlight the potential of DIAT for interpretable communication in complex multi-agent environments.
<div id='section'>Paperid: <span id='pid'>782, <a href='https://arxiv.org/pdf/2502.16449.pdf' target='_blank'>https://arxiv.org/pdf/2502.16449.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoran Su
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.16449">Facilitating Emergency Vehicle Passage in Congested Urban Areas Using Multi-agent Deep Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Emergency Response Time (ERT) is crucial for urban safety, measuring cities' ability to handle medical, fire, and crime emergencies. In NYC, medical ERT increased 72% from 7.89 minutes in 2014 to 14.27 minutes in 2024, with half of delays due to Emergency Vehicle (EMV) travel times. Each minute's delay in stroke response costs 2 million brain cells, while cardiac arrest survival drops 7-10% per minute.
  This dissertation advances EMV facilitation through three contributions. First, EMVLight, a decentralized multi-agent reinforcement learning framework, integrates EMV routing with traffic signal pre-emption. It achieved 42.6% faster EMV travel times and 23.5% improvement for other vehicles.
  Second, the Dynamic Queue-Jump Lane system uses Multi-Agent Proximal Policy Optimization for coordinated lane-clearing in mixed autonomous and human-driven traffic, reducing EMV travel times by 40%.
  Third, an equity study of NYC Emergency Medical Services revealed disparities across boroughs: Staten Island faces delays due to sparse signalized intersections, while Manhattan struggles with congestion. Solutions include optimized EMS stations and improved intersection designs.
  These contributions enhance EMV mobility and emergency service equity, offering insights for policymakers and urban planners to develop safer, more efficient transportation systems.
<div id='section'>Paperid: <span id='pid'>783, <a href='https://arxiv.org/pdf/2501.15495.pdf' target='_blank'>https://arxiv.org/pdf/2501.15495.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alberto Castagna
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.15495">Expert-Free Online Transfer Learning in Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reinforcement Learning (RL) enables an intelligent agent to optimise its performance in a task by continuously taking action from an observed state and receiving a feedback from the environment in form of rewards. RL typically uses tables or linear approximators to map state-action tuples that maximises the reward. Combining RL with deep neural networks (DRL) significantly increases its scalability and enables it to address more complex problems than before. However, DRL also inherits downsides from both RL and deep learning. Despite DRL improves generalisation across similar state-action pairs when compared to simpler RL policy representations like tabular methods, it still requires the agent to adequately explore the state-action space. Additionally, deep methods require more training data, with the volume of data escalating with the complexity and size of the neural network. As a result, deep RL requires a long time to collect enough agent-environment samples and to successfully learn the underlying policy. Furthermore, often even a slight alteration to the task invalidates any previous acquired knowledge. To address these shortcomings, Transfer Learning (TL) has been introduced, which enables the use of external knowledge from other tasks or agents to enhance a learning process. The goal of TL is to reduce the learning complexity for an agent dealing with an unfamiliar task by simplifying the exploration process. This is achieved by lowering the amount of new information required by its learning model, resulting in a reduced overall convergence time...
<div id='section'>Paperid: <span id='pid'>784, <a href='https://arxiv.org/pdf/2501.03187.pdf' target='_blank'>https://arxiv.org/pdf/2501.03187.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dennis Gross
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.03187">Turn-based Multi-Agent Reinforcement Learning Model Checking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we propose a novel approach for verifying the compliance of turn-based multi-agent reinforcement learning (TMARL) agents with complex requirements in stochastic multiplayer games. Our method overcomes the limitations of existing verification approaches, which are inadequate for dealing with TMARL agents and not scalable to large games with multiple agents. Our approach relies on tight integration of TMARL and a verification technique referred to as model checking. We demonstrate the effectiveness and scalability of our technique through experiments in different types of environments. Our experiments show that our method is suited to verify TMARL agents and scales better than naive monolithic model checking.
<div id='section'>Paperid: <span id='pid'>785, <a href='https://arxiv.org/pdf/2501.00165.pdf' target='_blank'>https://arxiv.org/pdf/2501.00165.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ben McClusky
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.00165">Dynamic Graph Communication for Decentralised Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work presents a novel communication framework for decentralized multi-agent systems operating in dynamic network environments. Integrated into a multi-agent reinforcement learning system, the framework is designed to enhance decision-making by optimizing the network's collective knowledge through efficient communication. Key contributions include adapting a static network packet-routing scenario to a dynamic setting with node failures, incorporating a graph attention network layer in a recurrent message-passing framework, and introducing a multi-round communication targeting mechanism. This approach enables an attention-based aggregation mechanism to be successfully trained within a sparse-reward, dynamic network packet-routing environment using only reinforcement learning. Experimental results show improvements in routing performance, including a 9.5 percent increase in average rewards and a 6.4 percent reduction in communication overhead compared to a baseline system. The study also examines the ethical and legal implications of deploying such systems in critical infrastructure and military contexts, identifies current limitations, and suggests potential directions for future research.
<div id='section'>Paperid: <span id='pid'>786, <a href='https://arxiv.org/pdf/2412.21088.pdf' target='_blank'>https://arxiv.org/pdf/2412.21088.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Reza Azadeh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.21088">Advances in Multi-agent Reinforcement Learning: Persistent Autonomy and Robot Learning Lab Report 2024</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-Agent Reinforcement Learning (MARL) approaches have emerged as popular solutions to address the general challenges of cooperation in multi-agent environments, where the success of achieving shared or individual goals critically depends on the coordination and collaboration between agents. However, existing cooperative MARL methods face several challenges intrinsic to multi-agent systems, such as the curse of dimensionality, non-stationarity, and the need for a global exploration strategy. Moreover, the presence of agents with constraints (e.g., limited battery life, restricted mobility) or distinct roles further exacerbates these challenges. This document provides an overview of recent advances in Multi-Agent Reinforcement Learning (MARL) conducted at the Persistent Autonomy and Robot Learning (PeARL) lab at the University of Massachusetts Lowell. We briefly discuss various research directions and present a selection of approaches proposed in our most recent publications. For each proposed approach, we also highlight potential future directions to further advance the field.
<div id='section'>Paperid: <span id='pid'>787, <a href='https://arxiv.org/pdf/2412.20116.pdf' target='_blank'>https://arxiv.org/pdf/2412.20116.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Benedikt Valentin Meylahn
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.20116">Multi-agent reinforcement learning in the all-or-nothing public goods game on networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We study interpersonal trust by means of the all-or-nothing public goods game between agents on a network. The agents are endowed with the simple yet adaptive learning rule, exponential moving average, by which they estimate the behavior of their neighbors in the network. Theoretically we show that in the long-time limit this multi-agent reinforcement learning process always eventually results in indefinite contribution to the public good or indefinite defection (no agent contributing to the public good). However, by simulation of the pre-limit behavior, we see that on complex network structures there may be mixed states in which the process seems to stabilize before actual convergence to states in which agent beliefs and actions are all the same. In these metastable states the local network characteristics can determine whether agents have high or low trust in their neighbors. More generally it is found that more dense networks result in lower rates of contribution to the public good. This has implications for how one can spread global contribution toward a public good by enabling smaller local interactions.
<div id='section'>Paperid: <span id='pid'>788, <a href='https://arxiv.org/pdf/2411.17724.pdf' target='_blank'>https://arxiv.org/pdf/2411.17724.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aslan S. Dizaji
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.17724">Incentives to Build Houses, Trade Houses, or Trade House Building Skills in Simulated Worlds under Various Governing Systems or Institutions: Comparing Multi-agent Reinforcement Learning to Generative Agent-based Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>It has been shown that social institutions impact human motivations to produce different behaviours, such as amount of working or specialisation in labor. With advancement in artificial intelligence (AI), specifically large language models (LLMs), now it is possible to perform in-silico simulations to test various hypotheses around this topic. Here, I simulate two somewhat similar worlds using multi-agent reinforcement learning (MARL) framework of the AI-Economist and generative agent-based model (GABM) framework of the Concordia. In the extended versions of the AI-Economist and Concordia, the agents are able to build houses, trade houses, and trade house building skill. Moreover, along the individualistic-collectivists axis, there are a set of three governing systems: Full-Libertarian, Semi-Libertarian/Utilitarian, and Full-Utilitarian. Additionally, in the extended AI-Economist, the Semi-Libertarian/Utilitarian system is further divided to a set of three governing institutions along the discriminative axis: Inclusive, Arbitrary, and Extractive. Building on these, I am able to show that among governing systems and institutions of the extended AI-Economist, under the Semi-Libertarian/Utilitarian and Inclusive government, the ratios of building houses to trading houses and trading house building skill are higher than the rest. Furthermore, I am able to show that in the extended Concordia when the central government care about equality in the society, the Full-Utilitarian system generates agents building more houses and trading more house building skill. In contrast, these economic activities are higher under the Full-Libertarian system when the central government cares about productivity in the society. Overall, the focus of this paper is to compare and contrast two advanced techniques of AI, MARL and GABM, to simulate a similar social phenomena with limitations.
<div id='section'>Paperid: <span id='pid'>789, <a href='https://arxiv.org/pdf/2411.14496.pdf' target='_blank'>https://arxiv.org/pdf/2411.14496.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bao Nguyen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.14496">Multi-agent reinforcement learning strategy to maximize the lifetime of Wireless Rechargeable</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The thesis proposes a generalized charging framework for multiple mobile chargers to maximize the network lifetime and ensure target coverage and connectivity in large scale WRSNs. Moreover, a multi-point charging model is leveraged to enhance charging efficiency, where the MC can charge multiple sensors simultaneously at each charging location. The thesis proposes an effective Decentralized Partially Observable Semi-Markov Decision Process (Dec POSMDP) model that promotes Mobile Chargers (MCs) cooperation and detects optimal charging locations based on realtime network information. Furthermore, the proposal allows reinforcement algorithms to be applied to different networks without requiring extensive retraining. To solve the Dec POSMDP model, the thesis proposes an Asynchronous Multi Agent Reinforcement Learning algorithm (AMAPPO) based on the Proximal Policy Optimization algorithm (PPO).
<div id='section'>Paperid: <span id='pid'>790, <a href='https://arxiv.org/pdf/2410.11642.pdf' target='_blank'>https://arxiv.org/pdf/2410.11642.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiamian Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.11642">Improve Value Estimation of Q Function and Reshape Reward with Monte Carlo Tree Search</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reinforcement learning has achieved remarkable success in perfect information games such as Go and Atari, enabling agents to compete at the highest levels against human players. However, research in reinforcement learning for imperfect information games has been relatively limited due to the more complex game structures and randomness. Traditional methods face challenges in training and improving performance in imperfect information games due to issues like inaccurate Q value estimation and reward sparsity. In this paper, we focus on Uno, an imperfect information game, and aim to address these problems by reducing Q value overestimation and reshaping reward function. We propose a novel algorithm that utilizes Monte Carlo Tree Search to average the value estimations in Q function. Even though we choose Double Deep Q Learning as the foundational framework in this paper, our method can be generalized and used in any algorithm which needs Q value estimation, such as the Actor-Critic. Additionally, we employ Monte Carlo Tree Search to reshape the reward structure in the game environment. We compare our algorithm with several traditional methods applied to games such as Double Deep Q Learning, Deep Monte Carlo and Neural Fictitious Self Play, and the experiments demonstrate that our algorithm consistently outperforms these approaches, especially as the number of players in Uno increases, indicating a higher level of difficulty.
<div id='section'>Paperid: <span id='pid'>791, <a href='https://arxiv.org/pdf/2409.03052.pdf' target='_blank'>https://arxiv.org/pdf/2409.03052.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Christopher Amato
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.03052">An Introduction to Centralized Training for Decentralized Execution in Cooperative Multi-Agent Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent reinforcement learning (MARL) has exploded in popularity in recent years. Many approaches have been developed but they can be divided into three main types: centralized training and execution (CTE), centralized training for decentralized execution (CTDE), and Decentralized training and execution (DTE).
  CTDE methods are the most common as they can use centralized information during training but execute in a decentralized manner -- using only information available to that agent during execution. CTDE is the only paradigm that requires a separate training phase where any available information (e.g., other agent policies, underlying states) can be used. As a result, they can be more scalable than CTE methods, do not require communication during execution, and can often perform well. CTDE fits most naturally with the cooperative case, but can be potentially applied in competitive or mixed settings depending on what information is assumed to be observed.
  This text is an introduction to CTDE in cooperative MARL. It is meant to explain the setting, basic concepts, and common methods. It does not cover all work in CTDE MARL as the subarea is quite extensive. I have included work that I believe is important for understanding the main concepts in the subarea and apologize to those that I have omitted.
